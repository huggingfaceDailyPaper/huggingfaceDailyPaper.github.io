<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 05, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 05, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25616" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25616" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The growing success of Vision-Language-Action (VLA) models stems from the promise that pretrained Vision-Language Models (VLMs) can endow agents with transferable world knowledge and vision-language (VL) grounding, laying a foundation for action models with broader generalization. Yet when these VLMs are adapted to the action modality, it remains unclear to what extent their original VL representations and knowledge are preserved. In this work, we conduct a systematic study of representation retention during VLA fine-tuning, showing that naive action fine-tuning leads to degradation of visual representations. To characterize and measure these effects, we probe VLA's hidden representations and analyze attention maps, further, we design a set of targeted tasks and methods that contrast VLA models with their counterpart VLMs, isolating changes in VL capabilities induced by action fine-tuning. We further evaluate a range of strategies for aligning visual representations and introduce a simple yet effective method that mitigates degradation and yields improved generalization to out-of-distribution (OOD) scenarios. Taken together, our analysis clarifies the trade-off between action fine-tuning and the degradation of VL representations and highlights practical approaches to recover inherited VL capabilities. Code is publicly available: https://blind-vla-paper.github.io</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Naive action supervised fine-tuning (SFT) of Vision-Language-Action (VLA) models degrades inherited vision‚Äìlanguage (VL) representations, causing attention sink and representation collapse, which harm out-of-distribution (OOD) generalization (see attention maps on page 4, Figure 4, and t-SNE on page 5, Figure 5).<br>‚Ä¢ VLAs exhibit domain-specific forgetting, especially for symbolic/abstract visual concepts absent from robot data (traffic signs, arrows, public info, weather), as quantified by the VL-Think suite (page 7, Table 2).<br>‚Ä¢ Existing mitigation strategies mostly target large-scale pretraining (auxiliary reasoning, web co-training, frozen backbones) and are costly/constraining; they do not address degradation during task-specific SFT, where drift occurs (pages 1‚Äì2, Related Works on page 2).<br>‚Ä¢ Freezing the vision encoder during SFT fails to preserve usable perception-control coupling, leading to near-zero task performance (page 6, Table 1), while limited and narrow robotics datasets amplify overfitting (pages 1‚Äì2).<br>‚Ä¢ There is a lack of targeted diagnostics to directly measure VL retention in VLAs during adaptation; current benchmarks emphasize execution, not representation retention (pages 3‚Äì4, Figure 3 and Section 4).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A lightweight Visual Representation Alignment regularizer anchors mid-layer VLA visual tokens to a frozen foundation vision teacher via a frozen MLP projector and cosine-similarity loss (Backbone2Enc), jointly optimized with the action SFT objective to prevent representation drift (pages 1‚Äì2, Figures 1‚Äì2; Section 6, Eq. 7‚Äì10). The paper also introduces VL-Think, a diagnostic task suite isolating VL understanding from control to quantify retention and forgetting (pages 3‚Äì4, Figure 3).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Platonic Multi-Teacher Alignment for VLAs: Combine and weight multiple vision teachers (e.g., DINOv2, SigLIP, C-RADIOv3) across layers to distill a richer shared latent space and further improve OOD robustness beyond single-teacher alignment.<br>‚Ä¢ Geometry-Aware 3D/Video Alignment for Embodied Agents: Extend alignment to 3D-consistent and temporal features (multi-view, depth, scene graphs) to address spatial reasoning and reduce attention sink under viewpoint and motion shifts.<br>‚Ä¢ Curriculum and RL-Aware Alignment for Robust OOD Control: Develop adaptive schedules for alignment strength (Œª) and integrate with online RL/SFT to jointly preserve VL semantics and optimize control under evolving task distributions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02778" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02778" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Code has emerged as a precise and executable medium for reasoning and action in the agent era. Yet, progress has largely focused on language-centric tasks such as program synthesis and debugging, leaving visual-centric coding underexplored. Inspired by how humans reason over sketches, we advocate SVG code as a compact, interpretable, and executable visual representation. We introduce VCode, a benchmark that reframes multimodal understanding as code generation: given an image, a model must produce SVG that preserves symbolic meaning for downstream reasoning. VCode covers three domains - general commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel evaluation protocol in which a policy model answers questions over rendered SVGs; correct answers indicate faithful symbolic preservation. Empirically, frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap between language-centric and visual-centric coding. To close this gap, we introduce VCoder, an agentic framework that augments VLMs along two axes: (i) Thinking with Revision, which iteratively analyzes discrepancies and refines SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply structured cues such as objects, shapes, and text beyond the model's intrinsic capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning. VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation. The benchmark and code are available at https://github.com/CSU-JPG/VCode.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Visual-centric coding is underexplored: current progress centers on language-only coding (program synthesis/debugging), leaving image-to-code generation for real-world scenes largely unaddressed.<br>‚Ä¢ Pixel RGB representations lack symbolic abstraction: they capture appearance but not compact, interpretable structure (objects, counts, spatial relations) needed for reasoning and agentic action.<br>‚Ä¢ Existing multimodal code benchmarks focus on synthetic assets (charts, UIs, icons) and guided prompts, not natural images; evaluations rarely test whether generated code preserves the original image‚Äôs semantics.<br>‚Ä¢ Frontier VLMs struggle to produce faithful SVGs from natural images, revealing a gap between language-centric and visual-centric coding; core difficulties include long-context code generation, lack of render-time feedback, and weak fine-grained shape/edge/text capture.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VCode reframes multimodal understanding as image-to-SVG generation and evaluates semantic fidelity via CodeVQA (answering questions using only the rendered SVG), alongside embedding similarity (SigLIP) and code-length metrics. VCoder augments VLMs with iterative render-and-revise (Thinking with Revision) and tool-augmented perception (detectors, segmentation, OCR; Acting with Visual Tools) to inject structured cues into SVG generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ End-to-End Vision‚ÄìLanguage Coders with Differentiable Rendering: Train coders directly against CodeVQA via differentiable rendering or RL to maximize semantic fidelity of SVGs.<br>‚Ä¢ Tool-Calling Policies for Visual Coding Agents: Learn policies that select and sequence detectors/segmenters/OCR during coding to optimize accuracy‚Äìefficiency trade-offs.<br>‚Ä¢ 3D-Aware SVG for Depth and Spatial Reasoning: Extend vector code with depth layers and spatial annotations to improve 3D relations on vision-centric benchmarks.<br>‚Ä¢ Compress-to-Code: Token-Efficient Symbolic Rendering of Natural Images: Develop objectives and primitives that minimize SVG token length while preserving CodeVQA performance.<br>‚Ä¢ Neural Primitive Libraries for Faithful Vectorization: Learn reusable shape/path/text primitives for irregular boundaries and textures to boost fidelity on natural scenes.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02779" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02779" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We propose MIRA, a new benchmark designed to evaluate models in scenarios where generating intermediate visual images is essential for successful reasoning. Unlike traditional CoT methods that rely solely on text, tasks in MIRA require models to generate and utilize intermediate images - such as sketches, structural diagrams, or path drawings - to guide their reasoning process. This setup closely mirrors how humans solve complex problems through "drawing to think". To solve this, MIRA focuses on tasks that are intrinsically challenging and involve complex structures, spatial relationships, or reasoning steps that are difficult to express through language alone. To ensure that our evaluation data is of high-quality, we include 546 multimodal problems, annotated with intermediate visual images and final answers. We also propose a unified evaluation protocol for MIRA that spans three levels of evaluation input: direct input with image and question only, text-only CoT input with image and thinking prompts, and Visual-CoT input with both annotated image clues and textual thinking prompts. To probe the upper bound of model capacity on our benchmark, we also report pass@k and majority voting accuracies under different k settings. Experimental results show that existing multimodal large language models, including strongest private models as well as strong open-weight models, perform poorly when relying solely on textual prompts. However, when intermediate visual cues are provided, model performance improves consistently, yielding an average relative gain of 33.7% across all models and tasks. We also probe the upper bound by expanding the search space and designing textual prompts aligned with Visual-CoT, but both yield only limited improvements compared to our Visual-CoT setting. These results underscore the critical role of imagined visual information in enabling successful reasoning on MIRA.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Text-only chain-of-thought is a lossy medium for inherently visual reasoning (spatial geometry, physics, multi-step state tracking), where humans typically ‚Äúdraw to think.‚Äù<br>‚Ä¢ Existing multimodal benchmarks emphasize perception (image-in, text-out) or text-only CoT; they rarely require generating/using intermediate images, and tool-based approaches are bounded by external tools.<br>‚Ä¢ There is no standardized benchmark or protocol to isolate and quantify the contribution of intermediate visual cues to reasoning or to assess ‚Äúthink-while-drawing‚Äù capabilities.<br>‚Ä¢ Empirically, leading MLLMs that excel on standard suites perform poorly on such tasks (<20% with direct inputs), and Text-CoT can even degrade performance; providing visual-CoT cues yields large gains (average +33.7% relative), revealing a fundamental capability gap.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MIRA is a curated benchmark of 546 multimodal problems across 20 task types, each paired with human-annotated intermediate visual states, plus a three-level evaluation protocol‚ÄîDirect, Text-CoT, and Visual-CoT‚Äîto isolate the effect of intermediate images on reasoning. The authors standardize prompts and auto-grading, and probe model upper bounds via pass@k, majority voting, and task-specialized CoT prompts across diverse closed and open-weight MLLMs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Think-While-Drawing: End-to-End Training for Interleaved Visual‚ÄìTextual Chain-of-Thought: Train unified MLLMs to generate, edit, and reference their own intermediate diagrams during reasoning without external tool orchestration.<br>‚Ä¢ AutoSketch: Programmatic Synthesis of Visual Chain-of-Thought at Scale: Automatically generate tasks and matched step-wise visual scratchpads to create large-scale Visual-CoT corpora for pretraining and finetuning.<br>‚Ä¢ Differentiable Sketchpads for Geometry and Physics Reasoning: Integrate differentiable drawing/simulation modules that models can call iteratively to construct and update visual states, improving faithfulness and accuracy on spatial/causal tasks.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02243" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02243" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multimodal large language models (MLLMs) must resolve conflicts when different modalities provide contradictory information, a process we term modality following. Prior work measured this behavior only with coarse dataset-level statistics, overlooking the influence of model's confidence in unimodal reasoning. In this paper, we introduce a new framework that decomposes modality following into two fundamental factors: relative reasoning uncertainty (the case-specific confidence gap between unimodal predictions) and inherent modality preference( a model's stable bias when uncertainties are balanced). To validate this framework, we construct a controllable dataset that systematically varies the reasoning difficulty of visual and textual inputs. Using entropy as a fine-grained uncertainty metric, we uncover a universal law: the probability of following a modality decreases monotonically as its relative uncertainty increases. At the relative difficulty level where the model tends to follow both modalities with comparable probability what we call the balance point, a practical indicator of the model's inherent preference. Unlike traditional macro-level ratios, this measure offers a more principled and less confounded way to characterize modality bias, disentangling it from unimodal capabilities and dataset artifacts. Further, by probing layer-wise predictions, we reveal the internal mechanism of oscillation: in ambiguous regions near the balance point, models vacillate between modalities across layers, explaining externally observed indecision. Together, these findings establish relative uncertainty and inherent preference as the two governing principles of modality following, offering both a quantitative framework and mechanistic insight into how MLLMs resolve conflicting information.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ MLLMs lack a principled account of how they resolve conflicts when image and text contradict (modality following).<br>‚Ä¢ Existing evaluations use coarse dataset-level ratios (TFR/VFR) that conflate unimodal capability, dataset artifacts, and inherent modality preference.<br>‚Ä¢ Even when filtering for cases solvable by either modality, prior work ignores confidence gaps; differing unimodal uncertainties still drive divergent multimodal choices.<br>‚Ä¢ No unified, comparable uncertainty metric has been used to explain case-wise decisions across modalities.<br>‚Ä¢ Limited mechanistic understanding of internal dynamics (e.g., why models hesitate or average) when modalities are similarly uncertain.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper decomposes modality following into relative unimodal reasoning uncertainty (measured via answer-token entropy under text-only and vision-only inputs, combined as a normalized ŒîHrel) and an inherent modality preference quantified by the balance point where text/vision following is equiprobable. It validates a monotonic law between ŒîHrel and following probability using a controllable conflict dataset with independently tunable visual/textual difficulty, and reveals layerwise ‚Äúoscillations‚Äù near the balance point via LogitLens-style probing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Balance-Point Aware Training for Conflict-Resilient MLLMs: Learn objectives that shift or regularize the balance point to desired preferences while preserving unimodal competence.<br>‚Ä¢ Uncertainty-Calibrated Multimodal Fusion via Relative Entropy Gating: Design fusion/decoding mechanisms that adaptively route or weight modalities based on ŒîHrel rather than fixed heuristics.<br>‚Ä¢ Diagnosing and Steering Layerwise Oscillations in MLLMs: Detect, interpret, and intervene on mid-layer oscillations to stabilize decisions in ambiguous regions without harming clear-case performance.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">The Collaboration Gap</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02687" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02687" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The trajectory of AI development suggests that we will increasingly rely on agent-based systems composed of independently developed agents with different information, privileges, and tools. The success of these systems will critically depend on effective collaboration among these heterogeneous agents, even under partial observability. Despite intense interest, few empirical studies have evaluated such agent-agent collaboration at scale. We propose a collaborative maze-solving benchmark that (i) isolates collaborative capabilities, (ii) modulates problem complexity, (iii) enables scalable automated grading, and (iv) imposes no output-format constraints, preserving ecological plausibility. Using this framework, we evaluate 32 leading open- and closed-source models in solo, homogeneous, and heterogeneous pairings. Our results reveal a "collaboration gap": models that perform well solo often degrade substantially when required to collaborate. Collaboration can break down dramatically; for instance, small distilled models that solve mazes well alone may fail almost completely in certain pairings. We find that starting with the stronger agent often improves outcomes, motivating a "relay inference" approach where the stronger agent leads before handing off to the weaker one, closing much of the gap. Our findings argue for (1) collaboration-aware evaluation, (2) training strategies developed to enhance collaborative capabilities, and (3) interaction design that reliably elicits agents' latent skills, guidance that applies to AI-AI and human-AI collaboration.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of collaboration-aware evaluation: Existing LM benchmarks rarely test dynamic, multi-agent collaboration under partial observability and without rigid protocols; the paper introduces a task that isolates collaboration ability with scalable auto-grading (see system diagram on page 3, Fig. 2.1).<br>‚Ä¢ Real-world need for flexible AI‚ÄìAI communication: Current systems lean on fixed protocols or centralized orchestration, which are too rigid for open-world, heterogeneous agents with different tools, privileges, and knowledge.<br>‚Ä¢ Ecological plausibility and scalability gaps: Prior studies often enforce tight output formats or use settings (e.g., role simulations, negotiations) that lack controlled, measurable outcomes or introduce conflicting incentives.<br>‚Ä¢ Empirical blind spot: Solo performance overestimates deployability‚Äîmodels that perform well alone often fail when collaborating, a phenomenon the paper terms the collaboration gap (plot on page 6, Fig. 4.1).<br>‚Ä¢ Underspecified grounding and schema alignment: Without fixed conventions, agents struggle to align coordinate systems, actions, and map representations, leading to failures (example on page 4, Fig. 2.2).<br>‚Ä¢ Heterogeneous deployments are sensitive to ordering: Who speaks first strongly affects outcomes; weaker models can drag down stronger ones, and cross-family pairings show distinct interaction patterns (heatmaps on page 7, Fig. 4.3).<br>‚Ä¢ Distillation misses a key axis: Distilled/smaller models disproportionately lose collaborative skill, suggesting current distillation pipelines neglect teamwork capabilities.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A scalable collaborative maze-solving benchmark that distributes partial information across two agents, imposes minimal communication constraints, and auto-grades free-form dialogues via an LLM grader with robust schema normalization to extract agreed routes. The paper also proposes relay inference‚Äîhaving a stronger agent seed the early turns before handing off to a weaker one‚Äîwhich closes much of the collaboration gap (priming and recovery effects shown on page 8, Fig. 4.6).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Collaboration-Aware Distillation: Preserving Teamwork Skills in Small Models: Design distillation objectives and curricula that explicitly retain grounding, turn-taking, and conflict-resolution behaviors lost in current small/distilled models.<br>‚Ä¢ Learning to Lead: Adaptive Role and Turn-Ordering Policies for Heterogeneous Agent Teams: Train agents or orchestrators to infer who should speak/act first and when to hand off, leveraging the strong ordering effects observed across families.<br>‚Ä¢ Grounding-First Prompts: Automatic Schema Alignment for Open-World Multi-Agent Dialogue: Develop prompt and meta-dialogue strategies that induce shared coordinate/action conventions and reduce grounding failures without rigid protocols.<br>‚Ä¢ Beyond Mazes: Distributed-Information Benchmarks for Realistic Collaborative Tasks: Generalize the benchmark template to coding, planning, and tool-use tasks with partial observability and auto-grading of unstructured dialogues.<br>‚Ä¢ RL for Collaborative Grounding: Training Agents to Resolve Perceptual and Procedural Conflicts: Use reinforcement learning to reward accurate grounding, conflict detection, and resolution under noisy or mismatched partner behaviors.<br>‚Ä¢ Robust LLM Graders for Unstructured Multi-Agent Transcripts: Build ensemble or cross-model graders with uncertainty estimation and schema discovery to further reduce evaluation bias/noise at scale.<br>‚Ä¢ Measuring and Mitigating Imitation Pitfalls in Multi-Agent Communication: Quantify and reduce harmful style imitation/sycophancy that causes stronger models to adopt weaker partners‚Äô suboptimal protocols.<br>‚Ä¢ Cost-Aware Relay Inference in the Wild: Policies for Seeding, Handoff, and Recovery in Mixed-Quality Agent Teams: Learn when to prime with a strong model, how long to lead, and when recovery is still effective under budget constraints.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25976" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25976" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reconstructing images seen by people from their fMRI brain recordings provides a non-invasive window into the human brain. Despite recent progress enabled by diffusion models, current methods often lack faithfulness to the actual seen images. We present "Brain-IT", a brain-inspired approach that addresses this challenge through a Brain Interaction Transformer (BIT), allowing effective interactions between clusters of functionally-similar brain-voxels. These functional-clusters are shared by all subjects, serving as building blocks for integrating information both within and across brains. All model components are shared by all clusters & subjects, allowing efficient training with a limited amount of data. To guide the image reconstruction, BIT predicts two complementary localized patch-level image features: (i)high-level semantic features which steer the diffusion model toward the correct semantic content of the image; and (ii)low-level structural features which help to initialize the diffusion process with the correct coarse layout of the image. BIT's design enables direct flow of information from brain-voxel clusters to localized image features. Through these principles, our method achieves image reconstructions from fMRI that faithfully reconstruct the seen images, and surpass current SotA approaches both visually and by standard objective metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve results comparable to current methods trained on full 40-hour recordings.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Diffusion-based fMRI decoders often produce realistic but unfaithful reconstructions, missing objects or distorting structure (position, color, layout)<br>‚Ä¢ Existing models compress whole-brain signals into a single global embedding, ignoring the brain‚Äôs distributed, localized representations and hindering patch-level guidance<br>‚Ä¢ Cross-subject methods align at scan level rather than voxel level, limiting data sharing across individuals and making transfer to new subjects data-hungry<br>‚Ä¢ Weak coupling between brain signals and generative priors allows diffusion to drift from the actually seen image<br>‚Ä¢ Low-level structural cues are underutilized; few methods directly reconstruct coarse image layout from fMRI<br>‚Ä¢ Scarce per-subject data necessitates architectures that share parameters across subjects and leverage external images effectively</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Brain-IT introduces a Brain Interaction Transformer (BIT) that maps fMRI to shared functional voxel clusters and transforms their Brain Tokens into localized features: adapted CLIP tokens for semantics and VGG features for structure. A dual-branch pipeline inverts VGG features via Deep Image Prior to create a coarse layout that initializes an unCLIP-style diffusion model conditioned on the predicted CLIP tokens, yielding reconstructions that are both structurally faithful and semantically accurate.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ End-to-End Learnable Voxel-to-Cluster Mapping for Brain-IT: Jointly optimize the V2C clustering and BIT to maximize reconstruction fidelity without relying on precomputed voxel embeddings<br>‚Ä¢ Temporal Brain-IT: Video Reconstruction from fMRI via Recurrent BIT: Extend BIT with temporal attention to decode dynamic scenes and motion from fMRI time series<br>‚Ä¢ Few-Shot Personalization of Brain-IT with Lightweight Subject Adapters: Add adapter layers or meta-learning to rapidly adapt BIT to new subjects using minutes of data<br>‚Ä¢ Interpretable Brain Tokens: Mapping BIT Clusters to Retinotopic and Semantic Cortical Maps: Probe and align Brain Tokens with neuroanatomical/functional maps for interpretability and neuroscientific validation<br>‚Ä¢ Self-Supervised Cross-Modal Pretraining for BIT at Scale: Pretrain with unlabeled fMRI and large image corpora using contrastive/Masked modeling to improve data efficiency and generalization</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02650" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02650" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large multimodal models (LMMs) often suffer from severe inference inefficiency due to the large number of visual tokens introduced by image encoders. While recent token compression methods, such as pruning and merging, have shown promise in reducing redundancy, their evaluation remains fragmented and inconsistent. In this work, we present UniPruneBench, a unified and extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench provides standardized protocols across six ability dimensions and ten datasets, covering ten representative compression algorithms and three families of LMMs (LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates system-level metrics such as runtime and prefilling latency to provide a holistic view. Our experiments uncover several key findings: (1) random pruning is a surprisingly strong baseline, (2) no single method consistently outperforms others across scenarios, (3) pruning sensitivity varies significantly across tasks, with OCR being most vulnerable, and (4) pruning ratio is the dominant factor governing performance degradation. We believe UniPruneBench will serve as a reliable foundation for future research on efficient multimodal modeling.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LMMs suffer from severe inference inefficiency because images introduce hundreds of often redundant visual tokens, inflating compute, memory, and latency and hindering real-time or large-scale deployment.<br>‚Ä¢ Existing token compression (pruning/merging) works lack a fair, unified, and reproducible evaluation; prior studies cover few methods, models, and tasks.<br>‚Ä¢ Inconsistent evaluation protocols (prompting, scoring metrics, retention ratios, toolkits) make cross-paper comparisons unreliable.<br>‚Ä¢ Prior evaluations neglect system-level metrics (e.g., total runtime, prefill latency, compression overhead), providing an incomplete view of efficiency‚Äìaccuracy trade-offs.<br>‚Ä¢ Implementations are tightly coupled to specific architectures, limiting modularity and extensibility to new LMMs and pruning locations.<br>‚Ä¢ There is limited understanding of when/why pruning works across abilities (e.g., OCR vs. instruction following) and how pruning ratio governs degradation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes UniPruneBench, a unified, extensible benchmark that standardizes the evaluation of visual token compression across six capability dimensions (ten datasets), three model families (LLaVA-v1.5, InternVL3, Qwen2.5-VL), and ten plug-and-play algorithms spanning ViT-only, LLM-only, and hybrid pruning. It decouples pruning logic from model architectures and reports both task accuracy and system-level metrics (total time, method time, prefill latency) under controlled pruning ratios and fixed pruning locations for fair comparison.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Task-Aware Visual Token Pruning for OCR-Intensive Multimodal Tasks: Design OCR-preserving pruning and merging that prioritize fine-grained text regions to mitigate the pronounced OCR sensitivity observed under pruning.<br>‚Ä¢ Learning to Prune: Joint Train-Time and Test-Time Co-Design for Visual Token Compression: Co-train encoders/connectors with pruning-aware objectives and distillation to recover performance at high sparsity.<br>‚Ä¢ Adaptive Instance-Level Token Budgeting with Lightweight Controllers: Learn per-input token allocation policies that dynamically balance accuracy and latency given a target compute budget.<br>‚Ä¢ Coordinated Multi-Stage Compression Across ViT and LLM: Develop principled orchestration of pre-LLM and intra-LLM pruning to avoid redundant discards and maximize complementary benefits.<br>‚Ä¢ Pruning-Robust LMM Architectures and Connectors: Redesign vision encoders and multimodal adapters to be inherently robust to token sparsification and to shrink prefill-time costs more effectively.<br>‚Ä¢ Random-Plus: Simple Yet Strong Baselines for Visual Token Compression: Systematically enhance random pruning with minimal heuristics (e.g., spatial stratification, duplication-aware sampling) to close the gap with complex methods.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LTD-Bench: Evaluating Large Language Models by Letting Them Draw</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02347" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02347" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current evaluation paradigms for large language models (LLMs) represent a critical blind spot in AI research--relying on opaque numerical metrics that conceal fundamental limitations in spatial reasoning while providing no intuitive understanding of model capabilities. This deficiency creates a dangerous disconnect between reported performance and practical abilities, particularly for applications requiring physical world understanding. We introduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation from abstract scores to directly observable visual outputs by requiring models to generate drawings through dot matrices or executable code. This approach makes spatial reasoning limitations immediately apparent even to non-experts, bridging the fundamental gap between statistical performance and intuitive assessment. LTD-Bench implements a comprehensive methodology with complementary generation tasks (testing spatial imagination) and recognition tasks (assessing spatial perception) across three progressively challenging difficulty levels, methodically evaluating both directions of the critical language-spatial mapping. Our extensive experiments with state-of-the-art models expose an alarming capability gap: even LLMs achieving impressive results on traditional benchmarks demonstrate profound deficiencies in establishing bidirectional mappings between language and spatial concept--a fundamental limitation that undermines their potential as genuine world models. Furthermore, LTD-Bench's visual outputs enable powerful diagnostic analysis, offering a potential approach to investigate model similarity.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Opaque, aggregate metrics hide LLMs‚Äô spatial reasoning limits and offer little intuitive understanding for users and developers, despite growing deployment in robotics, autonomy, and design (pp. 1‚Äì2).<br>‚Ä¢ Existing benchmarks emphasize text-to-symbol skills (e.g., knowledge, math, code) and lack visually interpretable tests; they do not evaluate the bidirectional mapping between language and spatial concepts (perception ‚Üî imagination) (pp. 3‚Äì4).<br>‚Ä¢ Current evaluations rarely localize capability thresholds; they do not reveal where models begin to fail as spatial complexity increases (Easy ‚Üí Normal ‚Üí Hard) (pp. 4‚Äì6).<br>‚Ä¢ Traditional metrics provide poor diagnostics for model similarity; they fail to capture stylistic regularities in generated visuals that could reveal model-family tendencies (pp. 9‚Äì10).<br>‚Ä¢ Overall, there is a disconnect between high benchmark scores and practical spatial abilities required for understanding and acting in the physical world (pp. 1‚Äì2).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>LTD-Bench is a dual-path, visually interpretable benchmark that requires LLMs to both generate visual artifacts from text (dot matrices or Python drawing code) and recognize characters/objects from matrices/code across three difficulty tiers (discrete grids, continuous curves, open-ended objects). Outputs are rendered and evaluated via human judges and GPT-4.1, enabling precise assessment of spatial perception and imagination and facilitating style-based model similarity analysis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Scaling LTD-Bench to 3D and Dynamics: A Comprehensive Benchmark for Spatiotemporal Reasoning in Language Models: Extend tasks to 3D scenes, motion, and multi-object layouts to stress-test compositional and temporal spatial reasoning.<br>‚Ä¢ Visual Style as a Metric: Quantifying Model Similarity via Style Signatures in Open-Ended Drawings: Develop quantitative, reproducible metrics to measure inter-model similarity using stylistic features of generated images, beyond accuracy.<br>‚Ä¢ Learning to Draw with Feedback: Execution-Grounded RL for Spatial Code Generation in LLMs: Use code execution signals and image-based rewards to train models that reliably map language to spatial drawings and improve imagination under constraints.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01937" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01937" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) trained for step-by-step reasoning often become excessively verbose, raising inference cost. Standard Reinforcement Learning with Verifiable Rewards (RLVR) pipelines filter out ``easy'' problems for training efficiency, leaving the model to train primarily on harder problems that require longer reasoning chains. This skews the output length distribution upward, resulting in a model that conflates ``thinking longer'' with ``thinking better''. In this work, we show that retaining and modestly up-weighting moderately easy problems acts as an implicit length regularizer. Exposing the model to solvable short-chain tasks constrains its output distribution and prevents runaway verbosity. The result is \emph{emergent brevity for free}: the model learns to solve harder problems without inflating the output length, despite the absence of any explicit length penalization. RLVR experiments using this approach on Qwen3-4B-Thinking-2507 (with a 16k token limit) achieve baseline pass@1 AIME25 accuracy while generating solutions that are, on average, nearly twice as short. The code is available at https://github.com/MBZUAI-Paris/Frugal-AI{GitHub}, with datasets and models on https://huggingface.co/collections/MBZUAI-Paris/k2-think-mini-68dcfa8b114686a4bd3dc2bc{Hugging Face}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ RLVR-trained reasoning models become excessively verbose, inflating inference cost, latency, and memory usage under long reasoning chains.<br>‚Ä¢ Standard RLVR pipelines filter out ‚Äúeasy‚Äù problems, biasing training toward medium/hard instances that encourage long outputs and conflate ‚Äúthinking longer‚Äù with ‚Äúthinking better‚Äù.<br>‚Ä¢ With binary verifiable rewards and no length penalty, models can reduce answer uncertainty by emitting more tokens (information-theoretic shortcut), inducing upward drift in output length.<br>‚Ä¢ Hard samples often yield truncated or failed generations within fixed context limits, providing weak gradients; easy samples are discarded, removing concise positive signals.<br>‚Ä¢ Existing methods focus on reward design, larger models, or curriculum on harder data, but lack simple data-centric mechanisms that directly regularize output length without hurting accuracy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Keep and modestly upweight moderately easy math problems in GRPO-based RLVR under a fixed 16k-token budget so they act as implicit length regularizers, yielding concise reasoning without explicit length penalties; then apply a short curriculum RLVR phase on a filtered DeepMath-103 subset to broaden difficulty while preserving brevity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Easy‚ÄìHard Mixers for RLVR: Online reweighting of sample difficulty to maintain a target brevity‚Äìaccuracy trade-off across training.<br>‚Ä¢ Implicit+Explicit Length Regularization for Reasoning RL: Combine easy-sample exposure with light length-aware rewards or constraints for finer control of output length.<br>‚Ä¢ Frugal Reasoning Beyond Math: Applying easy-sample regularization to coding, logic, and multimodal reasoning with verifiable rewards.<br>‚Ä¢ Theory of Emergent Brevity in Autoregressive RL: Formal analysis of how dataset difficulty distributions and context limits shape length dynamics and entropy.<br>‚Ä¢ Dynamic Context Budget Scheduling in RLVR: Allocate and adapt context windows per-sample to maximize efficiency-adjusted accuracy under fixed compute budgets.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">CodeClash: Benchmarking Goal-Oriented Software Engineering</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.00839" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.00839" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current benchmarks for coding evaluate language models (LMs) on concrete, well-specified tasks such as fixing specific bugs or writing targeted tests. However, human programmers do not spend all day incessantly addressing isolated tasks. Instead, real-world software development is grounded in the pursuit of high-level goals, like improving user retention or reducing costs. Evaluating whether LMs can also iteratively develop code to better accomplish open-ended objectives without any explicit guidance remains an open challenge. To address this, we introduce CodeClash, a benchmark where LMs compete in multi-round tournaments to build the best codebase for achieving a competitive objective. Each round proceeds in two phases: agents edit their code, then their codebases compete head-to-head in a code arena that determines winners based on objectives like score maximization, resource acquisition, or survival. Whether it's writing notes, scrutinizing documentation, analyzing competition logs, or creating test suites, models must decide for themselves how to improve their codebases both absolutely and against their opponents. We run 1680 tournaments (25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal that while models exhibit diverse development styles, they share fundamental limitations in strategic reasoning. Models also struggle with long-term codebase maintenance, as repositories become progressively messy and redundant. These limitations are stark: top models lose every round against expert human programmers. We open-source CodeClash to advance the study of autonomous, goal-oriented code development.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing code benchmarks target narrow, well-specified tasks (unit tests, single-function fixes, targeted tests) rather than high-level, outcome-driven goals and adversarial adaptation, misaligning evaluation with real-world software development (pp.1‚Äì3).<br>‚Ä¢ Real-world SWE requires iterative goal decomposition, strategy, opponent adaptation, and learning from noisy feedback; current evaluations rarely test long-horizon planning or competitive dynamics (Fig.1 on p.2; ¬ß2.3).<br>‚Ä¢ Prior benchmarks provide explicit instructions and binary pass/fail signals that quickly saturate, offering little room to assess self-directed improvement, validation practices, or meta-tools (logs analysis, self-play) (¬ß6).<br>‚Ä¢ Existing setups largely ignore persistent memory and repo health over time; models are not evaluated on building self-crafted memory, maintaining organized codebases, or avoiding redundancy (¬ß2.1; Fig.6‚Äì7 on pp.7‚Äì8).<br>‚Ä¢ A standardized, reproducible, multi-arena, head-to-head framework with relative metrics (win rate, Elo) is missing to measure strategic strength under non-deterministic outcomes and diverse objectives (Table 1 on p.5; ¬ßC.3).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>CodeClash benchmarks goal-oriented SWE by having LMs iteratively edit their own codebases in multi-round tournaments, then compete in diverse code arenas (e.g., BattleSnake, Core War, Poker, RoboCode). Feedback is limited to arena logs copied back into the repo (codebase-as-memory), edits are made via a bash-only agent interface, and performance is measured by round/tournament wins and an Elo maximum-likelihood fit.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Self-Play Fine-Tuning for CodeClash Agents: Use arena outcomes and logs to train opponent-aware strategies via RL/self-play, improving comeback rates and counter-strategy design.<br>‚Ä¢ Multimodal CodeClash: VLMs for Visual Arena Feedback: Incorporate visual traces/replays to enable perception-driven reasoning and richer failure analysis beyond text logs.<br>‚Ä¢ Scaling CodeClash to Real-World Systems: City-Scale, Multi-Objective Arenas: Build larger, multi-service codebases (e.g., planning, disaster response, cybersecurity) to test coordination across subsystems.<br>‚Ä¢ Reading the Rival: Learning to Exploit Transparent Opponent Code: Systematically study/code methods for opponent-code inspection, weakness detection, and tailored countermeasures.<br>‚Ä¢ Repository-Centric Memory for Long-Horizon Agents: Design reusable, self-maintaining memory artifacts (notes, tests, tools) that persist and compress across rounds without clutter.<br>‚Ä¢ Hygienic Auto-Refactoring in Self-Evolving Repos: Methods to curb file sprawl, deduplicate scripts, and enforce structure, improving file reuse and long-term maintainability.<br>‚Ä¢ Human‚ÄìAI Tournaments in CodeClash: Measuring Competitive and Collaborative Dynamics: Evaluate static and co-evolving human bots, co-design workflows, and skill transfer.<br>‚Ä¢ Multi-Player Strategy Learning: Coalitions, Positional Play, and Risk in CodeClash: Study strategic volatility, coalition formation, and risk management in 3+ player arenas.<br>‚Ä¢ Tool-Augmented vs. Bash-Only Agent Scaffolds: A Controlled Study: Quantify gains and biases from tool-rich frameworks versus minimal ACIs under identical arenas.<br>‚Ä¢ Elo-as-a-Service for Agentic SWE: Robust Online Rating and Variance Modeling: Develop live rating systems, uncertainty tracking, and match scheduling for evolving agent pools.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02832" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02832" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large-scale data has driven breakthroughs in robotics, from language models to vision-language-action models in bimanual manipulation. However, humanoid robotics lacks equally effective data collection frameworks. Existing humanoid teleoperation systems either use decoupled control or depend on expensive motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid teleoperation and data collection system that preserves full whole-body control while advancing scalability. Our system leverages PICO4U VR for obtaining real-time whole-body human motions, with a custom 2-DoF robot neck (cost around $250) for egocentric vision, enabling holistic human-to-humanoid control. We demonstrate long-horizon dexterous and mobile humanoid skills and we can collect 100 demonstrations in 15 minutes with an almost 100% success rate. Building on this pipeline, we propose a hierarchical visuomotor policy framework that autonomously controls the full humanoid body based on egocentric vision. Our visuomotor policy successfully demonstrates whole-body dexterous manipulation and dynamic kicking tasks. The entire system is fully reproducible and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also open-sourced at https://twist-data.github.io .</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Humanoid robotics lacks scalable, portable data collection; existing systems either decouple upper/lower body or rely on expensive, non-portable motion capture, limiting real-world deployment (Table I on page 2).<br>‚Ä¢ VR-based methods with partial control capture only simple locomotion and miss dynamic, coordinated whole-body skills (arms‚Äìtorso‚Äìlegs‚Äìfeet) needed for dexterous loco-manipulation (pages 1‚Äì2).<br>‚Ä¢ Absence of egocentric active vision and neck DoFs leads to poor depth estimation and limited field of view, degrading long-horizon performance (ablation in Figure 12 on page 8; neck design in Section III-B, page 4).<br>‚Ä¢ High setup/calibration overhead, multi-operator workflows, and system latency hinder rapid, large-scale data collection (single-operator design on page 6; 1-minute setup on page 1; <0.1s delay on page 6).<br>‚Ä¢ Prior low-level controllers often use complex teacher‚Äìstudent training and simplify lower-body to root-velocity commands, preventing precise foot/leg control and legged manipulation (Section III-A/E, pages 3 and 5).<br>‚Ä¢ No existing framework learns full whole-body visuomotor policies from egocentric vision; previous works control subsets or issue coarse base commands (Figure 7 on page 6; discussion on pages 2‚Äì3).<br>‚Ä¢ Importance: Large-scale data underpins LLMs/VLA; enabling fast, holistic, egocentric humanoid data collection is critical to unlock long-horizon dexterous and mobile manipulation (Abstract/Introduction, pages 1‚Äì2).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TWIST2 combines PICO4U VR whole-body streaming with a low-cost 2-DoF add-on neck and stereo egocentric vision, a modified GMR retargeter (pelvis-centric, lower-body positional constraints), and a PPO-trained general motion tracker to track full-body commands; on top, a Diffusion Policy maps egocentric images and proprioceptive history to whole-body joint commands for autonomous control (see system overview in Figure 2 on page 4).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ TWIST3: Dynamic Whole‚ÄëBody Teleoperation for High‚ÄëSpeed Humanoids: Extend the motion tracker and retargeting to robustly handle sprinting, rapid transitions, and impacts for dynamic whole‚Äëbody skills.<br>‚Ä¢ Learning Active Egocentric Gaze for Humanoids: Joint Neck‚ÄëBody Policies: Train policies that actively control the neck/camera to maximize task success under long‚Äëhorizon loco‚Äëmanipulation with stereo depth.<br>‚Ä¢ Cross‚ÄëEmbodiment Whole‚ÄëBody VLA: Scaling Vision‚ÄëLanguage‚ÄëAction to Full Humanoids: Build a universal command interface and retargeting to transfer TWIST2 data/policies across humanoid platforms and integrate language grounding.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">iFlyBot-VLA Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01914" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01914" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model trained under a novel framework. The main contributions are listed as follows: (1) a latent action model thoroughly trained on large-scale human and robotic manipulation videos; (2) a dual-level action representation framework that jointly supervises both the Vision-Language Model (VLM) and the action expert during training; (3) a mixed training strategy that combines robot trajectory data with general QA and spatial QA datasets, effectively enhancing the 3D perceptual and reasoning capabilities of the VLM backbone. Specifically, the VLM is trained to predict two complementary forms of actions: latent actions, derived from our latent action model pretrained on cross-embodiment manipulation data, which capture implicit high-level intentions; and structured discrete action tokens, obtained through frequency-domain transformations of continuous control signals, which encode explicit low-level dynamics. This dual supervision aligns the representation spaces of language, vision, and action, enabling the VLM to directly contribute to action generation. Experimental results on the LIBERO Franka benchmark demonstrate the superiority of our frame-work, while real-world evaluations further show that iFlyBot-VLA achieves competitive success rates across diverse and challenging manipulation tasks. Furthermore, we plan to open-source a portion of our self-constructed dataset to support future research in the community</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Autoregressive VLMs struggle to produce precise, smooth continuous control signals; naive discretization (e.g., binning) scales poorly and hurts numerical accuracy for long-horizon manipulation.<br>‚Ä¢ End-to-end training with randomly initialized diffusion/flow action experts can degrade the VLM‚Äôs perception and language reasoning; a training recipe is needed that preserves and leverages VLM generalization.<br>‚Ä¢ Training solely on manipulation data erodes language and spatial reasoning abilities; mixing manipulation with spatial VQA is required to maintain 3D understanding crucial for generalization.<br>‚Ä¢ Misalignment between vision, language, and action spaces impedes direct action generation; existing methods often capture either high-level intent or low-level dynamics, but not both.<br>‚Ä¢ Inference becomes inefficient when many discrete action tokens are generated; compact action conditioning is needed for real-time robot control.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>iFlyBot-VLA introduces dual-level action supervision‚Äîlatent action tokens from a VQ-VAE trained on large-scale human/robot videos plus FAST-encoded discrete action tokens‚Äîto align vision, language, and action in a Qwen2.5-VL backbone. A flow-matching diffusion transformer, conditioned on the backbone‚Äôs KV cache (only latent-action KVs), generates continuous action windows; mixed VQA+manipulation pre-training with gradient truncation and task-specific fine-tuning preserve perception while learning precise control.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Reinforcement Learning on Top of Dual-Level VLA for Robust Out-of-Distribution Recovery: Augment iFlyBot-VLA with on-policy RL or offline-to-online fine-tuning to improve recovery, robustness, and long-horizon stability beyond imitation.<br>‚Ä¢ 3D-Spatially Grounded VLA: Integrating Explicit 3D Scene Representations into Dual-Level Action Supervision: Fuse depth/scene graphs/3D Gaussians into the VLM and latent-action training to strengthen 3D perception, spatial reasoning, and manipulation in cluttered scenes.<br>‚Ä¢ Universal Cross-Embodiment Latent Action Codebooks for Zero-Shot Transfer: Scale and disentangle the latent action codebook across humans and diverse robot morphologies to enable zero-shot transfer between single- and dual-arm platforms.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02490" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02490" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As the global burden of Alzheimer's disease (AD) continues to grow, early and accurate detection has become increasingly critical, especially in regions with limited access to advanced diagnostic tools. We propose BRAINS (Biomedical Retrieval-Augmented Intelligence for Neurodegeneration Screening) to address this challenge. This novel system harnesses the powerful reasoning capabilities of Large Language Models (LLMs) for Alzheimer's detection and monitoring. BRAINS features a dual-module architecture: a cognitive diagnostic module and a case-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on cognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain volume metrics -- to perform structured assessments of Alzheimer's risk. Meanwhile, the Case Retrieval Module encodes patient profiles into latent representations and retrieves similar cases from a curated knowledge base. These auxiliary cases are fused with the input profile via a Case Fusion Layer to enhance contextual understanding. The combined representation is then processed with clinical prompts for inference. Evaluations on real-world datasets demonstrate BRAINS effectiveness in classifying disease severity and identifying early signs of cognitive decline. This system not only shows strong potential as an assistive tool for scalable, explainable, and early-stage Alzheimer's disease detection, but also offers hope for future applications in the field.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Early and accurate Alzheimer‚Äôs detection is critical but limited by access to neuroimaging and specialist assessment, especially in low-resource settings.<br>‚Ä¢ Conventional diagnostics (MMSE/CDR scoring, MRI volumetrics) are resource-intensive, require expert interpretation, and subtle structural changes are hard to quantify.<br>‚Ä¢ Real-world clinical data (e.g., MMSE, CDR, eTIV, nWBV, demographics; see Table I on page 2) are heterogeneous, variable, and often incomplete, challenging robust automated reasoning.<br>‚Ä¢ Existing AI systems often rely on rigid feature engineering and lack case-based contextual reasoning, interpretability, and robustness to real-world variability; LLMs also face context-window limits when integrating multiple reference cases.<br>‚Ä¢ There is a need for scalable, explainable tools that integrate heterogeneous structured data to support early detection and precise staging across diverse clinical environments.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>BRAINS is a retrieval-augmented LLM system that encodes a patient‚Äôs structured neurocognitive profile, retrieves top-K similar cases from a FAISS-based vector database (with dense reranking), and fuses them via a cross-attention Case Fusion Layer that replaces a special <RAGHere> token before inference by a fine-tuned LLaMA2-13B (architecture illustrated in Fig. 1 on page 3). It is domain-pretrained and fine-tuned with LoRA and dynamic masking, achieving 77.3% accuracy and improved F1 over non-retrieval baselines (Table II on page 4).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Longitudinal BRAINS: Forecasting Alzheimer‚Äôs Progression with Retrieval-Augmented Trajectory Modeling: Extend BRAINS to retrieve and fuse multi-visit histories to predict conversion risk and future MMSE/CDR trajectories.<br>‚Ä¢ Multimodal BRAINS: Joint Text‚ÄìImage‚ÄìSpeech Retrieval-Augmented LLMs for Early Alzheimer‚Äôs Detection: Integrate raw MRI/PET and speech biomarkers with structured scores to boost early-stage sensitivity and generalizability.<br>‚Ä¢ Federated BRAINS: Privacy-Preserving Cross-Site Case Retrieval for Neurodiagnostic LLMs: Develop federated vector stores and training to enable cross-hospital retrieval and learning without sharing patient data, improving privacy, fairness, and robustness.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ChartM^3: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02415" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02415" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Complex chart understanding tasks demand advanced visual recognition and reasoning capabilities from multimodal large language models (MLLMs). However, current research provides limited coverage of complex chart scenarios and computation-intensive reasoning tasks prevalent in real-world applications. This study proposes an automated multi-stage code-driven pipeline for systematically generating visual reasoning datasets to address these limitations. The pipeline integrates retrieval-augmented generation (RAG) to retrieve professional chart templates and employs chain-of-thought (CoT) strategies to generate reasoning codes that simulate real data distributions, thereby driving chart rendering and question-related statistical computations. Through model-based evaluation, the pipeline enhances chart diversity and data quality. Using this framework, we construct ChartM^3, a multi-dimensional and multi-step dataset containing 38K charts and 142K Q&A pairs for training, along with 2,871 high-quality evaluation samples for enabling practical performance assessment. Supervised fine-tuning (SFT) and reinforcement learning (RL) experiments demonstrate that our dataset significantly improves reasoning capabilities and cross-domain generalization performance, enabling smaller models to achieve performance comparable to larger-scale models in complex chart comprehension.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Multimodal LLMs perform poorly on real-world, text-rich charts despite strong natural-image VQA results, creating a deployment gap.<br>‚Ä¢ Existing chart datasets have limited chart-type and visual complexity (few types; weak coverage of scatter/heatmap/box/dual‚Äëaxis/multi‚Äësubplot), hurting generalization.<br>‚Ä¢ Questions are mostly low-level perception, lacking multi-step, computation-heavy, and multi-chart business analytics reasoning.<br>‚Ä¢ Datasets rarely include interpretable, stepwise reasoning and executable code, making answers hard to verify and models hard to train/explain.<br>‚Ä¢ Prior synthetic pipelines yield distributional monotony, weak real-world styling, and misalignment between plotting code and rendered images; numerical reasoning is error-prone.<br>‚Ä¢ Evaluation pipelines are not sufficiently objective or difficulty-calibrated, and human annotation cost limits scalability and coverage.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ChartM3 introduces a multi-stage, code-driven pipeline that uses RAG to retrieve professional chart templates and CoT to generate data-generation code, rendering code, and Q&A with executable analytical code; all code is executed to produce verifiable answers and reasoning. Model-based quality control and difficulty rating filter outputs, and the dataset supports SFT and RL with verifiable rewards to strengthen multi-step chart reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Cross-Language ChartM3: A Code-Driven, Multi-Backend Pipeline for ggplot2, Vega-Lite, and ECharts: Extend the synthesis pipeline beyond Python to multiple visualization grammars to boost style diversity, realism, and cross-platform generalization.<br>‚Ä¢ Beyond Statistical Charts: A Unified Benchmark for Reasoning over Flowcharts, Process Diagrams, and Networks: Adapt code-driven generation and verifiable reasoning to schematic and relational visuals requiring structural and logical reasoning.<br>‚Ä¢ Scaling RLVR for Chart Understanding: From Small Models to Billion-Scale Policies with Formal Verifiers: Scale RL with verifiable rewards using larger models/data, integrate programmatic/solver-based checks to reduce judge-LMM reliance, and study transfer to out-of-domain chart tasks.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.17950" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.17950" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Testing on real machines is indispensable for robotic control algorithms. In the context of learning-based algorithms, especially VLA models, demand for large-scale evaluation, i.e. testing a large number of models on a large number of tasks, is becoming increasingly urgent. However, doing this right is highly non-trivial, especially when scalability and reproducibility is taken into account. In this report, we describe our methodology for constructing RoboChallenge, an online evaluation system to test robotic control algorithms, and our survey of recent state-of-the-art VLA models using our initial benchmark Table30.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of scalable, reproducible real-robot evaluation for VLA models: simulator-only benchmarks miss real-world factors, and existing online systems host only a few tasks/machines, limiting coverage and generalization testing (see overview on page 1‚Äì2).<br>‚Ä¢ Inflexible submission paradigms and compute portability: model/docker/API-call submissions suffer CUDA/Python mismatches, debugging friction, NAT/public IP barriers, and constrain fine-grained timing needed by real-time methods (section 2.1).<br>‚Ä¢ High variance and tester-induced bias in real-robot tests: human setup differences lead to dramatic swings in success rates and the sweet-spot effect where object placement is exploited (illustrated on page 3‚Äì4), undermining reproducibility and fairness.<br>‚Ä¢ Coarse metrics hinder insight: a single success-rate metric overlooks partial progress and retries, masking meaningful differences in behavior (section 3.2).<br>‚Ä¢ Missing standardized, diverse, real-robot task suites with training data: broad, real-world tasks with released demonstrations are needed to fine-tune and fairly compare models (Table30 task list on page 7 and data release noted on page 2).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RoboChallenge: a remote-robot evaluation system exposing low-level, asynchronous APIs that stream precisely timestamped observations and accept FIFO action queues, plus job-scheduling, a visual task reproduction protocol for controlled resets, and a progress-based grading metric; deployed across a fleet of UR5, Franka, ALOHA, and ARX-5 robots with the 30-task Table30 benchmark and released demonstrations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ FairBench-Robo: A Blind Comparative Protocol for Real-Robot VLA Evaluation: Randomized, tester-blind post-selection to measure fairness and reduce operator bias beyond the current benchmark protocol.<br>‚Ä¢ TrustEval: Secure Model Attestation for Remote Robot Benchmarks: Cryptographic attestation and robot-side provenance (logs/video cross-checks) to verify model identity and prevent human-in-the-loop cheating noted as a limitation.<br>‚Ä¢ Table30++: Long-Horizon, Temporal-Memory and Deformable-Object Benchmarking with Force/Tactile Sensing: Extend Table30 with longer multi-stage tasks, soft-body manipulation, and added force/torque sensing to evaluate temporal VLA architectures against single-frame baselines.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02712" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02712" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Understanding and predicting emotion from videos has gathered significant attention in recent studies, driven by advancements in video large language models (VideoLLMs). While advanced methods have made progress in video emotion analysis, the intrinsic nature of emotions poses significant challenges. Emotions are characterized by dynamic and cues-dependent properties, making it difficult to understand complex and evolving emotional states with reasonable rationale. To tackle these challenges, we propose a novel affective cues-guided reasoning framework that unifies fundamental attribute perception, expression analysis, and high-level emotional understanding in a stage-wise manner. At the core of our approach is a family of video emotion foundation models (VidEmo), specifically designed for emotion reasoning and instruction-following. These models undergo a two-stage tuning process: first, curriculum emotion learning for injecting emotion knowledge, followed by affective-tree reinforcement learning for emotion reasoning. Moreover, we establish a foundational data infrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG) consisting of 2.1M diverse instruction-based samples. Emo-CFG includes explainable emotional question-answering, fine-grained captions, and associated rationales, providing essential resources for advancing emotion understanding tasks. Experimental results demonstrate that our approach achieves competitive performance, setting a new milestone across 15 face perception tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Emotions in video are dynamic and context-dependent, making it hard to model evolving states with clear, defensible rationale.<br>‚Ä¢ Existing VideoLLMs struggle to compose low-level facial attributes into high-level, fine-grained emotional interpretations (integration gap between attributes‚Üíexpressions‚Üíemotions).<br>‚Ä¢ Current systems often lack interpretability, failing to provide structured, explainable reasoning for their emotion predictions.<br>‚Ä¢ Datasets are limited in scope (few categories, static perception) and lack fine-grained, rationale-annotated supervision; a comprehensive emotion-centric data infrastructure is missing.<br>‚Ä¢ There is a significant performance gap in fine-grained sentiment/emotion analysis (e.g., Gemini 2.0 achieves only 26.3% accuracy; see Figure 2 on page 2), underscoring the need for new approaches.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VidEmo introduces a stage-wise affective cues-guided framework: curriculum emotion learning (attribute‚Üíexpression‚Üíemotion) injects emotion knowledge, followed by GRPO post-training with mixed rewards (rule-based QA, model-based caption, and an affective-tree reward via tree edit distance) to align fine-grained captions and rationales. At inference, a hierarchical best-of sampling over attributes, expressions, and emotions yields interpretable, rationale-grounded outputs (pipeline illustrated in Figure 3 on page 4).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Audio-Enhanced VidEmo: Integrating Speech Prosody and Acoustic Events into Affective-Tree Reasoning for Multimodal Emotion Understanding: Extend the tree to include audio cues and study cross-modal alignment in rewards and inference.<br>‚Ä¢ Personalized Affective Trees: Modeling Individual and Cross-Cultural Variability in Emotion Reasoning: Learn user/culture-conditioned priors and adaptive curricula to reduce bias and improve personalization.<br>‚Ä¢ Causal VidEmo: Interventional Affective-Tree Reinforcement Learning for Robust Emotion Attribution: Use causal graphs and interventional training to disentangle confounders and improve out-of-distribution generalization.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02219" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02219" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Complex reasoning over tabular data is crucial in real-world data analysis, yet large language models (LLMs) often underperform due to complex queries, noisy data, and limited numerical capabilities. To address these issues, we propose \method, a framework consisting of: (1) a query decomposer that breaks down complex questions, (2) a table sanitizer that cleans and filters noisy tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates executable code to derive the final answer from the sanitized table. To ensure unbiased evaluation and mitigate data leakage, we introduce a new dataset, CalTab151, specifically designed for complex numerical reasoning over tables. Experimental results demonstrate that \method consistently outperforms existing methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and 19.87% accuracy improvement on TAT-QA, TableBench, and \method, respectively. Moreover, our framework integrates seamlessly with mainstream LLMs, providing a robust solution for complex tabular numerical reasoning. These findings highlight the effectiveness of our framework in enhancing LLM performance for complex tabular numerical reasoning. Data and code are available upon request.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Multi-hop complexity in table QA: complex, conjunctive queries often require multiple calculation steps; LLMs frequently answer only part of the question, yielding incomplete or incorrect results, which is critical for real-world analytics (e.g., finance).<br>‚Ä¢ Noisy/irregular table structures: visual-to-text conversions introduce multi-level headers, segmented tables, nulls, and mixed-type numeric columns (e.g., "1.24(approx)") that break PoT/SQL pipelines and cause runtime errors.<br>‚Ä¢ Limited numerical computation in LLMs: CoT mimics patterns rather than computing; performance degrades under numeric perturbations; precise arithmetic is unreliable without external tools.<br>‚Ä¢ Limitations of existing methods: pre-trained/fine-tuned models require large high-quality data and generalize poorly to unseen tables; prompt-only methods usually ignore explicit decomposition and sanitization, leading to brittle reasoning and code failures.<br>‚Ä¢ Evaluation leakage: contamination in public datasets can inflate reported performance; a leakage-resistant benchmark is needed to assess genuine numerical reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TABDSR is a three-agent, prompt-based pipeline: a Query Decomposer splits complex questions into tractable sub-questions; a Table Sanitizer reconstructs nested/segmented headers and cleans cells/types with a parser-in-the-loop reflection; and a PoT-based Reasoner generates and executes Pandas code on the sanitized table to compute final answers. The approach is evaluated with CALTAB151, a leakage-resistant dataset for complex numerical reasoning over tables.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Hybrid Table-Aware Question Decomposition for Robust Multi-hop TQA: Combine question-only decomposition with selective table signals to improve sub-question fidelity without overfitting to table noise.<br>‚Ä¢ Execution-Guided Table Sanitization with Self-Repair and Schema Constraints: Develop multi-iteration, verifier-in-the-loop sanitization that enforces JSON schemas, type contracts, and automatic repair based on runtime errors.<br>‚Ä¢ Constrained Program-of-Thought with Verified Arithmetic for Tabular Reasoning: Introduce type-safe, unit-tested, and property-checked code generation with incremental execution and result validation to guarantee numeric correctness.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">D2D: Detector-to-Differentiable Critic for Improved Numeracy in Text-to-Image Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.19278" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.19278" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Text-to-image (T2I) diffusion models have achieved strong performance in semantic alignment, yet they still struggle with generating the correct number of objects specified in prompts. Existing approaches typically incorporate auxiliary counting networks as external critics to enhance numeracy. However, since these critics must provide gradient guidance during generation, they are restricted to regression-based models that are inherently differentiable, thus excluding detector-based models with superior counting ability, whose count-via-enumeration nature is non-differentiable. To overcome this limitation, we propose Detector-to-Differentiable (D2D), a novel framework that transforms non-differentiable detection models into differentiable critics, thereby leveraging their superior counting ability to guide numeracy generation. Specifically, we design custom activation functions to convert detector logits into soft binary indicators, which are then used to optimize the noise prior at inference time with pre-trained T2I models. Our extensive experiments on SDXL-Turbo, SD-Turbo, and Pixart-DMD across four benchmarks of varying complexity (low-density, high-density, and multi-object scenarios) demonstrate consistent and substantial improvements in object counting accuracy (e.g., boosting up to 13.7% on D2D-Small, a 400-prompt, low-density benchmark), with minimal degradation in overall image quality and computational overhead.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Diffusion-based text-to-image models frequently fail to generate the exact number of objects requested in prompts, leaving a persistent numeracy gap even after generic alignment methods.<br>‚Ä¢ Accurate numeracy is crucial for semantic faithfulness‚Äîespecially in common low-density scenes‚Äîwhere count errors are conspicuous and harm user trust and controllability.<br>‚Ä¢ Existing inference-time count-correction approaches rely on differentiable, regression-based counters (e.g., RCC, CLIP-Count), which underperform in low-density settings where detectors excel.<br>‚Ä¢ Strong detector-based counters (e.g., OWLv2, YOLO variants) are non-differentiable due to count-by-enumeration, preventing their gradients from guiding generation.<br>‚Ä¢ Prior strategies that adjust intermediate latents or attention layouts can degrade image quality and/or add substantial compute, and generic preference optimization alone does not resolve counting.<br>‚Ä¢ There is a need for a backbone-agnostic, low-overhead method that leverages detectors‚Äô superior counting ability without sacrificing image quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>D2D converts non-differentiable detector outputs into a differentiable count critic by applying steep, thresholded sigmoid activations to detector logits and logit-weighted scaling to produce strong, informative gradients, then uses these gradients to tune the initial noise via a lightweight Latent Modifier Network at inference time. This backbone-agnostic, noise-optimization approach improves object counting accuracy with minimal quality degradation and computational overhead, and extends to multi-class prompts.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive D2D: Learning Thresholds and Activation Shapes for Robust Numeracy: Jointly learn detector thresholds and sigmoid steepness per class/image to adapt gradients across densities and object scales.<br>‚Ä¢ D2D-Align: Multi-Objective Noise Optimization for Numeracy and Semantic Preferences: Combine D2D‚Äôs critic with human-preference, layout, and caption-consistency rewards in a unified, Pareto-aware optimization.<br>‚Ä¢ Open-Vocabulary Multi-Object Layout with Differentiable Detectors: Extend D2D to jointly control counts, spatial layouts, and inter-object relations for open-vocabulary prompts using detector-driven spatial objectives.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02374" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02374" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current large language models excel at broad, general-purpose tasks, but consistently underperform when exposed to highly specialized domains that require deep cultural, linguistic, and subject-matter expertise. In particular, traditional medical systems such as Ayurveda embody centuries of nuanced textual and clinical knowledge that mainstream LLMs fail to accurately interpret or apply. We introduce AyurParam-2.9B, a domain-specialized, bilingual language model fine-tuned from Param-1-2.9B using an extensive, expertly curated Ayurveda dataset spanning classical texts and clinical guidance. AyurParam's dataset incorporates context-aware, reasoning, and objective-style Q&A in both English and Hindi, with rigorous annotation protocols for factual precision and instructional clarity. Benchmarked on BhashaBench-Ayur, AyurParam not only surpasses all open-source instruction-tuned models in its size class (1.5--3B parameters), but also demonstrates competitive or superior performance compared to much larger models. The results from AyurParam highlight the necessity for authentic domain adaptation and high-quality supervision in delivering reliable, culturally congruent AI for specialized medical knowledge.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ General-purpose LLMs underperform on Ayurveda, a culturally and linguistically nuanced medical domain, leading to misinterpretation and weak clinical reasoning in sensitive healthcare contexts.<br>‚Ä¢ Insufficient bilingual (English/Hindi) support limits accessibility and accuracy for Indian users and practitioners.<br>‚Ä¢ Existing training data lack grounded, domain-specific supervision; models hallucinate and miss classical conventions due to heterogeneous, non-curated corpora.<br>‚Ä¢ Prior Ayurveda-oriented systems are limited in scope, scale, and evaluation rigor, offering inadequate reasoning and multilingual coverage.<br>‚Ä¢ Need for trustworthy, culturally congruent AI validated on domain benchmarks; e.g., on BhashaBench-Ayur, general 3B models lag (Llama-3.2-3B: 33.20%) versus AyurParam-2.9B at 39.97% (Table 1, page 8), underscoring the value of domain specialization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Fine-tune a 2.9B-parameter base model (Param-1-2.9B) on a rigorously curated, bilingual (EN/HI), span-grounded Ayurveda corpus (4.75M examples) built via a pipeline: taxonomy-guided collection with license governance, high-quality OCR (Surya), normalization, constrained Q&A synthesis with a larger LLM, rule-based + LLM adjudication, and targeted expert audits. Supervised fine-tuning with custom bilingual dialogue templates and task-specific tokens yields state-of-the-art results in its size class on BhashaBench-Ayur and competitive performance with larger models (Tables 1‚Äì3, pages 8‚Äì9).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Continual-AyurParam: Lifelong Integration of Contemporary Ayurvedic Research and Guidelines: Introduce continual learning to ingest recent publications, clinical guidelines, and practice updates for temporal relevance.<br>‚Ä¢ Shabda-to-Samprapti: Balanced Multilingual Pretraining for Sanskrit/Hindi-First Ayurvedic Reasoning: Targeted augmentation, tokenizer improvements, and language-specific fine-tuning to close the Hindi/Sanskrit gap.<br>‚Ä¢ ClinSafe-Ayur: Safety Guardrails, Uncertainty Quantification, and Refusal Policies for Traditional-Medicine LLMs: Add safety layers (disclaimers, calibrated uncertainty, refusal triggers) and evaluate on safety suites.<br>‚Ä¢ AyuGraph: Knowledge-Graph-Augmented Reasoning for Panchakarma, Rasayana, and Classical Literature: Fuse structured knowledge graphs with LLMs to improve reasoning on difficult classical subdomains.<br>‚Ä¢ Human-in-the-Clinic: Prospective Practitioner Evaluation and Clinical Validation of a Bilingual Ayurvedic LLM: Conduct expert reviews and simulated/real consultations to assess accuracy, safety, and usability.<br>‚Ä¢ Patient-Aware AyurParam: Privacy-Preserving Personalization of Ayurvedic Recommendations: Incorporate patient context via federated or on-device methods while enforcing privacy and contraindication checks.<br>‚Ä¢ LicensedData-Ayur: Secure Integration of Licensed Clinical and Modern Protocol Data for Robust Domain Coverage: Partner with institutions to safely leverage authoritative datasets for improved reliability.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02366" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02366" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In this work, we propose LiveSecBench, a dynamic and continuously updated safety benchmark specifically for Chinese-language LLM application scenarios. LiveSecBench evaluates models across six critical dimensions (Legality, Ethics, Factuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in the Chinese legal and social frameworks. This benchmark maintains relevance through a dynamic update schedule that incorporates new threat vectors, such as the planned inclusion of Text-to-Image Generation Safety and Agentic Safety in the next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs, providing a landscape of AI safety in the context of Chinese language. The leaderboard is publicly accessible at https://livesecbench.intokentech.cn/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Static, English-centric safety benchmarks miss Chinese linguistic and cultural nuances (indirect expressions, idioms, homophonic puns, taboos) and Chinese legal context, yielding misleading safety assessments (p.2).<br>‚Ä¢ AI safety is dynamic and event-driven; models overfit to static test sets and new jailbreaks emerge, making fixed benchmarks quickly obsolete (p.2; references to LiveBench/LiveCodeBench).<br>‚Ä¢ Existing evaluations under-emphasize privacy leakage and nuanced adversarial attacks (context-embedded, format-embedded, combined strategies), which are prevalent in real use (Section 2.1, p.2‚Äì3).<br>‚Ä¢ Most benchmarks judge only final answers; unsafe chain-of-thought reasoning can persist even when outputs look safe, leaving hidden risks (Reasoning Safety, Section 2.1, p.3).<br>‚Ä¢ Lack of a transparent, continuously updated leaderboard for Chinese-language safety hampers comparative monitoring and rapid mitigation (Abstract; Section 2, p.2).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>LiveSecBench is a dynamic, Chinese-context safety benchmark that evaluates LLMs across six dimensions (Legality, Ethics, Factuality, Privacy, Adversarial Robustness, Reasoning Safety), using culturally grounded, manually curated questions and inspecting chain-of-thought where applicable. Models are ranked via head-to-head ELO with Swiss-system pairing (Section 2.3, p.4), the dataset is periodically refreshed, and planned expansions include Text-to-Image and Agentic Safety (Section 4.1, p.5), with passive, non-public test sets for submissions (Section 4.2, p.5).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ LiveSecBench-Image: Dynamic Safety Benchmarking for Chinese Text-to-Image Systems: Extend LiveSecBench to multimodal prompts/outputs and evaluate cultural, legal, and harmful visual content filtering (aligned with Section 4.1, p.5).<br>‚Ä¢ AgentBench-CN: Assessing Agentic Safety and Tool-Use Robustness in Chinese LLM Agents: Systematically test tool-use, instruction chaining, and memory poisoning threats in agent frameworks (Section 4.1, p.5).<br>‚Ä¢ CoTSafe-CN: Auditing and Mitigating Unsafe Reasoning Paths in Chinese LLMs: Develop automated detectors and interventions that identify and correct unsafe chain-of-thought despite safe final outputs (Section 2.1, p.3).<br>‚Ä¢ AutoJailbreak-CN: Generating Evolving, Culturally Nuanced Adversarial Prompts for Live Benchmarks: Use programmatic/LLM-based generators to continuously create indirect, idiomatic, and format-embedded attacks tailored to Chinese contexts.<br>‚Ä¢ PrivacyProbe-CN: Measuring and Reducing Memorization-Based PII Leakage in Chinese LLMs: Standardize extraction audits and differential privacy defenses for Chinese PII and legal compliance within LiveSecBench.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01502" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01502" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Unsupervised learning of depth and ego-motion, two fundamental 3D perception tasks, has made significant strides in recent years. However, most methods treat ego-motion as an auxiliary task, either mixing all motion types or excluding depth-independent rotational motions in supervision. Such designs limit the incorporation of strong geometric constraints, reducing reliability and robustness under diverse conditions. This study introduces a discriminative treatment of motion components, leveraging the geometric regularities of their respective rigid flows to benefit both depth and ego-motion estimation. Given consecutive video frames, network outputs first align the optical axes and imaging planes of the source and target cameras. Optical flows between frames are transformed through these alignments, and deviations are quantified to impose geometric constraints individually on each ego-motion component, enabling more targeted refinement. These alignments further reformulate the joint learning process into coaxial and coplanar forms, where depth and each translation component can be mutually derived through closed-form geometric relationships, introducing complementary constraints that improve depth robustness. DiMoDE, a general depth and ego-motion joint learning framework incorporating these designs, achieves state-of-the-art performance on multiple public datasets and a newly collected diverse real-world dataset, particularly under challenging conditions. Our source code will be publicly available at mias.group/DiMoDE upon publication.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing unsupervised joint depth‚Äìego-motion methods indiscriminately mix motion types (rotation, tangential and radial translations) or simply drop rotation, preventing strong, component-specific geometric constraints from being exploited.<br>‚Ä¢ Rotational flows are irregular and depth-independent, while tangential and radial translation induce distinct, depth-dependent regularities; mixing them yields inconsistent gradients, hinders convergence, and degrades robustness.<br>‚Ä¢ Prior works often estimate rotation with external algorithms/networks, adding compute/storage overhead, breaking end-to-end training, and allowing rotational errors to propagate.<br>‚Ä¢ Photometric, per-pixel supervision is fragile under adverse conditions (night, weather, motion blur, texture-less regions, occlusions), limiting reliability and stability.<br>‚Ä¢ Hybrid VO methods achieve accuracy via back-end optimization but are slow and resource-heavy, restricting real-world deployment.<br>‚Ä¢ Domain-adaptation-based robust depth methods depend on fixed data splits, hampering scalable use of diverse, continually collected data.<br>‚Ä¢ Accurate, robust depth and ego-motion are fundamental for 3D perception; improving their reliability across diverse conditions is essential for practical autonomy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DiMoDE discriminatively treats motion components by aligning optical axes and imaging planes with PoseNet outputs, transforming optical flows accordingly, and imposing geometric regularity losses for tangential and radial components to refine pose. It then uses closed-form depth‚Äìtranslation relationships to build constraint cycles that jointly supervise DepthNet and recover translations end-to-end, improving robustness and convergence.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Geometry-Aware DiMoDE Under Extreme Illumination: Integrate illumination-invariant correspondences (e.g., event-based cues or learned low-light flow) so axis/plane alignments and constraint cycles remain reliable in near-darkness.<br>‚Ä¢ Interpretable Motion-Basis DiMoDE for Generalization: Replace raw SE(3) regression with learned motion bases and sparsity priors to regularize component estimation and improve transfer to complex, non-driving environments.<br>‚Ä¢ Self-Calibrating and Uncertainty-Aware DiMoDE: Jointly estimate intrinsics (focal, principal point) and model uncertainty in component-wise constraints to enhance scale consistency and training stability.<br>‚Ä¢ Multi-Sensor DiMoDE: IMU/LiDAR-Augmented Constraint Cycles: Fuse inertial and sparse LiDAR cues to stabilize axis/plane alignments and flow transformations in high-rotation, high-speed or texture-poor scenes.<br>‚Ä¢ Dynamic-Scene DiMoDE with Multi-Body Decomposition: Segment and assign rigid-motion components to moving objects, applying per-body axis/plane alignments to maintain valid constraints in dynamic environments.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01450" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01450" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent studies have identified Direct Preference Optimization (DPO) as an efficient and reward-free approach to improving video generation quality. However, existing methods largely follow image-domain paradigms and are mainly developed on small-scale models (approximately 2B parameters), limiting their ability to address the unique challenges of video tasks, such as costly data construction, unstable training, and heavy memory consumption. To overcome these limitations, we introduce a GT-Pair that automatically builds high-quality preference pairs by using real videos as positives and model-generated videos as negatives, eliminating the need for any external annotation. We further present Reg-DPO, which incorporates the SFT loss as a regularization term into the DPO objective to enhance training stability and generation fidelity. Additionally, by combining the FSDP framework with multiple memory optimization techniques, our approach achieves nearly three times higher training capacity than using FSDP alone. Extensive experiments on both I2V and T2V tasks across multiple datasets demonstrate that our method consistently outperforms existing approaches, delivering superior video generation quality.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Preference data bottleneck: Constructing high-quality video preference pairs is costly and hard‚Äîautomatic evaluators are unreliable, human scoring is expensive, and generated samples often differ subtly, yielding few strong pairs.<br>‚Ä¢ DPO instability in video diffusion: Standard DPO only optimizes preference gaps without supervising per-sample distributions, leading to gradient vanishing, rapid convergence, distribution shift, and training collapse with visual artifacts.<br>‚Ä¢ Scaling and memory limits: Video DPO with large models (>10B) and a frozen reference policy incurs heavy GPU memory usage due to multi-frame inputs and paired data, limiting batch size and model scale; prior work largely targets small (~2B) models.<br>‚Ä¢ Image-to-video mismatch: Directly porting image-domain DPO paradigms fails to address the spatiotemporal dependencies and complexity unique to video.<br>‚Ä¢ RLHF limitations: Reward-model-based approaches are complex, costly, unstable, and prone to reward hacking, especially for longer videos.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes GT-Pair to automatically create high-quality preference pairs by treating real videos as positives and their model-generated counterparts as negatives, eliminating external annotation. It further introduces Reg-DPO, which augments the DPO loss with an SFT-style regularization on positives (with dynamic weighting) to stabilize training and improve fidelity, and combines FSDP with Flash Attention, Context Parallelism, and Pair Parallelism to triple effective training capacity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Reg-DPO with Learnable SFT Weighting: Learn a data- and training-state‚Äìdependent schedule for the SFT regularization weight to maximize stability and performance across diverse video lengths and motions.<br>‚Ä¢ GT-Pair++: Multi-Granularity Preference Pairing for Long-Form Video: Construct hierarchical pairs at shot/scene/video levels to better capture long-range temporal coherence and narrative consistency.<br>‚Ä¢ Hard-Negative Mining for Video DPO: Automatically curate and schedule negatives of increasing difficulty to sharpen preference discrimination and avoid early saturation.<br>‚Ä¢ Online GT-Pair from In-the-Wild Streams: Continual, weakly supervised harvesting of real videos with on-the-fly filtering and generation to build scalable, up-to-date preference corpora without manual labels.<br>‚Ä¢ Theoretical Convergence of SFT-Regularized DPO in Diffusion Models: Provide formal stability and generalization guarantees, bounding distribution shift and gradient decay under Reg-DPO.<br>‚Ä¢ Memory-Efficient Dual-Policy Training for 10B+ Video Generators: Compress, distill, or offload the reference policy and integrate finer-grained pair/context parallelism for further reductions in memory and communication cost.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RiddleBench: A New Generative Reasoning Benchmark for LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24932" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24932" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models have demonstrated strong performance on many established reasoning benchmarks. However, these benchmarks primarily evaluate structured skills like quantitative problem-solving, leaving a gap in assessing flexible, multifaceted reasoning abilities that are central to human intelligence. These abilities require integrating logical deduction with spatial awareness and constraint satisfaction, which current evaluations do not measure well. To address this, we introduce RiddleBench, a benchmark of 1,737 challenging puzzles in English designed to probe these core reasoning capabilities. Evaluation of state-of-the-art models on RiddleBench shows fundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3, and Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and 63.16%). Analysis further reveals deep failures, including hallucination cascades (accepting flawed reasoning from other models) and poor self-correction due to a strong self-confirmation bias. Their reasoning is also fragile, with performance degrading significantly when constraints are reordered or irrelevant information is introduced. RiddleBench functions as a diagnostic tool for these issues and as a resource for guiding the development of more robust and reliable language models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing reasoning benchmarks mostly test pattern recognition or memorization and narrow skills (e.g., math word problems, commonsense QA, formal logic), not flexible, multifaceted reasoning that synthesizes logical deduction, spatial awareness, and constraint satisfaction.<br>‚Ä¢ There is no focused evaluation of compositional, constraint-satisfying reasoning that requires constructing spatial/relational layouts and satisfying multiple interacting constraints; current datasets emphasize single-path, explicit solutions and miss hybrid reasoning.<br>‚Ä¢ Modern LLMs exhibit critical, under-measured failure modes‚Äîhallucination cascades in model-as-judge settings, poor self-correction driven by self-confirmation bias, and fragile logic sensitive to reordered constraints and irrelevant information‚Äîdemanding a diagnostic benchmark.<br>‚Ä¢ A high-fidelity, public benchmark is needed to reliably expose these weaknesses and guide the development of more robust, dependable reasoning systems.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces RiddleBench, a 1,737-problem English benchmark spanning sequential reasoning, seating arrangements, blood relations, and coding‚Äìdecoding, curated via OCR plus meticulous human verification to target multi-step deduction, spatial reasoning, and constraint satisfaction. It evaluates leading LLMs with standardized zero-shot prompts and probes reliability through cross-model judging, self-correction tests, and robustness perturbations (constraint shuffling and red-herring noise).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Multilingual RiddleBench: Evaluating and Aligning Compositional Reasoning Across Languages: Translate and culturally adapt RiddleBench to major Indian and global languages to test language-agnostic reasoning and cross-lingual transfer.<br>‚Ä¢ Anti-Hallucination Judges: Proof-Grounded Verifiers That Re-solve Rather Than Endorse: Build verifier/refuter architectures that independently re-solve problems, verify premises, and use formal proof or constraint checks to prevent hallucination cascades.<br>‚Ä¢ Self-Correction That Works: Calibrated Introspection and Error Detection for LLM Reasoning: Develop self-evaluation loops with uncertainty calibration, contradiction detection, and external constraint solvers to overcome self-confirmation bias.<br>‚Ä¢ Order- and Noise-Invariant Reasoners: Training Objectives and Data Augmentations for Robust Logical Parsing: Enforce invariance to constraint order and irrelevant information via adversarial shuffling/noise, contrastive objectives, and consistency regularization.<br>‚Ä¢ Structure-Aware Spatial Solvers: Integrating CSP/Graph Modules and Diagram Induction into LLMs: Combine LLMs with constraint programming, graph-based backends, or diagram generation to handle seating/relations tasks with explicit state tracking.<br>‚Ä¢ Benchmarking and Mitigating Error Fixation: Iterative Debate and Multi-Agent Cross-Checks on RiddleBench: Use multi-agent debate, role rotation, and adjudication protocols to reverse entrenched errors and improve reliability on complex puzzles.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 8</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-05 23:10:53 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 8;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>