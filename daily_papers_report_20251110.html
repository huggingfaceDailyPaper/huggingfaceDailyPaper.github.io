<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 10, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 10, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Too Good to be Bad: On the Failure of LLMs to Role-Play Villains</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.04962" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.04962" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) are increasingly tasked with creative generation, including the simulation of fictional characters. However, their ability to portray non-prosocial, antagonistic personas remains largely unexamined. We hypothesize that the safety alignment of modern LLMs creates a fundamental conflict with the task of authentically role-playing morally ambiguous or villainous characters. To investigate this, we introduce the Moral RolePlay benchmark, a new dataset featuring a four-level moral alignment scale and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs with role-playing characters from moral paragons to pure villains. Our large-scale evaluation reveals a consistent, monotonic decline in role-playing fidelity as character morality decreases. We find that models struggle most with traits directly antithetical to safety principles, such as ``Deceitful'' and ``Manipulative'', often substituting nuanced malevolence with superficial aggression. Furthermore, we demonstrate that general chatbot proficiency is a poor predictor of villain role-playing ability, with highly safety-aligned models performing particularly poorly. Our work provides the first systematic evidence of this critical limitation, highlighting a key tension between model safety and creative fidelity. Our benchmark and findings pave the way for developing more nuanced, context-aware alignment methods.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLMs struggle to authentically role-play morally ambiguous and antagonistic personas due to a fundamental conflict between safety alignment (helpful, harmless, honest) and the behaviors required for villainy.<br>‚Ä¢ Existing datasets and evaluations lack a structured, balanced benchmark across moral alignments; villains are underrepresented, and there is no trait-level fidelity assessment.<br>‚Ä¢ General chatbot leaderboards (e.g., Arena) are poor predictors of villain role-playing ability, masking a critical capability gap that impacts creative applications.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces the Moral RolePlay benchmark: a four-level moral alignment scale with a balanced test set of 800 characters, annotated traits, and rich scene contexts, coupled with an actor-framed, zero-shot prompting strategy. It evaluates character fidelity using an LLM-as-judge rubric with a deduction-based score formula, revealing a monotonic decline as morality decreases and establishing the Villain RolePlay leaderboard.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Context-Aware Safety Alignment for Fictional Antagonists: Design alignment schemes that permit trait-consistent negative behaviors within clearly bounded role-play while preventing real-world harm and misuse.<br>‚Ä¢ Trait-Conditioned Fine-Tuning for Egoist and Villain Personas: Use trait-level supervision, control tokens, and constrained decoding to safely emulate manipulative and deceitful behaviors with nuanced fidelity.<br>‚Ä¢ Hybrid Human‚ÄìLLM Evaluation for Moral Role-Play Fidelity: Build calibrated evaluation protocols combining human raters and multi-LLM committees to reduce bias and better detect nuanced inconsistencies in antagonistic portrayals.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DeepEyesV2: Toward Agentic Multimodal Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.05271" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.05271" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Agentic multimodal models should not only comprehend text and images, but also actively invoke external tools, such as code execution environments and web search, and integrate these operations into reasoning. In this work, we introduce DeepEyesV2 and explore how to build an agentic multimodal model from the perspectives of data construction, training methods, and model evaluation. We observe that direct reinforcement learning alone fails to induce robust tool-use behavior. This phenomenon motivates a two-stage training pipeline: a cold-start stage to establish tool-use patterns, and reinforcement learning stage to further refine tool invocation. We curate a diverse, moderately challenging training dataset, specifically including examples where tool use is beneficial. We further introduce RealX-Bench, a comprehensive benchmark designed to evaluate real-world multimodal reasoning, which inherently requires the integration of multiple capabilities, including perception, search, and reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative benchmarks, demonstrating its effectiveness across real-world understanding, mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2 exhibits task-adaptive tool invocation, tending to use image operations for perception tasks and numerical computations for reasoning tasks. Reinforcement learning further enables complex tool combinations and allows model to selectively invoke tools based on context. We hope our study can provide guidance for community in developing agentic multimodal models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing multimodal LLMs are passive and cannot actively invoke external tools (code execution, web search) or integrate them into reasoning, limiting performance on real-world tasks requiring perception, search, and reasoning.<br>‚Ä¢ Single-tool approaches (e.g., cropping-only or search-only) fail when fine-grained perception and up-to-date information retrieval must be combined.<br>‚Ä¢ Direct reinforcement learning on base models does not yield stable, reliable tool-use behaviors, indicating the need for a cold-start phase to establish tool-use patterns.<br>‚Ä¢ Current benchmarks mostly silo abilities; there is a lack of comprehensive evaluation that tests cross-capability integration in realistic settings.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DeepEyesV2 integrates executable Python code and web search as interleavable tools within an iterative reasoning loop, converting tool outputs into observations that are incorporated into subsequent steps. It uses a two-stage training pipeline‚Äîcold-start supervised fine-tuning on curated, tool-beneficial data to establish patterns, followed by RL with simple accuracy and format rewards to refine task-adaptive tool invocation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning to Orchestrate Multi-Tool Chains via Hierarchical Policies in Agentic MLLMs: Develop hierarchical RL and planning mechanisms to coordinate complex sequences of image operations, computations, and searches with step-level credit assignment.<br>‚Ä¢ RealX-Bench++: A Multi-Modal, Multi-Hop Benchmark for Verifiable Tool-Augmented Reasoning: Extend RealX-Bench with grounded evidence annotations, temporal updates, and verifiability metrics to rigorously test tool choice and evidence integration.<br>‚Ä¢ Safety and Robustness in Tool-Driven Multimodal Agents: Runtime Sandboxing and Failure Recovery: Investigate safe execution, error detection, and recovery strategies for code and web interactions, including adversarial robustness and misuse prevention.<br>‚Ä¢ Self-Discovering Tools: Automatic Tool Induction from Interaction Logs for Multimodal Agents: Mine successful trajectories to automatically derive new tool abstractions and APIs, expanding the toolbox without manual engineering.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Visual Spatial Tuning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.05491" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.05491" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Capturing spatial relationships from visual inputs is a cornerstone of human-like general intelligence. Several previous studies have tried to enhance the spatial awareness of Vision-Language Models (VLMs) by adding extra expert encoders, which brings extra overhead and usually harms general capabilities. To enhance the spatial ability in general architectures, we introduce Visual Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with human-like visuospatial abilities, from spatial perception to reasoning. We first attempt to enhance spatial perception in VLMs by constructing a large-scale dataset termed VST-P, which comprises 4.1 million samples spanning 19 skills across single views, multiple images, and videos. Then, we present VST-R, a curated dataset with 135K samples that instruct models to reason in space. In particular, we adopt a progressive training pipeline: supervised fine-tuning to build foundational spatial knowledge, followed by reinforcement learning to further improve spatial reasoning abilities. Without the side-effect to general capabilities, the proposed VST consistently achieves state-of-the-art results on several spatial benchmarks, including 34.8% on MMSI-Bench and 61.2% on VSIBench. It turns out that the Vision-Language-Action models can be significantly enhanced with the proposed spatial tuning paradigm, paving the way for more physically grounded AI.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Vision-Language Models (VLMs) lack robust 3D spatial perception and reasoning across single images, multi-view sets, and videos, limiting grounded interaction in robotics, AR/VR, and autonomous systems.<br>‚Ä¢ Prior approaches often add expert 3D encoders, increasing complexity and compute while degrading general multimodal capabilities.<br>‚Ä¢ Existing datasets and training pipelines target isolated skills (e.g., single-image SFT only), under-cover multi-view and spatiotemporal relationships, and provide weak support for advanced spatial reasoning.<br>‚Ä¢ There is a need for a scalable, encoder-free way to inject 3D spatial knowledge into standard VLM backbones and to strengthen spatial reasoning with verifiable signals, without harming general abilities.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Visual Spatial Tuning (VST) combines a large perception dataset (VST-P, 4.1M samples over 19 skills across single/multi-image/video) and a curated reasoning dataset (VST-R, 135K CoT and rule-checkable items) with a progressive pipeline: supervised fine-tuning ‚Üí CoT cold-start ‚Üí reinforcement learning (GRPO with accuracy/format rewards), aided by BEV-guided CoT generation and FoV unification for consistent 3D detection. This encoder-free tuning yields state-of-the-art spatial understanding and improves VLA performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ BEV-Guided Chain-of-Thought at Scale for Multi-View Spatial Reasoning: Systematically expand BEV-based prompting and evaluation to diverse scenes and viewpoints, studying robustness and generalization of textual layout reconstruction.<br>‚Ä¢ FoV-Unified Spatial Perception in the Wild: Self-calibrating and domain-adaptive methods to generalize FoV unification to unknown cameras, reducing geometric mismatch and improving 3D detection across heterogeneous devices.<br>‚Ä¢ From VST to SLAM: Language-Supervised Map Building for Embodied Foundation Models: Introduce explicit intermediate spatial maps (BEV/scene graphs) learned with textual supervision to bridge VST and SLAM for improved navigation and manipulation.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.04662" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.04662" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but they cannot reliably verify their own logic. Even when they reach correct answers, the underlying reasoning may be flawed, undermining trust in high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a neuro-symbolic method that extracts and verifies formal logical arguments from CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order logic and identifies premises that ground the argument in source context, commonsense knowledge, or prior reasoning steps. The symbolic representation enables automated solvers to verify logical validity while the NL premises allow humans and systems to identify ungrounded or fallacious reasoning steps. Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT effectively identifies flawed reasoning, and serves as a strong predictor of final answer correctness. We also leverage VeriCoT's verification signal for (1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct preference optimization (DPO) using verification-based pairwise rewards, further improving reasoning validity and accuracy.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLM Chain-of-Thought (CoT) often contains logically invalid steps even when final answers are correct, undermining trust in high-stakes domains (biomedical, legal).<br>‚Ä¢ Current LMs lack explicit mechanisms to verify the logical validity of their reasoning or to surface the implicit premises that support each step.<br>‚Ä¢ Prior self-refinement and symbolic checking approaches do not guarantee validity of the entire CoT and rarely ground every step in source context or commonsense.<br>‚Ä¢ Existing autoformalization is limited to specific tasks/domains (e.g., NLI, math/code), leaving general-domain CoT verification under-addressed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VERICOT autoformalizes each CoT step into first-order logic (SMT-LIB), incrementally infers supporting premises from source context and commonsense, and uses the Z3 solver to check entailment/consistency, flagging steps as ungrounded, contradictory, or untranslatable while filtering premises via an LLM-as-judge. It then leverages the verification signal for inference-time self-reflection and fine-tuning (SFT/DPO) to improve reasoning validity and accuracy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Probabilistic VERICOT: Uncertainty-Aware Neuro-Symbolic Chain-of-Thought Validation: Integrate probabilistic or fuzzy logic with calibrated confidence over premises and entailment to quantify trust in verified reasoning.<br>‚Ä¢ Citation-Grounded VERICOT: Retrieval-Augmented Premise Generation with Evidence Attribution: Couple premise generation with retrieval and explicit source-span citation to reduce confabulations and strengthen grounding.<br>‚Ä¢ VERICOT-RL: Reinforcement Learning from Formal Verification Rewards: Use solver-based rewards and counterfactual checks to train models to produce maximally verifiable CoTs beyond DPO.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Dense Motion Captioning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.05369" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.05369" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in 3D human motion and language integration have primarily focused on text-to-motion generation, leaving the task of motion understanding relatively unexplored. We introduce Dense Motion Captioning, a novel task that aims to temporally localize and caption actions within 3D human motion sequences. Current datasets fall short in providing detailed temporal annotations and predominantly consist of short sequences featuring few actions. To overcome these limitations, we present the Complex Motion Dataset (CompMo), the first large-scale dataset featuring richly annotated, complex motion sequences with precise temporal boundaries. Built through a carefully designed data generation pipeline, CompMo includes 60,000 motion sequences, each composed of multiple actions ranging from at least two to ten, accurately annotated with their temporal extents. We further present DEMO, a model that integrates a large language model with a simple motion adapter, trained to generate dense, temporally grounded captions. Our experiments show that DEMO substantially outperforms existing methods on CompMo as well as on adapted benchmarks, establishing a robust baseline for future research in 3D motion understanding and captioning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ 3D motion understanding is under-explored compared to text-to-motion generation, lacking models that can temporally localize and describe actions within long, continuous motion sequences.<br>‚Ä¢ Existing motion-language datasets are short, lack precise temporal annotations, or provide noisy/fragmented labels, making them unsuitable for dense, temporally grounded captioning.<br>‚Ä¢ State-of-the-art single-motion captioning/retrieval models fail to generalize to complex sequences with multiple sub-actions, showing a notable performance drop on such cases.<br>‚Ä¢ Prior tasks (e.g., temporal action localization with fixed labels) do not support free-form, segment-level captions with accurate timestamps, limiting detailed motion understanding.<br>‚Ä¢ Applications require body-centric, temporally precise descriptions (e.g., lifting video to 3D motion for nuanced analysis), which current benchmarks and models do not support.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>They define Dense Motion Captioning (DMC) and introduce CompMo, a 60k-sequence dataset with 2‚Äì10 actions per clip, each densely captioned with precise temporal boundaries. They propose DEMO, a baseline that couples a large language model with a simple motion adapter and trains it in two stages‚Äîmotion-language alignment and dense, temporally grounded caption generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Streaming Dense Motion Captioning for Online 3D Pose: Develop an incremental captioner that localizes and describes actions in real time from streaming 3D motion input.<br>‚Ä¢ Multimodal Fusion for Dense Motion Captioning: Extend DMC by fusing 3D motion with synchronized RGB/audio (√† la MotionX++) to improve temporal localization and semantic richness.<br>‚Ä¢ Evaluating and Refining Metrics for 3D Dense Motion Captioning: Design DMC-specific evaluation metrics that jointly assess temporal alignment and caption quality beyond video-centric measures like SODA.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.05017" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.05017" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In this work, we identify an inherent bias in prevailing LVLM architectures toward the language modality, largely resulting from the common practice of simply appending visual embeddings to the input text sequence. To address this, we propose a simple yet effective method that refines textual embeddings by integrating average-pooled visual features. Our approach demonstrably improves visual grounding and significantly reduces hallucinations on established benchmarks. While average pooling offers a straightforward, robust, and efficient means of incorporating visual information, we believe that more sophisticated fusion methods could further enhance visual grounding and cross-modal alignment. Given that the primary focus of this work is to highlight the modality imbalance and its impact on hallucinations -- and to show that refining textual embeddings with visual information mitigates this issue -- we leave exploration of advanced fusion strategies for future work.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LVLMs frequently hallucinate (object, action, attribute, relation, temporal) because generations lean on language priors rather than visual evidence<br>‚Ä¢ Standard fusion by simply appending visual embeddings to LLM token streams yields modality imbalance, with self-attention disproportionately favoring textual tokens<br>‚Ä¢ Pre-trained LLM backbones are inherently language-biased, causing under-utilization of visual cues during fine-tuning on VL data<br>‚Ä¢ Heavier cross-attention fusion (e.g., Flamingo/BLIP-2) increases complexity and cost, while simple token appending maintains efficiency but exacerbates the bias<br>‚Ä¢ Reducing hallucinations is critical for safety-critical applications (e.g., healthcare, autonomous systems) where visually grounded reasoning is required</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VisAlign: integrate visual information directly into each textual token by concatenating the averaged visual embedding to every token embedding, followed by a projection layer to form vision-conditioned textual embeddings. This lightweight refinement encourages balanced attention and better visual grounding within the existing LVLM architecture.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Dynamic Layer-wise Vision-Conditioned Token Refinement: Extend VisAlign by injecting per-layer, per-token visual summaries with learnable gates to adaptively balance modalities across the transformer stack<br>‚Ä¢ Contrastive Grounding Loss for Hallucination Suppression in LVLMs: Add contrastive objectives aligning answers with matched visuals and repelling plausible-but-unsupported textual priors<br>‚Ä¢ Token-level Visual Attribution Regularization for Balanced Attention: Regularize attention maps to maintain a target proportion of attention on visual-conditioned tokens under adversarial or ambiguous prompts<br>‚Ä¢ Multi-Granularity Visual Context for Text Embedding Refinement: Fuse global, region-level, and temporal segment embeddings into textual tokens to capture fine- and coarse-grained cues<br>‚Ä¢ Uncertainty-Aware Visual Conditioning to Detect and Mitigate Hallucinations: Incorporate uncertainty estimation over visual-conditioned tokens to calibrate generations and abstain under low visual support<br>‚Ä¢ Benchmarking Temporal Hallucinations with Vision-Conditioned Token Methods: Build a dedicated dataset and protocol to evaluate how token-level visual conditioning impacts temporal hallucinations in videos</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Real-Time Reasoning Agents in Evolving Environments</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.04898" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.04898" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Agents in the real world must make not only logical but also timely judgments. This requires continuous awareness of the dynamic environment: hazards emerge, opportunities arise, and other agents act, while the agent's reasoning is still unfolding. Despite advances in language model reasoning, existing approaches fail to account for this dynamic nature. We introduce real-time reasoning as a new problem formulation for agents in evolving environments and build Real-Time Reasoning Gym to demonstrate it. We study two paradigms for deploying language models in agents: (1) reactive agents, which employ language models with bounded reasoning computation for rapid responses, and (2) planning agents, which allow extended reasoning computation for complex problems. Our experiments show that even state-of-the-art models struggle with making logical and timely judgments in either paradigm. To address this limitation, we propose AgileThinker, which simultaneously engages both reasoning paradigms. AgileThinker consistently outperforms agents engaging only one reasoning paradigm as the task difficulty and time pressure rise, effectively balancing reasoning depth and response latency. Our work establishes real-time reasoning as a critical testbed for developing practical agents and provides a foundation for research in temporally constrained AI systems, highlighting a path toward real-time capable agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLM agents are typically evaluated in static or turn-based settings, leading to untimely actions when the environment evolves during reasoning.<br>‚Ä¢ Single-paradigm agents (reactive or planning) cannot balance speed and reasoning depth; reactive agents falter under high cognitive load, while planning agents degrade under time pressure.<br>‚Ä¢ There is no reproducible, hardware-agnostic testbed to study real-time reasoning; existing budget-control techniques struggle to precisely constrain generation without quality loss.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce Real-Time Reasoning Gym, a dynamic evaluation environment that enforces time pressure via token-count per step, and propose AgileThinker, a dual-thread agent where a planning LLM streams long-horizon reasoning while a reactive LLM issues timely actions using partial planning traces within per-step budgets to balance latency and depth.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Compute Scheduling for Dual-Thread Agents: Learn policies that dynamically allocate per-step token budgets between reactive and planning threads based on time pressure and cognitive load.<br>‚Ä¢ End-to-End Cross-Thread Reasoning with Shared Memory: Develop trainable architectures enabling cross-thread attention and synchronized state updates to reduce stale-plan errors in evolving environments.<br>‚Ä¢ Real-Time Multi-Agent Coordination with Theory of Mind: Extend the Gym and design agents that infer partner intentions and communicate under time pressure for robust collaboration in dynamic tasks.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Jailbreaking in the Haystack</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.04707" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.04707" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in long-context language models (LMs) have enabled million-token inputs, expanding their capabilities across complex tasks like computer-use agents. Yet, the safety implications of these extended contexts remain unclear. To bridge this gap, we introduce NINJA (short for Needle-in-haystack jailbreak attack), a method that jailbreaks aligned LMs by appending benign, model-generated content to harmful user goals. Critical to our method is the observation that the position of harmful goals play an important role in safety. Experiments on standard safety benchmark, HarmBench, show that NINJA significantly increases attack success rates across state-of-the-art open and proprietary models, including LLaMA, Qwen, Mistral, and Gemini. Unlike prior jailbreaking methods, our approach is low-resource, transferable, and less detectable. Moreover, we show that NINJA is compute-optimal -- under a fixed compute budget, increasing context length can outperform increasing the number of trials in best-of-N jailbreak. These findings reveal that even benign long contexts -- when crafted with careful goal positioning -- introduce fundamental vulnerabilities in modern LMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Safety risks from million-token context windows are underexplored despite rapid deployment in agents and complex workflows.<br>‚Ä¢ It is unclear whether increased jailbreak susceptibility comes from long context itself or from agentic/distributional changes; prior work did not isolate this variable.<br>‚Ä¢ Existing jailbreak methods often require overtly adversarial or explicitly harmful content, making them more detectable, less transferable, and often more resource-intensive.<br>‚Ä¢ The impact of goal positioning within long contexts on safety has been largely unstudied, leaving practitioners without concrete prompt-structuring guidance.<br>‚Ä¢ There is no compute-aware framework for optimizing attack (and defense) strategies across context length versus best-of-N sampling under fixed compute budgets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>NINJA appends a long, benign, thematically relevant, model-generated ‚Äúhaystack‚Äù around a harmful ‚Äúneedle‚Äù goal‚Äîplaced at the very beginning‚Äîto exploit long-context-induced safety degradation and positional bias, yielding high, transferable, and harder-to-detect jailbreak rates. The paper also derives a compute-aware scaling law showing that, under fixed compute, extending context length can outperform increasing best-of-N trials for maximizing attack success.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Positional Safety for Long-Context Language Models: Architectures and training schemes that enforce position-invariant safety (e.g., prompt rewriting to place user goals last, positional adversarial training, and attention regularizers) to neutralize goal-position vulnerabilities.<br>‚Ä¢ Benign-Context Adversarial Training Against Needle-in-Haystack Jailbreaks: A curriculum that synthesizes benign long contexts with embedded harmful goals across positions to improve refusal robustness while preserving capabilities and minimizing false positives.<br>‚Ä¢ Compute-Aware Defense Strategies for Long-Context Threats: A defense framework that optimally allocates compute between context processing, chunked safety checks, and sampling policies, guided by scaling laws analogous to those proposed for NINJA.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">HAFixAgent: History-Aware Automated Program Repair Agent</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01047" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01047" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Automated program repair (APR) has recently shifted toward large language models and agent-based systems, yet most systems rely on local snapshot context, overlooking repository history. Prior work shows that repository history helps repair single-line bugs, since the last commit touching the buggy line is often the bug-introducing one. In this paper, we investigate whether repository history can also improve agentic APR systems at scale, especially for complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing Agent that injects blame-derived repository heuristics into its repair loop. A preliminary study of all 854 real-world bugs from Defects4J motivates our design, showing that bug-relevant history is both widely available and highly concentrated. Empirical comparison of HAFixAgent with two state-of-the-art baselines shows: (1) Effectiveness: HAFixAgent significantly improves over the agent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2) Efficiency: history does not significantly increase agent steps and keeps token costs comparable, with notably lower median costs for complex multi-file-multi-hunk bugs. (3) Practicality: combining different historical heuristics repairs more bugs, offering a clear cost-benefit trade-off. HAFixAgent offers a practical recipe for history-aware agentic APR: ground the agent in version control history, prioritize diff-based historical context, and integrate complementary heuristics when needed.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Agent-based APR systems largely rely on the current code snapshot and test feedback, overlooking rich repository history that could guide fixes.<br>‚Ä¢ Complex multi-hunk/multi-file bugs remain difficult for LLM-based APR, requiring coordinated, non-local edits that current context strategies fail to support well.<br>‚Ä¢ It is unclear whether history signals (e.g., blame) are sufficiently available and concentrated at scale to be practical for agentic workflows.<br>‚Ä¢ Existing history-aware APR methods mostly target single-line bugs or pattern mining and are not integrated into iterative, tool-using agents.<br>‚Ä¢ Integrating history risks inflating agent steps and token costs; practical heuristics and fallbacks (for blameless insertions) are needed to keep costs manageable.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>HAFixAgent is a minimal history-aware APR agent that grounds its Reason-Act loop in a single, blame-derived commit and diff/function-level historical heuristics, combined with deterministic extraction and a fallback for blameless insertions. It executes via a compact bash toolset (grep/sed/find/tests), keeping loops guarded and costs comparable while leveraging concentrated repository history.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning to Select Historical Heuristics for Agentic APR: A meta-learning approach that dynamically chooses and weights diff- vs. function-level history cues per bug to maximize repair success under token budgets.<br>‚Ä¢ Cross-Language, Real-World Evaluation of History-Aware APR: Scaling HAFixAgent to multiple programming languages and CI pipelines, assessing robustness to flaky tests, code moves, and large monorepos.<br>‚Ä¢ Causality-Aware History Mining for Program Repair: Beyond blame‚Äîcombine SZZ variants, change-impact analysis, and developer rationale (PRs/issues) to extract causal historical signals that improve multi-hunk repair precision.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24505" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24505" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Accurate confidence calibration in Large Language Models (LLMs) is critical for safe use in high-stakes domains, where clear verbalized confidence enhances user trust. Traditional methods that mimic reference confidence expressions often fail to capture the reasoning needed for accurate confidence assessment. We propose natural language critiques as a solution, ideally suited for confidence calibration, as precise gold confidence labels are hard to obtain and often require multiple generations. This paper studies how natural language critiques can enhance verbalized confidence, addressing: (1) What to critique: uncertainty (question-focused) or confidence (answer-specific)? Analysis shows confidence suits multiple-choice tasks, while uncertainty excels in open-ended scenarios. (2) How to critique: self-critique or critique calibration training? We propose Self-Critique, enabling LLMs to critique and optimize their confidence beyond mere accuracy, and CritiCal, a novel Critique Calibration training method that leverages natural language critiques to improve confidence calibration, moving beyond direct numerical optimization. Experiments show that CritiCal significantly outperforms Self-Critique and other competitive baselines, even surpassing its teacher model, GPT-4o, in complex reasoning tasks. CritiCal also shows robust generalization in out-of-distribution settings, advancing LLM's reliability.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Ensuring accurate verbalized confidence is critical for safe, trustworthy LLM use, yet current models are often miscalibrated.<br>‚Ä¢ Precise gold confidence labels are hard and costly to obtain (often requiring multiple generations), making direct numerical supervision brittle and impractical.<br>‚Ä¢ Existing approaches either mimic reference confidence expressions without modeling the underlying reasoning or rely on computationally expensive multi-sample consistency; most self-improvement focuses on accuracy rather than calibration.<br>‚Ä¢ Prior work conflates uncertainty and confidence and overlooks their task-dependent suitability; there is no systematic exploration of using natural-language critiques to calibrate confidence.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces Self-Critique (a prompting method where the model analyzes the question, its reasoning, answer, and stated confidence to refine calibration) and CritiCal (a critique-calibration training framework that uses GPT-4o natural-language critiques comparing the model‚Äôs reasoning to references, trained via SFT or DPO). These approaches align verbalized confidence with reasoning quality rather than optimizing numeric scores directly, yielding better in- and out-of-distribution calibration.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Task-Aware Critique Routing for Calibration Across Question Types: Learn a meta-controller that selects uncertainty- vs confidence-focused critiques based on task and input characteristics to maximize calibration.<br>‚Ä¢ Teacher-Free CritiCal: Bootstrapping Natural-Language Critiques Without Proprietary Models: Replace GPT-4o with a lightweight critic trained via self-play and distillation to generate high-quality critiques at scale.<br>‚Ä¢ CritiCal-M: Extending Critique-Based Calibration to Multimodal and Tool-Use LLMs: Adapt critique generation and training to calibrate verbalized confidence in vision-language and tool-augmented reasoning scenarios.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 4</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-10 23:03:28 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 4;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>