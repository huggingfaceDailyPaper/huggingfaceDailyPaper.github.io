<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 17, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 17, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">DoPE: Denoising Rotary Position Embedding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.09146" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.09146" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Rotary Position Embedding (RoPE) in Transformer models has inherent limits that weaken length extrapolation. We reinterpret the attention map with positional encoding as a noisy feature map, and propose Denoising Positional Encoding (DoPE), a training-free method based on truncated matrix entropy to detect outlier frequency bands in the feature map. Leveraging the noise characteristics of the feature map, we further reparameterize it with a parameter-free Gaussian distribution to achieve robust extrapolation. Our method theoretically reveals the underlying cause of the attention sink phenomenon and its connection to truncated matrix entropy. Experiments on needle-in-a-haystack and many-shot in-context learning tasks demonstrate that DoPE significantly improves retrieval accuracy and reasoning stability across extended contexts (up to 64K tokens). The results show that the denoising strategy for positional embeddings effectively mitigates attention sinks and restores balanced attention patterns, providing a simple yet powerful solution for improving length generalization. Our project page is Project: https://The-physical-picture-of-LLMs.github.io</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ RoPE-driven long-context degradation: low-frequency alignment in RoPE induces outlier channels and bright-band attention sinks, harming retrieval and reasoning stability when extrapolating far beyond training lengths (e.g., 64K tokens).<br>â€¢ Brittleness under noise: attention sink tokens (e.g., BOS) adjacent to targets severely disrupt retrieval accuracy, exposing the need for robust positional encoding under noisy or adversarial inputs.<br>â€¢ Limitations of existing fixes: NTK spectrum tweaks, learnable PE (FIRE), or data-dependent PE (DAPE) either require training, add parameters, or remain unstable at extreme lengths; NoPE removes positional cues entirely and can underperform.<br>â€¢ Lack of principled selection: prior methods do not provide a training-free, theoretically grounded criterion to identify and suppress the specific frequency bands/heads causing sinks and structural sparsity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DOPE is a training-free denoising scheme that uses truncated matrix entropy to detect low-rank, spiky RoPE frequency bands and heads, then masks or replaces their positional componentsâ€”optionally with variance-matched Gaussian noiseâ€”to restore isotropic attention and improve length extrapolation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Entropy-Guided Positional Encoding: A Learnable DOPE for Online Long-Context Adaptation: Introduce a lightweight, learnable controller that adjusts entropy thresholds per layer/head during inference or fine-tuning to adapt masking to content and length.<br>â€¢ Provable Links Between Attention Sink and Truncated Matrix Entropy: Tight Bounds and Generalization Beyond RoPE: Formalize the cone-condition and spectral bounds connecting sinks to entropy, extend proofs to ALiBi/Kerple/NoPE, and derive guarantees for extreme-length stability.<br>â€¢ DOPE Across Modalities: Denoising Positional Encoding for Vision-Language and Long-Video Transformers: Apply entropy-based denoising to multimodal RoPE variants, evaluating long-document and long-video retrieval/reasoning under noise and distribution shift.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11434" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11434" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in unified multimodal models (UMMs) have enabled impressive progress in visual comprehension and generation. However, existing datasets and benchmarks focus primarily on single-turn interactions, failing to capture the multi-turn, context-dependent nature of real-world image creation and editing. To address this gap, we present WEAVE, the first suite for in-context interleaved cross-modality comprehension and generation. Our suite consists of two complementary parts. WEAVE-100k is a large-scale dataset of 100K interleaved samples spanning over 370K dialogue turns and 500K images, covering comprehension, editing, and generation tasks that require reasoning over historical context. WEAVEBench is a human-annotated benchmark with 100 tasks based on 480 images, featuring a hybrid VLM judger evaluation framework based on both the reference image and the combination of the original image with editing instructions that assesses models' abilities in multi-turn generation, visual memory, and world-knowledge reasoning across diverse domains. Experiments demonstrate that training on WEAVE-100k enables vision comprehension, image editing, and comprehension-generation collaboration capabilities. Furthermore, it facilitates UMMs to develop emergent visual-memory capabilities, while extensive evaluations on WEAVEBench expose the persistent limitations and challenges of current approaches in multi-turn, context-aware image generation and editing. We believe WEAVE provides a view and foundation for studying in-context interleaved comprehension and generation for multi-modal community.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Real-world image creation and editing are multi-turn and context-dependent, but most datasets and benchmarks only support single-turn interactions.<br>â€¢ There is a lack of high-quality interleaved, multi-turn datasets that capture temporal dependencies and iterative workflows, limiting open-source UMMs' ability to develop visual memory and context coherence.<br>â€¢ No standardized, human-annotated benchmark or evaluation framework exists to assess multi-turn, interleaved comprehensionâ€“generation, including visual memory and world-knowledge reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces WEAVE, comprising WEAVE-100k (a 100K-sample interleaved dataset across comprehension, editing, and generation with 370K turns and 500K images) and WEAVEBench (a 100-task benchmark with a hybrid VLM judge that scores alignment to reference images and fidelity to original images plus editing instructions to evaluate multi-turn generation, visual memory, and world-knowledge reasoning). Training UMMs on WEAVE-100k elicits emergent visual memory and improves performance on existing benchmarks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Memory-Augmented Unified Multimodal Models for Long-Horizon Editing: Integrate retrieval and external memory modules into UMMs to sustain visual consistency and mitigate performance degradation as context length grows.<br>â€¢ WEAVEBench++: A Human-Calibrated, Counterfactual Benchmark for Multi-Turn Visual Editing and Reasoning: Extend the evaluation suite with counterfactual references, adversarial edits, and calibrated human scoring to better stress-test context-aware generation and reasoning.<br>â€¢ Curriculum and Compression: Training Strategies for Scalable Interleaved Multimodal Generation: Develop curriculum learning, context compression, and replay-based training pipelines to efficiently learn from long interleaved sequences while preserving visual memory.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11134" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11134" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing multimodal benchmarks largely assess understanding (discrimination) or generation in isolation, failing to evaluate integrated generative reasoning that couples comprehension, planning, and creation.<br>â€¢ Current tests rarely require constructing formally correct artifacts (e.g., diagrams) under constraints; they favor answer selection or unconstrained generation, missing constructive competence.<br>â€¢ Lack of verifiable alignment across modalitiesâ€”most datasets do not provide executable codeâ€”prevents precise process-level and outcome-level verification.<br>â€¢ Geometric problem solving inherently demands multi-step spatial reasoning and precise visual generation; state-of-the-art UMMs struggle with this constructive aspect, revealing a critical evaluation gap.<br>â€¢ There is a need for a tri-modal, stepwise benchmark that diagnoses end-to-end abilities: parse language, plan formally, generate code, and render diagrams that satisfy constraints.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces GGBench, a tri-modal (textâ€“codeâ€“image) benchmark for geometric generative reasoning where each problem couples stepwise natural-language plans with executable GeoGebra code and rendered diagrams for deterministic, code-based verification. The dataset is built via an LLM-assisted pipeline (collection, tagging/filtering, construction-oriented adaptation, synchronized stepwise text + GGB code generation, rendering) followed by automated checks (executability, logical consistency, diagram correctness) and expert review, resulting in 1,411 high-quality multi-step construction tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Program-Supervised Training of Unified Multimodal Models for Geometric Construction: Train UMMs to generate GeoGebra code from text/images using executability- and geometry-aware rewards for end-to-end constructive reasoning.<br>â€¢ Process-Level Judges and Self-Refinement for Multimodal Generative Reasoning: Develop automatic tri-modal judges that enforce textâ€“codeâ€“image consistency and drive iterative planningâ€“execution correction loops.<br>â€¢ Beyond 2D Geometry: A Verifiable Multi-Domain Generative Reasoning Suite: Extend the tri-modal, code-verified paradigm to 3D geometry, charts, CAD/sketches, and circuit diagrams with domain-specific executable validators.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.08195" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.08195" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>User interface (UI) programming is a core yet highly complex part of modern software development. Recent advances in visual language models (VLMs) highlight the potential of automatic UI coding, but current approaches face two key limitations: multimodal coding capabilities remain underdeveloped, and single-turn paradigms make little use of iterative visual feedback. We address these challenges with an interactive UI-to-code paradigm that better reflects real-world workflows and raises the upper bound of achievable performance. Under this paradigm, we present UI2Code^N, a visual language model trained through staged pretraining, fine-tuning, and reinforcement learning to achieve foundational improvements in multimodal coding. The model unifies three key capabilities: UI-to-code generation, UI editing, and UI polishing. We further explore test-time scaling for interactive generation, enabling systematic use of multi-turn feedback. Experiments on UI-to-code and UI polishing benchmarks show that UI2Code^N establishes a new state of the art among open-source models and achieves performance comparable to leading closed-source models such as Claude-4-Sonnet and GPT-5. Our code and models are available at https://github.com/zai-org/UI2Code_N.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Multimodal coding in VLMs is underdeveloped, making it hard to translate complex UI layouts, nested components, and subtle visual details into long, executable code.<br>â€¢ Single-turn generation paradigms underutilize iterative visual feedback, diverging from real-world UI development workflows and capping achievable quality.<br>â€¢ Existing open-source and proprietary VLMs underperform on UI-to-code and UI polishing benchmarks, revealing the need for interactive and test-time scalable approaches.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>UI2CodeN is an interactive visual language model trained via staged pretraining, supervised fine-tuning, and reinforcement learning that unifies UI-to-code generation, UI editing, and UI polishing. It employs test-time scaling with multi-turn visual feedback to iteratively refine code and improve multimodal coding performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Test-Time Scaling for Interactive UI-to-Code Generation: Design uncertainty-aware, budget-sensitive strategies to allocate iterations and select visual feedback for maximal quality gains.<br>â€¢ Cross-Platform UI2Code: Generalizing Interactive VLMs to Mobile, Web, and Desktop Frameworks: Extend the paradigm across toolkits and enforce platform-specific constraints with runtime validation.<br>â€¢ Human-in-the-Loop Reinforcement Learning for UI Assistants: Integrate designer/user feedback and visual fidelity metrics into RL fine-tuning to improve reliability, usability, and safety in practical settings.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11257" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11257" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Scarce labeled ionic liquid (IL) data and the need to leverage unlabeled, multimodal information to improve property prediction accuracy and out-of-distribution (OOD) generalization.<br>â€¢ Existing methods (MD/quantum chemistry, simple linear models, and unimodal deep learning) are either computationally expensive, domain-limited, or fail to capture molecular multimodality, resulting in poor predictive performance.<br>â€¢ Fragmented, manual workflows and the vast combinatorial cationâ€“anion space make IL screening and design infeasible without automation; generative models often produce unrealistic candidates due to data scarcity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>AIonopedia is an LLM-driven ReAct agent that orchestrates a multimodal IL foundation modelâ€”aligning SMILES, molecular graphs, and descriptors via self-supervised modality alignment followed by fine-tuning on a curated IL datasetâ€”to predict soluteâ€“solvent interaction and bulk properties. It integrates RAG-based literature/web search, chemical structure retrieval and canonicalization, and a hierarchical similarity-guided beam search to perform IL modification and scalable screening, validated with wet-lab experiments.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ AutoIL: Closed-loop Robotic Discovery with an LLM-Agent for Ionic Liquids: Integrate AIonopedia with high-throughput microfluidics and Bayesian optimization for autonomous designâ€“makeâ€“testâ€“learn cycles.<br>â€¢ Multimodal IL Foundation Models with 3D Quantum Representations and Uncertainty-Aware Multitask Learning: Extend the predictor with 3D geometries, quantum descriptors, and calibrated uncertainty to enable active learning and safer OOD deployment.<br>â€¢ GreenIL: Multi-objective Optimization of Ionic Liquids for Performance and Environmental Safety: Couple property prediction with toxicity/biodegradability and life-cycle metrics to discover greener, high-performing ILs under regulatory constraints.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Virtual Width Networks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11238" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11238" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Virtual Width Networks (VWN), a framework that delivers the benefits of wider representations without incurring the quadratic cost of increasing the hidden size. VWN decouples representational width from backbone width, expanding the embedding space while keeping backbone compute nearly constant. In our large-scale experiment, an 8-times expansion accelerates optimization by over 2 times for next-token and 3 times for next-2-token prediction. The advantage amplifies over training as both the loss gap grows and the convergence-speedup ratio increases, showing that VWN is not only token-efficient but also increasingly effective with scale. Moreover, we identify an approximately log-linear scaling relation between virtual width and loss reduction, offering an initial empirical basis and motivation for exploring virtual-width scaling as a new dimension of large-model efficiency.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Naively widening Transformer hidden size improves expressivity but incurs quadratic growth in parameters and compute, making it impractical at scale.<br>â€¢ Conventional MoE expands only FFN inner dimensions while keeping the backbone hidden size fixed, leaving a width bottleneck and a performance gap vs. truly wider models.<br>â€¢ Prior Hyper-/Frac-Connections underutilize expanded dimensions or lack flexible, efficient coupling between expanded representations and a fixed-width backbone; no unified, general formulation.<br>â€¢ There is a need for higher token-efficiency and faster convergence without increasing backbone compute, and for a new, predictable scaling axis (virtual width) beyond parameters/data.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Virtual Width Networks (VWN) decouple representational width from backbone width by using Over-Width Embeddings and Generalized Hyper-Connections (GHC) that compress over-width states to backbone width for attention/FFN and re-expand them with lightweight matrices, enabling rÃ— virtual widening at near-constant compute. Paired with multi-token prediction, VWN better exercises the expanded space, accelerating optimization and improving accuracy while exhibiting an approximately log-linear relation between virtual width and loss.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Toward a Theory of Virtual-Width Scaling Laws: Formalize and validate the observed log-linear relation between virtual width and loss across model sizes, data regimes, and objectives.<br>â€¢ Adaptive Virtual Width Scheduling for Efficient Pretraining: Learn or schedule layer-/token-wise virtual width (and GHC routing) during training to optimize computeâ€“performance trade-offs.<br>â€¢ Co-Design of Tokenization and Virtual Width: Jointly optimize over-tokenization, n-gram embeddings, and MTP with VWN to maximize utilization of expanded embedding spaces and downstream generalization.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">LiteAttention: A Temporal Sparse Attention for Diffusion Transformers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11062" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11062" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion Transformers, particularly for video generation, achieve remarkable quality but suffer from quadratic attention complexity, leading to prohibitive latency. Existing acceleration methods face a fundamental trade-off: dynamically estimating sparse attention patterns at each denoising step incurs high computational overhead and estimation errors, while static sparsity patterns remain fixed and often suboptimal throughout denoising. We identify a key structural property of diffusion attention, namely, its sparsity patterns exhibit strong temporal coherence across denoising steps. Tiles deemed non-essential at step t typically remain so at step t+Î´. Leveraging this observation, we introduce LiteAttention, a method that exploits temporal coherence to enable evolutionary computation skips across the denoising sequence. By marking non-essential tiles early and propagating skip decisions forward, LiteAttention eliminates redundant attention computations without repeated profiling overheads, combining the adaptivity of dynamic methods with the efficiency of static ones. We implement a highly optimized LiteAttention kernel on top of FlashAttention and demonstrate substantial speedups on production video diffusion models, with no degradation in quality. The code and implementation details will be publicly released.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Quadratic attention complexity dominates inference latency in video Diffusion Transformers, making generation prohibitively slow.<br>â€¢ Dynamic per-step sparse attention requires repeated profiling and incurs estimation noise, while skipping only partial computations leaves major bottlenecks (e.g., softmax, memory transfers) intact.<br>â€¢ Static sparsity patterns are fixed and often suboptimal as attention evolves across denoising steps, leading to misalignment and quality risks.<br>â€¢ Existing cross-sequence caching exploits feature redundancy, not attention sparsity, and introduces memory overhead and approximation errors.<br>â€¢ Prior work does not exploit the temporal coherence of attention sparsity across denoising steps, missing an opportunity for full iteration elimination.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>LiteAttention leverages temporal coherence of attention sparsity by identifying skippable tiles early and propagating a persistent skip mask across timesteps to fully bypass QK, softmax, and PV for marked tiles, integrating efficiently atop FlashAttention3 with lightweight per-timestep calibration to control accumulated error.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Theoretical Analysis of Temporal Sparsity Coherence in Diffusion Attention: Formalize conditions under which attention sparsity persists across timesteps and derive complexity/accuracy bounds tied to the noise schedule and transformer dynamics.<br>â€¢ Learning Early Predictors for Evolutionary Skip Masks in DiTs: Train lightweight predictors to infer skip masks from early-step statistics, combining supervised/self-supervised objectives with per-layer/timestep calibration to maximize speed without quality loss.<br>â€¢ Unified Feature Caching and Evolutionary Attention Skipping for DiT Inference: Integrate token/feature caching with tile-level skip propagation across self- and cross-attention, designing joint routing policies that optimize memory, latency, and fidelity.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Simulating the Visual World with Artificial Intelligence: A Roadmap</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.08585" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.08585" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a "window" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of a unified, precise definition of "world model" and a coherent taxonomy, making progress hard to measure and compare<br>â€¢ Video generation models focus on visual fidelity but lack intrinsic physical plausibility, real-time interactivity, and long-horizon planning<br>â€¢ Existing approaches (3D/4D and video generation) excel only in partial capabilities, resulting in superficial faithfulness and limited controllability<br>â€¢ Missing structural components: formal navigation modes, controllable video-to-world pipelines, and comprehensive evaluation metrics<br>â€¢ Rapid advances in video generation outpace conceptual frameworks, creating an urgent need for a roadmap that links visual generation to world modeling</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes a conceptual roadmap and taxonomy that decomposes video foundation models into an implicit world model (latent simulation engine) and a video renderer, formally defining world modeling as next-scene prediction conditioned by a triplet (Current Scene, Navigation Mode, Prior Information). It organizes progress into four generationsâ€”Faithfulness, Interactiveness, Planning, and Stochasticityâ€”and formalizes navigation modes and evaluation perspectives to guide research.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Intrinsic Physics-Constrained Video World Models: Integrate differentiable physics priors and constraint learning into latent world models to ensure physically intrinsic faithfulness across diverse environments.<br>â€¢ Agent-in-the-Loop Benchmarks for Interactiveness and Planning: Develop standardized evaluation suites with embodied agents to quantify real-time multimodal interaction and multi-scale planning in video-based world models.<br>â€¢ Stochasticity-Aware Long-Horizon Simulation for Rare Events: Build generative world models that capture outlier and low-probability events with calibrated uncertainty, enabling safer training and testing for robotics and autonomous driving.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07403" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07403" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ MLLMs still struggle with true 3D spatial understanding (localization, objectâ€“relation reasoning), which is critical for robotics, navigation, and AR.<br>â€¢ Existing spatial methods are data-hungry or impractical: they rely on massive VQA corpora, explicit 3D inputs (depth/point clouds), or architecture-specific tokens and modules.<br>â€¢ RL with sparse, end-accuracy rewards provides weak guidance for visually grounded multi-step reasoning and fails to teach where to look and how to localize.<br>â€¢ Scene graphs are typically used as external pre-processing rather than integrated into end-to-end reasoning, limiting grounding and generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SpatialThinker builds question-focused scene subgraphs and trains an MLLM with online RL using a lexicographically gated dense reward (format adherence, regional count control, answer accuracy, CIoU-based localization) to enforce observeâ€“localizeâ€“thinkâ€“answer reasoning. A compact, synthesized STVQA-7K dataset supplies high-quality spatial VQA supervision, enabling strong gains over SFT and sparse-RL with only 7K samples.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Spatio-TemporalThinker: Dense-Reward RL for Video-Based Spatial Reasoning: Extend scene-graph grounding and spatial rewards to temporal relations and motion in videos.<br>â€¢ Embodied SpatialThinker: Interactive RL with Spatial Rewards for Real-World Robot Manipulation: Close the loop by coupling spatial reasoning with action and feedback in embodied environments.<br>â€¢ Uncertainty-Aware Spatial Rewards: Probabilistic Scene-Graph Grounding for Robust 3D Reasoning: Model detection/relation uncertainty and calibrate rewards to improve robustness and OOD generalization.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">HI-TransPA: Hearing Impairments Translation Personal Assistant</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.09915" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.09915" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>To provide a unified and flexible solution for daily communication among hearing-impaired individuals, we introduce the Omni-Model paradigm into assistive technology and present HI-TransPA, an instruction-driven audio-visual personal assistant. The model fuses indistinct speech with high-frame-rate lip dynamics, enabling both translation and dialogue within a single multimodal framework. To tackle the challenges of noisy and heterogeneous raw data and the limited adaptability of existing Omni-Models to hearing-impaired speech, we construct a comprehensive preprocessing and curation pipeline that detects facial landmarks, isolates and stabilizes the lip region, and quantitatively assesses multimodal sample quality. These quality scores guide a curriculum learning strategy that first trains on clean, high-confidence samples and progressively incorporates harder cases to strengthen model robustness. We further adopt a SigLIP encoder combined with a Unified 3D-Resampler to efficiently encode high-frame-rate lip motion. Experiments on our purpose-built HI-Dialogue dataset show that HI-TransPA achieves state-of-the-art performance in both literal accuracy and semantic fidelity. This work establishes a foundation for applying Omni-Models to assistive communication technology, providing an end-to-end modeling framework and essential processing tools for future research.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Standard ASR struggles with atypical, indistinct pronunciation patterns common in hearing-impaired speech, leading to poor recognition and communication barriers.<br>â€¢ Existing assistive tools are largely one-way (hearing-to-text) and do not support hearing-impaired usersâ€™ expression via robust translation plus conversational assistance.<br>â€¢ Generic multimodal/Omni-models are not adapted to high-frame-rate lip dynamics, limiting their ability to disambiguate impaired speech.<br>â€¢ Real-world audio-visual data for hearing-impaired users is noisy and heterogeneous; current pipelines lack quality-aware curation and training strategies.<br>â€¢ Multimodal datasets pairing impaired speech with aligned lip motion are scarce, constraining model development and evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>HI-TransPA is an instruction-driven audio-visual assistant that fuses impaired speech with high-frame-rate lip dynamics using a SigLIP vision encoder plus a unified 3D-Resampler atop a Qwen2.5-Omni-3B backbone, enabling both translation and dialogue. A quality-aware pipeline (landmark-based lip stabilization, audio/video scoring with ASR confidence, SNR, and motion) drives rejection sampling and a two-stage curriculum to train from easy to hard samples, complemented by a three-stage alignment (image/video resampler tuning, audio-visual co-adaptation, and mixed translate/chat instruction fine-tuning).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Personalizing Omni-Assistants for Hearing-Impaired Speech: Speaker-Adaptive and Impairment-Aware Modeling: Develop personalization and adaptation layers to capture individual articulation patterns and improve robustness.<br>â€¢ Streaming HI-TransPA: Low-Latency Audio-Visual Translation with Online Lip Dynamics: Architect for real-time inference with token-efficient visual resampling and synchronized audio-visual streaming.<br>â€¢ Privacy-Preserving On-Device HI Assistants via Federated and Distillation Learning: Train and deploy on edge devices with federated learning and model compression to safeguard user privacy.<br>â€¢ Multilingual and Cross-Lingual HI Translation with Unified Audio-Lip Modeling: Extend to multiple languages, leveraging cross-lingual transfer and lip-informed disambiguation.<br>â€¢ Beyond CER: Human-Centered Evaluation of Empathy, Helpfulness, and Safety in HI Assistants: Create standardized metrics and user studies for conversational quality, inclusivity, and safety.<br>â€¢ Robustness to Occlusions and Domain Shifts in Visual Speech: Data Augmentation and Self-Supervised Pretraining: Use occlusion-aware augmentations and large-scale self-supervised audio-visual pretraining.<br>â€¢ Unified Expression Pipeline: Joint Lip-to-Speech Synthesis and Translation for Hearing-Impaired Users: Integrate lip-to-speech generation with translation to enhance clarity and downstream understanding.<br>â€¢ Active and Curriculum Data Curation for HI Datasets: Uncertainty- and Quality-Driven Sampling at Scale: Automate data selection with uncertainty/quality estimators to efficiently expand datasets.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11373" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11373" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Single-model output length limits and quadratic Transformer scaling cap reasoning depth and make RLVR-based test-time scaling costly.<br>â€¢ Verifierâ€“Corrector (Vâ€“C) multi-agent pipelines fail to transfer to open-source LLMs due to weak critiquing and correction abilities.<br>â€¢ Trajectory-level rewards in multi-agent RL cause severe credit assignment noise (e.g., incorrect Verifier judgments getting positive reward).<br>â€¢ Ultra-long multi-agent rollouts (nÃ—L tokens) create training inefficiency and long-tail latency that current methods do not address.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MarsRL jointly optimizes Solver, Verifier, and Corrector with agent-specific verifiable rewards to decouple credit assignment, and uses agentic pipeline parallelism with grouped rollouts and segment decoding (from UloRL) plus adaptive sampling to efficiently train ultra-long trajectories.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Causal Credit Assignment for Multi-Agent LLMs: Beyond Verifiable Rewards: Develop causal inference-based reward shaping to properly attribute credit across agents and time, enabling training when reference answers are unavailable.<br>â€¢ MarsRL++: Distributed Agentic Pipeline Parallelism for Ultra-Long Reasoning at Scale: Build a distributed, dynamically scheduled agent-level pipeline with cross-agent load balancing to further reduce rollout latency and compute costs.<br>â€¢ Universal Verifierâ€“Corrector Pretraining for Cross-Domain Reasoning Systems: Pretrain modular Verifier/Corrector agents across math, code, and science tasks with meta-adaptive sampling to generalize across diverse Solvers.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.10984" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.10984" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Evaluation of discourse-level and expert-domain translation is inadequate; current methods emphasize segment-level accuracy/fluency and neglect long-form coherence and terminological precision.<br>â€¢ Existing benchmarks (e.g., WMT, FLORES, Redtrans) focus on short segments and cannot assess sustained coherence, domain-intensive terminology handling, or expert stylistic standards required in real professional scenarios.<br>â€¢ Conventional reference-based metrics and single-judge LLM assessments are unreliable for long-form texts; a robust, reference-free, explainable metric aligned with human judgments is needed to fairly evaluate LLMs on professional-grade translation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces DiscoX, a professionally curated long-form Chineseâ€“English benchmark (200 texts across seven expert domains) and Metric-S, a reference-free, multi-agent LLM evaluation workflow that scores accuracy, fluency, and appropriateness via pre-checking, error attribution/deduplication, and severity weighting, achieving strong alignment with human judgments.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Extending DiscoX to Multilingual Expert Domains: Build and evaluate discourse-level expert translation benchmarks across diverse language pairs and domains to test generalization and robustness.<br>â€¢ Training MT Systems with Discourse-Aware Objectives and Metric-S Feedback: Develop LLM/MT models optimized for coherence and terminology consistency using structured objectives and reinforcement learning driven by Metric-S signals.<br>â€¢ Reliability and Explainability in Reference-Free MT Evaluation: Calibrating Multi-Agent Judges: Systematically study judge calibration, standardize error taxonomies, and integrate human-in-the-loop processes to improve Metric-S reliability and interpretability.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">Experience-Guided Adaptation of Inference-Time Reasoning Strategies</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11519" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11519" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Inference-time strategies in agentic AI are largely static, failing to learn from past successes/failures and repeatedly incurring unnecessary compute or making the same mistakes.<br>â€¢ Memory-based steering methods (e.g., Dynamic Cheatsheet, Buffer of Thoughts) only modify text prompts and cannot adapt sampling parameters, tool sets, control logic, or switch paradigms; they also cannot cache and reuse full computational procedures.<br>â€¢ Offline strategy optimization approaches require costly training, remain fixed after deployment, and cannot continually incorporate new experience.<br>â€¢ Lack of a unified representation to adapt all strategy components with explicit state hinders compositional cost tracking, execution tracing, and continual learning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>EGUR uses an LLM-based meta-strategy that, at inference time, proposes multiple complete, executable strategies (prompts, sampling parameters, tool configurations, and control logic) conditioned on the current problem and a structured memory via a Guide. A Consolidator integrates execution feedback to refine future generations; strategies are formalized as compositions of stateful processes, enabling comparison, caching, and continual adaptation for improved accuracy and efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Towards Theory and Guarantees for Experience-Guided Inference-Time Strategy Adaptation: Develop regret bounds and computeâ€“accuracy trade-off analyses for meta-strategies that generate stateful processes under feedback.<br>â€¢ Safety- and Cost-Aware EGUR: Constrained meta-strategy optimization that incorporates risk, latency, and tool-use budgets while adapting strategies online.<br>â€¢ Multimodal EGUR for Tool-Augmented Agents: Extend experience-guided strategy generation to vision, speech, and robotics with heterogeneous tools and real-time control constraints.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11002" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11002" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Emotion plays a pivotal role in video-based expression, but existing video generation systems predominantly focus on low-level visual metrics while neglecting affective dimensions. Although emotion analysis has made progress in the visual domain, the video community lacks dedicated resources to bridge emotion understanding with generative tasks, particularly for stylized and non-realistic contexts. To address this gap, we introduce EmoVid, the first multimodal, emotion-annotated video dataset specifically designed for creative media, which includes cartoon animations, movie clips, and animated stickers. Each video is annotated with emotion labels, visual attributes (brightness, colorfulness, hue), and text captions. Through systematic analysis, we uncover spatial and temporal patterns linking visual features to emotional perceptions across diverse video forms. Building on these insights, we develop an emotion-conditioned video generation technique by fine-tuning the Wan2.1 model. The results show a significant improvement in both quantitative metrics and the visual quality of generated videos for text-to-video and image-to-video tasks. EmoVid establishes a new benchmark for affective video computing. Our work not only offers valuable insights into visual emotion analysis in artistically styled videos, but also provides practical methods for enhancing emotional expression in video generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Video generation models prioritize visual coherence and realism while neglecting emotional expressiveness, leading to emotionally flat outputs.<br>â€¢ There is no dedicated, multimodal, emotion-annotated video dataset for stylized/non-realistic media (animations, stickers, cinematic clips), creating a gap between emotion understanding and generation.<br>â€¢ Existing work focuses on realistic, dialog-centric videos; creative domains with rich affect (animation, memes, film editing) are underexplored.<br>â€¢ The community lacks benchmarks and protocols to measure emotion accuracy in text-to-video (T2V) and image-to-video (I2V) tasks and to integrate emotion conditioning into generation pipelines.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The authors introduce EmoVid, a multimodal dataset of cartoons, movie clips, and animated stickers annotated with Mikelsâ€™ eight emotions, color attributes (brightness, colorfulness, hue), and VLM-generated captions, then analyze spatiotemporal emotionâ€“visual correlations and establish a benchmark for T2V/I2V emotion evaluation. They fine-tune Wan2.1 with explicit emotion conditioning, yielding improved emotional expressiveness and visual quality in generated videos.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Emotion-Conditioned Video Generation via Multimodal Control: Extend conditioning beyond text to include audio prosody, sentiment cues, and color palettes for fine-grained affective control.<br>â€¢ Affective Metrics for Stylized Video Synthesis: Design standardized automatic and human-in-the-loop metrics to quantify emotion accuracy and intensity in creative video domains.<br>â€¢ Learning Colorâ€“Emotion Priors for Controllable Video Editing: Build models that map hue/brightness/colorfulness to target emotions for affect-aware color grading and style transfer.<br>â€¢ Temporal Emotion Trajectory Modeling for Narrative Videos: Generate and edit videos with controllable multi-phase emotion arcs using sequence models and transition priors.<br>â€¢ Domain-Generalization of Emotion Recognition in Artistic Media: Develop robust emotion recognition that generalizes across animation, stickers, and film via cross-domain adaptation.<br>â€¢ Personalized and Culture-Aware Emotion Video Generation: Adapt emotion conditioning to user and cultural preferences, mitigating bias in stylized affective content.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RF-DETR: Neural Architecture Search for Real-Time Detection Transformers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.09554" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.09554" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the "tunable knobs" for NAS to improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves on prior state-of-the-art real-time methods on COCO and Roboflow100-VL. RF-DETR (nano) achieves 48.0 AP on COCO, beating D-FINE (nano) by 5.3 AP at similar latency, and RF-DETR (2x-large) outperforms GroundingDINO (tiny) by 1.2 AP on Roboflow100-VL while running 20x as fast. To the best of our knowledge, RF-DETR (2x-large) is the first real-time detector to surpass 60 AP on COCO. Our code is at https://github.com/roboflow/rf-detr</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Open-vocabulary detectors struggle to generalize to out-of-distribution classes and modalities; fine-tuning heavy VLMs improves in-domain performance but hurts runtime and open-vocab generalization.<br>â€¢ Specialist real-time detectors are implicitly over-optimized for COCO (architectures, schedulers, augmentations) and transfer poorly to diverse real-world datasets with different distributions.<br>â€¢ Existing NAS for detection often targets backbones or requires retraining per hardware; there is a lack of end-to-end, weight-sharing NAS that yields accuracy-latency Pareto curves without retraining.<br>â€¢ Latency reporting is irreproducible across papers due to GPU power throttling and heterogeneous protocols, hindering fair comparison.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>RF-DETR trains a DINOv2-based DETR supernet with weight-sharing over input, architectural, and inference knobs (e.g., image resolution, patch size, windowed/non-windowed attention, decoder depth, query tokens), then selects Pareto-optimal subnets via validation grid search without retraining for real-time deployment. It is scheduler-free, extends to instance segmentation with a lightweight mask head, and standardizes latency evaluation by buffering inference to mitigate GPU power throttling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ RF-DETR-OV: Distilling Visionâ€“Language Priors into Real-Time DETRs: Integrate lightweight text encoders or distill open-vocabulary knowledge into RF-DETR to retain zero-shot generalization while preserving real-time speed.<br>â€¢ Dynamic Budgeted RF-DETR: Adaptive Inference-Time Architecture Selection: Learn policies that choose resolution, decoder layers, and query counts per image/device to meet changing latency/energy budgets with minimal accuracy loss.<br>â€¢ DETR-Bench: A Standardized, Reproducible Latency Suite for Detection Transformers: Establish cross-hardware benchmarking protocols and tooling (e.g., buffering, warm-up, batching) for fair, reproducible accuracyâ€“latency comparisons.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.10492" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.10492" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner. Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Industrial recommenders optimize mainly for accuracy/engagement, neglecting beyond-accuracy objectives (diversity, novelty, personalization), leading to misalignment (e.g., popularity bias, polarization) and poor discovery.<br>â€¢ Valuable structured domain knowledge (human priors such as taxonomies, temporal patterns, interaction types) is applied post-hoc in ranking, remaining decoupled from core representation learningâ€”especially problematic for end-to-end generative models.<br>â€¢ Current generative recommenders learn user intent in an unsupervised manner, ignoring human priors and requiring costly post-hoc heuristics that make the system a black box and hard to control/interpret.<br>â€¢ Alternatives (multi-interest, disentanglement methods) often need architecture-specific modifications, have limited industrial applicability, and still discard or underuse existing human priors.<br>â€¢ Models struggle to effectively leverage longer contexts and larger model sizes without guided structure, limiting scalability and alignment with real-world objectives.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A backbone-agnostic multi-head decoding framework uses lightweight, prior-conditioned adapter heads to inject structured human priors (semantic, behavioral, temporal, graph) directly into end-to-end training, guiding the model to disentangle user intent along human-understandable axes. A hierarchical composition strategy models interactions among different priors, enabling controllable, interpretable generation that improves both accuracy and beyond-accuracy metrics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Prior Weighting for Generative Recommenders: Learn user- and context-dependent gates to dynamically weight and select prior-conditioned heads for optimal alignment across objectives.<br>â€¢ Learning Priors from Data: From Human Priors to Hybrid Discoveries: Automatically induce, refine, or expand priors (e.g., taxonomies, temporal regimes) via weak supervision and structure learning to complement human-specified priors.<br>â€¢ Causal Human Priors for Robust and Fair Recommendation: Integrate causal graphs and counterfactual priors into multi-head decoding to reduce bias, improve robustness, and support what-if reasoning.<br>â€¢ Online Continual Composition of Priors in Foundation Recommenders: Develop streaming and continual learning methods that update adapter heads and their compositions in real time without catastrophic forgetting.<br>â€¢ Cross-Domain Transfer of Structured Priors for Zero-Shot Recommendation: Transfer and adapt prior-conditioned heads across domains and modalities, enabling zero/few-shot recommendation with minimal retraining.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Workload Schedulers -- Genesis, Algorithms and Differences</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.10258" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.10258" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This paper presents a novel approach to categorization of modern workload schedulers. We provide descriptions of three classes of schedulers: Operating Systems Process Schedulers, Cluster Systems Jobs Schedulers and Big Data Schedulers. We describe their evolution from early adoptions to modern implementations, considering both the use and features of algorithms. In summary, we discuss differences between all presented classes of schedulers and discuss their chronological development. In conclusion we highlight similarities in the focus of scheduling strategies design, applicable to both local and distributed systems.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ The scheduling landscape is fragmented across operating systems, cluster managers, and big data frameworks; practitioners lack a unified categorization that clarifies goals, constraints, and trade-offs.<br>â€¢ Existing schedulers often rely on simple heuristics (e.g., FCFS, round robin) or limited visibility, leading to inefficiencies under heterogeneity, dynamic loads, and fairness constraints.<br>â€¢ Process schedulers face strict latency/overhead limits and limited tunability; cluster schedulers contend with unpredictable workloads, complex policies, heterogeneity, and power consumption; distributed schedulers face concurrency bottlenecks and insufficient queuing/fault handling.<br>â€¢ There is a need to trace the evolution of algorithms to distill common design principles and differences, enabling better selection, tuning, and future unification of scheduling strategies.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A survey-driven, chronological taxonomy that compares operating system process schedulers, cluster job schedulers, and big data schedulers, analyzing their algorithms, goals, and constraints to identify differences, commonalities, and a potential unifying perspective on scheduling strategy design.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Toward a Unified Scheduling Abstraction for Local and Distributed Systems: Define and evaluate a cross-domain API and policy layer that harmonizes fairness, latency, and throughput from CFS-like local schedulers to cluster/job schedulers.<br>â€¢ Energy-Aware Backfilling and Consolidation in Large-Scale Clusters: Integrate power models and dynamic consolidation into FCFS/backfilling frameworks to minimize energy while maintaining SLAs and fairness.<br>â€¢ Optimistic Multi-Scheduler Concurrency for Two-Level Resource Managers: Design lock-free, conflict-resolving offer evaluation to alleviate head-of-line blocking and improve throughput in Mesos-like systems.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07448" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07448" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena. Unlike standard scientific reasoning or general creative generation, idea generation in science is a multi-objective and open-ended task, where the novelty of a contribution is as essential as its empirical soundness. Large language models (LLMs) have recently emerged as promising generators of scientific ideas, capable of producing coherent and factual outputs with surprising intuition and acceptable reasoning, yet their creative capacity remains inconsistent and poorly understood. This survey provides a structured synthesis of methods for LLM-driven scientific ideation, examining how different approaches balance creativity with scientific soundness. We categorize existing methods into five complementary families: External knowledge augmentation, Prompt-based distributional steering, Inference-time scaling, Multi-agent collaboration, and Parameter-level adaptation. To interpret their contributions, we employ two complementary frameworks: Boden's taxonomy of Combinatorial, Exploratory and Transformational creativity to characterize the level of ideas each family expected to generate, and Rhodes' 4Ps framework-Person, Process, Press, and Product-to locate the aspect or source of creativity that each method emphasizes. By aligning methodological advances with creativity frameworks, this survey clarifies the state of the field and outlines key directions toward reliable, systematic, and transformative applications of LLMs in scientific discovery.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLM-based scientific ideation is a multi-objective task that must jointly optimize novelty and scientific soundness, yet current LLM creativity is inconsistent and poorly understood.<br>â€¢ Existing surveys emphasize pipelines, agents, or task coverage, lacking a creativity-centered synthesis grounded in cognitive science frameworks.<br>â€¢ There is no common framework to map methods to creativity levels (combinatorial, exploratory, transformational) and creativity sources (person, process, press, product), impeding systematic design and comparison.<br>â€¢ Evaluation of scientific ideas is subjective and non-standardized; reliable metrics for novelty, feasibility, and impact are scarce, limiting reproducibility and progress tracking.<br>â€¢ Knowledge augmentation (RAG/graphs) improves grounding but biases outputs toward safe, incremental (combinatorial) ideas and can reduce diversity.<br>â€¢ Alignment and safety tuning often narrow output distributions, constraining divergent exploration even under creativity-oriented prompting.<br>â€¢ Inference-time search methods lack a unified view on explorationâ€“feasibility trade-offs and often operate at low abstraction levels, limiting exploratory/transformational creativity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes a creativity-centered survey and taxonomy that categorizes LLM-driven scientific ideation into five method families (external knowledge augmentation, prompt-based steering, inference-time scaling, multi-agent collaboration, and parameter-level adaptation) and interprets them through Bodenâ€™s creativity levels and Rhodesâ€™ 4Ps. It maps each family to expected creativity outcomes and sources, analyzes evaluation practices, and identifies gaps that hinder transformational creativity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Agent-Level Open-Ended Search for Transformational Scientific Creativity: Evolve agentsâ€™ roles, goals, and skills (not just ideas) to escape local optima and enable paradigm-shifting hypotheses.<br>â€¢ Creativity-Aware Retrieval: Relational and Diversity-Promoting RAG for Scientific Ideation: Combine knowledge graphs with novelty/diversity objectives and adjustable groundingâ€“divergence controls to foster cross-domain leaps.<br>â€¢ Standardized Product-Side Evaluation for Scientific Ideas: A Benchmark Suite for Novelty, Feasibility, and Impact: Define composite metrics, gold standards, and expert-in-the-loop protocols for reliable, comparable assessment.<br>â€¢ Dual-Objective Alignment for Science: RLHF/RLAIF Optimizing Both Novelty and Soundness: Train with simulators, rule-checkers, and expert feedback to balance originality with empirical correctness.<br>â€¢ Abstraction-Centric Test-Time Search: Tree Search Over Conceptual Spaces and Experimental Plans: Run MCTS/bandit search at higher-level nodes (concepts, mechanisms, workflows) with multi-signal heuristics.<br>â€¢ Multi-Agent Peer Review and Debate for Cross-Disciplinary Hypothesis Discovery: Orchestrate specialized critic/synthesizer agents that emulate scientific collaboration to surface unconventional ideas.<br>â€¢ Beyond Autoregression: Architectural and Memory Augmentations for Creative Scientific Reasoning: Explore planning modules, external memory, and non-autoregressive components to overcome structural creativity limits.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">Building the Web for Agents: A Declarative Framework for Agent-Web Interaction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11287" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11287" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The increasing deployment of autonomous AI agents on the web is hampered by a fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, a web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces <tool> and <context> tags, allowing developers to explicitly define available actions and relevant state, thereby creating a clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the framework's practicality, learnability, and expressiveness in a three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides a foundational mechanism for realizing the Agentic Web, enabling a future of seamless and secure human-AI collaboration on the web.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Agents must infer affordances from human-oriented UIs (DOM scraping/screenshots), causing brittle, inefficient, and unreliable web interactions.<br>â€¢ Significant security and privacy risks arise from uncontrolled data exposure and unintended actions; developers lack mechanisms to constrain what agents can see or do.<br>â€¢ The web lacks standardized, machine-native affordances and explicit, auditable contracts for agent behavior, disempowering site owners and creating unstable integrations.<br>â€¢ Centralized approaches (site-managed LLM backends or universal scraping agents) shift control away from either users or developers and do not scale safely.<br>â€¢ Implementing robust multimodal agent interactions is complex without a common framework, leading to fragmented, non-portable solutions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>VOIX is a web-native, declarative framework that introduces <tool> and <context> HTML elements to explicitly expose invokable actions and task-relevant state to agents. A browser agent discovers these elements, sends a scoped catalog to a user-chosen inference provider (LLM), and dispatches tool calls back to the page via events with optional asynchronous returns, preserving privacy through client-side control and explicit context scoping.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Toward a Web Standard for Agentic Interfaces: Formalizing <tool> and <context> in HTML and Browser APIs: Define specifications, compatibility requirements, and conformance tests to standardize VOIX-like semantics across browsers and agent providers.<br>â€¢ Safe-by-Design Agent Contracts: Static Analysis and Runtime Policies for VOIX Tool Schemas: Develop type systems, policy languages, and verification/runtime guards to prevent unsafe tool invocations and constrain data flows.<br>â€¢ Declarative vs. Perceptual Agents: A Benchmarking Study of Task Success, Efficiency, and Safety on the Agentic Web: Empirically compare VOIX-based agents to DOM/screenshot-based agents across long-horizon tasks, measuring reliability, privacy exposure, and developer effort.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11168" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11168" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vehicle-to-Vehicle (V2V) cooperative perception has great potential to enhance autonomous driving performance by overcoming perception limitations in complex adverse traffic scenarios (CATS). Meanwhile, data serves as the fundamental infrastructure for modern autonomous driving AI. However, due to stringent data collection requirements, existing datasets focus primarily on ordinary traffic scenarios, constraining the benefits of cooperative perception. To address this challenge, we introduce CATS-V2V, the first-of-its-kind real-world dataset for V2V cooperative perception under complex adverse traffic scenarios. The dataset was collected by two hardware time-synchronized vehicles, covering 10 weather and lighting conditions across 10 diverse locations. The 100-clip dataset includes 60K frames of 10 Hz LiDAR point clouds and 1.26M multi-view 30 Hz camera images, along with 750K anonymized yet high-precision RTK-fixed GNSS and IMU records. Correspondingly, we provide time-consistent 3D bounding box annotations for objects, as well as static scenes to construct a 4D BEV representation. On this basis, we propose a target-based temporal alignment method, ensuring that all objects are precisely aligned across all sensor modalities. We hope that CATS-V2V, the largest-scale, most supportive, and highest-quality dataset of its kind to date, will benefit the autonomous driving community in related tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of real-world V2V cooperative perception data under complex adverse traffic scenarios (rain, snow, fog, night, direct sunlight, work zones), where single-vehicle perception is most fragile.<br>â€¢ Existing V2X datasets suffer from imperfect hardware synchronization (20â€“50 ms) and stamp-based alignment that ignores mechanical LiDARâ€™s intra-scan timing, causing cross-modal misalignment.<br>â€¢ Limited multi-view, high-frame-rate cameras and sparse, non-time-consistent annotations (missing global IDs, BEV, HD maps) impede detection, tracking, mapping, and cross-modal learning in CATS.<br>â€¢ Cross-agent calibration and registration are error-prone without LiDAR deskewing and refined inter-vehicle transforms, reducing the reliability of multi-vehicle fusion.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces CATS-V2V, a large-scale real-world V2V dataset with two hardware-synchronized vehicles (â‰ˆ1 ms), 128-beam LiDAR (10 Hz), seven 30 Hz cameras, and RTK-fixed INS, providing deskewed LiDAR, HD maps, and time-consistent 3D boxes with global IDs. It proposes a target-based temporal alignment that computes per-object average LiDAR timestamps and aligns them to the nearest camera frame, with motion compensation and GICP-refined cross-agent registration to ensure precise multi-modal and multi-vehicle consistency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Uncertainty-Aware Fusion for V2V Cooperative Perception in Complex Adverse Traffic Scenarios: Learn weather/lighting-aware fusion and communication policies that adapt to sensor degradation and bandwidth using CATS-V2V.<br>â€¢ Target-Centric Temporal Alignment Networks for Multi-View LiDARâ€“Camera Synchronization: A learning-based model that predicts per-object temporal offsets to further reduce residual misalignment beyond deterministic target-based alignment.<br>â€¢ Weather-Robust Cross-Modal Depth and BEV Scene Reconstruction via Multi-Pass CATS-V2V: Exploit repeated routes under different conditions and RTK-fixed poses to train robust depth estimation and BEV reconstruction across adverse scenarios.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 7</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-17 23:09:07 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 7;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>