<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - January 09, 2026</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">January 09, 2026</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05242" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05242" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Multi-reward RL is needed to align LLM behaviors with diverse human preferences, but widely used GRPO has been adopted without assessing suitability for multi-reward settings.<br>‚Ä¢ GRPO‚Äôs normalization collapses distinct reward combinations into identical advantages, eroding signal resolution and masking relative differences among rewards.<br>‚Ä¢ This collapse leads to unstable training, suboptimal convergence, and even early failure, with numerical ranges entangled with reward count rather than reflecting true preference structure.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>GDPO decouples normalization by performing group-wise normalization for each individual reward, then applies batch-wise advantage normalization to maintain a stable numerical range independent of reward count, preserving relative reward differences and improving training stability and multi-reward optimization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Reward Weighting for GDPO: Integrate uncertainty- and variance-aware weighting to dynamically balance multiple rewards during training.<br>‚Ä¢ Theoretical Foundations of Advantage Collapse in Multi-Reward RL: Formalize conditions under which normalization induces advantage degeneracy and derive guarantees for GDPO‚Äôs stability.<br>‚Ä¢ Pareto-Efficient GDPO for Multi-Objective LLM RL: Extend GDPO with Pareto front tracking to optimize trade-offs among correctness, format, and length constraints.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.04890" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.04890" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Matrix-layer scales in LLMs are trapped by the weight-decay‚Äìnoise equilibrium (‚à•W‚à• ‚àù ‚àö(Œ∑/Œª)), making norms depend on optimizer hyperparameters rather than data.<br>‚Ä¢ Existing ¬µP-based scaling and multiplier tuning are compute-intensive, brittle across data mixes, and break under width scaling when WD is present.<br>‚Ä¢ Constrained row/column and block-output norms limit representation richness and lead to performance drops when block scales mismatch (e.g., MLP vs. attention/SSM).<br>‚Ä¢ Adding multipliers introduces symmetry-induced drift (Q/K multiplicative symmetry, residual normalization symmetry) and adverse interactions with global gradient clipping.<br>‚Ä¢ Need an optimizer- and architecture-agnostic approach that improves training dynamics and downstream performance for both Adam and Muon across attention, SSM, and MLP blocks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Reparameterize matrix layers with learnable scalar multipliers (W ‚Üí sW) and per-row/per-column vector multipliers (W_ij = r_i W_ij c_j) to free feature scales from WD‚Äìnoise equilibrium, relying on gradient aggregation to avoid Brownian expansion; stabilize training by light WD on multipliers and excluding them from global clip-norm, with placement guidelines to avoid redundant symmetries.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Symmetry-Aware Optimization for Learnable Multipliers in Transformers: Develop principled constraints or regularizers that prevent drift along multiplicative (Q/K) and normalization symmetries without relying on weight decay.<br>‚Ä¢ Learnable Multipliers for Width-Consistent Scaling: Theory and Practice: Formalize width scaling laws under WD‚Äìnoise equilibrium with LRMs, explain alignment decay, and derive robust hyperparameter transfer rules.<br>‚Ä¢ Controlling Lazy Training with Learnable Projector Multipliers: Design training protocols and head parameterizations that preserve feature learning while allowing LM-head scale adaptation, avoiding shortcut solutions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05249" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05249" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Nighttime low-light AWB is undermined by extreme illumination scarcity, high ISO chroma noise, and mixed light sources, breaking assumptions of classic color constancy.<br>‚Ä¢ Existing methods either rely on fixed statistical heuristics that fail under noise and demand tedious manual parameter tuning, or deep models that require large labeled nighttime data and generalize poorly across sensors/ISPs, causing color shifts.<br>‚Ä¢ Robust cross-sensor nighttime AWB is critical for mobile, surveillance, and automotive imaging, yet evaluation is hampered by the lack of multi-camera nighttime benchmarks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RL-AWB couples a new statistical estimator (SGP-LRD: salient gray-pixel detection plus local reflectance differences) with a Soft Actor-Critic agent that adaptively tunes two hyperparameters (gray-pixel percentage N and Minkowski order p) per image using RGB-uv histogram and action-history features with a relative angular-error reward. A two-stage curriculum enables sample-efficient training (5-shot) and strong cross-sensor generalization without camera-specific calibration.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Test-Time Self-Supervised RL for AWB Without Ground Truth: Train RL-AWB using proxy rewards (grayness consistency, illumination sparsity, cycle constancy) on unlabeled nighttime data.<br>‚Ä¢ Multi-Illuminant RL-AWB with Spatially Varying Policies: Learn region-wise or pixel-wise parameter policies to handle mixed lighting and local AWB in complex scenes.<br>‚Ä¢ Meta-RL for Cross-Sensor White Balance Adaptation: Develop sensor-agnostic priors and rapid few-shot adaptation across diverse cameras and ISPs for robust deployment.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05241" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05241" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Real-world robot manipulation datasets are hard to scale due to demanding hardware setups, camera calibration, and sensor synchronization, limiting diversity and quantity needed for robust policy learning.<br>‚Ä¢ Existing augmentation methods operate on single-frame, single-view images, failing to provide temporally coherent and multi-view observations needed by modern VLA and visuomotor policies (e.g., for tasks where pre/post states look identical).<br>‚Ä¢ Text prompts alone poorly specify scene setups; dataset captions are simplistic, VLMs hallucinate, and text cannot capture low-level visual details, leading to misaligned and unrealistic augmentations.<br>‚Ä¢ Off-the-shelf object/robot segmentation is unreliable for dynamic wrist-camera views with rapid motion and narrow FOV, especially when target objects are initially invisible.<br>‚Ä¢ Lack of a scalable, automated pipeline to curate visual identity assets from large robotics datasets for controllable, diverse augmentation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RoboVIP introduces a multi-view, temporally coherent video inpainting diffusion framework conditioned on curated visual identity images, coupled with an action-oriented segmentation pipeline (leveraging gripper-state timing) and an automated agentic process to build a million-scale visual identity pool. This plug-and-play system augments manipulation videos (including dynamic wrist-cam) to diversify backgrounds and tabletop scenes while preserving action trajectories, improving VLA and visuomotor policy performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ RoboVIP-3D: Geometry-Aware Multi-View Video Augmentation for Robot Manipulation: Enforce cross-view and 3D scene consistency via camera pose and implicit geometry to further improve spatial realism and multi-view coherence.<br>‚Ä¢ Interactive Identity Prompting: Closed-loop Augmentation Guided by Policy Feedback: Adaptively select and weight visual identities based on training dynamics and failure modes to optimize augmentation for downstream policy learning.<br>‚Ä¢ Physics- and Action-Aware Video Inpainting for Contact-Rich Manipulation: Integrate force/proprioception and causal constraints into generation to preserve physical plausibility and task-relevant object states during augmentation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RelayLLM: Efficient Reasoning via Collaborative Decoding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05167" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05167" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively "relaying" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLMs deliver strong reasoning but incur high computational cost and latency, while SLMs are efficient yet struggle on complex tasks.<br>‚Ä¢ Existing collaboration (cascading/routing) is coarse-grained at the query level, offloading entire problems to LLMs and wasting compute when SLMs can handle most steps.<br>‚Ä¢ Token-level routing baselines (e.g., CITER) require external controllers, adding latency and complexity, and do not let the SLM internally manage help-seeking.<br>‚Ä¢ SLMs lack mechanisms to request targeted, minimal expert intervention at critical positions and to balance independence with strategic assistance.<br>‚Ä¢ Training challenges include teaching the SLM to produce valid call commands, decide when/how long to call, and optimizing a cost‚Äìaccuracy trade-off without distribution shift.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RelayLLM enables token-level collaborative decoding where the SLM controls generation and inserts a special <call>n</call> command to invoke the LLM for n tokens, then resumes reasoning; a two-stage training (supervised warm-up for command syntax, followed by GRPO-based RL with difficulty-aware rewards and data filtering) teaches strategic, low-cost help-seeking.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Multi-Expert RelayLLM: Token-Level Routing Across Specialized Teachers: Extend RelayLLM to select among multiple expert LLMs/tools per token, learning expert choice and call length with multi-objective rewards.<br>‚Ä¢ Confidence-Aware RelayLLM: Uncertainty-Guided Collaborative Decoding: Integrate calibrated confidence and self-verification signals to trigger calls and adapt n dynamically, reducing unnecessary interventions under tight budgets.<br>‚Ä¢ Cross-Family and Multimodal RelayLLM: Robust Collaboration with Heterogeneous Tokenizers and Modalities: Design model-agnostic relay protocols for different LLM families and vision‚Äìlanguage tasks, aligning vocabularies and context formats for stable token-level collaboration.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AT^2PO: Agentic Turn-based Policy Optimization via Tree Search</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.04767" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.04767" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Limited exploration diversity in multi-turn agent rollouts; existing chain/tree expansions are random or heuristic and fail to prioritize high-uncertainty/high-potential turns under tight budgets.<br>‚Ä¢ Sparse, trajectory-level rewards impede precise credit assignment to intermediate actions, yielding weak turn-wise supervision.<br>‚Ä¢ Misaligned policy optimization objectives: current token- or sequence-level methods ignore the turn-based sampling structure with interleaved tool responses, causing unstable gradients and inefficient learning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>AT2PO combines entropy-guided turn-level tree expansion, turn-wise credit assignment via entropy-weighted backward propagation over the tree, and Agentic Turn-based Policy Optimization (ATPO) with per-turn importance sampling and clipping, aligning updates with the natural decision granularity of multi-turn agent interactions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning-to-Expand: A Meta-Policy for Node Selection in Agentic Tree Search: Replace entropy heuristics with a learned expansion policy that optimizes exploration‚Äìexploitation under budget constraints.<br>‚Ä¢ Turn-Level Advantage Critics for Agentic RL: Introduce learned turn-value/advantage estimators to complement entropy-weighted credit assignment and reduce variance.<br>‚Ä¢ ATPO for Multimodal and Tool-Augmented Agents: Extend turn-based optimization to multimodal inputs and heterogeneous tool feedback with modality-aware importance weighting and credit propagation.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Token-Level LLM Collaboration via FusionRoute</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05106" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05106" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Achieve strong, balanced performance across diverse domains without the prohibitive training and deployment costs of very large general-purpose LLMs.<br>‚Ä¢ Robustly coordinate multiple smaller, specialized LLMs whose strengths are uneven and whose generalization outside training distributions is limited.<br>‚Ä¢ Address limitations of existing paradigms: MoE requires costly joint training and architectural homogeneity; sequence-level multi-agent systems are coarse, inefficient, and hard to allocate; model merging is hyperparameter-sensitive and suffers parameter interference.<br>‚Ä¢ Overcome instability of prior token-level collaboration that relies solely on fixed expert outputs and breaks when experts underperform or selection is wrong.<br>‚Ä¢ Resolve the theoretical limitation that pure expert-only routing cannot, in general, realize the optimal decoding policy without strong global coverage assumptions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>FusionRoute trains a lightweight router LLM that, at each decoding step, selects the best expert and adds a complementary logit to the expert‚Äôs next-token distribution via logit fusion, refining or correcting the expert output. This dual expert selection plus trainable complementation expands the policy class, enabling optimal decoding under mild conditions while avoiding expensive joint expert training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ FusionRoute++: Uncertainty-Aware Complementary Routing for Reliable Token-Level Collaboration: Integrate calibrated uncertainty estimates to adaptively weight expert selection and complementary logits, improving robustness on ambiguous or out-of-distribution tokens.<br>‚Ä¢ Multimodal FusionRoute: Token-Level Collaboration Across Text, Code, and Vision: Extend the router and logit fusion to coordinate heterogeneous multimodal experts, enabling fine-grained collaboration in vision-language and code-with-context tasks.<br>‚Ä¢ Online FusionRoute: Continual Expert Discovery and Adaptation in Dynamic Domains: Develop algorithms for online expert addition/removal and router adaptation without gradient access to experts, supporting non-stationary task distributions and lifelong learning.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21815" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21815" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Vision-language models are highly vulnerable to small pixel-space perturbations that can cause semantic drift and harmful outputs, posing risks in safety-critical applications.<br>‚Ä¢ Existing entropy-based attacks maximize uncertainty globally across all decoding steps, implicitly assuming equal token importance and leading to inefficient budget use that overlooks key autoregressive decision points.<br>‚Ä¢ There is a lack of selective and transferable attack frameworks that exploit the small subset (~20%) of high-entropy ‚Äúfork‚Äù tokens that disproportionately steer generation trajectories across diverse VLM architectures.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Propose Entropy-Guided Adversarial attacks (EGA) that selectively maximize next-token entropy at top high-entropy positions via l‚àû-bounded PGD (HiEnt-PGD), and introduce a transferable variant (HiEnt-Bank) that uses an offline flip-rate token bank to target shared vulnerable tokens without computing entropy on the target model.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Entropy-Aware Safety Alignment for VLMs: Regularize or downweight high-entropy decision tokens during training to reduce harmful branching under perturbations.<br>‚Ä¢ Certified Defenses Focused on High-Entropy Forks: Provide provable robustness guarantees that bound next-token uncertainty at selected positions under pixel-space attacks.<br>‚Ä¢ Universal Vulnerability Token Banks for Cross-Model Defense: Build and evaluate shared flip-rate token banks to anticipate transferable attacks and guide targeted defenses.<br>‚Ä¢ Modeling and Mitigating Autoregressive Harmful Content Propagation: Analyze how harmful probability mass spreads along sequences and design interventions to interrupt early propagation.<br>‚Ä¢ Black-Box Estimation of High-Entropy Positions in VLMs: Develop query-efficient surrogate methods to locate decision tokens without gradients for selective attack and defense.<br>‚Ä¢ Entropy-Guided Decoding Policies: Adapt temperature, sampling, and safety filters dynamically at detected high-entropy steps to prevent harmful trajectories.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05175" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05175" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ CoT‚Äôs necessity in video understanding is underexplored; direct answering in RL-trained video models often matches or surpasses CoT while CoT incurs high computational cost and potential overthinking.<br>‚Ä¢ Always-thinking strategies are inefficient, producing long reasoning traces (hundreds of tokens), increasing latency and inference costs for perception-heavy video tasks.<br>‚Ä¢ Existing auto-thinking methods are hard to transfer to video: weak correlation between explicit reasoning and accuracy, scarcity of must-think samples, and unstable training that collapses to always-think or never-think when enforcing binary mode labels.<br>‚Ä¢ Need a stable, label-free adaptive reasoning mechanism that reasons only when necessary while maintaining or improving accuracy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VideoAuto-R1 trains with a thinking once, answering twice paradigm: the model outputs an initial answer, then a single reasoning trace, and a reviewed answer, supervising both answers with verifiable (R1-style) rewards weighted toward the final answer. At inference, a length-normalized mean log-probability of the initial answer serves as a confidence score to early-exit (direct answer) or continue with reasoning, delivering state-of-the-art accuracy with ~3.3√ó shorter responses.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Confidence-Aware Early-Exit Policies for Video Reasoning: Train calibrated or learned confidence estimators/policies (beyond mean log-prob) to optimize accuracy‚Äìlatency trade-offs and reduce erroneous mode switches.<br>‚Ä¢ Auto-Perception-Driven Frame Selection for VideoAuto-R1: Integrate adaptive reasoning with dynamic frame/segment selection and re-observation to couple when-to-think with what-to-look-at for long, noisy videos.<br>‚Ä¢ Budget-Aware Reinforcement Learning for Adaptive Video CoT: Use cost-sensitive rewards to jointly optimize correctness and token/compute budget, enabling controllable reasoning depth under latency or energy constraints.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05138" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05138" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Video world models lack a unified, precise interface to control both camera and multi-object motion, as dynamics are typically modeled in the 2D image plane.<br>‚Ä¢ 2D control cues (points, optical flow, masks, boxes) are view-dependent and fragile under large viewpoint changes and occlusions.<br>‚Ä¢ Existing 3D-aware controls (depth maps, sparse 3D trajectories, 3D boxes, SMPL-X) are fragmented, rigid, category-limited, or noisy, and do not form a compact, editable 4D world state aligned with camera control.<br>‚Ä¢ Most controllable video methods assume static or weakly dynamic scenes, leaving realistic multi-object dynamics underexplored.<br>‚Ä¢ There is a scarcity of large-scale real-world training data with explicit 4D annotations (camera and object trajectories), limiting model generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VerseCrafter introduces a unified 4D Geometric Control state composed of a static background point cloud and per-object 3D Gaussian trajectories, rendered into RGB+depth control maps with a soft mask and injected via a lightweight GeoAdapter into a frozen Wan2.1-14B video diffusion backbone for precise, view-consistent control of camera and multi-object motion. It further builds VerseControl4D, an automatic data engine and dataset that extracts these 4D controls from in-the-wild videos to enable large-scale training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ End-to-End 4D Geometric Control Estimation from Raw Video: Replace external depth/mask pipelines with a differentiable network that infers background point clouds and Gaussian trajectories directly from monocular sequences.<br>‚Ä¢ Physics-Aware Video World Models with Gaussian Occupancy Dynamics: Integrate collision, constraints, and force-aware dynamics into 3D Gaussian trajectories to simulate interactions and physically plausible motion.<br>‚Ä¢ Long-Horizon Dynamic World Modeling with Persistent 4D Memory: Extend VerseCrafter with explicit 4D memory to maintain scene state over long sequences, enabling consistent world editing and multi-episode navigation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">The Illusion of Specialization: Unveiling the Domain-Invariant "Standing Committee" in Mixture-of-Experts Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03425" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03425" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ The divide-and-conquer intuition of MoE specialization is misleading: routed experts self-organize into domain-invariant "Standing Committees" that dominate computation across tasks, which is crucial for understanding capacity use and performance.<br>‚Ä¢ Existing interpretability and routing analyses focus on individual experts and activation frequency, missing stable co-activation and group-level structure; this gap obscures the true organization of computation in MoE models.<br>‚Ä¢ Standard load-balancing losses that enforce uniform expert utilization conflict with an emergent centralization bias, potentially harming convergence, wasting FLOPs, and limiting performance‚Äîeven architectures with shared experts do not eliminate this bias.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes COMMITTEEAUDIT, a model-agnostic post hoc framework that aggregates full routing distributions to compute an Expert Contribution Index per domain, assesses task-specificity, and identifies Pareto-optimal, cross-domain-stable expert committees ("Standing Committees") via rank stability across tasks. This quantifies group-level organization beyond individual activation statistics and reveals centralized computation patterns.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Committee-Aligned Load Balancing for MoE: Design training objectives that reinforce stable committees instead of uniform expert utilization to improve convergence, efficiency, and downstream performance.<br>‚Ä¢ Hierarchical Committee Routing in MoE Architectures: Build gates and layers that explicitly model a persistent generalist committee with a task-specific periphery, enabling dynamic top-k allocation across depth and domains.<br>‚Ä¢ Audited Pruning and Capacity Reallocation for MoE: Use COMMITTEEAUDIT metrics to prune or demote peripheral experts and reallocate FLOPs toward committee members, optimizing efficiency without sacrificing accuracy.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Plenoptic Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05239" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05239" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Multi-view inconsistency: existing camera-controlled video re-rendering methods succeed in single-view but fail to maintain synchronized spatio-temporal hallucinations across viewpoints due to diffusion stochasticity and limited long-range memory.<br>‚Ä¢ Na√Øve multi-view conditioning is computationally prohibitive: enlarging context windows to include many input views quickly leads to out-of-memory issues and unstable training.<br>‚Ä¢ Error accumulation in autoregressive generation: long-range sequential synthesis degrades over time (artifacts, over-exposure) without robustness to imperfect intermediate outputs.<br>‚Ä¢ Lack of principled context selection: frame-level or ad-hoc retrieval misses global co-visibility, yielding suboptimal cross-view conditioning and desynchronization.<br>‚Ä¢ Difficulty in long video generation: maintaining spatial coherence across chunk boundaries and precise camera control (including focal length changes) is challenging.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>PlenopticDreamer is an autoregressive multi-in‚Äìsingle-out video diffusion transformer that retrieves top-k past video‚Äìcamera pairs via a 3D field-of-view co-visibility mechanism and encodes cameras with Pl√ºcker raymaps to synchronize cross-view hallucinations. Progressive context scaling, self-conditioned training, and long-video conditioning stabilize convergence, improve robustness to error accumulation, and enable extended, coherent video generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Self-Forcing Autoregressive Plenoptic Video Generation: Integrate self-forcing training to reduce long-range error accumulation and improve temporal stability and exposure control across multi-view sequences.<br>‚Ä¢ 3D Scene Memory-Augmented Plenoptic Re-rendering: Fuse explicit 3D memory (e.g., surfels/point clouds or NeRFs) with FOV-based retrieval to enforce geometry-aware consistency and better handle large viewpoint changes.<br>‚Ä¢ Compressive Context for Scalable Long-Form Plenoptic Video: Develop compressive memory and context distillation to expand context size and resolution efficiently while preserving cross-view synchronization over very long videos.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">Agent-as-a-Judge</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05111" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05111" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Single-pass LLM-as-a-Judge suffers from parametric biases (e.g., verbosity, self-preference) and shallow reasoning, leading to unreliable judgments on complex tasks.<br>‚Ä¢ Lack of executable verification and evidence gathering causes ‚Äúhallucinated correctness,‚Äù especially in specialized, multi-step, or tool-using domains.<br>‚Ä¢ Cognitive overload in multi-criteria evaluation yields coarse, global scores that miss fine-grained errors and nuances.<br>‚Ä¢ Evaluands are increasingly complex, specialized, and multi-step, demanding robust, verifiable, and nuanced assessment beyond static LLM judges.<br>‚Ä¢ The field lacks a unified framework/taxonomy to navigate rapidly emerging agentic evaluation systems, hindering comparability and progress.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A comprehensive survey that formalizes the shift from LLM-as-a-Judge to Agent-as-a-Judge, proposing a developmental taxonomy (Procedural ‚Üí Reactive ‚Üí Self-Evolving) and organizing methods into five dimensions: multi-agent collaboration, planning, tool integration, memory/personalization, and optimization paradigms. It maps methodologies to applications across general and professional domains and distills challenges and research directions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Stability-Guaranteed Self-Evolving Judge Agents: Design self-modifying judges with formal stability and safety constraints on rubric evolution and memory updates.<br>‚Ä¢ Tool-Verified Evaluation with Formal Guarantees: Develop a unified, auditable tool-integration framework (execution, search, provers) with correctness proofs and reliability metrics for agentic judgments.<br>‚Ä¢ Standardized Meta-Evaluation for Agent-as-a-Judge: Create benchmarks and protocols that measure robustness, bias, verifiability, and fine-grained accuracy of agentic judges across domains and modalities.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">CoV: Chain-of-View Prompting for Spatial Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05172" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05172" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached. We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\% improvement in LLM-Match, with a maximum gain of +13.62\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\% average improvement, peaking at +3.73\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ EQA requires gathering distributed and partially occluded context across multiple viewpoints; fixed-frame VLMs cannot explore open view spaces at inference, limiting grounded spatial reasoning in robotics, navigation, and HCI.<br>‚Ä¢ Existing methods perform one-shot answering from a finite, predetermined view set, struggling with multi-step spatial reasoning and missing occluded or fine-grained details.<br>‚Ä¢ Video inputs contain many redundant frames; without question-aligned view selection, irrelevant content dilutes signal, confuses the agent, and wastes compute.<br>‚Ä¢ Training/fine-tuning large multimodal models is costly; a training-free, model-agnostic approach that improves performance via test-time scaling is needed.<br>‚Ä¢ 2D VLMs rely on passive observation and lack camera control, leading to poor alignment between visual evidence and question semantics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>CoV is a training-free, test-time prompting framework that turns a VLM into an active viewpoint reasoner via a coarse-to-fine process: a View Selection agent filters frames to pick question-aligned anchors, then a Chain-of-View agent interleaves reasoning with discrete camera actions (translations, rotations, view switches) mapped to SE(3) transforms over a 3D scene‚Äîoptionally aided by a bird‚Äôs-eye view‚Äîuntil sufficient context or a step budget is reached.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning to Plan Views for Embodied QA: Reinforcement Policies for Chain-of-View: Replace prompt-driven actions with a learned policy that optimizes information gain and answer correctness, reducing redundant steps and improving efficiency.<br>‚Ä¢ Scene-Graph Memory Augmented Chain-of-View for Long-Horizon Spatial Reasoning: Build a dynamic scene graph from accumulated views to guide exploration, resolve occlusions, and provide principled stop criteria for answering.<br>‚Ä¢ Uncertainty-Aware Test-Time Scaling in Embodied QA: Adaptive Action Budgets for CoV: Estimate answer confidence and calibrate compute to adapt minimum/maximum action steps, balancing accuracy and latency under resource constraints.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DocDancer: Towards Agentic Document-Grounded Information Seeking</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05163" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05163" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Long, multi-modal documents require iterative exploration and comprehension; existing systems struggle with accuracy and efficiency at long context lengths.<br>‚Ä¢ OCR-only pipelines and naive document parsing lose structural/visual cues and fail on visually rich layouts.<br>‚Ä¢ Embedding-based RAG decouples retrieval and reasoning, is single-shot, and is brittle for complex, multi-step queries.<br>‚Ä¢ Existing agent-based DocQA systems are prompt-engineered, closed-source, and lack learnable, autonomous behaviors.<br>‚Ä¢ High-quality, agent-supervised DocQA training data is scarce; many datasets lack sufficiently annotated training splits.<br>‚Ä¢ Common XML-style outlines contain structural/content inaccuracies and omit retrieval-aware visual information, limiting agent effectiveness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DocDancer is an end-to-end trained, ReAct-style single-agent DocQA framework with a minimal toolset: a global keyword Search and a fine-grained multimodal Read that integrates text, images, tables, and page-layout signals over enhanced MinerU2.5-based document outlines with generated visual captions. An Exploration-then-Synthesis data pipeline produces grounded trajectories and QA pairs to train open-source backbones (e.g., Qwen3-4B/30B), achieving state-of-the-art results on MMLongBench-Doc and DocBench.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Paper Title 1: Towards Adaptive Tool Discovery for Agentic DocQA: Learn to expand, compose, or specialize tools beyond Search/Read via meta-learning and interaction feedback to improve efficiency and coverage.<br>‚Ä¢ Paper Title 2: Layout-Aware Multimodal Reasoning for Long Documents: Pretrain and fine-tune models to jointly exploit hierarchical structure, visual cues, and cross-page context for stronger grounding and robustness.<br>‚Ä¢ Paper Title 3: Scaling Exploration-then-Synthesis: Data-Centric Training for Document Agents: Automate quality control, curriculum scheduling, and safety checks to synthesize diverse, harder DocQA trajectories across domains and languages.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05124" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05124" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ In-context image generation/editing (ICGE) needs precise understanding of interleaved image‚Äìtext inputs and faithful execution, yet current models often produce images misaligned with their reasoning.<br>‚Ä¢ Existing unified multimodal models struggle with complex, ambiguous prompts‚Äîespecially reference attribution and confusion across multiple input images.<br>‚Ä¢ Reinforcement learning for visual generation is mostly designed for text-only conditioning, lacking effective, scalable rewards for ICGE; task-specific reward design is costly and complex.<br>‚Ä¢ Bridging understanding and generation is crucial because pure text cannot capture visual concepts embodied in reference images, a key requirement for subject-driven generation and editing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Re-Align introduces a structured In-Context Chain-of-Thought (IC-CoT) that decouples semantic guidance (an explicit out_caption) from reference association (relation tags per reference image) to bridge understanding and generation, then applies GRPO with a surrogate reward computed via image‚Äìtext similarity between the predicted caption and the generated image, plus a reasoning-induced diversity strategy. The model is trained in two stages: supervised fine-tuning on the Re-Align-410K ICGE dataset with IC-CoT annotations, followed by RL-based reasoning‚Äìgeneration alignment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Multimodal Consistency Critics for ICGE: Train a dedicated reward model that jointly assesses semantic guidance and reference attribution to provide richer alignment signals than caption‚Äìimage similarity.<br>‚Ä¢ Hierarchical IC-CoT with Spatial Grounding for Complex Compositions: Extend IC-CoT to include scene graphs or bounding-box layouts, enabling stepwise planning and explicit spatial placement of referenced entities.<br>‚Ä¢ Re-Align-Video: Structured Reasoning-Guided Alignment for In-Context Video Generation and Editing: Generalize IC-CoT and surrogate rewards to temporal sequences, handling cross-frame reference consistency and motion-aware editing.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03559" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03559" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Chain-of-Thought (CoT) reasoning improves multi-step mathematical problem solving in large language models but remains vulnerable to exposure bias and error accumulation, as early mistakes propagate irreversibly through autoregressive decoding. In this work, we propose DiffCoT, a diffusion-styled CoT framework that reformulates CoT reasoning as an iterative denoising process. DiffCoT integrates diffusion principles at the reasoning-step level via a sliding-window mechanism, enabling unified generation and retrospective correction of intermediate steps while preserving token-level autoregression. To maintain causal consistency, we further introduce a causal diffusion noise schedule that respects the temporal structure of reasoning chains. Extensive experiments on three multi-step CoT reasoning benchmarks across diverse model backbones demonstrate that DiffCoT consistently outperforms existing CoT preference optimization methods, yielding improved robustness and error-correction capability in CoT reasoning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Chain-of-Thought (CoT) suffers from exposure bias: models are trained on correct prefixes via teacher forcing but must decode from potentially erroneous prefixes at inference.<br>‚Ä¢ Early mistakes accumulate irreversibly in autoregressive decoding, leading to fragile multi-step reasoning and incorrect final answers.<br>‚Ä¢ Existing search-based methods (e.g., MCTS + critics) incur heavy inference-time overhead and rely on post-hoc pruning rather than improving the decoding policy.<br>‚Ä¢ Preference Optimization methods align locally at the step level, lack global trajectory revision, and fail to leverage future signals to correct past errors.<br>‚Ä¢ There is a mismatch between training supervision and inference-time reasoning dynamics, motivating a globally revisable CoT paradigm.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DiffCoT reframes CoT reasoning as iterative denoising with a diffusion-style sliding window that retrospectively refines prior steps while autoregressively generating the next. It defines step-level noise via reward-ranked candidate steps and applies a causal noise schedule to respect temporal dependencies, preserving token-level autoregression while enabling unified generation and correction.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Adaptive Causal Noise Schedules for Diffusion-Styled Chain-of-Thought: Automatically learn step-dependent noise schedules that optimize correction‚Äìcausality trade-offs across tasks.<br>‚Ä¢ Retrieval-Augmented Diffusion CoT: Integrate external knowledge retrieval into the sliding-window denoising loop to correct and ground reasoning under noisy trajectories.<br>‚Ä¢ Efficient Data-Free DiffCoT via Synthetic Candidate Generation: Replace MCTS-based candidate collection with lightweight synthetic sampling and self-evaluation to reduce training overhead while maintaining denoising efficacy.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.04754" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.04754" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Render-supervised 2D-to-3D distillation misaligns semantics because supervision arrives after rendering/compositing, causing feature‚Äìregion mismatches and view-dependent instability.<br>‚Ä¢ Registration-based 3DGS methods lack explicit cross-view consistency and intra-mask cohesion, leading to noisy, fragmented semantics across viewpoints.<br>‚Ä¢ Existing pipelines often depend on iterative densification and gradient-based fine-tuning, increasing computation, runtime (minutes to hours), and complexity.<br>‚Ä¢ Open-vocabulary 3D understanding demands both accurate geometry and coherent semantics for applications in robotics, navigation, and AR, with efficient, direct 3D querying.<br>‚Ä¢ Current 3DGS approaches typically learn semantics per-view and fuse post hoc, which weakens object-level reasoning and hampers clean, consistent retrieval.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ProFuse performs a dense correspondence‚Äìguided pre-registration to triangulate accurate Gaussian geometry and cluster warped SAM masks into cross-view 3D Context Proposals; it then mass-aggregates proposal CLIP features and registers them to Gaussians via visibility-weighted accumulation, yielding coherent, open-vocabulary per-Gaussian descriptors without render-supervised training or densification.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Dynamic ProFuse: Cross-View Context Fusion for 4D Gaussian Splatting: Extend correspondence-guided proposals and visibility-weighted registration to dynamic scenes with temporal consistency and object tracking.<br>‚Ä¢ End-to-End Proposal Formation: Joint Learning of Dense Correspondence, Segmentation, and Registration in 3DGS: Integrate the matcher, SAM, and feature fusion into a differentiable pipeline to optimize grouping and descriptors jointly.<br>‚Ä¢ Hierarchical Context Proposals for Large-Scale Open-Vocabulary 3DGS: Develop multi-resolution, graph-based proposals to handle city-scale or multi-room environments with scalable retrieval and memory-efficient indexing.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03362" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03362" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Mixed foreground‚Äìbackground pixels in soft boundaries (e.g., thin hairs, semi-transparent structures) cause depth and correspondence ambiguity, degrading 3D vision tasks.<br>‚Ä¢ State-of-the-art monocular depth models miss or break fine boundary details and often predict depth behind true surfaces, leading to detached strands and error propagation to stereo and novel views.<br>‚Ä¢ Implicit generative stereo/novel view methods hallucinate textures and produce inconsistent details at soft boundaries; latent-space designs compress and degrade high-frequency textures.<br>‚Ä¢ Lack of datasets with high-quality soft-boundary depth annotations and reliance on hand-crafted cues (e.g., trimaps) hinder robust training and generalization.<br>‚Ä¢ Need for a plug-and-play solution that preserves high-fidelity textures and geometric consistency specifically in soft boundary regions across depth, stereo, and novel view synthesis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>HairGuard leverages matting datasets to train a gated-residual depth fixer that automatically localizes and corrects soft-boundary regions while preserving global depth, then performs forward warping, generative inpainting (scene painter), and dual-skip color fusion to blend warped and inpainted results for geometrically consistent, fine-grained novel views.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Joint Alpha‚ÄìDepth Estimation for Mixed Pixels: Learn opacity (alpha) and depth jointly with composition-aware losses to improve soft-boundary modeling and downstream warping/inpainting.<br>‚Ä¢ Temporal HairGuard: Soft-Boundary-Aware Stereo and Novel View Synthesis for Videos: Introduce temporal gating and motion-consistent fusion to maintain strand-level coherence across frames.<br>‚Ä¢ Soft-Boundary-Aware Diffusion Priors for View Synthesis: Integrate matting-informed losses and gated attention into diffusion models to reduce hallucinations and preserve high-frequency textures at thin structures.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03111" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03111" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ RL for LLM reasoning currently depends on thousands of high-quality samples, making training costly and data-intensive; the paper challenges this assumption by seeking extreme data efficiency.<br>‚Ä¢ Prior one-sample RL results are math-specific and preliminary; the paper addresses whether a single math sample can generalize improvements across diverse domains (physics, chemistry, biology) and identifies what makes an optimal sample.<br>‚Ä¢ Existing approaches often require complex reward modeling or large-scale datasets that may harm generalization; the paper proposes "sample engineering" to improve reasoning via deliberate sample design rather than data volume.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Polymath learning: select or synthesize a single math reasoning sample emphasizing salient skills (notably algebra and precalculus) and multidisciplinary context, then train an LLM with RL using verifiable, rule-based rewards on that one sample to induce cross-domain reasoning gains.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Automated Polymath Sample Synthesis via Skill-Aware Optimization: Develop algorithms that automatically construct meta-samples maximizing salient skill coverage and predicted RL impact.<br>‚Ä¢ Mechanisms of Cross-Domain Transfer in One-Sample RL: Use interpretability and probing to uncover how a single-sample RL update alters internal reasoning circuits across domains.<br>‚Ä¢ Minimal Meta-Sample Scaling Laws for RLVR: Characterize robustness, safety, and generalization trade-offs when moving from one to a small set of engineered meta-samples without curriculum.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Memorization in 3D Shape Generation: An Empirical Study</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23628" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23628" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of a standard, validated metric to quantify memorization/novelty in 3D shape generation; existing metrics (e.g., Uni3D-Score, FPD) assess fidelity/distributional similarity but not training-set replication.<br>‚Ä¢ Risk of training data leakage and poor generalization if 3D generative models memorize training shapes; need principled auditing.<br>‚Ä¢ Prior memorization studies focus on images; the 3D modality introduces unique factors (representations, rotations, conditioning) whose effects on memorization are unknown.<br>‚Ä¢ Need to understand how data modality, data diversity, conditioning granularity, guidance scale, latent design, and augmentations affect memorization and how to mitigate it without harming quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Proposes an evaluation framework that defines object-level memorization via 3D retrieval metrics calibrated to human judgments‚Äîidentifying Light Field Distance (LFD) as most accurate‚Äîand aggregates to model-level memorization for generated sets. Applies this framework to existing 3D generators and conducts controlled experiments with a latent vector-set (Vecset) diffusion model to quantify how modality, data diversity, conditioning granularity, guidance, latent length, and rotation augmentation influence and can reduce memorization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Privacy-Preserving 3D Generative Modeling with Memorization Auditing: Integrate differential privacy and post-hoc auditing (LFD-calibrated or improved metrics) to bound leakage while quantifying quality‚Äìprivacy trade-offs.<br>‚Ä¢ Symmetry- and Rotation-Aware Regularization for Memorization Mitigation in 3D Diffusion: Design group-invariant regularizers and augmentation schedules that curb memorization (e.g., rotation/symmetry consistency) without degrading fidelity.<br>‚Ä¢ A Unified Benchmark and Metric Suite for Novelty in 3D Shape Generation: Expand beyond LFD with learned multi-view descriptors and geometry-aware distances, standardized datasets, and human studies to establish a community benchmark for 3D memorization.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">Multi-Scale Local Speculative Decoding for Image Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05149" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05149" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Autoregressive image generation suffers from high latency due to strictly sequential next-token decoding and quadratic sequence growth with resolution.<br>‚Ä¢ Existing speculative decoding methods (primarily designed for text) do not exploit images‚Äô spatial structure and multi-scale hierarchy, limiting acceleration in vision.<br>‚Ä¢ Visual token distributions are ambiguous (flat, interchangeable VQ-VAE codebook tokens), making token-level acceptance rules inefficient and lowering acceptance rates.<br>‚Ä¢ Raster-scan rejection/resampling wastes computation by re-decoding long sequences instead of focusing on local spatial corrections.<br>‚Ä¢ Multi-scale AR models accelerate sampling but rely on bespoke next-scale objectives and schedules that integrate poorly with unified next-token MLLMs and efficient KV-cache usage.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MuLo-SD combines a low-resolution drafter with learned up-samplers to propose multi-scale candidate image tokens, then verifies them in parallel with a high-resolution target model using locally scoped rejection/resampling over spatial neighborhoods and relaxed probability pooling. This multi-scale, locality-aware speculative decoding yields up to 1.7√ó speedups over baselines while maintaining semantic alignment and perceptual quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ ZipAR-Augmented MuLo-SD: Parallel Multi-Scale Speculative Decoding for Images: Integrate ZipAR-style inter-row parallelism into MuLo-SD‚Äôs drafting and local verification to further reduce forward passes and latency.<br>‚Ä¢ Self-Distilled Multi-Scale Speculative Decoding for Unified MLLMs: Reuse internal layers of the target model as the drafter/up-sampler to eliminate extra models, improve KV-cache efficiency, and maintain exact next-token objectives.<br>‚Ä¢ Adaptive Local Verification with Confidence-Guided Neighborhood Expansion: Learn dynamic neighborhood sizes and probability pooling based on uncertainty to optimize the trade-off between acceptance rate, fidelity, and compute.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.04792" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.04792" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Multi-step inference in state-of-the-art video diffusion models is computationally expensive, making high-quality video generation impractical for latency- and compute-constrained settings.<br>‚Ä¢ Open-source pyramidal video models trained from scratch typically underperform in visual quality compared to SOTA; there is no established way to convert pretrained SOTA models into pyramidal ones without quality loss.<br>‚Ä¢ Step distillation strategies in pyramidal setups are underexplored‚Äîespecially when the teacher itself is pyramidal‚Äîand prior works do not systematically compare pyramidal vs non-pyramidal teachers or assess PPF for few-step generation.<br>‚Ä¢ Existing pyramidal stage-transition operators (e.g., average pooling and nearest-neighbor) are ad hoc; a more general, principled framework is needed to broaden and optimize resolution transitions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Finetune a pretrained video diffusion transformer (Wan2.1-1.3B) into a three-stage spatiotemporal pyramidal model using PyramidalFlow, then study and apply step distillation (distribution matching and adversarial) within this pyramidal setup and with PPF for few-step inference; additionally, generalize stage transitions to arbitrary orthogonal-transform-based up/downsampling, subsuming common pooling/upsampling as special cases.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Orthogonal-Transform Stage Transitions for Pyramidal Video Diffusion: Parameterize and learn orthogonal up/downsampling operators (e.g., wavelet/DCT families) to optimize information preservation and stage coupling across domains.<br>‚Ä¢ Content-Adaptive Pyramidal Scheduling for Few-Step Video Generation: A controller that selects stage schedules, noise ranges, and guidance per prompt/content to balance latency and fidelity under compute budgets.<br>‚Ä¢ Hybrid PyramidalFlow‚ÄìPatchification for On-Device Few-Step Video Synthesis: A unified architecture that combines flow-based pyramidal resolution changes with patch-pyramidal token reduction, co-trained for 1‚Äì2 step generation and mobile deployment.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.04620" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.04620" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ In-agent self-improvement and population-based search produce unstable, hard-to-audit improvement trajectories.<br>‚Ä¢ Lack of non-regression guarantees makes it difficult to detect and prioritize pass‚Üífail regressions and reason about failures across versions.<br>‚Ä¢ Improvements often boost averages while masking brittle per-case behavior, undermining production reliability.<br>‚Ä¢ Existing methods rarely yield reproducible, auditable artifacts or clear engineering specifications for why performance changed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>AgentDevel reframes agent improvement as a regression-aware release engineering pipeline: it runs the current agent, extracts implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate via script-based executable diagnosis, and promotes it under flip-centered gating that prioritizes non-regression (pass‚Üífail) and verified fixes (fail‚Üípass). This maintains a single canonical version line and produces auditable, reproducible artifacts.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Formal Flip-Centered Gating: Statistical Guarantees for Non-Regression in LLM Agent Releases: Develop formal tests and confidence bounds for flip-based gating, including canarying and risk-aware promotion policies.<br>‚Ä¢ Scalable Executable Diagnosis: Learning Domain-General Script Synthesis from Agent Traces: Train models to automatically generate and validate diagnosis scripts that aggregate dominant symptom patterns across tasks and domains.<br>‚Ä¢ Release Engineering for Multi-Agent Systems: Coordinated Non-Regression Pipelines and Cross-Agent Impact Analysis: Extend AgentDevel to multi-agent settings with dependency tracking, cross-agent flip analysis, and coordinated promotion.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.04575" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.04575" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Build a general-purpose, real-time game-playing agent that runs on consumer GPUs, avoiding bespoke RL environments and reward engineering.<br>‚Ä¢ Address behavior cloning‚Äôs core weaknesses‚Äîdistributional shift and causal confusion‚Äîby studying how scaling model depth, parameters, and data improves causal policy learning.<br>‚Ä¢ Overcome the latency and poor control performance of current multimodal LLM/VLM systems in video games.<br>‚Ä¢ Provide an open, reproducible foundation (8300+ hours of human gameplay, code, checkpoints) to accelerate research on game-playing models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Train a behavior-cloned foundation model on 8,300+ hours of diverse 3D game footage, mapping raw image observations to keyboard/mouse actions, with architectures scaled up to ~1.2B parameters optimized for real-time inference on consumer GPUs. Systematically vary data size, model depth/parameters, and training steps‚Äîvalidated first on a toy causal task‚Äîto characterize scaling laws and show that deeper models with more data learn more causal policies.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Counterfactual Behavior Cloning: Data Augmentation to Reduce Causal Confusion in Real-Time Game Agents: Introduce intervention-based and time-aligned counterfactual samples to improve causal identification and robustness in BC.<br>‚Ä¢ Online Adaptation for Behavior-Cloned Game Agents Under Distributional Shift: Lightweight on-policy correction (e.g., DAgger-style, self-training) with safety constraints to maintain performance during deployment.<br>‚Ä¢ Hierarchical Planning atop Scaled Behavior Cloning for Long-Horizon 3D Tasks: Combine short-horizon BC controllers with hierarchical memory/planning modules to tackle tasks requiring strategic reasoning and temporal abstraction.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.04342" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.04342" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Quadratic time and memory of full softmax attention in Diffusion Transformers severely limits scalability to long-duration/high-resolution videos and prevents practical on-device generation.<br>‚Ä¢ Pure linear attention offers linear complexity and RNN-style constant memory but loses fidelity (weaker fine-grained/local dependencies), often requiring costly retraining and yielding lower quality.<br>‚Ä¢ Existing hybrid attention designs remain quadratic and non-recurrent, failing to deliver the compute/memory benefits needed for long video generation.<br>‚Ä¢ Re-training SOTA bidirectional softmax models from scratch with alternative attention is prohibitively expensive; a lightweight distillation path to an efficient recurrent model is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ReHyAt introduces temporally chunked hybrid attention‚Äîsoftmax within overlapping local chunks and linear attention globally‚Äîmade causal to enable an RNN reformulation with linear time and constant memory. A two-stage pipeline distills a bidirectional softmax teacher by learning per-block polynomial kernel maps (œïq, œïk) and then lightly fine-tunes the DiT, achieving near-SOTA quality with ~160 GPU-hours.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ AdaHyAt: Content-Adaptive Hybrid Attention for Long-Form Video Diffusion: Dynamically allocate softmax tokens and overlap per chunk based on motion/complexity to improve fidelity-efficiency trade-offs.<br>‚Ä¢ EdgeHyAt: Quantization- and Sparsity-Aware Recurrent Hybrid Attention for On-Device Video Generation: Combine ReHyAt with low-bit quantization, operator fusion, and learned sparsity to achieve real-time long video generation on mobile/edge.<br>‚Ä¢ CrossHyAt: Hybrid Attention Distillation for Cross-Attention and Multimodal Conditioning in Video DiTs: Extend the distillation recipe to cross-attention and integrate audio/depth/segmentation controls while maintaining linear complexity.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.04300" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.04300" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing post-training alignment methods (e.g., DPO/GRPO) rely on coarse scalar rewards or binary preferences, which cannot capture hierarchical, fine-grained human expertise.<br>‚Ä¢ Binary image-level signals fail to disentangle positive and negative attributes, limiting multi-objective optimization and precise control over generation quality.<br>‚Ä¢ Current approaches lack domain-knowledge injection and attribute-level decoupling in diffusion models, leading to suboptimal alignment‚Äîespecially in specialized domains like painting.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A two-stage framework: first, supervised fine-tuning of an auxiliary diffusion model with expert-defined hierarchical attributes; second, Complex Preference Optimization (CPO), an extension of DPO that aligns the target diffusion model by simultaneously increasing the probability of positive attributes and decreasing that of negative attributes using the auxiliary model‚Äôs guidance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Attribute Trees Automatically for Fine-grained Diffusion Alignment: Induce hierarchical attribute criteria from data via semi-supervised/LLM-based structure learning to reduce expert annotation burden.<br>‚Ä¢ Cross-Domain CPO: Generalizing Complex Preference Optimization to Video and 3D Generative Models: Extend attribute decoupling and CPO to multimodal generative tasks (video, 3D, audio) with domain-specific hierarchies.<br>‚Ä¢ Active Expert Feedback for Continual Fine-grained Alignment of Diffusion Models: Build an interactive, expert-in-the-loop CPO pipeline with active sampling and iterative updates to maintain alignment under domain shifts.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02016" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02016" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Achieving high object detection accuracy typically requires heavy architectures, long training, and large annotated datasets, while existing detectors underuse rich supervisory signals latent in annotations.<br>‚Ä¢ Knowledge distillation mainly targets model compression with identical inputs and does not exploit training-only privileged cues; LUPI is underexplored for object detection and lacks a general, model-agnostic approach.<br>‚Ä¢ There is a need to boost accuracy and generalization across datasets and object scales without increasing inference-time complexity or model size, and to identify which privileged signals and supervision strengths are most effective.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A model-agnostic LUPI teacher‚Äìstudent framework: train a teacher on RGB plus privileged channels (e.g., bounding box masks, saliency, depth) and distill intermediate features to an RGB-only student via a joint detection loss and cosine-distance alignment at a chosen backbone layer. The student keeps the baseline architecture and inference cost while benefiting from privileged-context representations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Privileged Super-Resolution Distillation for Small Object Detection: Use high-resolution crops, multi-scale priors, or synthetic zoom-in privileged channels to target small objects and distill scale-aware features to the student.<br>‚Ä¢ Self-Supervised Privileged Information Generation for Object Detection: Learn pseudo privileged maps (depth, saliency, segmentation, textual rationales) via self-/cross-modal supervision to enable LUPI without extra annotations.<br>‚Ä¢ Curriculum LUPI with Adaptive Alpha and Layer-wise Feature Alignment: Automatically schedule teacher guidance strength and select architecture-specific distillation layers to maximize gains while avoiding over-reliance on privileged cues.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05125" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05125" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ VrDU lacks interpretable, task-aligned analyses of visual embedding spaces, making it hard to assess model feasibility and understand latent structure.<br>‚Ä¢ Synthetic training data are often judged by human photorealism rather than whether they align with the model‚Äôs visual‚Äìsemantic distribution, leading to domain gaps and suboptimal performance.<br>‚Ä¢ Existing methods provide limited tools to locate error-prone regions and guide targeted data augmentation; on-premise models often trail SaaS solutions without principled, model-centric data enhancement.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VERSE extracts frozen visual embeddings, reduces them via PCA into a Reduced Embedding Space, and overlays clusters with human-observed features and per-sample F1 to identify error-inducing regions. It then guides targeted synthetic data generation and retraining to boost performance and generalization, validated on MERIT with Donut and Idefics2.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Automated VERSE: Active, Cluster-Guided Synthetic Data Generation for VrDU: Automate cluster detection and feature conditioning to continuously generate and curate training samples for error-prone regions.<br>‚Ä¢ Nonlinear VERSE: Manifold-Preserving Reduction for Visual Embedding Analysis: Evaluate UMAP/TSNE and contrastive projections to better preserve local/global structure and disentangle features in RES.<br>‚Ä¢ VERSE-MultiPage: Embedding Space Exploration for Multi-Page Document Reasoning: Extend VERSE to sequences of pages, modeling cross-page layout, table flow, and long-range visual‚Äìtextual dependencies.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Learning User Preferences Through Interaction for Long-Term Collaboration</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02702" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02702" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing evaluations of memory-equipped conversational agents emphasize recall of past facts rather than whether agents learn and leverage user interaction preferences to improve collaboration across sessions.<br>‚Ä¢ There is no benchmark that systematically tests multi-session adaptation with realistic user personas and preferences, tied to downstream collaboration outcomes (task success, efficiency, user effort).<br>‚Ä¢ Agents lack mechanisms and training signals to persist, refine, and act on user preference memory over time, reducing repeated user burden and enhancing long-term user experience.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce MULTISESSIONCOLLAB, a multi-session benchmark with realistic user simulators and preferences, and propose long-term collaborative agents with persistent, self-refining preference memory. The agents use learning signals from simulator behavior to produce richer reflections and update memory, improving alignment and collaboration efficiency over repeated sessions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Uncertainty-Aware Preference Memory for Multi-Session Dialogue: Model confidence over inferred preferences and use active elicitation to reduce misalignment and overfitting.<br>‚Ä¢ Privacy-Preserving Continual Preference Learning in Conversational Agents: Enable user-controlled, on-device, and selective retention/forgetting of preference memory under privacy constraints.<br>‚Ä¢ Generalizing Interaction Preferences Across Tasks and Domains: Develop transfer/meta-learning methods so learned interaction preferences adapt robustly to new tasks and contexts without negative transfer.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-11">

    <div class="paper">
        <h2 class="paper-title">Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.01887" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.01887" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Fine-tuning attacks can override safety alignment in LMaaS with minimal cost and few harmful examples, creating serious reliability and liability risks.<br>‚Ä¢ Existing defenses require large safety datasets or calibration sets, incur significant computation, and often degrade downstream utility.<br>‚Ä¢ There is a need for a minimal-cost, fast, and general method to restore safety without sacrificing utility, regardless of model size or harmful fine-tuning scale.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>One-shot safety recovery: patch a compromised LLM by fine-tuning for a few epochs on a single, carefully selected safety instance, formulated via bi-level optimization to target the low-rank safety gradient subspace that opposes harmful updates. Exploiting the low intrinsic dimensionality of alignment signals enables neutralizing harmful gradients with negligible compute and no utility loss.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Automatic Selection of One-Shot Safety Exemplars for LMaaS: Develop algorithms to automatically choose or synthesize the most effective single safety instance using gradient proxies or metadata.<br>‚Ä¢ Theoretical Bounds on One-Shot Safety Patching in Low-Rank Alignment Subspaces: Establish formal conditions, convergence rates, and robustness guarantees for one-shot recovery under adversarial fine-tuning.<br>‚Ä¢ One-Shot Safety Patching for Multilingual and Multimodal LLMs: Extend and evaluate the approach across languages and vision-language models to test generality and constraints.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.04233" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.04233" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Scarcity of large-scale, high-quality multilingual speech corpora with reliable word-level timestamps; existing resources are largely monolingual or small-scale multilingual and lack fine-grained temporal supervision.<br>‚Ä¢ Web-crawled multilingual datasets have highly variable quality and unreliable annotations, limiting their utility for precision-critical generative tasks.<br>‚Ä¢ Generative speech models exhibit alignment drift, accent leakage, and degraded intelligibility in low-resource languages due to weak temporal supervision and data imbalance.<br>‚Ä¢ Traditional forced alignment and language-specific G2P/lexicons are brittle for noisy, in-the-wild, and code-switched data, hindering robust multilingual training and editing.<br>‚Ä¢ Lack of unified benchmarks and resources to evaluate and train zero-shot multilingual TTS and word-level speech editing at scale.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>LEMAS releases a 150k-hour, 10-language dataset with precise word-level timestamps built via a unified pipeline that romanizes text, applies MMS CTC forced alignment, and filters using confidence, duration/pause, speech-rate, and language checks. Using this corpus, the authors train LEMAS-TTS (flow-matching TTS with unified phonetic inputs, CTC alignment and accent-adversarial losses, and prosody conditioning) and LEMAS-Edit (autoregressive masked infilling with repetition penalties, adaptive re-generation, and robust multilingual ASR/align/denoise front-end) for robust zero-shot synthesis and smooth, boundary-accurate editing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Confidence-Aware Multilingual TTS with Word-Level Alignment Priors: Use LEMAS confidence scores for curriculum learning and dynamic loss weighting to further reduce alignment drift and improve cross-lingual intelligibility.<br>‚Ä¢ Scalable Code-Switching and Accent-Control in Flow-Matching TTS: Extend language-tagged phonetics and adversarial disentanglement to explicit, controllable code-switching and accent style transfer.<br>‚Ä¢ Unified Discrete‚ÄìContinuous Generative Speech Models Trained on LEMAS: Combine codec-token autoregression with flow-matching refinement to jointly optimize intelligibility, prosody, and editability in multilingual settings.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24160" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24160" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Automated optical inspection (AOI) still suffers from high false-alarm rates, poor adaptability to novel/rare defects, and limited generalization across diverse manufacturing contexts.<br>‚Ä¢ Existing industrial datasets are small, narrow in domain coverage, and lack multimodal (image‚Äìtext) annotations, hindering vision‚Äìlanguage alignment and fine-grained, interpretable defect understanding.<br>‚Ä¢ General-purpose VLMs trained on natural images miss subtle, localized industrial defects and domain-specific terminology, limiting open-vocabulary capability and semantic grounding.<br>‚Ä¢ Specialized detectors (e.g., YOLO variants) require costly pixel-level labels, struggle with long-tail defects, and do not unify discriminative and generative capabilities.<br>‚Ä¢ There is a need for data-efficient, adaptable foundation models that can be fine-tuned with minimal task-specific data for scalable industrial inspection and synthesis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>They introduce IMDD-1M (1.24M expert-verified image‚Äìtext pairs across 63 domains and 421 defect types) and train from scratch a text-conditioned diffusion U-Net with an implicit captioner and a Mask2Former-based mask generator to unify generation with open-vocabulary segmentation/detection. The model enables data-efficient adaptation to new industrial tasks, leveraging cross-attention text conditioning and pseudo text embeddings when captions are absent.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Parameter-Efficient Domain Adaptation of Industrial Diffusion VLMs: Explore adapters/LoRA and task arithmetic to adapt the foundation model to new factories and defect taxonomies with <1% labeled data.<br>‚Ä¢ Causally-Grounded Vision‚ÄìLanguage Defect Analysis with Process Metadata: Integrate process logs, materials, and machine settings to enable root-cause reasoning, counterfactuals, and actionable recommendations.<br>‚Ä¢ Robustness, Uncertainty, and Open-Set Safety for Industrial VLMs: Develop evaluation suites and training methods for OOD detection, calibration, and safety under sensor, illumination, and distribution shifts.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 11</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button><button class="page-btn" onclick="goToPage(11)">11</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2026-01-09 23:09:02 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 11;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>