<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - December 18, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">December 18, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Step-GUI Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.15431" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.15431" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Efficiently acquiring reliable, high-quality training data for multi-turn GUI agents; traditional step-level annotations are subjective, costly, and hard to scale, while model-generated labels lack verifiable ground truth.<br>â€¢ Standardizing LLMâ€“device interaction across heterogeneous platforms (Android/iOS/Windows/macOS/Ubuntu) and protecting user privacy during GUI automation.<br>â€¢ Evaluating agents on ecologically valid, everyday mobile scenarios; existing benchmarks skew toward static or synthetic tasks and under-cover high-frequency applications.<br>â€¢ Bridging general multimodal competence with specialized GUI agency, ensuring models retain broad knowledge while learning precise grounding and action execution.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A self-evolving training pipeline centered on the Calibrated Step Reward System (CSRS) that calibrates trajectory-level success/failure signals and extracts rich LLM-generated reasoning to create reliable multi-dimensional training data within Mid-Train, Cold-Start, and RLVR stages. The work also introduces Step-GUI models, a hierarchical GUI-MCP protocol for standardized and privacy-preserving deployment, and AndroidDaily for realistic evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Federated GUI-MCP: Privacy-Preserving Cross-Device Coordination for GUI Agents: Extend GUI-MCP with federated learning and secure aggregation to train and operate agents entirely on-device.<br>â€¢ World-Modeling for GUIs: Learning Latent State Dynamics and Affordances: Develop explicit latent world models that predict GUI state transitions and functional affordances to improve planning and grounding.<br>â€¢ Adaptive CSRS for Continual App Evolution: A Data Flywheel for Nonstationary GUI Environments: Make task generation, calibration, and curricula adaptive to app/version drift for continual learning.<br>â€¢ AndroidDaily++: Personalization and Long-Horizon Mobile Tasks at Scale: Expand the benchmark with user-specific preferences, multi-app workflows, and longer task horizons to stress real deployments.<br>â€¢ Trustworthy GUI Agents: Formal Verification and Auditing within GUI-MCP: Integrate formal verifiers and audit logs to ensure safe, compliant, and reproducible GUI actions across platforms.<br>â€¢ Accessibility-Aware GUI Agents: Fusing Accessibility Trees with Visual Perception: Combine structured accessibility signals with vision to enhance robustness, efficiency, and inclusivity in GUI control.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DEER: Draft with Diffusion, Verify with Autoregressive Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.15176" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.15176" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Autoregressive (AR) decoding is a latency bottleneck for long-context, agentic, and reasoning tasks.<br>â€¢ Existing speculative decoding with AR drafters suffers from step-wise uncertainty accumulation (collapse of trust), sharply limiting acceptance length and speedup.<br>â€¢ AR drafters decode sequentially and cannot exploit parallel generation, capping achievable throughput.<br>â€¢ Naively using diffusion LLMs (dLLMs) for drafting causes distribution mismatch (trained for global generation, not prefix-conditioned continuation), leading to poor verifier alignment.<br>â€¢ Prior diffusion-based drafting often uses continuous, multi-step denoising with weak temperature control and drift from the AR verifier.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DEER drafts with a discrete-space diffusion LLM using one-step, blockwise generation and verifies via standard AR rejection sampling; a two-stage alignment (AR-style continuation distillation with SEP tokens, then exponentially weighted suffix refinement) adapts the dLLM to prefix-conditioned continuation, enabling long accepted blocks and higher speedups.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Verifier-Aware Training for Diffusion Drafters: Jointly optimize dLLMs with acceptance-aware objectives to directly maximize verified tokens per step under AR verifiers.<br>â€¢ Dynamic Block Scheduling for Speculative Diffusion Decoding: Learn to adapt draft block sizes online based on prefix uncertainty and verifier feedback to balance acceptance rate and throughput.<br>â€¢ Systems Kernels for Discrete Diffusion Drafting with KV Cache: Develop fused GPU kernels and cache-friendly runtimes to efficiently integrate dLLM block drafting with AR KV-cached verification at scale.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Fast and Accurate Causal Parallel Decoding using Jacobi Forcing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14681" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14681" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ AR (autoregressive) decoding is inherently sequential, causing high latency and poor utilization of modern accelerators for long outputs.<br>â€¢ dLLMs-based parallel decoding underperforms and adapts AR models via bidirectional attention and NELBO, creating a pretrain-to-posttrain mismatch that conflicts with causal priors and hinders exact KV cache reuse.<br>â€¢ Masked post-training data distributions deviate from natural pretraining distributions, making AR-to-dLLM adaptation costly and fragileâ€”especially at large block sizes where quality drops and speedup fails to scale.<br>â€¢ Existing consistency-distillation methods for Jacobi decoding rarely increase the number of correct tokens per iteration as block size grows and require many forward/backward passes, limiting practical acceleration.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Jacobi Forcing progressively distills AR models on their own Jacobi trajectories using a cyclic progressive noise schedule and a progressive consistency loss computed with noise-aware causal attention and sequence packing (reducing passes to O(1)), combined with an AR loss to preserve quality; it is iteratively refined with trajectories from progressively larger block sizes and paired with rejection recycling and multi-block decoding to boost acceptance per iteration and wall-clock speed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Adaptive Noise Schedules for Jacobi Forcing: Automatically optimize per-block noise ratios and window sizes to maximize token acceptance and stability across tasks and model scales.<br>â€¢ KV Cache-Aware Multi-Block Jacobi Decoding for Distributed LLM Inference: Design cache sharing and scheduling policies that exploit causal attention to minimize memory movement and maximize throughput in multi-block parallel decoding.<br>â€¢ Convergence and Error Bounds for Noise-Aware Causal Parallel Decoding: Provide theoretical guarantees on fixed-point convergence, acceptance rates, and quality under progressive distillation and rejection recycling.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14052" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14052" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ On-device MLLMs suffer prohibitive latency and memory from ViTâ€™s quadratic complexity on high-resolution inputs common in UI/OCR/document scenarios.<br>â€¢ Large numbers of visual tokens inflate LLM compute and KV-cache footprint, blocking deployment on memory- and power-constrained mobile devices.<br>â€¢ Existing speedups (e.g., aggressive CNN downsampling/token reduction) often degrade accuracy and fine-grained perception.<br>â€¢ Lack of adaptive, content-aware resolution control causes redundant high-resolution computation on simple/low-information images.<br>â€¢ Single, fixed-capacity visual encoders cannot balance accuracy vs. latency across diverse tasks/devices; dynamic switching needs semantic consistency across branches.<br>â€¢ Practical, plug-and-play accelerators and system-level validation on real devices are missing in many prior works.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>HyperVL combines image tiling with a lightweight Visual Resolution Compressor that predicts per-image scaling (10â€“100%) to reduce visual tokens and latency. Dual Consistency Learning aligns SigLIP2-Base/Large encoders to a shared Qwen3-1.7B LLM via alternating training and distillation, enabling dynamic switching between fast and accurate visual branches with minimal performance loss.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Task-Aware Visual Resolution Scheduling for On-Device Multimodal LLMs: Learn a policy that jointly selects compression ratios and encoder branches given task/device budgets to meet accuracy under latency constraints.<br>â€¢ Self-Supervised Resolution Compressors for Multimodal Perception: Remove supervised labeling by using contrastive or RL signals to predict optimal scaling without teacher MLLM sweeps.<br>â€¢ Mixture-of-ViTs: A Router-Based Multi-Encoder Framework for Mobile VLMs: Generalize DCL to multiple visual experts with a learned router for finer accuracyâ€“efficiency trade-offs.<br>â€¢ Compression-Aware KV-Cache and Decoder Co-Design for Edge VLMs: Jointly optimize visual token compression with KV-cache management and speculative decoding to minimize end-to-end latency.<br>â€¢ Overlap-Aware Tiling and Cross-Tile Attention for Document VLMs: Use adaptive tile sizes/overlaps with lightweight cross-tile attention to retain long-range context at low memory.<br>â€¢ Quantization-Aware Dual Consistency Learning for 4-bit Mobile VLMs: Integrate QAT/PTQ into DCL to preserve cross-branch alignment under aggressive quantization.<br>â€¢ Federated Learning of Adaptive Resolution and Routing Policies in Privacy-Preserving VLMs: Train VRC and branch routers from on-device signals without exporting user data.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Puzzle Curriculum GRPO for Vision-Centric Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14944" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14944" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Obtaining verifiable, vision-centric rewards for RL post-training is costly and noisy, often requiring hand-curated labels or external verifiers<br>â€¢ Vanilla GRPO suffers from flat and sparse rewards and vanishing group-relative advantages, leading to weak learning signals and difficulty-agnostic optimization<br>â€¢ Chain-of-thought failures in VLMsâ€”shortcutting, overthinking, and reasoningâ€“answer inconsistencyâ€”persist, with RAC typically degrading as training progresses<br>â€¢ Benchmark noise (label errors, underspecified prompts) obscures true progress and evaluation in visual reasoning</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>PC-GRPO is a supervision-free RLVR framework that replaces annotations with verifiable self-supervised puzzles (PatchFit, Rotation, Jigsaw with graded partial-credit rewards) and applies a difficulty-aware curriculum that emphasizes medium difficulty via reward variance and permutation diversity, while monitoring and boosting Reasoningâ€“Answer Consistency to improve stability and downstream accuracy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Puzzle Generation for Curriculum RLVR in VLMs: Automatically synthesize and schedule puzzles with controllable difficulty to maximize group-relative advantages and mitigate reward sparsity<br>â€¢ Consistency-Optimized GRPO for Vision-Language Models: Directly integrate RAC-based signals into the optimization objective (e.g., via consistency-aware rewards or constraints) to couple reasoning traces and final answers<br>â€¢ Scalable VLM Committee Auditing for Clean Visual Reasoning Benchmarks: Build automated auditor ensembles to detect and correct label noise and underspecified prompts, producing verifiable, high-fidelity evaluation sets</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Universal Reasoning Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14693" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14693" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Clarify the true sources of performance gains in Universal Transformer (UT) variants on ARC-AGI/Sudoku, disentangling recurrent inductive bias and nonlinearity from elaborate architectural designs.<br>â€¢ Address the inefficiency of vanilla Transformers for multi-step, algorithmic reasoning where scaling depth/width yields diminishing returns under fixed parameter/FLOPs budgets.<br>â€¢ Improve UT optimization stability when many recurrent loops are used, mitigating noisy/unstable long-horizon gradients.<br>â€¢ Enhance expressivity of UT transitions by introducing effective local token mixing, as conventional point-wise MLPs may underuse short-range context.<br>â€¢ Provide a parameter-efficient, small model trained from scratch that achieves state-of-the-art performance without relying on test-time scaling or ensembling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>URM is a decoder-only Universal Transformer enhanced with ConvSwiGLUâ€”adding a depthwise short 1D convolution (k=2) after MLP expansion to strengthen nonlinear, local token mixingâ€”and Truncated Backpropagation Through Loops (TBPTL), which backpropagates only through later recurrent iterations to stabilize and accelerate training while preserving iterative refinement.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning to Truncate: Adaptive Backpropagation Horizons for Universal Transformers: Develop data- and step-dependent policies (e.g., via meta-learning or ACT signals) to dynamically choose gradient truncation windows per sequence and training phase.<br>â€¢ Globalâ€“Local Nonlinearity Fusion for Iterative Reasoning: Systematically combine ConvSwiGLU with alternative token-mixing modules (e.g., larger-k depthwise convs, dilated/shifted convs, RWKV-like time-mix) and study optimal placements within UT transitions.<br>â€¢ Scaling Laws of Recurrent Inductive Bias in Reasoning Models: Quantify trade-offs between loops, ACT budgets, parameters, and FLOPs to map regimes where recurrence outperforms added depth/width, guiding compute allocation for algorithmic tasks.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.15635" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.15635" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We propose IC-Effect, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning 15 high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Achieve automated video VFX editing that injects complex effects while strictly preserving the original background and temporal coherenceâ€”a requirement unmet by current video editing/generation models.<br>â€¢ Remove dependence on pixel-accurate masks; existing mask-based methods conflict with automation and instruction-driven localization.<br>â€¢ Learn diverse, physically coherent effect patterns under limited paired data, addressing the scarcity of high-quality VFX editing datasets.<br>â€¢ Reduce the quadratic computational cost of DiT conditioning; direct concatenation of full-resolution condition tokens is inefficient and leads to fidelity degradation due to noiseâ€“condition mixing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>IC-Effect uses DiT-based in-context conditioning by treating the source video as clean spatiotemporal condition tokens under a causal attention mask, combined with a two-stage LoRA strategy (high-rank for general editing/instruction following, low-rank Effect-LoRA for per-effect style) and spatiotemporal sparse tokenization with position correction to preserve structure and motion while cutting compute.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Real-Time IC-Effect: Streaming DiT with Adaptive Sparse Conditioning for Online Video VFX Editing: Design a streaming DiT and dynamic token sparsification to enable low-latency, frame-by-frame effect injection.<br>â€¢ Cross-Effect Generalization: Meta-Learning Unified Editors for Few-Shot Video VFX: Develop meta-learning or hypernetwork-based LoRA to generalize across effect families and minimize per-effect fine-tuning.<br>â€¢ Physics-Aware VFX Editing: Integrating Physical Priors and Simulation into DiT In-Context Learning: Incorporate physics-informed constraints (e.g., fluid, particle, fire dynamics) to improve realism and scene-aware effect interaction.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.15693" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.15693" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Misuse of AI-driven video generation threatens social safety, creating urgent need for reliable and explainable detectors.<br>â€¢ Existing detectors are mostly binary classifiers without human-interpretable, grounded evidence, and they miss subtle, physics-violating artifacts.<br>â€¢ General MLLMs achieve near-random performance, overemphasize superficial cues (e.g., quality, lighting), and lack fine-grained spatio-temporal reasoning.<br>â€¢ Current datasets/benchmarks suffer from realâ€“fake discrepancies (duration/FPS, domain/style), limited diversity, vague artifact taxonomies, and scarce valid annotationsâ€”promoting shortcut learning and unfair evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Skyra is a specialized video MLLM trained to identify human-perceivable artifacts and use spatio-temporally grounded evidence (timestamps and bounding boxes) for both detection and explanation. It leverages the ViF-CoT-4K dataset for supervised fine-tuning and a second-stage reinforcement learning to mine discriminative artifacts, and is evaluated on the aligned ViF-Bench.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Physics-Aware Artifact Detection in Generative Videos: Formalize and learn physics-grounded priors to detect causality and commonsense violations across evolving video generators.<br>â€¢ Multi-Modal Grounded Artifact Reasoning with Audio-Visual-Text Alignment: Integrate audio and prompt/context cues into Skyra-style spatio-temporal grounding to improve subtle artifact perception and explanations.<br>â€¢ Continual RL Adaptation for Open-World AI-Generated Video Forensics: Develop online, domain-adaptive RL pipelines that continually adapt detectors to unseen generators while resisting shortcut signals.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.15603" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.15603" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose Qwen-Image-Layered, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling inherent editability, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on https://github.com/QwenLM/Qwen-Image-Layered{https://github.com/QwenLM/Qwen-Image-Layered}</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Pixel-space entanglement in raster images causes semantic drift and geometric misalignment during editing, making it hard to preserve unedited content.<br>â€¢ Global editing approaches resample the whole image and inherit stochastic inconsistencies; mask-guided local editing struggles with occlusions and soft boundaries, making the true edit region ambiguous.<br>â€¢ Prior decomposition methods rely on segmentation/matting/inpainting and recursive inference, leading to error propagation, poor alpha quality, and limits to simple foreground/background splits.<br>â€¢ Separate RGB/RGBA encoders create a latent distribution gap between inputs and outputs, hindering stable end-to-end decomposition.<br>â€¢ Lack of high-quality, real-world multilayer (RGBA) training data; existing datasets are synthetic or have simple layouts with few semi-transparent layers.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Qwen-Image-Layered is an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers via a shared RGBA-VAE latent space and a Variable Layers Decomposition MMDiT with Layer3D RoPE for variable-length outputs. A multi-stage curriculum (Text-to-RGB/RGBA â†’ Text-to-Multi-RGBA â†’ Image-to-Multi-RGBA) and a PSD-derived multilayer dataset enable robust training and high-fidelity, inherently editable layers.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Layered Video Decomposition: Temporally consistent RGBA layers for video editing using temporal extensions of Layer3D RoPE and cross-frame attention.<br>â€¢ Unsupervised Multilayer Decomposition in the Wild: Self-supervised recomposition and cycle-consistency losses to learn layered decompositions without PSD supervision.<br>â€¢ Physics-Aware Layer Decomposition: Joint estimation of alpha, albedo, and illumination to better handle semi-transparent/reflective materials and improve editability.<br>â€¢ Layer-Aware Text-to-Image Generation: Unified model that generates explicitly controllable, semantically disentangled layers directly from prompts for fine-grained design workflows.<br>â€¢ Benchmarks and Metrics for Layer Editability: A standardized dataset and metrics assessing alpha fidelity, inter-layer disentanglement, and edit stability under common edit operations.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Robust and Calibrated Detection of Authentic Multimedia Content</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.15182" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.15182" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Generative models can synthesize highly realistic content, so-called deepfakes, that are already being misused at scale to undermine digital media authenticity. Current deepfake detection methods are unreliable for two reasons: (i) distinguishing inauthentic content post-hoc is often impossible (e.g., with memorized samples), leading to an unbounded false positive rate (FPR); and (ii) detection lacks robustness, as adversaries can adapt to known detectors with near-perfect accuracy using minimal computational resources. To address these limitations, we propose a resynthesis framework to determine if a sample is authentic or if its authenticity can be plausibly denied. We make two key contributions focusing on the high-precision, low-recall setting against efficient (i.e., compute-restricted) adversaries. First, we demonstrate that our calibrated resynthesis method is the most reliable approach for verifying authentic samples while maintaining controllable, low FPRs. Second, we show that our method achieves adversarial robustness against efficient adversaries, whereas prior methods are easily evaded under identical compute budgets. Our approach supports multiple modalities and leverages state-of-the-art inversion techniques.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Post-hoc deepfake detectors are unreliable because resynthesis indistinguishability makes many real images inseparable from synthetic ones, leading to unbounded false positive rates when high trust is required.<br>â€¢ Existing detectors lack robustness and generalizationâ€”adversaries can evade them with minimal compute, and they fail under distribution shifts, common social-media transformations, and targeted perturbations.<br>â€¢ Watermarking requires generator modifications and is vulnerable to removal and distribution-level attacks; thus a calibrated, high-precision (bounded-FPR) alternative to binary real/fake decisions is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes a resynthesis-based Authenticity Index that uses reconstruction-free inversion into modern generators and aggregates pixel-level, structural, and semantic similarity metrics to decide whether an input is authentic or plausibly deniable, with thresholds calibrated to control FPR. It evaluates robustness with adversarial objectives tailored to the inversion pipeline and demonstrates modality-agnostic extensions (including video) and better generalization to real-world data.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Certified Robust Plausible Deniability Detection via Reconstruction-Free Inversion: Develop provable robustness guarantees for the A-index under compute-bounded adversaries and common perturbations.<br>â€¢ A Multimodal Authenticity Index for Audioâ€“Videoâ€“Image Streams: Extend the calibrated resynthesis framework to audio and fully multimodal content with cross-modal similarity metrics and joint inversion.<br>â€¢ Integrating Provenance Signals with the Authenticity Index for End-to-End Trust: Combine watermarking/provenance (e.g., C2PA) with inversion-based plausibility to achieve complementary, calibrated authenticity assessments.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13874" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13874" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in a single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling any-horizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing video reasoning models mostly follow a single-turn (DIRECT) paradigm, processing many frames at once and lacking adaptive, any-horizon behavior, which is resource-intensive and inflexible for tasks of varying difficulty and video length.<br>â€¢ Collecting high-quality QnA data for long videos is costly and slow; bottom-up subclip-based synthetic pipelines are resource-heavy and struggle to ensure coverage across entire videos.<br>â€¢ Current agent systems over-rely on temporal grounding and existing RL recipes (often MCQ/string overlap) are ill-suited for open-ended video tasks; there is a need for robust RL with verifiable rewards and realistic benchmarks capturing long, entertainment-style videos.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SAGE trains an orchestrator VLM (SAGE-MM) to adapt between single-turn answering and multi-turn tool-driven reasoning (web search, speech transcription, segment-level temporal grounding and frame extraction), enabling any-horizon video reasoning. It uses Gemini-2.5-Flash to synthesize QnA and tool trajectories for cold-start SFT, then applies GRPO-based RL with multi-reward signals and an LLM-as-a-judge to instill flexible, open-ended reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ SAGE++: Hierarchical Any-Horizon Planning with Coarse-to-Fine Temporal Grounding: Learn a hierarchical planner that first narrows coarse time windows, then performs fine-grained event grounding to reduce search space and improve efficiency.<br>â€¢ VeriVideo: Verifiable Reward Models for Open-Ended Long-Video QA: Develop multimodal reward models that judge answer correctness and evidence alignment without external LLMs, enabling scalable RL on open-ended tasks.<br>â€¢ StreamSAGE: Real-Time Any-Horizon Agents for Live and Streaming Video: Extend agent policies to low-latency, streaming settings with incremental reasoning, adaptive tool scheduling, and partial-observation handling.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.15687" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.15687" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Exploration signals in LLM RL are extrinsic (entropy bonuses, outcome rarity, external semantic comparators) and misaligned with the modelâ€™s optimization geometry, yielding surface-level diversity without novel update directions.<br>â€¢ This mismatch leads to diffuse and fragile exploration under sparse/verifiable rewards common in math and general reasoning, harming sample efficiency, stability, and PPO-style training dynamics with KL control.<br>â€¢ External semantic similarity can misjudge learning value: semantically novel trajectories may add no new gradient information, while semantically similar ones may be crucialâ€”exposing a semanticâ€“optimization mismatch in current methods.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>G2RL augments GRPO with gradient-guided exploration by constructing sequence-level features from the modelâ€™s final-layer first-order sensitivity (obtainable from a standard forward pass) and upweighting trajectories that introduce novel gradient directions via a bounded groupwise reward scaler. It integrates seamlessly with PPO-style clipping and KL control, requiring no extra backward passes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Curvature-Aware G2RL: Second-Order Geometry for Stable LLM Exploration: Extend G2RL with Hessian/Fisher information to account for curvature, enabling more stable and informative gradient-space exploration under tight KL constraints.<br>â€¢ Step-Level Gradient Diversity for Chain-of-Thought Credit Assignment: Decompose sequence-level sensitivity into step-wise contributions to reward trajectories that add novel gradient directions at critical reasoning steps, improving credit assignment and sample efficiency.<br>â€¢ Cross-Model Gradient-Guided Exploration for Teacherâ€“Student RLHF: Align exploration across teacher and student models by sharing or distilling gradient-space features, enhancing guidance quality while maintaining semantic coherence and stability.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13884" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13884" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a dataset-creation pipeline that scales the teacher-student paradigm to 91 languages and 25 scripts. Building on FineWeb-Edu, our approach trains regression models to identify NER-relevant passages and annotates them with multilingual LLMs, resulting in about 225k passages with 235k distinct entity labels. Our experiments show that the regression model achieves more than 84 F1, and that models trained on FiNERweb obtain comparable or improved performance in zero shot transfer settings on English, Thai, and Swahili, despite being trained on 19x less data than strong baselines. In addition, we assess annotation quality using LLM-as-a-judge and observe consistently high scores for both faithfulness (3.99 out of 5) and completeness (4.05 out of 5), indicating reliable and informative annotations. Further, we release the dataset with both English labels and translated label sets in the respective target languages because we observe that the performance of current state-of-the-art models drops by 0.02 to 0.09 F1 when evaluated using target language labels instead of English ones. We release FiNERweb together with all accompanying artifacts to the research community in order to facilitate more effective student-teacher training for multilingual named entity recognition.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of a reusable, scalable multilingual NER dataset that combines broad language/script coverage with a rich, fine-grained label space; existing human-labeled resources trade off one dimension (e.g., PAN-X vs DynamicNER).<br>â€¢ Prompt-based NER with LLMs is inefficient and misaligned with token-level extraction (per-label prompts, costly postprocessing); need synthetic supervision for compact student models.<br>â€¢ Prior synthetic NER datasets are ad hoc by-products without systematic pipeline or quality controls, limiting reproducibility and reuse.<br>â€¢ Web-scale corpora are noisy; no automatic filtering to select passages rich in diverse entities and suitable for NER annotation.<br>â€¢ Cross-lingual label translation reduces label separability and hurts performance under classical losses, revealing limitations in current training objectives.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A three-stage pipeline: (1) use LLM preference ratings to train a multilingual XLM-R regression model that scores passage usefulness for NER, (2) filter FineWeb-2 into high-quality chunks per language, and (3) annotate with GPT-4o mini and Gemma3-27B, align and semantically merge spans, then translate label sets to target languages and release confidence-partitioned datasets and artifacts.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Label-Semantics-Aware Objectives for Multilingual Universal NER: Design losses and concept-ID mappings that treat translated, semantically equivalent labels as shared positives to mitigate negative sampling errors.<br>â€¢ Confidence-Guided Curriculum and Re-Annotation for Long-Tail Entity Types: Leverage FiNERwebâ€™s confidence splits to upsample hard cases and trigger targeted LLM/human re-annotation to improve completeness.<br>â€¢ Script- and Segmentation-Robust NER Architectures for Low-Resource Languages: Develop preprocessing and model designs resilient to unsegmented scripts (e.g., Thai) and tokenization variability to ensure balanced multilingual transfer.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.10863" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.10863" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ There is no comprehensive, human-validated benchmark that holistically evaluates video-based spatial intelligence in MLLMs across perception, planning, prediction, and cross-video reasoning using continuous visual inputs.<br>â€¢ Spatial understanding over continuous video is critical for embodied, general-purpose assistants; without rigorous evaluation, progress and comparisons are unreliable.<br>â€¢ Existing benchmarks largely use single/multi-image inputs or narrow video tasks with template-generated questions, leading to limited temporal reasoning, reduced diversity, and template bias/overfitting.<br>â€¢ Prior datasets lack coverage of planning, long-horizon prediction, multi-view integration, and memory update, and have limited scene and source diversity, weakening real-world generalization.<br>â€¢ Typical practices (e.g., standard frame-sampling strategies, naive 3D cues, chain-of-thought prompting) and spatial fine-tuning have unclear or poor transfer, and the field lacks diagnostic insights into failure modes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces MMSI-Video-Bench, a fully human-authored, multi-choice benchmark spanning Perception (Spatial Construction), Motion Understanding, Planning, Prediction, and Cross-Video Reasoning, with 1,106 rationale-backed questions over 1,278 clips from 25 datasets and in-house videos, plus three domain sub-benchmarks (Indoor Scene Perception, Robot, Grounding). It provides rigorous multi-stage reviews, diverse data coverage, and comprehensive evaluation of 25 MLLMs with fine-grained error analyses, revealing a large humanâ€“AI gap and systematic weaknesses in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Reasoning-Aware Frame Selection for Video MLLMs: Learn question-conditioned, evidence-centric frame sampling to outperform generic strategies (e.g., AKS) on reasoning-intensive tasks in MMSI-Video-Bench.<br>â€¢ Spatio-Temporal Memory Networks for Cross-Video Reasoning: Develop persistent, queryable memory and correspondence modules to handle multi-view integration and temporal discontinuities for memory update and cross-video matching.<br>â€¢ Geometry-Grounded Video LLMs with Learnable 3D Priors: Fuse differentiable 3D scene representations/scene graphs with language models to improve global spatial construction and geometric reasoning beyond naive 3D cue injection.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.15713" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.15713" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Diffusion vision-language models (dVLMs) underperform advanced autoregressive VLMs due to weaker base diffusion LLMs, leading to a large performance gap on multimodal benchmarks.<br>â€¢ Existing dVLMs lack variable-length generation and efficient KV-cache reuse, resulting in inferior practical inference speed compared to AR-VLMs.<br>â€¢ Prior diffusion approaches in text rarely scale to large VLMs and often require architectural changes or complex training (e.g., annealing), limiting applicability.<br>â€¢ Training high-performance dVLMs typically demands massive data; a data-efficient method that leverages strong AR models is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DiffusionVL introduces a simple diffusion finetuning that converts any autoregressive model (AR-VLM or AR-LM) into a block diffusion VLM without architectural changes, using block-wise noise, a hybrid attention mask (intra-block bidirectional, inter-block causal), and block decoding with KV-cache reuse. For AR-VLMs it directly applies end-to-end diffusion finetuning; for AR-LMs it first aligns vision-text via AR training of the connector, then performs diffusion finetuning to complete modality and paradigm shift.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Hybrid ARâ€“Diffusion Decoding for Vision-Language Models: Dynamically switch between autoregressive and block diffusion decoding based on content complexity to optimize latencyâ€“quality trade-offs.<br>â€¢ Scaling Laws and Data Efficiency in DiffusionVL: A systematic study of model size, block size, denoising steps, and training data curricula to reach or surpass AR-VLM performance with minimal data.<br>â€¢ DiffusionVL for Video and Multi-Image Reasoning: Extend block diffusion to temporal and multi-image settings with cross-block temporal attention and cache sharing for efficient long-context multimodal generation.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.12072" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.12072" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLM-synthesized datasets often lack semantic diversity, exhibiting redundancy and mode collapse even under aggressive sampling.<br>â€¢ Sampling/decoding strategies act locally on next-token probabilities, lack a global view of dataset diversity, and typically require access to logits (incompatible with many closed models).<br>â€¢ Prompt-based diversity control relies on domain expertise, is hard to scale, and often yields topical rather than true semantic diversity.<br>â€¢ Post-training/RL methods that encode diversity are computationally expensive and restricted to open-weight models.<br>â€¢ There is a need for a principled, scalable, training-free approach that optimizes global diversity and works with black-box LLMs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>VOYAGER is a training-free, iterative framework that maximizes diversity by modeling it as the volume (determinant) of a kernel similarity matrix: it accepts generations that yield high marginal determinant gain relative to a fixed-size anchor set and prunes anchors via k-DPP sampling. Rejected samples trigger textual-gradient prompt refinement to spawn diverse â€œexplorers,â€ enabling closed-model compatibility without access to weights or logits.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Kernel Learning for DPP-Guided LLM Data Generation: Learn task- and domain-specific embedding/kernels that better capture semantic diversity for VOYAGER's volume objective.<br>â€¢ Quality-Diversity Optimization via Multi-Objective DPPs: Jointly optimize semantic diversity and output quality/faithfulness using constrained or weighted DPP formulations.<br>â€¢ Streaming Guarantees for Anchor-Set DPP Pruning: Provide theoretical bounds linking k-DPP anchor maintenance to maximum-volume submatrix and effective-rank guarantees in streaming generation.<br>â€¢ Self-Tuning VOYAGER: Bandit-Based Control of Thresholds and Beams: Automatically adapt Ï„, k, b, and batch size using online learning to balance acceptance rate, diversity, and cost.<br>â€¢ VOYAGER-MM: Training-Free Diverse Generation for Multimodal Data: Extend the framework to textâ€“image/audio/video datasets with cross-modal kernels and multimodal explorers.<br>â€¢ Determinant-Free Marginal Gain Estimation at Scale: Develop low-rank, NystrÃ¶m, or Hutchinson-based estimators for fast marginal gain without explicit determinant computations.<br>â€¢ Bias-Aware Diverse Synthesis with Fairness-Constrained DPPs: Incorporate fairness and coverage constraints so diversity gains do not amplify demographic or topical biases.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">End-to-End Training for Autoregressive Video Diffusion via Self-Resampling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.15702" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.15702" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Exposure bias in autoregressive video diffusion: trainâ€“test mismatch under teacher forcing causes errors to compound across time, degrading quality and collapsing long rollouts.<br>â€¢ Lack of end-to-end, teacher-free training: existing post-training methods (e.g., Self Forcing) depend on bidirectional teachers or online discriminators, hindering scalable training from scratch and risking future information leakage that breaks strict causality.<br>â€¢ Inefficient long-horizon attention: dense causal attention scales with history length; common sliding-window heuristics ignore varying relevance of past frames, weakening global consistency and increasing drift in long videos.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Resampling Forcing: a teacher-free, end-to-end training scheme that self-resamples history by continuing denoising from a sampled timestep with the online model (gradients detached), then trains per-frame diffusion under a sparse causal mask; coupled with parameter-free, dynamic topâ€‘k history routing to maintain near-constant attention complexity over long horizons.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learned Error Schedulers for Self-Resampling in Autoregressive Video Diffusion: Replace fixed logit-normal timestep sampling with a learned policy that adapts resampling strength to model competence and scene dynamics.<br>â€¢ Causality-Preserving Distillation without Future Leakage for Long-Video Generation: Design distillation objectives and architectures that enforce strict temporal causality while transferring quality from powerful teachers.<br>â€¢ Memory-Efficient Long-Horizon Video Diffusion via Adaptive History Routing: Extend dynamic topâ€‘k routing with content- and task-aware sparsity control and multi-granular descriptors to further reduce compute while preserving global consistency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VABench: A Comprehensive Benchmark for Audio-Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.09299" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.09299" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, a comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (text-video, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide a systematic analysis and visualization of the evaluation results, aiming to establish a new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Absence of a systematic, joint benchmark for synchronous audioâ€“video generation; existing evaluations largely focus on visual quality and neglect audio and cross-modal aspects.<br>â€¢ Existing AV benchmarks have limited dimensions and scenarios, overlooking multimodal coupling phenomena (e.g., Doppler effects, emotional synergy across modalities, alignment of background music with visual rhythm).<br>â€¢ Current evaluations often rely on reference-based V2A settings and manual assessment, which are unsuitable for T2AV/I2AV and hinder scalability; they also lack mechanisms to assess triangular consistency among text, video, and audio.<br>â€¢ Stereo audioâ€”now common in AV modelsâ€”is insufficiently evaluated, with spatial acoustic properties and sound field rendering largely ignored.<br>â€¢ These gaps are critical as new models (e.g., Sora 2, Veo 3, Wan 2.5) aim for synchronized, realistic audioâ€“video generation; robust evaluation must balance quality, cross-modal semantic consistency, physical plausibility, and emotional expressiveness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces VABench, a comprehensive benchmark for audioâ€“video generation covering T2AV, I2AV, and stereo tasks, with a curated test set spanning seven content categories and 15 fine-grained metrics (8 expert-model based, 7 MLLM-based) assessing cross-modal similarity, synchronization, lipâ€“speech consistency, QA-based semantics, and dual-channel stereo spatial rendering. Data are filtered via human workers and LLMs, enabling scalable, reference-free, multi-dimensional evaluation tailored to synchronous audioâ€“video outputs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning to Optimize Audioâ€“Video Generators with VABench-Driven Feedback: Use VABench metrics as rewards (e.g., RLHF or proxy losses) to directly improve synchronization, cross-modal consistency, and spatial audio rendering.<br>â€¢ Stereo-Aware Diffusion for Spatially Consistent Audioâ€“Video Generation: Develop generative architectures that jointly model video motion and dual-channel stereo cues to achieve physically plausible spatial acoustics.<br>â€¢ Physics- and Emotion-Aware Consistency Metrics for Tri-Modal Generation: Design new reference-free metrics capturing Doppler effects, material interactions, and emotional coherence across text, audio, and video.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">In Pursuit of Pixel Supervision for Visual Pre-training</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.15715" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.15715" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed "Pixio", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Pixel-based self-supervision is underexplored at web scale; existing practice favors latent-space objectives (e.g., DINO/JEPA), potentially discarding low-level cues crucial for many tasks.<br>â€¢ Original MAE design is suboptimal in large-data/large-model regimes: shallow decoders force encoders to model low-level details, single-patch masking enables shortcut copying, and a single [CLS] token undercaptures diverse global properties.<br>â€¢ Heavy benchmark-centric data curation in latent-space methods introduces bias and fragility; there is a need for minimally curated, diverse web-scale data with scalable, low-bias supervision grounded in pixels.<br>â€¢ Many downstream tasks (depth estimation, feed-forward 3D reconstruction, semantic segmentation, robot learning) require preserving fine-grained geometry and appearance that latent invariances may suppress.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Pixio is an enhanced masked autoencoder trained purely with pixel reconstruction that raises task difficulty and model capacity via a deeper lightweight decoder, larger blockwise masking, and multiple class tokens, and scales training to 2B images using loss-based self-curation and color-entropy filtering. This simple, stable setup yields transferable features that match or surpass latent-space SOTA on dense and geometric tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Temporal Pixio: Pixel-Space Self-Supervised Pretraining for Video with Spatio-Temporal Blockwise Masking: Extend Pixio to videos to exploit temporal consistency, motion cues, and 3D geometry for stronger representations.<br>â€¢ AutoMask: Learning Instance- and Scale-Adaptive Masking Policies for Pixel Supervision: Learn masking ratio and granularity online based on per-image difficulty to prevent shortcuts and optimize transfer.<br>â€¢ Pixio+Latents: Joint Pixel Reconstruction and Latent Consistency for Robust and Less Biased Visual Pretraining: Combine pixel reconstruction with lightweight latent invariance objectives to capture semantics without heavy curation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.15649" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.15649" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Scaling LLM context windows is computationally and memory-intensive; existing long-context techniques (efficient attention, positional extrapolation, prompt compression, external memory) degrade at very long lengths.<br>â€¢ Vision-text compression (rendering text as images) achieves 3Ã—â€“20Ã— token compression, but its impact on core long-context understanding in VLMs (retrieval, associative reasoning, long-term memory) is largely unknown.<br>â€¢ Prior VTC evaluations emphasize OCR and literal matching, failing to probe associative reasoning with minimal lexical overlap and comprehensive dialogue memory.<br>â€¢ VLMs exhibit VTC-specific fragilities (lost-in-the-middle positional bias, sensitivity to font size/placement, architectural mismatches like thumbnail encoders), necessitating a dedicated benchmark to diagnose limitations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces VTCBench and VTCBench-Wild, comprehensive benchmarks that evaluate VLMsâ€™ long-context understanding under vision-text compression across Retrieval (NIAH variants), Reasoning (low lexical overlap), and Memory (multi-turn dialogue) with two controlled settings: fixed compression ratio via font-size tuning and fixed rendering presets. It standardizes image inputs and metrics, systematically varies context length and needle depth, and conducts ablations and error analyses to isolate rendering, positional, and architectural effects.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Layout-Aware 2D Attention for Long-Context VTC: Design position-invariant 2D attention and layout encodings to mitigate lost-in-the-middle and spatial biases in dense text images.<br>â€¢ Jointly Optimized Vision-Text Renderer and VLM for Token-Efficient Comprehension: Co-train a learnable rendering operator with the VLM to maximize legibility and semantic retention under fixed rVTC.<br>â€¢ Multi-Scale, Text-Centric Vision Encoders Without Thumbnails: Build encoders tailored to dense text using high-res tiling and selective token allocation, avoiding low-information thumbnails.<br>â€¢ Associative Reasoning Pretraining for VTC: Curate VTC-specific datasets with minimal lexical overlap and train objectives that strengthen multi-hop, commonsense associations and reduce refusals.<br>â€¢ Hybrid VTC + External Memory for Long Dialogues: Integrate differentiable memory or retrieval modules with VTC to store, aggregate, and query long-term dialogue facts under compression.<br>â€¢ Style-Robust VTC via Augmentation and Contrastive Objectives: Employ diverse rendering augmentations (fonts, colors, line-height) and contrastive losses to improve robustness across in-the-wild visual styles.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.15110" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.15110" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapid evolution of text-to-image generation models has revolutionized visual content creation. While commercial products like Nano Banana Pro have garnered significant attention, their potential as generalist solvers for traditional low-level vision challenges remains largely underexplored. In this study, we investigate the critical question: Is Nano Banana Pro a Low-Level Vision All-Rounder? We conducted a comprehensive zero-shot evaluation across 14 distinct low-level tasks spanning 40 diverse datasets. By utilizing simple textual prompts without fine-tuning, we benchmarked Nano Banana Pro against state-of-the-art specialist models. Our extensive analysis reveals a distinct performance dichotomy: while Nano Banana Pro demonstrates superior subjective visual quality, often hallucinating plausible high-frequency details that surpass specialist models, it lags behind in traditional reference-based quantitative metrics. We attribute this discrepancy to the inherent stochasticity of generative models, which struggle to maintain the strict pixel-level consistency required by conventional metrics. This report identifies Nano Banana Pro as a capable zero-shot contender for low-level vision tasks, while highlighting that achieving the high fidelity of domain specialists remains a significant hurdle.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Can a large commercial text-to-image generator serve as a generalist, zero-shot solver for classic low-level vision tasks (restoration, enhancement, fusion) without task-specific training?<br>â€¢ There is a gap between human-perceived quality and conventional reference-based metrics (e.g., PSNR, SSIM), which penalize plausible but non-aligned generative outputs.<br>â€¢ Existing evaluations of generative models on low-level tasks are fragmented; a comprehensive, standardized zero-shot benchmark across many tasks/datasets is missing.<br>â€¢ Specialist models require task-specific pipelines and fine-tuning, limiting scalability and often yielding less perceptually pleasing results despite strong metrics.<br>â€¢ Understanding the strengths, limits, and failure modes of generative models in low-level vision is necessary to guide method development and metric design.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A comprehensive zero-shot evaluation using simple, fixed prompts applies Nano Banana Pro to 14 low-level tasks over 40 datasets, comparing against specialist SOTA models with standard reference-based metrics and qualitative analysis, without fine-tuning or multi-round selection.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Perception-Aligned Metrics for Generative Low-Level Vision: Design metrics robust to pixel misalignment that better track human judgments for restoration, enhancement, and fusion.<br>â€¢ Prompt-Conditioning and Control for Zero-Shot Restoration with T2I Models: Develop prompt tuning, control signals, and guidance schedules to improve fidelity and controllability across degradations.<br>â€¢ Data-Consistency-Aware Diffusion for Low-Level Tasks: Integrate physics/measurement models and consistency constraints into sampling to reduce stochastic drift and boost PSNR/SSIM without sacrificing perceptual quality.<br>â€¢ Generalistâ€“Specialist Hybrid Pipelines for Low-Level Vision: Combine specialist pre/post-processing with generative priors to balance pixel-level accuracy and perceptual realism across diverse tasks.<br>â€¢ Benchmarking Protocols and Datasets for Generative Low-Level Vision: Establish standardized protocols (including subjective tests and distributional measures) expanding the 14-task, 40-dataset suite for fair comparisons.<br>â€¢ Instruction-Tuned Foundation Models for Unified Low-Level Vision: Adapt T2I backbones via instruction tuning or lightweight adapters to follow restoration instructions faithfully while retaining generative priors.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13190" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13190" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ AIS data are noisy and irregular (unstructured/misspelled destination/ETA, missing values, uneven sampling), creating spatio-temporal bias and undermining reliability for global trajectory modeling.<br>â€¢ Existing methods are constrained to ROIs and either process message-wise sequences (overfit to dense regions due to irregular intervals) or coarse grid tokens (lose local detail and demand learning spatial relations from scratch), making long-range, global destination estimation difficult.<br>â€¢ Lack of port-to-port annotation and many-to-many training on a single destination label induces feedback bias over variable-length trajectories; models need to predict daysâ€“weeks ahead while handling multivariate (kinematic and non-kinematic) features.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>WAY reformulates AIS trajectories into a nested, grid-based sequence with a 4â€‘channel multi-feature representation and processes it via Channel-Aggregative Sequential Processing blocks that combine multi-head channel attention with a Transformer decoder for temporal propagation. An annotation pipeline extracts clean port-to-port segments, and Gradient Dropout regularizes many-to-many training to prevent length-induced feedback bias.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ WAY-ETA: Joint Learning of Destination and Estimated Time of Arrival in Global AIS Trajectories: Extend WAY to multitask training with shared representations to simultaneously predict destination and ETA, evaluating early prediction horizons.<br>â€¢ Adaptive Hierarchical Spatial Grids for Detail-Preserving Global Vessel Trajectory Modeling: Learn grid resolution dynamically based on traffic density and uncertainty to balance spatio-temporal bias mitigation with local detail retention.<br>â€¢ Gradient Dropout 2.0: Length-Aware Curriculum and Theoretical Analysis for Robust Training on Long AIS Sequences: Enhance GD with curriculum scheduling and formal analysis to optimize many-to-many training stability across variable sequence lengths.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">FrontierCS: Evolving Challenges for Evolving Intelligence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.15699" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.15699" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing code/reasoning benchmarks are saturated and focus on closed-form, single-answer, pass-or-fail tasks, failing to reflect open-ended CS problems where optimal solutions are unknown.<br>â€¢ The field lacks a comprehensive, cross-domain, expert-curated benchmark of unsolved yet objectively verifiable problems that measure incremental progress rather than binary correctness.<br>â€¢ Current evaluations rarely require executable solvers with resource constraints, missing assessment of algorithmic quality, trade-offs, and system design under realistic conditions.<br>â€¢ Prior open-ended efforts (e.g., narrow optimization contests or LLM-as-judge setups) have limited diversity or rely on subjective judging, hindering reproducibility and fair comparison.<br>â€¢ Frontier reasoning models show systematic gaps: diminishing returns from larger reasoning budgets, micro-optimization traps, and difficulty balancing engineering feasibility with true algorithmic optimization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>FrontierCS is a 156-task benchmark of expert-crafted, open-ended CS problems across algorithmic and research tracks where models submit executable programs scored by deterministic evaluators with partial, task-specific metrics and resource limits, supported by parametric instance generators, reference solutions, and scalable infrastructure. It provides objective, continuous scoring for unsolved problems and an update policy to evolve difficulty, enabling evaluation and training (e.g., RL) on measurable progress.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Heuristics with FrontierCS: Reinforcement Learning from Deterministic Evaluators for Open-Ended Algorithms: Use FrontierCS scores as rewards to train agents that discover high-quality heuristics and search strategies across NP-hard-style tasks.<br>â€¢ Agentic Multi-Round Solvers for FrontierCS: Tool-Augmented Iteration and Feedback-Driven Refinement: Develop agent frameworks that compile, run, diagnose, and iteratively improve solvers under resource limits to close the humanâ€“model gap.<br>â€¢ Prompting for Structure: Guiding Internal Representations to Avoid Micro-Optimization Traps in Open-Ended Programming: Study structural prompt interventions (e.g., mandated state representations) that shift models from superficial tweaks to algorithmically meaningful designs.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SCOPE: Prompt Evolution for Enhancing Agent Effectiveness</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.15374" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.15374" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce SCOPE (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an online optimization problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\% to 38.64\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLM agents face massive, dynamic contexts but static prompts lack mechanisms to manage and leverage this context effectively.<br>â€¢ Two pervasive failure modes: Corrective Failures (errors treated as generic alarms leading to loops or fabrication) and Enhancement Failures (missed optimization opportunities and silent quality issues without explicit errors).<br>â€¢ Modern multi-role, long-horizon agent systems exhibit heterogeneous failures across specialized sub-agents, making one-size-fits-all prompts obsolete.<br>â€¢ Memory-augmented test-time learning (e.g., DC, ACE) operates at task-level granularity, cannot adapt mid-execution, and mixes strategies into a single playbook.<br>â€¢ In-context correction (e.g., Reflexion, Self-Refine, ReAct) appends feedback rather than integrating actionable instruction, often causing repeated mistakes.<br>â€¢ Offline prompt optimization finds prompts pre-deployment and cannot evolve online, limiting robustness under distribution shifts.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SCOPE performs online prompt evolution by synthesizing guidelines from execution traces, selecting and routing them via a dual-stream (tactical, strategic) into memory, and consolidating strategic memory to update the agentâ€™s prompt. It further uses perspective-driven exploration to run multiple parallel prompts (e.g., Efficiency vs. Thoroughness), maximizing strategy coverage for diverse tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Meta-SCOPE: Learning Optimal Dual-Stream Routing for Online Prompt Evolution: Train meta-policies to adaptively decide tactical vs. strategic updates across roles using reinforcement and bandit feedback.<br>â€¢ Discovering Perspectives via Self-Play for Strategy Coverage in Agentic Systems: Automatically generate and select diverse perspectives with diversity-encouraging objectives instead of manually defined axes.<br>â€¢ Role-Aware Transfer of Strategic Guidelines in Heterogeneous Agent Architectures: Develop mechanisms to generalize, specialize, and safely transfer strategic memory across coder, browser, and analyzer agents while preventing negative transfer.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">Understanding and Improving Hyperbolic Deep Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14202" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14202" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the PoincarÃ© Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Reinforcement learning environments often produce hierarchical/relational data that are poorly captured by Euclidean embeddings, causing distortion and data inefficiency.<br>â€¢ Hyperbolic representations are well-suited to hierarchical structure but suffer from severe optimization instability in deep RL due to nonstationary targets and large-norm embeddings.<br>â€¢ Existing hyperbolic RL agents (e.g., PPO variants) experience trust-region violations and unstable policy updates because encoder gradients explode in both PoincarÃ© Ball and Hyperboloid models.<br>â€¢ Common remedies like heuristic clipping or unregularized training either fail to bound feature norms reliably or introduce a curse of dimensionality, leading to instability and slower learning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper analyzes gradients of core hyperbolic operations in PoincarÃ© Ball and Hyperboloid models to identify norm-induced instability, then proposes HYPER++: a hyperbolic PPO agent combining a categorical value loss, Euclidean feature regularization that guarantees bounded norms without high-dimensional clipping issues, and more optimization-friendly hyperbolic layers (including RMSNorm and a novel scaling layer).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Trust-Region-Safe Hyperbolic RL: Theoretical Guarantees and Algorithms: Formalize and prove trust-region compliance under hyperbolic regularization, deriving algorithms with stability and convergence guarantees for PPO and off-policy methods.<br>â€¢ Curvature-Adaptive Hyperbolic Networks for Deep RL: Learn task-dependent curvature and norm budgets online, enabling automatic control of embedding norms and geometry to maintain stability across diverse environments.<br>â€¢ Hyperbolic Representation Learning for Hierarchical Model-Based RL: Integrate hyperbolic encoders with world models and tree search to exploit hierarchical state structure, improving planning efficiency and generalization.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Hybrid Attribution Priors for Explainable and Robust Model Training</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14719" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14719" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing attribution methods for SLM text classification show homogenization and class confusion: they emphasize common keywords shared by semantically similar classes, providing weak discriminative signals and amplifying misclassification.<br>â€¢ Self-derived priors (e.g., Integrated Gradients) inherit model biases, capture only local gradient information, and lack external knowledge or label semanticsâ€”especially problematic in few-shot and fine-grained settings.<br>â€¢ Human-annotated priors are transparent but costly and unscalable; there is a need for reliable, scalable attribution priors that combine model-internal reasoning with external linguistic knowledge to enhance interpretability and robustness, including under adversarial perturbations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes Class-Aware Attribution Priors (CAP), which prompt an LLM with task instructions and the label space, then use word masking and a regularized least-squares fit over true-label probabilities to derive discriminative, class-aware token attributions. It further introduces CAP Hybrid, fusing CAP with existing priors (e.g., LIME/SHAP/IG) and aligning SLM self-attributions to the fused priors during training to improve interpretability and robustness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Bayesian Hybrid Attribution Fusion: A Principled Framework to Integrate LLM- and Model-Derived Priors<br>â€¢ Contrastive Attribution Alignment for Few-Shot Robust Text Classification: Enhancing Discriminative Signals via Contrastive Objectives<br>â€¢ Class-Aware Attribution Priors for Multilingual and Cross-Domain Settings: Scaling Explainable and Robust SLM Training</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.09851" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.09851" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ STS sensors typically require mode switching (illumination or mechanics), preventing simultaneous tactile-visual perception and adding control complexity.<br>â€¢ Tactile marker tracking is unreliable with transparent skins due to degraded contrast and cluttered external backgrounds, limiting shear/contact estimation in open environments.<br>â€¢ Integration of rich STS signals into modern learning pipelines is underexplored; prior work relies on hand-crafted controllers rather than general-purpose imitation learning.<br>â€¢ Single-modality sensing is insufficient: vision suffers from occlusion during contact, VBTS lacks pre-contact awareness, and proximity sensors are too coarse for fine-grained manipulation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>TacThru introduces a fully transparent elastomer with persistent illumination and concentric keyline markers, enabling simultaneous visual and tactile sensing with efficient Kalman-filtered tracking (6.08 ms/frame). TacThru-UMI integrates these multimodal observations into a Transformer-based Diffusion Policy (DINOv2 encoders + MLPs for marker deviations/proprioception) to learn policies that adaptively attend to visual and tactile cues for precise manipulation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ TacThru-Pretrain: Self-Supervised Multimodal Pretraining for See-Through-Skin Sensors: Pretrain visuo-tactile encoders on large-scale real+sim datasets to boost robustness and sample efficiency across contact-rich tasks.<br>â€¢ DexTacThru: Simultaneous Visuo-Tactile Servoing for Dexterous In-Hand Manipulation: Extend TacThru to multi-finger hands and develop continuous servoing policies fusing proximity vision and tactile shear for dynamic, precise in-hand manipulation.<br>â€¢ STS-RL: Reliability-Weighted Reinforcement Learning with Simultaneous Tactile-Visual Sensing: Learn modality confidence estimation to weight/switch cues online, enabling robust insertion/assembly under occlusion and variable lighting.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.15340" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.15340" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Human conversation involves continuous exchanges of speech and nonverbal cues such as head nods, gaze shifts, and facial expressions that convey attention and emotion. Modeling these bidirectional dynamics in 3D is essential for building expressive avatars and interactive robots. However, existing frameworks often treat talking and listening as independent processes or rely on non-causal full-sequence modeling, hindering temporal coherence across turns. We present TIMAR (Turn-level Interleaved Masked AutoRegression), a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual contexts. It fuses multimodal information within each turn and applies turn-level causal attention to accumulate conversational history, while a lightweight diffusion head predicts continuous 3D head dynamics that captures both coordination and expressive variability. Experiments on the DualTalk benchmark show that TIMAR reduces FrÃ©chet Distance and MSE by 15-30% on the test set, and achieves similar gains on out-of-distribution data. The source code will be released in the GitHub repository https://github.com/CoderChen01/towards-seamleass-interaction.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing paradigms split talking-head and listening-head generation, lacking a unified, bidirectional model of interactive 3D head dynamics.<br>â€¢ Non-causal, full-sequence approaches break cross-turn temporal coherence and are unsuitable for streaming, turn-by-turn interaction.<br>â€¢ Current methods neglect interleaved multimodal context and causal dependencies, yielding speech-driven motion without user feedback or reactive motion without linkage to speaking behavior, reducing naturalness and generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>TIMAR (Turn-level Interleaved Masked AutoRegression) models conversation as interleaved userâ€“agent audio-visual tokens, performing intra-turn bidirectional fusion and inter-turn causal attention. A lightweight diffusion head autoregressively predicts continuous 3D head motion, unifying talking and listening with causal continuity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Whole-Body TIMAR: Causal Turn-Level Modeling of Interactive 3D Gesture, Gaze, and Posture Dynamics: Extend TIMAR beyond head motion to coordinated upper-body gestures and gaze with the same interleaved causal framework.<br>â€¢ Style-Aware, Cross-Lingual TIMAR: Personalization and Language-Invariant Causal Modeling for Expressive 3D Head Dynamics: Incorporate speaker identity and prosodic style embeddings to adapt across users and languages while preserving causal coherence.<br>â€¢ Real-Time TIMAR for Embodied Agents: Low-Latency Streaming Synthesis of Head Dynamics with Micro-Expression and Gaze Control: Optimize the framework for deployment with tight latency budgets and richer nonverbal cues, maintaining turn-level causal consistency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LikeBench: Evaluating Subjective Likability in LLMs for Personalization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13077" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13077" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing personalization benchmarks primarily test memory (recalling user facts) and application of those facts, under-measuring the subjective, user-centric axis of likability.<br>â€¢ Users need models that adapt over time within ongoing dialogues; most evaluations are static or single-session and fail to capture dynamic preference adaptation.<br>â€¢ Coarse, high/low trait personas used in prior work are unrealistic and insufficiently discriminative; fine-grained, psychologically grounded personas are needed for credible assessments.<br>â€¢ Strong memory accuracy does not guarantee high likability, indicating a gap in diagnostics to pinpoint where models fall short (e.g., emotional tone, formality, humor, callbacks).<br>â€¢ Even SOTA models show limited robustness in longer, noisier interactions, highlighting the need for benchmarks that expose degradation in multi-turn personalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>LikeBench is a multi-session, dynamic evaluation framework where LLMs converse with a simulated, psychologically grounded persona and must infer and adapt to user preferences solely from the evolving dialogue. After each turn, the simulated user scores likability across seven diagnostic dimensionsâ€”emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callbackâ€”enabling fine-grained analysis of adaptation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning to Optimize Likability: Reinforcement Learning from Simulated Personas: Train LLMs to directly maximize LikeBenchâ€™s multi-dimensional likability scores, studying trade-offs between memory accuracy and stylistic adaptation.<br>â€¢ Beyond Simulated Users: Human-in-the-Loop Longitudinal Evaluation of LLM Likability: Validate and calibrate LikeBench metrics with real users over weeks, measuring stability, preference drift, and cross-user generalization.<br>â€¢ Robust Personalization in Noisy, Long-Horizon Dialogues: Architectures and Memory Policies for Sustained Adaptation: Develop mechanisms for selective memory, style control, and error recovery to maintain likability over extended, noisy interactions.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 10</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-12-18 23:15:32 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 10;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>