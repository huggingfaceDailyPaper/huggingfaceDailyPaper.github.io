<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - December 08, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">December 08, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05150" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05150" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by 100times with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Multi-step diffusion/flow models require 40‚Äì100 NFEs, making inference costly and limiting practical deployment of large (20B+) generative models.<br>‚Ä¢ Existing few-step accelerations rely on frozen teacher models or adversarial discriminators/auxiliary scorers, causing training instability, architectural complexity, and high GPU memory, hindering scalability.<br>‚Ä¢ Teacher-free consistency approaches show sharp quality degradation at very low steps (<4 NFEs), failing to deliver strong 1-step performance.<br>‚Ä¢ There is a need for a simple, scalable, teacher-free framework that achieves 1-NFE quality comparable to multi-step baselines, especially on large multimodal models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TwinFlow extends the flow-matching time domain to [-1,1] and trains twin trajectories‚Äîreal (t>0) and fake (t<0) generated by the model itself‚Äîcombining a self-adversarial loss on the fake path with a rectification loss that minimizes the velocity difference between fake and real flows (via stop-gradient). Integrated with the any-step base objective (N=2) and batch partitioning controlled by Œª, it enables 1-step generation without auxiliary networks or frozen teachers.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ TwinFlow-Video: One-step Generation for Temporally Coherent Video via Twin Trajectories: Extend velocity matching and twin trajectories to spatiotemporal domains to achieve coherent few-step video synthesis.<br>‚Ä¢ Adaptive TwinFlow: Curriculum and Lambda Scheduling for Stable 1-NFE Training at Scale: Develop dynamic strategies for time sampling and Œª balancing to improve stability and performance on ultra-large models.<br>‚Ä¢ Theory of Self-adversarial Flows: Convergence Guarantees and Diversity in One-step Generators: Provide formal analysis of KL-to-velocity matching, establish convergence conditions, and study diversity/mode-collapse behavior in self-adversarial flows.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">EditThinker: Unlocking Iterative Reasoning for Any Image Editor</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05965" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05965" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Single-turn instruction-based image editing is reactive and lacks deliberation, leading to low instruction-following success, missing attributes, and no opportunity for self-correction.<br>‚Ä¢ Existing RL and reward-model approaches largely provide post-hoc, outcome-only scalar feedback that fails to guide intermediate reasoning or planning during generation.<br>‚Ä¢ Real-world editing demands identity preservation, localized semantic changes, and global visual consistency under free-form instructions‚Äîcritical for content creation, avatars, and controllable world simulation.<br>‚Ä¢ Prior improvements focus on fine-tuning specific editors, offering limited generalization; a model-agnostic iterative controller is needed to boost any editor‚Äôs instruction adherence.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>EditThinker is a single MLLM that drives a Think-while-Edit loop‚ÄîCritique, Refine, Repeat‚Äîby jointly outputting a visual critique score, explicit reasoning, and refined instructions, which are fed to any editor until satisfactory. It is trained via SFT on the multi-turn THINKEDIT-140k dataset and RL (GRPO) with format, critic, and edit rewards to align reasoning with real editor behavior.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ End-to-End Co-Training of Editors and Thinkers for Iterative Image Editing: Jointly optimize the editor and EditThinker with shared, multi-component rewards to reduce the planning‚Äìexecution gap and failure modes.<br>‚Ä¢ Adaptive Turn Scheduling and Stopping Policies for Efficient Multi-Round Editing: Learn dynamic iteration budgets and confidence-based stopping to maximize quality gains under compute constraints.<br>‚Ä¢ Multimodal Reward Modeling Beyond Scalar Scores for Fine-Grained Editing Guidance: Develop region- and attribute-aware visual-text rewards that supervise intermediate reasoning and localized edits more effectively.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.02580" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.02580" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Mixing positive and negative advantage signals from the start yields conflicting gradients and high variance, causing unstable early RL post-training and limiting reasoning gains.<br>‚Ä¢ Existing curriculum learning strategies are static or heuristic (e.g., easy-to-hard by external difficulty or success rates), not competence-aware, and misaligned with the model‚Äôs evolving ability.<br>‚Ä¢ Advantage-based optimizers (PPO, GRPO, etc.) lack mechanisms to exploit advantage as a curriculum signal; early inclusion of negatives can induce entropy collapse and poor generalization.<br>‚Ä¢ There is a need for a unified, algorithm-agnostic mechanism that stabilizes early training while improving cross-domain generalization (math ‚Üí GUI) and out-of-distribution robustness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>CAPO is a two-phase, advantage-driven curriculum: Phase 1 performs imitation learning using only positive-advantage trajectories (with KL regularization) to reduce gradient variance and stabilize training, then a hard switch introduces Phase 2 with full-spectrum advantages to learn discriminative policies. It is plug-and-play compatible with GRPO, PPO, RLOO, and Reinforce++, and is theoretically motivated by a variance‚Äìbias tradeoff.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive CAPO: Online Advantage-Driven Switching and Weighting: Learn the switch point and positive/negative weights from batch-level advantage statistics (e.g., quantiles, entropy) to remove manual schedules and tailor curricula per task.<br>‚Ä¢ Hierarchical Curriculum Advantage for Multimodal and Tool-Use Reasoning: Extend CAPO to token/step/trajectory granularity and integrate tool calls and multimodal signals to enhance planning, perception, and control in GUI and other domains.<br>‚Ä¢ Provable Advantage Curricula under Non-Stationary Rewards and Off-Policy Sampling: Develop tighter convergence and generalization guarantees for CAPO under drifting reward models, preference noise, and importance sampling, with adaptive KL and variance control.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04810" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04810" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Unified multimodal architectures are inefficient due to excessive visual tokens and token-wise concatenation, causing long context lengths and slow training/inference, especially for image editing.<br>‚Ä¢ Mismatched compression ratios between understanding (patch-based) and generation (autoencoder) branches hinder balanced end-to-end training in a single architecture.<br>‚Ä¢ Existing unified designs lack a principled way to share parameters where tasks overlap while decoupling where they diverge (semantics vs high-frequency details), limiting mutual gains across tasks.<br>‚Ä¢ Visual understanding encoders struggle with diverse domains (e.g., STEM), reducing accuracy on broad benchmarks.<br>‚Ä¢ Current datasets and metrics for image editing (e.g., GEdit) inadequately measure subject consistency, and widely used data (e.g., GPT-Image-Edit-1.5M) can harm identity preservation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>EMMA aligns both visual encoders at 32√ó compression and fuses semantic and detail features via channel-wise concatenation, cutting visual tokens while preserving cross-branch information. A shared‚Äìdecoupled transformer (shared shallow QK, task-specific V; deep layers split) plus a MoE understanding encoder with a STEM expert, trained with next-token prediction for understanding and flow-matching for generation, delivers efficient, unified performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Dynamic Compression and Token Budgeting for Unified Multimodal Models: Learn adaptive compression ratios and channel widths per input/task to minimize context length without sacrificing fidelity.<br>‚Ä¢ Benchmarking Subject Consistency in Image Editing: A Dataset and Metrics for Unified Architectures: Introduce identity-preserving editing benchmarks and automated evaluators to address metric blind spots in GEdit.<br>‚Ä¢ Unified Video Understanding, Generation, and Editing with Channel-wise Latent Fusion: Extend EMMA‚Äôs 32√ó compression and channel-wise fusion to video/audio with temporal-aware encoders and shared‚Äìdecoupled backbones.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04784" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04784" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Consistent image generation requires faithfully preserving identities, styles, and logical coherence across multiple images, which is essential for applications such as storytelling and character design. Supervised training approaches struggle with this task due to the lack of large-scale datasets capturing visual consistency and the complexity of modeling human perceptual preferences. In this paper, we argue that reinforcement learning (RL) offers a promising alternative by enabling models to learn complex and subjective visual criteria in a data-free manner. To achieve this, we introduce PaCo-RL, a comprehensive framework that combines a specialized consistency reward model with an efficient RL algorithm. The first component, PaCo-Reward, is a pairwise consistency evaluator trained on a large-scale dataset constructed via automated sub-figure pairing. It evaluates consistency through a generative, autoregressive scoring mechanism enhanced by task-aware instructions and CoT reasons. The second component, PaCo-GRPO, leverages a novel resolution-decoupled optimization strategy to substantially reduce RL cost, alongside a log-tamed multi-reward aggregation mechanism that ensures balanced and stable reward optimization. Extensive experiments across the two representative subtasks show that PaCo-Reward significantly improves alignment with human perceptions of visual consistency, and PaCo-GRPO achieves state-of-the-art consistency performance with improved training efficiency and stability. Together, these results highlight the promise of PaCo-RL as a practical and scalable solution for consistent image generation. The project page is available at https://x-gengroup.github.io/HomePage_PaCo-RL/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Achieving consistent image generation across multiple images‚Äîpreserving identities, styles, and logical coherence‚Äîremains challenging and crucial for applications like storytelling, advertising, and character design.<br>‚Ä¢ Supervised approaches struggle due to the lack of large-scale datasets that capture visual consistency and the difficulty of modeling subjective human perceptual preferences with fixed losses.<br>‚Ä¢ Existing RL pipelines for image generation are computationally costly and unstable, and current reward models inadequately align with human judgments of visual consistency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>PaCo-RL integrates a pairwise consistency reward model (PaCo-Reward) trained via automated sub-figure pairing with generative, autoregressive scoring enhanced by task-aware instructions and chain-of-thought, and an efficient RL algorithm (PaCo-GRPO) featuring resolution-decoupled optimization and log-tamed multi-reward aggregation for stable, low-cost consistency optimization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ PaCo-Video: Reinforcement Learning for Consistent Character and Scene Generation in Video: Extend pairwise reward modeling and resolution-decoupled RL to temporal sequences to ensure identity/style continuity across frames.<br>‚Ä¢ Human-in-the-Loop Pairwise Reward Modeling for Subjective Visual Consistency: Incorporate active learning and preference elicitation to refine PaCo-Reward with human judgments, improving alignment on nuanced, context-dependent consistency.<br>‚Ä¢ Cross-Modal Consistency RL: Aligning Image Sets with Narrative and Style Constraints: Jointly optimize image sets against textual narratives and stylistic descriptors using multi-reward aggregation to enforce semantic and aesthetic coherence.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05905" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05905" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present SCAIL (Studio-grade Character Animation via In-context Learning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that SCAIL achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing pose-driven image-to-video methods fail under studio-grade demands‚Äîlarge motion variations, multi-person interactions, and cross-identity/domain transfers‚Äîleading to structural distortions, occlusion errors, and temporal inconsistency<br>‚Ä¢ 2D skeleton controls are noisy and depth-ambiguous and cannot encode occlusions; SMPL-based controls are identity-coupled and hard to augment/retarget, causing identity leakage and poor 3D-consistent motion transfer<br>‚Ä¢ Common pose injection (e.g., channel concatenation) provides only per-frame local cues in DiT backbones, missing global temporal context and misreading motion semantics (e.g., turning front/back)<br>‚Ä¢ Lack of curated, diverse, motion-rich training data and a comprehensive studio-grade benchmark to assess complex motions and cross-driven generalization</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SCAIL introduces an identity-agnostic 3D-consistent pose signal by estimating 3D joints, rendering bones as cylinders to preserve depth/occlusion, and aligning to the reference via projection optimization with skeletal/camera augmentations for robust retargeting. It injects the entire pose sequence as tokens into a DiT-based I2V model with Pose-Shifted RoPE and masking, enabling full spatio-temporal reasoning for reliable motion transfer.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ End-to-End 3D Pose‚ÄìGuided Video Diffusion: Jointly train the 3D pose estimator and the diffusion generator to reduce upstream pose noise and improve occlusion-aware conditioning<br>‚Ä¢ Contact-Aware Multi-Character Animation via Graph-Relational Diffusion: Integrate explicit contact, collision, and inter-person occlusion graphs into full-context DiT to enhance interaction realism<br>‚Ä¢ Topology-Agnostic Retargeting for Non-Human and Stylized Characters: Generalize cylindrical 3D pose controls and retargeting to varied skeletons (creatures, stylized avatars) with learned topology mappings</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05591" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05591" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an Entropy Ratio Clipping (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Off-policy RL in LLM post-training induces distribution shift that pushes updates beyond the trust region, causing instability.<br>‚Ä¢ Entropy instability (oscillating exploration) and gradient norm explosions/vanishing degrade convergence and optimization.<br>‚Ä¢ PPO-Clip only constrains sampled actions‚Äô importance ratios, leaving unsampled actions unconstrained and enabling global distribution drift.<br>‚Ä¢ KL-penalty (PPO-Penalty) requires sensitive coefficient tuning, risks over/under-regularization, and can suppress exploration.<br>‚Ä¢ High-variance importance sampling destabilizes update step sizes, exacerbating trust-region violations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce token-level entropy ratio œÅ_t = H(œÄ_new,t)/H(œÄ_old,t) as a global measure of distribution change, and apply bidirectional hard clipping that discards gradients when œÅ_t falls outside [1‚àíŒ≤_low, 1+Œ≤_high]. ERC complements PPO-Clip and integrates into DAPO/GPPO to stabilize entropy and gradients while preserving exploration, yielding consistent performance gains on math reasoning benchmarks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Entropy Ratio Clipping for LLM RL: Learn or schedule Œ≤ bounds from training signals (entropy trends, gradient variance) to dynamically balance exploration and stability.<br>‚Ä¢ Smooth ERC: A Differentiable Entropy-Ratio Regularizer: Replace hard clipping with a soft, differentiable penalty on entropy ratios to retain gradient flow and enable principled coefficient tuning; compare to KL and entropy regularization.<br>‚Ä¢ Entropy-Ratio Trust-Region Theory for Off-Policy RL: Establish formal links between entropy-ratio bounds, KL/trust-region guarantees, and convergence rates in large-action-space policies (e.g., LLMs).</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05044" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05044" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Generating interactive and dynamic 4D scenes from a single static image remains a core challenge. Most existing generate-then-reconstruct and reconstruct-then-generate methods decouple geometry from motion, causing spatiotemporal inconsistencies and poor generalization. To address these, we extend the reconstruct-then-generate framework to jointly perform Motion generation and geometric Reconstruction for 4D Synthesis (MoRe4D). We first introduce TrajScene-60K, a large-scale dataset of 60,000 video samples with dense point trajectories, addressing the scarcity of high-quality 4D scene data. Based on this, we propose a diffusion-based 4D Scene Trajectory Generator (4D-STraG) to jointly generate geometrically consistent and motion-plausible 4D point trajectories. To leverage single-view priors, we design a depth-guided motion normalization strategy and a motion-aware module for effective geometry and dynamics integration. We then propose a 4D View Synthesis Module (4D-ViSM) to render videos with arbitrary camera trajectories from 4D point track representations. Experiments show that MoRe4D generates high-quality 4D scenes with multi-view consistency and rich dynamic details from a single image. Code: https://github.com/Zhangyr2022/MoRe4D.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Generating coherent 4D scenes (consistent 3D geometry plus plausible motion) from a single image is severely under-constrained, requiring recovery of complete spatiotemporal information from limited 2D observations.<br>‚Ä¢ Generate-then-reconstruct pipelines rely on 2D video priors that lack explicit 3D structure, leading to multi-view inconsistencies and geometry collapse during subsequent 3D reconstruction.<br>‚Ä¢ Vanilla reconstruct-then-generate approaches decouple static geometry from motion, discard dynamic cues latent in the source image, and thus restrict motion to externally constrained, small-scale dynamics; additionally, there is a scarcity of large-scale, high-quality 4D datasets with dense point trajectories.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MoRe4D jointly couples motion generation with 3D reconstruction by predicting dense 4D point trajectories from a single image using a diffusion-based 4D Scene Trajectory Generator (4D-STraG) trained on the new TrajScene-60K dataset, enhanced with depth-guided motion normalization and a Motion Perception Module. The resulting 4D point tracks are rendered into multi-view-consistent dynamic videos via a 4D View Synthesis Module (4D-ViSM).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Physics-Guided Diffusion for Single-Image 4D Scene Synthesis: Integrate differentiable physics and collision constraints into the denoising process to enforce physically plausible, energy-consistent motions and interactions.<br>‚Ä¢ Self-Supervised 4D Trajectory Learning Without Dense Labels: Replace dense trajectory supervision with cycle-consistency, multi-view reprojection, and photometric priors to scale training beyond TrajScene-60K.<br>‚Ä¢ Text-Conditioned Motion Priors for Controllable 4D Generation: Fuse language guidance with motion priors to enable semantically controlled, user-directed dynamics while preserving geometric consistency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04563" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04563" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose COOPER, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average 6.91\% improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a 7.92\% gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Multimodal LLMs trained on 2D image‚Äìtext pairs lack 3D awareness, struggling to infer geometry, depth, and object boundaries for robust spatial reasoning.<br>‚Ä¢ Perception enhancement methods (adding depth/segmentation/point clouds via fixed pipelines) improve low/mid-level cues but yield limited gains in high-level spatial reasoning and decision-making.<br>‚Ä¢ Reasoning enhancement methods (text-only chain-of-thought and RL on spatial VQA) perform reasoning from raw 2D inputs without explicit 3D grounding, leading to brittle, inconsistent spatial understanding.<br>‚Ä¢ Unified MLLMs are optimized to generate RGB images and cannot natively produce non-RGB auxiliary modalities (depth, segmentation) that are crucial for spatial reasoning.<br>‚Ä¢ Existing unified frameworks lack adaptive, interleaved control to decide when, what, and how to generate images versus text, preventing tight coupling of perception and reasoning.<br>‚Ä¢ These issues hinder progress on foundational applications (robotics, autonomous driving, AR/VR) where precise spatial intelligence is critical.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>COOPER is a unified MLLM that first learns to generate auxiliary modalities by converting depth and segmentation into RGB pseudo-images and training with the model‚Äôs Rectified Flow pipeline, then acquires adaptive, interleaved reasoning via supervised fine-tuning on multimodal chain-of-thought and GRPO-based reinforcement learning with a Cooperative Perception‚ÄìReasoning reward to schedule capability use.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond-RGB COOPER: Unified Generation of Normals, Optical Flow, and 3D Point Clouds for Spatial Reasoning: Extend auxiliary modality generation to richer geometric signals and non-image latent spaces to further strengthen 3D-aware reasoning.<br>‚Ä¢ Hierarchical Policy Learning for Tool-Augmented Cooperative Perception and Reasoning: Integrate external 3D tools (SLAM, NeRF, stereo) with a learned hierarchical controller that dynamically calls tools and interleaves visual/textual CoT.<br>‚Ä¢ Benchmarking and Interpreting Multimodal Chain-of-Thought for 3D Spatial Intelligence: Create datasets, metrics, and interpretability analyses to evaluate and understand interleaved vision‚Äìlanguage CoT and its causal impact on spatial reasoning performance.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.00473" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.00473" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>With the continuous advancement of image generation technology, advanced models such as GPT-Image-1 and Qwen-Image have achieved remarkable text-to-image consistency and world knowledge However, these models still fall short in photorealistic image generation. Even on simple T2I tasks, they tend to produce " fake" images with distinct AI artifacts, often characterized by "overly smooth skin" and "oily facial sheens". To recapture the original goal of "indistinguishable-from-reality" generation, we propose RealGen, a photorealistic text-to-image framework. RealGen integrates an LLM component for prompt optimization and a diffusion model for realistic image generation. Inspired by adversarial generation, RealGen introduces a "Detector Reward" mechanism, which quantifies artifacts and assesses realism using both semantic-level and feature-level synthetic image detectors. We leverage this reward signal with the GRPO algorithm to optimize the entire generation pipeline, significantly enhancing image realism and detail. Furthermore, we propose RealBench, an automated evaluation benchmark employing Detector-Scoring and Arena-Scoring. It enables human-free photorealism assessment, yielding results that are more accurate and aligned with real user experience. Experiments demonstrate that RealGen significantly outperforms general models like GPT-Image-1 and Qwen-Image, as well as specialized photorealistic models like FLUX-Krea, in terms of realism, detail, and aesthetics. The code is available at https://github.com/yejy53/RealGen.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Advanced T2I models (e.g., GPT-Image-1, Qwen-Image) achieve strong text alignment and world knowledge but still produce non-photorealistic, "fake" images with detectable AI artifacts (e.g., overly smooth skin, oily sheen).<br>‚Ä¢ Existing training pipelines lack realism-targeted objectives; they do not directly penalize synthetic artifacts or optimize generation toward detector-verified realism at both semantic and feature levels.<br>‚Ä¢ Evaluation of photorealism is inconsistent and labor-intensive; scalable, human-free benchmarks aligned with user perception are missing, hindering reliable comparison and progress.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RealGen couples LLM-based prompt optimization with a diffusion generator and optimizes the end-to-end pipeline via reinforcement learning (GRPO) using detector-guided rewards from semantic-level and feature-level synthetic image detectors to directly penalize artifacts and boost realism. It further introduces RealBench with Detector-Scoring and Arena-Scoring for automated, human-free photorealism evaluation aligned with user experience.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Reward Shaping for Photorealistic Generation: Dynamically weight semantic- and feature-level detector signals and calibrate rewards with user preference modeling to further improve realism across diverse prompts and domains.<br>‚Ä¢ RealBench++: A Comprehensive Benchmark for Photorealism Across Modalities: Extend RealBench to multi-domain images and video with perceptual, temporal-consistency, and lighting/geometry-aware metrics, enabling standardized, scalable evaluation.<br>‚Ä¢ Detector-Guided RL for Text-to-Video Realism: Generalize RealGen to video by integrating temporal artifact detectors and consistency rewards, optimizing prompts and diffusion video models for photorealistic, temporally coherent synthesis.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05927" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05927" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Controllable video models often hallucinate physically inconsistent future frames, undermining trust in robotics tasks such as policy evaluation and visual planning.<br>‚Ä¢ Existing video generators cannot express calibrated, dense (spatial-temporal) confidence, limiting safe decision-making and error localization within frames.<br>‚Ä¢ Prior uncertainty quantification is largely task-level; pixel-space UQ is unstable and computationally expensive, leaving a gap for fine-grained, interpretable, and OOD-aware uncertainty estimates.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>C3 trains controllable video models for correctness and calibration using strictly proper scoring rules, estimating dense subpatch-level uncertainty directly in latent space. It then maps latent uncertainty to interpretable pixel-level RGB heatmaps, enabling calibrated confidence visualization and OOD detection.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Uncertainty-Aware Visual Planning with Calibrated World Models: Integrate C3‚Äôs calibrated frame-level confidence into planners to downweight unreliable predictions and reduce hallucination-induced failures.<br>‚Ä¢ Active Data Collection for Video World Models Guided by Dense Uncertainty: Use uncertainty heatmaps to drive targeted data acquisition and exploration, improving coverage and reducing model uncertainty.<br>‚Ä¢ Cross-Modal OOD Detection in Controllable Video Generation via Latent-Space Calibration: Extend C3 to multi-modal conditioning (text/action/sensor) and benchmark robust OOD detection across domains and tasks.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05343" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05343" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains a key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SpaceControl, a training-free test-time method for explicit spatial control of 3D generation. Our approach accepts a wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. A controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SpaceControl outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at https://spacecontrol3d.github.io/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of precise, intuitive, and easily editable geometric control in 3D generative modeling; text prompts are ambiguous and images are cumbersome to edit for fine-grained geometry.<br>‚Ä¢ Existing spatial control methods either require costly, class-specific fine-tuning that harms generalization (training-based) or rely on slow test-time optimization that constrains 3D structure only indirectly via 2D projections (guidance-based); detail-enrichment methods assume fine-grained input geometry.<br>‚Ä¢ Need for a training-free, efficient, and explicit 3D spatial conditioning mechanism that supports diverse geometry inputs (from superquadrics to meshes), integrates with modern disentangled structure/appearance models, and offers a tunable trade-off between geometric faithfulness and visual realism for practical creative workflows.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SpaceControl provides training-free test-time spatial guidance by voxelizing user geometry, encoding it with the pretrained Trellis encoder into structure latents, noising them to a chosen timestep (SDEdit-style rectified flow), and denoising under text/image conditioning‚Äîwhere the timestep controls the faithfulness‚Äìrealism trade-off‚Äîthen similarly guiding appearance latents and decoding to 3D Gaussians, radiance fields, or meshes. It integrates seamlessly with pre-trained 3D generative models (e.g., Trellis, SAM 3D) without any fine-tuning and accepts inputs from superquadrics to detailed meshes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Region-Wise Spatial Guidance for 3D Generative Models: Learn spatially variable guidance schedules (t0/œÑ0) per region to balance local fidelity and global realism.<br>‚Ä¢ Scene-Level SpaceControl: Layout-Constrained and Multi-Object 3D Generation: Extend test-time spatial control from single assets to full scenes with object relations, dimensions, and symmetry constraints.<br>‚Ä¢ Learning Task-Specific Geometry Encoders for SpaceControl: Train lightweight encoders for primitives, point clouds, and CAD inputs to improve robustness and latent alignment beyond voxelization.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.02835" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.02835" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Reasoning-centric VOS queries involve dynamics, causality, and temporal interactions, which are poorly handled by single-step latent mask prediction.<br>‚Ä¢ Existing VLM-based approaches force non-native outputs (e.g., special segmentation tokens), leading to opaque reasoning chains, limited interpretability, distribution shift, and heavy supervised data requirements.<br>‚Ä¢ Direct spatio-temporal grounding from raw video remains hard for current VLMs; test-time CoT and search strategies are insufficient for reliable segmentation.<br>‚Ä¢ Optimizing the multi-step decision process (video understanding, temporal selection, spatial grounding) lacks intermediate supervision; naive RL faces sparse rewards and coordination challenges.<br>‚Ä¢ There is a need for structured, VLM-native reasoning procedures that preserve pretrained alignment, improve generalization, and yield auditable reasoning trajectories.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ReVSeg explicitly decomposes reasoning VOS into a two-round chain within a single VLM: round 1 interprets the query to select a keyframe and concise object description; round 2 predicts a bounding box on that keyframe, which a tracker propagates to full video masks. The VLM is post-trained with GRPO-based reinforcement learning using reasoning-aligned rewards (format, temporal frame quality, and spatial IoU) to optimize multi-step decision quality without dense supervision.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Hierarchical ReVSeg: Multi-Keyframe Temporal Planning for Long Videos: Extend the policy to select multiple keyframes/intervals via hierarchical temporal decomposition to better handle long-range context and complex dynamics.<br>‚Ä¢ Preference-Driven Reward Modeling for Video Grounding: Learn data-driven reward models from human or synthetic preferences to improve credit assignment across reasoning steps and mitigate sparse, rule-based signals.<br>‚Ä¢ End-to-End Tracker-Reasoner Co-Training for Robust VOS: Jointly train the VLM and video tracker in a unified pipeline to align spatial grounding with propagation, improving robustness to occlusion, fast motion, and distribution shift.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Self-Improving VLM Judges Without Human Annotations</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05145" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05145" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present a framework to self-train a VLM judge model without any human preference annotations, using only self-synthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instruction-response pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves a Llama-3.2-11B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for a future self-judge that evolves alongside rapidly improving VLM capabilities.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Training VLM judges relies on large-scale human preference annotations that are costly, labor-intensive, and quickly become obsolete as models and tasks evolve.<br>‚Ä¢ Distilling from proprietary models (e.g., GPT-4o, Claude) indirectly depends on human labels and introduces reliance on closed-source systems.<br>‚Ä¢ Lack of scalable mechanisms to produce diverse, quality-graded multimodal data and consistent reasoning traces without human supervision.<br>‚Ä¢ Need for evaluators that can co-evolve with rapidly improving VLM capabilities across correctness, preference, reasoning, safety, and VQA domains without ongoing human annotation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>An iterative self-training framework that synthesizes multimodal instruction‚Äìresponse pairs at controlled quality levels, uses the previous-iteration judge to generate judgments and reasoning traces, filters cases using known preferences, and trains on aligned labels and rationales. For closed-ended tasks it pairs majority-vote answers against alternatives; for open-ended tasks it injects substantial, realistic errors (e.g., attribute or spatial changes) to form preference pairs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Curriculum Self-Training for VLM Judges Across Dynamic Task Distributions: Automatically adjust task coverage and difficulty in synthesis to maintain judge robustness as VLMs evolve.<br>‚Ä¢ Uncertainty-Aware Self-Judging with Confidence-Guided Filtering and Rationales: Incorporate calibration and confidence weighting to filter ambiguous synthetic labels and improve reasoning trace quality.<br>‚Ä¢ Multimodal Adversarial Error Injection for Robust Judge Learning: Generate diverse, hard perturbations (temporal, commonsense, multi-image consistency, compositional and safety-sensitive errors) to stress-test and strengthen judge reliability.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">M3DR: Towards Universal Multilingual Multimodal Document Retrieval</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.03514" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.03514" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing vision-based document retrieval is predominantly English-centric and performs poorly across diverse scripts and languages.<br>‚Ä¢ OCR-based pipelines lose layout/visual cues and are brittle to fonts/scripts, compounding errors in low-resource languages.<br>‚Ä¢ There is a scarcity of large-scale multilingual multimodal training data and standardized evaluation benchmarks for document retrieval.<br>‚Ä¢ Practical systems need models that align text queries and document images across languages and support both single-vector and multi-vector retrieval efficiently.<br>‚Ä¢ Deployment requires flexible embedding dimensionality to trade off accuracy, storage, and speed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>M3DR trains multilingual multimodal retrievers using a large synthetic, layout-aware parallel corpus (22 languages) and VLM-based query synthesis, optimizing contrastive objectives to align text and document-image embeddings. It introduces NetraEmbed (single dense vectors with Matryoshka 768/1536/2560 dims) and ColNetraEmbed (ColBERT-style multi-vector), plus the Nayana-IR benchmark, achieving state-of-the-art cross-lingual retrieval.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond-Synthetic: A Human-Curated Multilingual Multimodal Retrieval Benchmark: Build a real-world, human-authored benchmark with graded relevance to assess generalization and reduce synthetic bias.<br>‚Ä¢ Region-Aware Cross-Lingual Retrieval with Layout-Grounded Late Interaction: Incorporate region/box grounding and layout signals into late-interaction scoring to improve fine-grained cross-script matching and explainability.<br>‚Ä¢ Continual Multilingual Expansion via Low-Resource Adaptation for Document Retrieval: Explore parameter-efficient finetuning and active data synthesis to incrementally add new languages/scripts with minimal labeling.<br>‚Ä¢ Hybrid OCR‚ÄìVision Fusion for Robust Multilingual Document Retrieval: Combine OCR text when reliable with visual embeddings through learned fusion or late interaction to handle tables, charts, and noisy scans.<br>‚Ä¢ Efficient Billion-Scale Multilingual Retrieval with Matryoshka Distillation: Distill multi-vector models into truncatable single-vector embeddings and study compression/indexing strategies for planetary-scale deployments.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">AI & Human Co-Improvement for Safer Co-Superintelligence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05356" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05356" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Fully autonomous self-improving AI poses significant risks (misuse, misalignment, goal misspecification) due to limited human steerability and transparency.<br>‚Ä¢ Existing self-improvement efforts have focused mainly on weight optimization; architecture/code self-modification and autonomous research agents remain immature and potentially unsafe.<br>‚Ä¢ Current AI assistance (e.g., coding, drafting) is a byproduct of general capability training and not explicitly optimized for collaborative AI research workflows.<br>‚Ä¢ Lack of benchmarks, training data, and evaluation protocols for measuring and improving human‚ÄìAI research collaboration across the full research pipeline.<br>‚Ä¢ Need a faster and safer path to superintelligence that leverages complementary human‚ÄìAI strengths and keeps humans in-the-loop for guidance and value alignment.<br>‚Ä¢ Retreat from openness in AI research reduces reproducibility and collective progress; requires managed openness strategies to balance safety and scientific advancement.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Propose a co-improvement paradigm: design AI systems explicitly optimized to collaborate with humans across the end-to-end research pipeline (problem identification, benchmark creation, ideation, experiment design/execution, evaluation, safety co-design). Concretely, create targeted benchmarks and training data for collaborative skills, train models with human/AI feedback to optimize for joint research outcomes, and use iterative, bidirectional co-learning to steer capabilities and safety toward co-superintelligence.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Co-ImproveBench: A Comprehensive Benchmark Suite for Human‚ÄìAI Collaborative Research ‚Äî Evaluate problem discovery, benchmark creation, ideation, experiment design, execution, and error analysis with human-in-the-loop metrics.<br>‚Ä¢ Co-RLAIF: Reinforcement Learning from Human and AI Feedback for Research Collaboration ‚Äî Train models to optimize collaborative utility (clarity, steerability, reliability) rather than solo task scores.<br>‚Ä¢ Safety-by-CoDesign: Joint Development of Values, Constitutions, and Evaluators with Humans and AIs ‚Äî Methods and protocols for co-developing safety objectives and verifying them during research loops.<br>‚Ä¢ GroupMind: Multi-Human‚ÄìMulti-AI Debate and Consensus Synthesis for Research Planning ‚Äî Tools for structured aggregation of viewpoints and actionable consensus in large co-research teams.<br>‚Ä¢ Co-Exec: Shared-Autonomy Workflows and Reproducibility Tooling for AI‚ÄìHuman Experimentation ‚Äî Infrastructure for co-implementation, ablations, logging, and reproducible pipelines.<br>‚Ä¢ Provable Co-Superintelligence: AI-Augmented Formal Methods for Verifying Learning Algorithms and Training Recipes ‚Äî Leverage superhuman ML theory to obtain guarantees on safety and capability improvements.<br>‚Ä¢ Managed Openness for Co-Improvement: Governance and Release Protocols that Balance Safety and Scientific Progress ‚Äî Practical frameworks for open artifacts, red-teaming, and staged disclosure in co-research.<br>‚Ä¢ From AI to Science-at-Large: Extending Co-Improvement to Materials, Biomedicine, and Systems Engineering ‚Äî Generalize collaborative research agents to high-stakes scientific domains.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05277" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05277" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Temporal understanding in autonomous driving (AD) remains a significant challenge, even for recent state-of-the-art (SoTA) Vision-Language Models (VLMs). Prior work has introduced datasets and benchmarks aimed at improving temporal reasoning, but these have emphasized other video content, including sports, cooking, and movies. No existing benchmark focuses exclusively on the unique challenges of temporal understanding in ego-centric AD footage. To fill this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is presented, which evaluates VLMs' ability to capture the dynamic relationships between actions in AD. TAD comprises nearly 6,000 question-answer (QA) pairs, spanning 7 human-designed tasks. In addition, an evaluation is performed that consists of 9 closed- and open-source generalist models as well as SoTA AD specialist models. When applied to TAD, current SoTA models demonstrated substandard accuracies, largely due to imperfect fine-grained motion understanding. To improve motion understanding and overall accuracy on TAD, two novel training-free solutions are proposed: Scene-CoT, that leverages Chain-of-Thought (CoT) and TCogMap, which incorporates an ego-centric temporal cognitive map. The proposed approaches are integrated with existing VLMs and improve average accuracy on TAD by up to 17.72%. By introducing TAD, benchmarking multiple SoTA models, and proposing effective enhancements, this work aims to catalyze future research on temporal understanding in AD. The benchmark and evaluation code are available at https://huggingface.co/datasets/vbdai/TAD{Hugging Face} and https://github.com/vbdi/tad_bench{Github}, respectively.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Absence of an autonomous-driving‚Äìspecific benchmark for temporal understanding; existing video benchmarks focus on domains like sports/cooking/movies and miss AD‚Äôs unique challenges.<br>‚Ä¢ AD videos pose distinctive difficulties‚Äîlarge variation in temporal scales, ego-centric viewpoint (ego car not visible), and subtle fine-grained actions (e.g., gradual lane changes)‚Äîthat current VLMs fail to capture reliably.<br>‚Ä¢ State-of-the-art generalist and AD-specialist models show a significant gap to human performance on temporal reasoning, revealing limitations in fine-grained motion understanding, event ordering, and scene-level dynamics.<br>‚Ä¢ Need for training-free methods that enhance temporal reasoning in VLMs without costly retraining, and systematic evaluation across segment- and scene-level tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce the TAD benchmark (5,861 QA pairs across 150 NuScenes videos, spanning 7 tasks with both segment- and scene-level questions) and propose two training-free integrations‚ÄîScene-CoT (chain-of-thought decomposition of scene motions) and TCogMap (ego-centric temporal cognitive map derived from the ego trajectory)‚Äîto enrich VLM inputs and improve accuracy by up to 17.72%.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Temporal Cognitive Maps for Autonomous Driving with Weak Supervision: Train a learnable TCogMap module end-to-end with VLMs to jointly model trajectories, scene context, and reasoning.<br>‚Ä¢ Segment-to-Scene Pretraining: Multiscale Temporal Representation Learning for Ego-centric Driving Videos: Pretrain VLMs with coupled segment- and scene-level tasks to align fine-grained actions with long-horizon temporal dynamics.<br>‚Ä¢ Trajectory-Augmented Vision-Language Models for Robust AD Temporal Reasoning: Integrate vehicle kinematics, HD map priors, and spatial graphs into prompt structures or adapters to strengthen motion understanding and event ordering.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ProPhy: Progressive Physical Alignment for Dynamic World Simulation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05564" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05564" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Current video generation models often violate fundamental physical laws, especially in high-motion, large-scale, or multi-phenomena scenarios.<br>‚Ä¢ Existing approaches respond isotropically to physical prompts and lack explicit, discriminative physics-aware conditioning.<br>‚Ä¢ Prior MoPE methods (e.g., WISA) perform video-level routing and capture only global physical cues, failing to localize and coordinate multiple concurrent physical processes.<br>‚Ä¢ Distillation/RLHF-based methods (e.g., VideoREPA, PhysMaster) provide implicit alignment and insufficient fine-grained, token-level modeling of physical dynamics.<br>‚Ä¢ There is a need for fine-grained spatial anisotropy and progressive extraction/injection of physics-specific priors to build physically coherent world simulators.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ProPhy introduces a two-stage Mixture-of-Physics-Experts (MoPE): Semantic Experts extract physics-specific priors from text and fuse them with visual features, while Refinement Experts perform token-level routing to model localized dynamics. A physical alignment strategy transfers VLM-predicted spatial distributions into the Refinement Experts to enforce explicit, anisotropic, physics-aware generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Physics Priors from Multi-Modal Cues for Anisotropic Video Generation: Extend ProPhy to fuse audio, depth, and tactile signals to enrich semantic and token-level physics priors.<br>‚Ä¢ VLM-Free Fine-Grained Physical Alignment via Differentiable Physics and Counterfactual Supervision: Replace external VLM guidance with self-supervised signals from differentiable simulators and counterfactual perturbations to learn spatial physical distributions.<br>‚Ä¢ Continual Mixture-of-Physics-Experts for Open-World Dynamics: Develop a lifelong ProPhy that discovers, expands, and adapts experts to new physical phenomena and domains without catastrophic forgetting.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05409" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05409" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLM deployment is constrained by memory and compute; pushing PTQ below W8A8 often degrades accuracy while failing to realize theoretical speedups.<br>‚Ä¢ Hardware‚Äìalgorithm gap: GPUs lack native hybrid-precision (e.g., W4A8) paths, forcing fallbacks to W8A8 and negating throughput gains; existing 2:4 semi-structured sparsity is rarely used due to accuracy loss.<br>‚Ä¢ Uniform quantization and fixed sparsity cannot capture non-uniform information and outliers in weights/activations, causing large errors; dynamic activation selection adds runtime overhead not well supported by current GPUs.<br>‚Ä¢ Need a hardware-friendly unified format that jointly exploits sparsity and quantization, balances accuracy/throughput, and is compatible with existing and future accelerators.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce SQ-format, a bank-based sparse-quantized representation that splits one GEMM operand into a sparse high-precision part and a dense low-precision part with masks and per-bank sparsity, enabling hybrid-precision matrix multiplication. Provide PTQ algorithms for weights (Hessian/importance-based selection combining SmoothQuant + GPTQ) and activations (static masks from per-channel A¬∑W statistics), plus hardware designs that process low-precision dense and high-precision sparse paths in parallel with gather/scatter support.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Native SQ-format Tensor Cores: Co-designing Gather-Aware Hybrid-Precision Units for LLM Inference: Build and evaluate hardware units that natively support banked masks, dynamic activation selection, and parallel sparse/dense paths to fully unlock SQ-format throughput.<br>‚Ä¢ Training with SQ-format: End-to-end Sparse-Quantized Fine-Tuning for Robust Hybrid Precision: Integrate SQ-format into training/fine-tuning to learn masks, bank sizes, and precision splits jointly, improving accuracy and robustness beyond PTQ.<br>‚Ä¢ Adaptive SQ-format Autotuning: Layer-wise Optimization of Bank Size, Sparsity, and Precision Allocation: Develop calibration-driven or RL-based search to automatically set b, s, and hhigh/hlow per layer/channel for optimal accuracy‚Äìthroughput trade-offs.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04694" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04694" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Effective earthquake risk reduction relies on accurate site-specific evaluations. This requires models that can represent the influence of local site conditions on ground motion characteristics. In this context, data driven approaches that learn site controlled signatures from recorded ground motions offer a promising direction. We address strong ground motion generation from time-domain accelerometer records and introduce the TimesNet-Gen, a time-domain conditional generator. The approach uses a station specific latent bottleneck. We evaluate generation by comparing HVSR curves and fundamental site-frequency f_0 distributions between real and generated records per station, and summarize station specificity with a score based on the f_0 distribution confusion matrices. TimesNet-Gen achieves strong station-wise alignment and compares favorably with a spectrogram-based conditional VAE baseline for site-specific strong motion synthesis. Our codes are available via https://github.com/brsylmz23/TimesNet-Gen.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Need for accurate site-specific strong motion generation to improve seismic hazard assessment, capturing local geology-driven site effects in time-domain waveforms<br>‚Ä¢ Existing classical synthesis methods (empirical/semi-empirical/physics-based) either oversimplify or are computationally expensive, and struggle with nonstationary temporal‚Äìspectral behavior<br>‚Ä¢ Deep learning approaches lack a foundational model that jointly captures complex time-domain and spectral patterns of strong-motion records<br>‚Ä¢ Station-specific conditioning is underexplored; limited per-station data and traditional conditioning fail to reliably learn site-dependent signatures<br>‚Ä¢ Absence of robust, physically meaningful evaluation standards for generative seismic data; need distribution-level metrics tied to site response (HVSR and fundamental frequency f0)</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TimesNet-Gen adapts TimesNet into a time-domain conditional generator with station-ID feature modulation and a non-variational latent bottleneck, sampling via averaging multiple station-specific latent codes plus Gaussian noise. Trained with two-phase (unsupervised pretraining, station-conditioned fine-tuning) and evaluated using HVSR curves and f0-distribution confusion matrices, it outperforms a spectrogram-based conditional VAE baseline.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Physics-Guided TimesNet-Gen for Broadband Earthquake Synthesis: Integrate source, path, and site parameters and physical constraints to steer generation while preserving learned station-specific signatures.<br>‚Ä¢ Site-Aware Generative Evaluation: HVSR‚Äìf0 Metrics Augmented with P/S Arrivals and IMs: Develop comprehensive, uncertainty-aware metrics incorporating P/S arrival distributions, PGA, duration, and other intensity measures.<br>‚Ä¢ TimesNet-Gen Encoders for Downstream Seismic Intelligence: Phase Picking and Ground-Motion Parameter Estimation: Leverage learned latent representations for robust phase picking, Vs30/parameter estimation, and event classification with semi-supervised training.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.03667" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.03667" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Multimodal colonoscopy lacks a comprehensive, category-rich, and task-diverse dataset to train and evaluate models at scale.<br>‚Ä¢ Existing MLLMs show weak generalizability across colonoscopy tasks; closed-source models lead overall while open-source variants sometimes excel in safety monitoring, revealing a specialist‚Äôs paradox for medical-specific models.<br>‚Ä¢ Clinical reliability is undermined by text-dominance biases, including on-image text manipulation and case-contradicting or emotionally charged instructions.<br>‚Ä¢ Reasoning-enabled models often improve interpretability without improving decision accuracy, highlighting a gap between multimodal understanding and trustworthy clinical reasoning.<br>‚Ä¢ R1-style optimization in data-scarce settings suffers from intra-group advantage vanishing, causing unstable training and limiting real-world deployment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper builds a large-scale multimodal colonoscopy foundation (COLONVQA), evaluates generalizability (COLONEVAL) and reliability under perturbations (COLONPERT), and advances clinical reasoning by curating COLONREASON and training COLONR1‚Äîan R1-styled model with task-adaptive rewards, negative sampling, and self-evolving prompting to stabilize optimization and boost accuracy under data scarcity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Robust Multimodal Fusion to Neutralize Text-Dominance in Endoscopic AI: Develop causality-aware fusion and on-image text detection/denoising to mitigate implicit/explicit text biases in clinical outputs.<br>‚Ä¢ Clinician-of-Thought: Auditable, Guideline-Aligned Reasoning Traces for Colonoscopy Decision Support: Align model reasoning with clinical guidelines, calibrate risk, and provide verifiable rationales for safety-critical decisions.<br>‚Ä¢ Continual R1 Training with Human-in-the-Loop for Real-Time Colonoscopy Assistance: Design online, task-adaptive reinforcement learning that leverages streaming data and expert feedback to maintain robustness across evolving clinical distributions.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">From FLOPs to Footprints: The Resource Cost of Artificial Intelligence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04142" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04142" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As computational demands continue to rise, assessing the environmental footprint of AI requires moving beyond energy and water consumption to include the material demands of specialized hardware. This study quantifies the material footprint of AI training by linking computational workloads to physical hardware needs. The elemental composition of the Nvidia A100 SXM 40 GB graphics processing unit (GPU) was analyzed using inductively coupled plasma optical emission spectroscopy, which identified 32 elements. The results show that AI hardware consists of about 90% heavy metals and only trace amounts of precious metals. The elements copper, iron, tin, silicon, and nickel dominate the GPU composition by mass. In a multi-step methodology, we integrate these measurements with computational throughput per GPU across varying lifespans, accounting for the computational requirements of training specific AI models at different training efficiency regimes. Scenario-based analyses reveal that, depending on Model FLOPs Utilization (MFU) and hardware lifespan, training GPT-4 requires between 1,174 and 8,800 A100 GPUs, corresponding to the extraction and eventual disposal of up to 7 tons of toxic elements. Combined software and hardware optimization strategies can reduce material demands: increasing MFU from 20% to 60% lowers GPU requirements by 67%, while extending lifespan from 1 to 3 years yields comparable savings; implementing both measures together reduces GPU needs by up to 93%. Our findings highlight that incremental performance gains, such as those observed between GPT-3.5 and GPT-4, come at disproportionately high material costs. The study underscores the necessity of incorporating material resource considerations into discussions of AI scalability, emphasizing that future progress in AI must align with principles of resource efficiency and environmental responsibility.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing AI sustainability assessments largely ignore the material footprint of specialized hardware, focusing mainly on energy and water.<br>‚Ä¢ Rapid scaling of AI training increases demand for GPUs containing heavy and toxic metals, posing upstream extraction and downstream e-waste risks.<br>‚Ä¢ There is no standardized link between digital compute metrics (FLOPs/MFU) and physical resource consumption, hindering accurate footprint estimates.<br>‚Ä¢ Lack of transparent reporting on model FLOPs utilization (MFU) and realistic hardware lifespans limits reliability of current evaluations.<br>‚Ä¢ Efficiency gains can trigger Jevons effects, so improvements may increase total resource use without material-conscious planning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Disassemble an Nvidia A100 SXM 40GB GPU and quantify its elemental composition via ICP-OES, then map model training FLOP budgets to GPU lifetime throughput (adjusted by MFU and lifespan) to estimate GPUs required and scale elemental masses to derive material footprints.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Global Material Ledger for AI: From Training Runs to Industry-Wide Inventories: Combine GPU shipment data, training telemetry, and end-of-life flows to quantify sector-level material use and waste.<br>‚Ä¢ MFU Disclosure Standard for Frontier Model Training: Define and validate a reporting protocol for MFU, throughput, and failure rates to improve compute-to-material footprint accuracy.<br>‚Ä¢ Full-Stack Footprints: Incorporating Networking, Storage, and Cooling into AI LCAs: Extend the method beyond GPUs to include all data center hardware for comprehensive material accounting per model.<br>‚Ä¢ Thermal Co-Design for Longevity: Waste-Heat Reuse and Cooling Architectures that Extend Accelerator Lifespan: Evaluate cooling and thermal strategies that reduce wear, lengthen GPU lifetimes, and cut material demand.<br>‚Ä¢ Capability-per-Kilogram: Benchmarking AI Progress by Material Efficiency: Introduce metrics linking benchmark gains to kilograms of (toxic) materials to guide resource-efficient model development.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05774" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05774" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Long video queries hinge on sparse, temporally dispersed cues hidden amid hours of redundant content, making dense processing inefficient and brittle.<br>‚Ä¢ Caption-based agentic frameworks rely on query-agnostic captioners, wasting computation and blurring fine-grained temporal/spatial details, leading to imprecise grounding and weaker causal tracing.<br>‚Ä¢ Existing methods lack query-adaptive, iterative observation (deciding what/when/where to look) with sufficiency assessment, limiting efficiency and reliability in complex LVU.<br>‚Ä¢ There is a need to improve accuracy while drastically reducing inference time and input tokens on realistic long-video benchmarks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>AVP is a training-free, agentic framework that iteratively plans‚Äìobserves‚Äìreflects: a planner issues query-conditioned actions (what/where/how), an observer extracts compact, time-stamped evidence directly from pixels, and a reflector evaluates sufficiency to stop or trigger further observation‚Äîachieving efficient, fine-grained grounding without reliance on caption databases.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Query-Driven Active Video Policies via Reinforcement Learning: Train the planner to optimize cost-aware what/where/how decisions under efficiency and accuracy constraints, surpassing prompt-only heuristics.<br>‚Ä¢ Uncertainty-Calibrated Reflection for Reliable Stopping in Agentic Video Reasoning: Integrate Bayesian or calibrated confidence models to improve sufficiency estimation and stopping criteria for long, ambiguous queries.<br>‚Ä¢ Tool-Augmented Active Video Perception with Fine-Grained Pixel-Level Grounding: Combine AVP with specialized tools (tracking, detection, OCR) to enhance precision and causal tracing while maintaining compact, query-relevant evidence.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05339" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05339" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLMs can still produce unsafe content despite post-training alignment, necessitating robust, real-time guardrails on both inputs and outputs.<br>‚Ä¢ Existing guardrails rely on fixed, static safety taxonomies, making them brittle to context (age group, culture, regulation, platform) and causing over- or under-blocking.<br>‚Ä¢ Classifier-based APIs and programmable rule systems struggle with long, complex LLM contexts and cannot learn or generalize to nuanced violations.<br>‚Ä¢ Adding or evolving safety categories typically requires re-labeling or retraining, limiting scalability in fast-changing environments.<br>‚Ä¢ Public safety benchmarks are narrow, often lack response-level labels and rich taxonomies, and saturation plus proprietary datasets hinder transparent, reproducible evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Roblox Guard 1.0 is an instruction fine-tuned Llama-3.1-8B guardrail that performs binary moderation on prompts and responses, trained on a large mixed corpus using a three-stage synthetic data pipeline (policy-conditioned prompt generation, multi-model response generation, LLM-as-judge labeling) plus open-source datasets, and enhanced with chain-of-thought rationales and input inversion to enable taxonomy-adaptive generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Continual Taxonomy Adaptation for LLM Guardrails via Parameter-Efficient Updates: Develop online/continual learning methods to incorporate evolving safety policies without full retraining.<br>‚Ä¢ Multimodal Guardrails: Taxonomy-Adaptive Safety Across Text, Image, and Voice: Extend taxonomy-adaptive moderation to multimodal inputs with unified cross-domain reasoning.<br>‚Ä¢ Policy-Conditioned Moderation at Inference: Conditioning Guardrails on Explicit Product Policies: Feed product-specific policy documents at inference to dynamically tailor guardrail behavior to context and jurisdiction.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 8</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-12-08 23:08:04 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 8;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>