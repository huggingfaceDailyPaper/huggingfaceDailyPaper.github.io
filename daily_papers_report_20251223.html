<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - December 23, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">December 23, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.16676" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.16676" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\% execution accuracy in Text-to-SQL over SynSQL, +7\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLM data preparation is fragmented and adâ€‘hoc, lacking principled dataflow abstractions and making workflows hard to reproduce, debug, and reuse (Section 1).<br>â€¢ Existing systems are largely extraction/filtering-oriented and offer limited support for iterative, model-in-the-loop generation with fine-grained semantic control (Table 1, p.7).<br>â€¢ General big-data ETL tools (e.g., Spark/Dask/Hadoop) are ill-suited for unstructured, semantics-heavy text workflows and provide no native support for GPU-efficient LLM batching or token-level operations (Section 2.2).<br>â€¢ There is no unified, backend-agnostic programming model that composes operators, prompt templates, storage, and serving into debuggable pipelines with compile-time validation and resumption (Sections 3â€“4).<br>â€¢ The growing need for high-quality synthetic data across domains requires standardized, reusable pipelines and an ecosystem that ensures reliability, reproducibility, and community sharing (Sections 1, 7).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DataFlow is a PyTorch-like, LLM-first framework that unifies storage, serving, prompt templates, and composable operators (generate/evaluate/filter/refine) into debuggable pipelines with compile-time validation and stepwise resumption; DataFlow-Agent (LangGraph-based) translates natural-language specs into executable DAGs via retrieveâ€“reuseâ€“synthesize operator construction and sandboxed verification.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ DataFlow-Multimodal: Unifying LLM-Driven Data Preparation for Text, Tables, Graphs, and Vision: Extend core abstractions and operators to new modalities with modality-aware validation and conversions.<br>â€¢ AutoFlow: Cost- and Quality-Aware Compilation and Optimization of LLM Data Pipelines: Learn to schedule, batch, and route across LLM backends with compile-time/static analysis to minimize tokens/$ while maximizing dataset quality.<br>â€¢ AgentFlow: Self-Improving Agents for Automatic Operator Synthesis, Verification, and Reuse: Enhance the agentic layer with RAG+RL/self-reflection, formal specs, and static checks to reliably synthesize, debug, and generalize pipelines.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.19693" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.19693" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Fragmented pipelines between semantic encoders (for understanding) and pixel encoders/VAEs (for generation) yield heterogeneous, conflicting latents that reduce training efficiency and make unified models brittle.<br>â€¢ Existing unification attempts either transplant semantic encoders into generators (faster convergence but limited fine detail) or bolt semantics onto pixel encoders (coexistence via trade-offs), falling short of genuine integration of abstraction and fidelity.<br>â€¢ Missing spectral perspective: empirical analyses show semantic encoders emphasize low-frequency content while pixel encoders preserve high-frequency detail; this mismatch causes representational conflicts across modalities and is underexploited by current tokenizers.<br>â€¢ Need a single tokenizer/latent that is both semantically expressive and pixel-faithful (diffusion-friendly) to support reconstruction, generation, and recognition in one space with minimal compromise.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Unified Autoencoding (UAE) initializes from a semantic encoder and factorizes its latent features into a low-frequency semantic base and residual high-frequency bands via an FFT-based band projector with iterative residual split; it aligns only the low band to the teacher (semantic-wise loss) while reconstructing images with a spectral-transform ViT decoder and robustness-improving noise injection on high-frequency bands to harmonize semantics and pixel fidelity in one latent space.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Prism-Guided Multimodal Tokenizers for Audioâ€“Videoâ€“3D: Generalize UAEâ€™s band factorization to audio, video, and 3D/depth so modalities share a low-frequency semantic base with modality-specific high-frequency bands.<br>â€¢ Learned Anisotropic Band Partitioning and Task-Conditioned Modulation: Replace fixed radial masks with learnable, content- and task-aware spectral partitions and gates to better allocate frequency capacity for detection, segmentation, and editing.<br>â€¢ Frequency-Curricular Diffusion and Autoregressive Sampling on UAE Latents: Design training/sampling schedules that progressively traverse bands from low to high with adaptive compute, improving speedâ€“fidelity trade-offs and controllability.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Region-Constraint In-Context Generation for Instructional Video Editing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.17650" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.17650" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Text-only instructional video editing often fails to localize the true edit region, causing unintended changes outside the target area.<br>â€¢ During joint denoising, tokens from editing and non-editing regions interfere, degrading novel object generation and background coherence.<br>â€¢ Prior methods commonly require masks or task-specific controls; training-free/inversion approaches lack temporal consistency and generalization; image in-context editing does not directly address videoâ€™s temporal and cross-region interactions.<br>â€¢ A major bottleneck is the lack of large-scale, high-quality instructionâ€“video pairs for supervised training.<br>â€¢ There is a need for a unified editor that supports local object add/remove/replace and global style transfer with precise edits, preserved backgrounds, and high temporal stability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>ReCo reformulates instruction-based video editing as in-context joint denoising by width-wise concatenating source and target videos in a video DiT, and introduces two region-constraint regularizers on one-step denoised latents and attention maps to amplify differences in the edit region, suppress interference from source-edit tokens, and preserve non-edit areas. Training uses flow-matching with an auxiliary video condition branch and the 500K-sample ReCo-Data for broad coverage of add/remove/replace/style tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Mask-Free Region Discovery for Instructional Video Editing via Self-Supervised Attention: Learn edit-region masks directly from textâ€“video cues without ground-truth masks, enabling fully mask-agnostic training and inference.<br>â€¢ Cross-Region Causality for Token Interference Suppression in Video Diffusion: Impose causal, temporally-aware attention constraints to further prevent leakage from source edit regions and improve coherence across frames.<br>â€¢ Unified Instruction-Following Video Editor with Multimodal Planning and In-Context Feedback: Integrate a VLM planner to parse instructions, propose region candidates, and iteratively refine edits via side-by-side in-context comparisons and uncertainty-guided updates.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.19134" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.19134" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Dynamic RAG methods trigger retrieval using model-internal signals (probabilities/entropy/attention) that are ill-calibrated, leading to confident hallucinations and mis-triggered retrieval (pp. 1â€“2; Fig. 1).<br>â€¢ Static or fixed-schedule retrieval fails on multi-hop questions where information needs arise during generation (p. 1).<br>â€¢ Continuous internal uncertainty scores lack clear thresholds and transfer poorly across models/domains, yielding inconsistent performance and efficiency (pp. 2, 5â€“6; Tab. 1).<br>â€¢ LLMs struggle with long-tail knowledge and unseen relations; rare entities and zero co-occurrence pairs are high-risk but undetectable from model states alone (pp. 2â€“4).<br>â€¢ Need a principled, model-agnostic, and efficient trigger grounded in objective evidence from the pre-training corpus that shapes the modelâ€™s knowledge (pp. 2â€“3).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>QuCo-RAG quantifies uncertainty from pre-training corpus statistics via two stages: (1) pre-generation knowledge assessment that triggers retrieval when question entities are low-frequency, and (2) runtime claim verification that triggers retrieval/regeneration when generated headâ€“tail entity co-occurrence in the corpus is zero. Both stages use Infini-gram for millisecond-latency frequency and co-occurrence queries over a 4T-token corpus, with a lightweight triplet extractor to form verification targets (Fig. 2, pp. 3â€“4).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Time-Aware QuCo-RAG: Temporal Corpus Statistics for Evolving Knowledge: Build time-stamped indices and recency-aware co-occurrence to detect stale vs. emerging facts and trigger timely retrieval.<br>â€¢ Beyond Entities: Relation- and Number-Aware Corpus Verification for Dynamic RAG: Extend verification beyond entity pairs to canonicalized relations and normalized numerical claims using entity linking to reduce alias errors and catch numerical hallucinations.<br>â€¢ Proxy-Corpus Theory for Dynamic RAG: Formal Bounds on Cross-Model Transferability: Develop information-theoretic analyses and diagnostics predicting when statistics from a proxy web-scale corpus reliably signal uncertainty for models trained on overlapping but undisclosed data.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.17040" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.17040" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production. A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations. To address this, existing methods either train trajectory-conditioned video generation model on trajectory-video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions. Nevertheless, existing methods struggle to generate camera-pose-faithful, high-quality videos for two main reasons: (1) reprojection-based approaches are highly susceptible to errors caused by inaccurate depth estimation; and (2) the limited diversity of camera trajectories in existing datasets restricts learned models. To address these limitations, we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity. The framework integrates two key components: (1) infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model. Conditioning on this noise-free rotational information, the residual parallax term is predicted through end-to-end training to achieve high camera-pose fidelity; and (2) a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths. Experimental results demonstrate that InfCam outperforms baseline methods in camera-pose accuracy and visual fidelity, generalizing well from synthetic to real-world data. Link to our project page:https://emjay73.github.io/InfCam/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Depthâ€‘reprojection approaches rely on monocular depth; inaccuracies cause cascading reprojection errors and artifacts, and depth networks are not updated endâ€‘toâ€‘end during training, hurting pose fidelity (see Fig. 2 on page 3).<br>â€¢ Trajectoryâ€‘conditioned models inherit dataset biases (limited and synchronized trajectories with identical first frames), leading to overâ€‘preservation of the sourceâ€™s initial frame and poor generalization to new motions and intrinsics (pages 2, 5â€“6; Fig. 4 on page 5).<br>â€¢ Existing datasets have limited cameraâ€‘motion diversity and fixed focal lengths, biasing models to keep the source focal length and reducing robustness to FOV changes (pages 2, 5â€“6).<br>â€¢ Practically important: creators need precise, temporally consistent camera control in postâ€‘production without reshoots; methods must maintain appearance, handle occlusions, and follow exact trajectories (Abstract; page 2).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>InfCam conditions a video diffusion model on infinite-homographyâ€“warped source latents to inject noiseâ€‘free rotation and learns the residual parallax (translation/depth effect) endâ€‘toâ€‘end via a homographyâ€‘guided selfâ€‘attention layer, while a trajectory and intrinsic (focalâ€‘length) augmentation pipeline diversifies training to remove dataset biases.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ InfCamâ€‘XL: Longâ€‘Horizon Cameraâ€‘Controlled Video Generation with Recurrent Memory â€” Extend InfCam to minutesâ€‘long sequences using recurrent/kvâ€‘cache memory and keyframe anchors to maintain pose fidelity and temporal coherence.<br>â€¢ Selfâ€‘Calibrating InfCam: Joint Intrinsics and Trajectory Estimation from Inâ€‘theâ€‘Wild Videos â€” Integrate intrinsics (fx, fy, cx, cy) and pose estimation into the latent homography pipeline to remove reliance on external calibration at inference.<br>â€¢ Dynamicâ€‘Parallax InfCam: Uncertaintyâ€‘Aware Residual Parallax for Nonâ€‘Rigid and Occlusionâ€‘Heavy Scenes â€” Model a probabilistic/deformable parallax field with uncertainty to better handle moving objects, disocclusions, and complex scene geometry.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.18880" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.18880" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Cold-start in item difficulty estimation: operational IRT calibration needs large-scale field testing, delaying deployment and raising costs, yet difficulty is foundational for adaptive testing, curriculum design, and controlled item generation (Introduction; noted across datasets). <br>â€¢ Gap between solving and perceiving difficulty: even high-performing LLMs may not recognize human cognitive hurdles; alignment with student struggles is unknown and crucial for using LLMs in assessment (Introduction). <br>â€¢ Limitations of prior IDP: supervised/text-based models depend on historical response data and are domain-specific; expert ratings are subjective and misaligned with psychometric difficulty (Related Work A.1). <br>â€¢ Systematic Humanâ€“AI misalignment: scaling/reasoning do not reliably improve difficulty prediction; models form a machine consensus that diverges from humansâ€”average Spearman â‰ˆ0.28, with USMLE â‰ˆ0.13 (Table 1, p.4), and predicted distributions are collapsed/optimistic (Figure 1, p.4). <br>â€¢ Simulation and ensembling are insufficient: persona prompts give noisy, inconsistent gains; ensembles are bounded and degrade when adding weaker models (Figure 2, p.5; Figure 3, p.5; Table 2, p.6). <br>â€¢ Curse of knowledge: model-derived IRT difficulty correlates even worse with human difficulty; many human-hard items are trivial for models (Savant rate up to 70.4% on USMLE) and overall saturation is high (Table 3, p.6). <br>â€¢ Lack of metacognitive self-awareness: predicted difficulty poorly separates a modelâ€™s own failures (AUROC â‰ˆ0.55 across domains), indicating difficulty judgments are decoupled from actual correctness (Table 5, p.8).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A large-scale empirical framework measures Humanâ€“AI Difficulty Alignment by evaluating LLMs as observers (zero-shot difficulty prediction; Spearman correlation to human ground truth) and as actors (derive machine difficulty via Rasch IRT by treating models as examinees; correlate with human difficulty), while probing proficiency simulation (low/medium/high persona prompts), ensembling, and metacognition (AUROC of difficulty vs. model correctness) across four domains.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Aligning LLM Difficulty Perception with Student Traces: Fine-Tuning on Real Response Logs: Train LLMs with student response trajectories to ground difficulty judgments in authentic human error patterns and reduce misalignment.<br>â€¢ Knowledge Masking for Authentic Proficiency Simulation: Toward Capability-Suppressed LLMs: Develop methods (e.g., adapter gating/knowledge masking/RL penalties) that reliably throttle competence to simulate low-proficiency cognition without leaking correct solutions.<br>â€¢ IRT-Aware Multi-Task Learning for Joint Solving and Difficulty Estimation: Jointly predict answers and human IRT parameters under calibration/consistency constraints to couple perception with capability.<br>â€¢ Active Calibration for Cold-Start Difficulty with Minimal Field Testing: A Bayesian Updating Approach: Use small, targeted human samples to rapidly calibrate LLM difficulty predictions and reweight model ensembles by calibration quality.<br>â€¢ Benchmarking Difficulty Alignment Across Domains and Subpopulations: Building a Field-Tested, Bias-Aware Suite: Curate broader, publicly available datasets with validated difficulty and subgroup labels to stress-test alignment, machine consensus, and fairness.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.19678" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.19678" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a "fill-and-revise" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: https://hyokong.github.io/worldwarp-page/{https://hyokong.github.io/worldwarp-page/}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Long-range, geometrically consistent novel view synthesis from a single image is hard because 3D-consistent pixel-space constraints clash with diffusion models that operate best in latent, camera-conditioned spaces.<br>â€¢ Existing camera-pose embeddings provide weak 3D scene content and generalize poorly to out-of-distribution trajectories, causing pose drift and incoherent geometry.<br>â€¢ Explicit 3D priors (meshes/point clouds/3DGS) used as static warps suffer from occlusions (holes), distortions, and irreversible error propagation, while standard inpainting/generation fails to simultaneously fix disocclusions and warped artifacts.<br>â€¢ Autoregressive causal generators are ill-suited for using future-view warped hints and tend to accumulate errors over long sequences.<br>â€¢ Maintaining fidelity over very long camera paths is critical for VR, telepresence, and interactive 3D exploration, yet current methods degrade quickly in quality and geometric stability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>WorldWarp generates video chunk-by-chunk by coupling an online-optimized 3D Gaussian Splatting (3DGS) cache that forward-warps recent history into future views with a non-causal Spatio-Temporal Diffusion (ST-Diff) refiner. ST-Diff uses a spatially-temporally varying noise schedule (full noise in occluded regions, partial noise in warped regions) to jointly fill disocclusions and revise warped content, maintaining long-term 3D consistency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ End-to-End Joint Geometry-and-Diffusion Training for View Extrapolation: Co-train depth/pose estimation and ST-Diff with differentiable warping to reduce reliance on external geometry and mitigate warp errors.<br>â€¢ Uncertainty-Aware 3DGS Cache with Adaptive Noise Scheduling: Estimate per-pixel/per-splat uncertainty in the cache and map it to noise strengths for more reliable fill-and-revise behavior.<br>â€¢ WorldWarp for Dynamic Scenes: Incorporate scene flow or dynamic Gaussian splats to handle moving objects and non-rigid changes while preserving multi-view consistency.<br>â€¢ Loop-Consistent Long-Horizon Video with Global Pose Optimization: Add loop-closure and global bundle-adjustment-like constraints across chunks to suppress long-term drift over thousands of frames.<br>â€¢ Real-Time WorldWarp via Distilled Few-Step Diffusion: Distill ST-Diff into a small-step or flow-matching model to approach real-time generation without sacrificing geometric fidelity.<br>â€¢ Text-Editable WorldWarp under 3D Constraints: Disentangle geometry and appearance to enable style/semantic edits guided by text while strictly maintaining 3D structure.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.19629" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.19629" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the https://steinate.github.io/logoplanner.github.io/{project page}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Traditional modular pipelines incur latency and cascading errors across perceptionâ€“localizationâ€“mappingâ€“planning, and are brittle in real deployments (e.g., legged robot vibrations degrade odometry and destabilize planning)<br>â€¢ Existing end-to-end planners still rely on explicit localization (SLAM/VO) and precise extrinsic calibration; short temporal windows cause drift and hinder cross-embodiment generalization<br>â€¢ Single-frame or scale-ambiguous geometry lacks long-term, metric-aware, dense 3D context (including occluded and rear regions), weakening obstacle avoidance and planning consistency<br>â€¢ Passing explicit poses/point clouds downstream propagates upstream errors, and scale ambiguity prevents aligning geometry with planned trajectories</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>LoGoPlanner fine-tunes a long-horizon video-geometry backbone with depth-derived scale priors to recover metric-scale scene geometry and implicit ego-state from RGB-D sequences, while decoupling camera and chassis frames for cross-embodiment robustness. A query-based planner fuses state and geometry tokens with the goal, and a diffusion head iteratively denoises actions to yield collision-free trajectories without explicit localization or mapping modules.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ ScaleReal: Large-Scale Metric-Scale Video-Geometry Pretraining for Real-World Navigation: Build and pretrain on diverse real-world RGB-D video datasets with metric supervision to boost reconstruction fidelity and sim-to-real robustness.<br>â€¢ AutoCalNav: Self-Calibrating End-to-End Navigation Across Arbitrary Sensor Embodiments: Learn online extrinsic calibration and sensor fusion (e.g., RGB-D+IMU) within the policy to maintain localization grounding under changing viewpoints and hardware.<br>â€¢ GeoMemory-Nav: Persistent Metric 3D Memory for Long-Horizon, Loop-Consistent Navigation: Integrate a persistent geometric memory with loop-closure reasoning to enable long-range planning, re-localization, and consistent map-aware trajectory optimization.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.17385" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.17385" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Code LLMs rely on large, curated instruction data or external code corpora that are expensive to collect, verify, and scale.<br>â€¢ Desire for post-training that improves models without any external corpus (not even unlabeled code), by exploiting internal knowledge and deterministic execution feedback.<br>â€¢ Existing â€˜unsupervisedâ€™ approaches still depend on open-source code datasets and synthetic QA; they lack a principled way to verify correctness without references and to extract reliable training signals from noisy self-generated outputs.<br>â€¢ Three core challenges: (1) automatically constructing diverse, well-specified programming problems; (2) verifying correctness without ground-truth solutions; (3) building a stable, iterative self-bootstrapping signal from noisy candidates.<br>â€¢ Importance: reduces dependency on labeled data and compute, enables training in resource-constrained settings, and shows internal model states contain useful signals for code quality/correctness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>UCoder introduces IPC, a six-stage, corpus-free self-bootstrapping pipeline: probe the model to generate problems and tests, densely sample solutions, and select high-quality code via execution-driven consensus clustering with theoretical guarantees, then iteratively fine-tune on the selected samples. Selection combines execution success, consensus strength, and code fluency to reliably identify correct implementations without any external data.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ SpecSynth: Formal Specification and Test Synthesis for Corpus-Free Code Post-Training: Integrate symbolic execution, property-based testing, and fuzzing to automate stronger test/spec generation beyond unit tests.<br>â€¢ Active Consensus: Cost-Efficient Sampling for Unsupervised Code Learning: Use uncertainty estimation and representation clustering to cut candidate count while preserving consensus reliability.<br>â€¢ Beyond Pass@k: Multi-Objective Unsupervised Training for Secure and Maintainable Code: Add static analysis, security linters, and style/maintainability metrics as auxiliary signals alongside execution.<br>â€¢ Polyglot-UCoder: Extending Internal Probing to Multi-Language and Typed Paradigms: Generalize the framework to Java/C++/JS with compilation- and type-aware test harnesses and cross-language transfer.<br>â€¢ File-to-Repo UCoder: Scaling from Functions to Multi-File, Project-Level Generation: Generate multi-module tasks and integration tests to handle package management, I/O, and end-to-end workflows.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.19682" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.19682" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective Î±-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to +40.3\% over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3times less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ High cost, slowness, and poor parallelizability of real-world interactions for training LLM agents; discovering failure modes (e.g., small UI changes) is expensive and brittle (page 2, â€œAdd to Basketâ€ example).<br>â€¢ Over-reliance on static, pre-collected expert trajectories that are a fixed snapshot of the world, limiting coverage and generalization even as dataset size grows (page 2).<br>â€¢ Existing synthetic data pipelines mostly produce static corpora that do not adapt to the agentâ€™s evolving weaknesses, leading to inefficient learning (page 2).<br>â€¢ Lack of an automated, difficulty-aligned curriculum that keeps tasks in the agentâ€™s zone of proximal development without manual scheduling (pages 3â€“4; Figure 3).<br>â€¢ Need for a scalable, low-cost alternative to real-world interaction via an LLM-based environment simulator that can generate diverse, evaluable tasks on demand (pages 3â€“5).<br>â€¢ Desire for data efficiency: outperform static teacher-based augmentation (e.g., Gemini 2.5 Pro) with far less data via targeted simulation (Figure 1b on page 1; Figure 6b on page 12).<br>â€¢ Requirement for stability and calibration so tasks are neither trivial nor impossible; difficulty should self-adjust as the agent improves (Figure 7 on page 13).<br>â€¢ Practical importance: large, cross-benchmark gains vs 7B baselines and competitiveness with larger models, indicating data-evolving training can rival model scaling (Figure 1a on page 1; Table 1 on page 9).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>GenEnv co-evolves an Agent Policy and an LLM Environment Policy: the simulator generates tasks and is rewarded to keep the agentâ€™s empirical success near a target Î± (via Renv = exp(âˆ’Î²(È·pâˆ’Î±)^2)), aligning task difficulty with the agentâ€™s current capability. The agent is optimized with GRPO on task rewards while the environment is updated via reward-weighted regression with KL regularization, yielding an adaptive, data-evolving curriculum that steadily increases task complexity (Figures 3, 5, 6; Table 2).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive-Î± GenEnv: Learning the Target Difficulty for Maximal Learning Signal: Meta-learn per-skill or per-domain Î± to optimize learning dynamics beyond a fixed 0.5 target.<br>â€¢ Multi-Agent GenEnv: Co-Evolving Collaborating and Competing Agents with a Shared Simulator: Extend the environment policy to generate tasks for teams or adversaries and study emergent curricula.<br>â€¢ Sim2Real GenEnv: Robust Transfer from Difficulty-Aligned Simulation to Real Environments: Combine domain randomization and fidelity control to maximize transfer to real web/UI tasks.<br>â€¢ Multi-Objective Curriculum Rewards for Safe and Efficient Agency: Augment Renv to jointly optimize difficulty, safety constraints, tool costs, and robustness to adversarial prompts.<br>â€¢ Scaling Laws for Difficulty-Aligned Simulators: Empirical and theoretical analysis of performance vs simulator capacity, rollout budget, and dataset size (building on Figure 6â€™s data-efficiency findings).<br>â€¢ Curriculum over Tools: Evolving API Graphs and Tool Compositions in the Environment Policy: Let the simulator restructure tool schemas and compositions to teach complex tool-use (API-Bank/BFCL).<br>â€¢ Beyond Bandits: Convergence Guarantees for Co-Evolution in MDPs with Partial Observability: Extend the paperâ€™s theoretical results to sequential decision-making with non-binary rewards.<br>â€¢ Human-in-the-Loop GenEnv: Steering Simulator Objectives with Sparse Feedback: Inject minimal human signals to bias task families, improve realism, and prevent reward misspecification.<br>â€¢ Counterfactual Evaluation and Off-Policy Diagnostics for GenEnv: Develop auditors and metrics to detect reward hacking, difficulty drift, and mis-calibration without halting training.<br>â€¢ Multimodal GenEnv: Difficulty-Aligned Co-Evolution in Vision, Speech, and UI Interaction: Generalize the simulator to multimodal tasks and study cross-modal curricula and transfer.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.17206" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.17206" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Exploration capacity shapes both inference-time performance and reinforcement learning (RL) training for large (vision-) language models, as stochastic sampling often yields redundant reasoning paths with little high-level diversity. This paper proposes Reasoning Palette, a novel latent-modulation framework that endows the model with a stochastic latent variable for strategic contextualization, guiding its internal planning prior to token generation. This latent context is inferred from the mean-pooled embedding of a question-answer pair via a variational autoencoder (VAE), where each sampled latent potentially encodes a distinct reasoning context. During inference, a sampled latent is decoded into learnable token prefixes and prepended to the input prompt, modulating the model's internal reasoning trajectory. In this way, the model performs internal sampling over reasoning strategies prior to output generation, which shapes the style and structure of the entire response sequence. A brief supervised fine-tuning (SFT) warm-up phase allows the model to adapt to this latent conditioning. Within RL optimization, Reasoning Palette facilitates structured exploration by enabling on-demand injection for diverse reasoning modes, significantly enhancing exploration efficiency and sustained learning capability. Experiments across multiple reasoning benchmarks demonstrate that our method enables interpretable and controllable control over the (vision-) language model's strategic behavior, thereby achieving consistent performance gains over standard RL methods.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Token-level sampling (temperature, nucleus) revisits near-duplicate chains of thought and lacks high-level strategic diversity, harming both inference-time performance and RL exploration efficiency for (V)LMs.<br>â€¢ Existing remedies (e.g., entropy regularization) encourage only local diversity and rarely shape internal planning, so models fail to probe distinct reasoning strategies with different structures and plans.<br>â€¢ NaÃ¯ve noise/prefix injection is often misaligned with the modelâ€™s embedding space and can degrade performance; a well-aligned, controllable latent context is needed to steer reasoning without heavy retraining.<br>â€¢ Efficient, interpretable, and controllable exploration is essential for RL with verifiable rewards (math, code, QA) and VLM grounding, yet current methods lack explicit, domain-aware control over reasoning modes.<br>â€¢ Empirically, strategic pre-generative perturbations matter: as shown in Fig. 1 (page 1), sampling a Gaussian noise token before the prompt boosts pass@k even with greedy decoding, highlighting the need to control planning-level variability rather than token-level randomness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Reasoning Palette learns a VAE over mean-pooled [question; answer] token embeddings to form a Gaussian latent space of reasoning contexts; sampled latents are decoded into short prefix embeddings (length L) prepended to the prompt to modulate internal planning before generation. A brief SFT warm-up makes the model responsive to these prefixes, and RL training mixes latent-guided and standard rollouts via two-phase or linear-decay schedules to enable structured, strategy-level exploration.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Disentangled Reasoning Palettes via Structured Priors: Learn more interpretable, factorized latents (e.g., VQ-VAE/Î²-VAE) that separate dimensions like decomposition depth, verification style, and domain.<br>â€¢ Adaptive Palette Scheduling for RLVR: Learn a curriculum that adaptively schedules latent-guided rollouts by difficulty and verifier feedback to optimize explorationâ€“exploitation over training.<br>â€¢ Palette-Guided Tree-of-Thought Search: Assign different latent palettes to branches in tree/graph search to diversify strategic modes during deliberate reasoning.<br>â€¢ Continual Palette Refinement from High-Reward Traces: Update the VAE online from successful RL rollouts to densify and sharpen effective regions of the latent space.<br>â€¢ Cross-Modal Palette Transfer for VLMs: Study transfer and alignment of text-trained palettes to multimodal tasks, measuring grounding gains and domain-robustness.<br>â€¢ Verifier-and-Palette Co-Training: Train verifiers that condition on latent context to better assess strategy-specific correctness and guide latent selection.<br>â€¢ Safe and Bias-Aware Palette Sampling: Constrain or steer latent sampling to avoid unsafe/biased reasoning modes while maintaining strategic diversity.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.16229" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.16229" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Confidence-driven decoding in dLLMs yields only ~1â€“3 tokens per forward pass (TPF) on math/coding tasks, leaving most parallelism unused and limiting single-sample throughput for latency-sensitive applications.<br>â€¢ Effective parallelism is highly sensitive to the Token Filling Order (TFO); greedy highest-confidence filling can trap decoding in trajectories with low future confidence, suppressing later parallel fills.<br>â€¢ Existing accelerators either require retraining/distillation or passively verify fixed drafts without proactively exploring better TFOs, thus failing to unlock higher future parallelism in a training-free way.<br>â€¢ System-level constraints (e.g., KV-cache for bidirectional attention, block-level causal masks, lack of multi-branch execution) impede translating algorithmic parallelism into wall-clock speedups.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>LoPA is a training-free lookahead decoder that spawns an anchor and k lookahead branches by sampling top-confidence positions, evaluates all branches in one parallel forward pass with a branch-confidence metric, and commits the branch with the highest future parallelization potential to maximize TPF. A co-designed Branch Parallel distributed inference system with cache-consistency protocols (CUDA and Ascend backends) translates this into 8â€“10+ TPF and ~800â€“1000+ tokens/s single-sample throughput.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning to Look Ahead: Policy Optimization for Token Filling Order in Diffusion LLMs: Train a lightweight policy to choose TFOs that maximize expected future parallelism versus heuristic top-k sampling.<br>â€¢ LoPA-Tree: Multi-Step Tree-Structured Lookahead for Non-Autoregressive Decoding: Extend one-step lookahead to deeper, bounded tree search with dynamic pruning guided by confidence.<br>â€¢ Adaptive Branch Budgeting for Real-Time dLLM Serving: Adjust branch count k online using confidence dispersion, latency budgets, and device load to balance speed and quality.<br>â€¢ Lossless Lookahead Decoding: Theoretical Guarantees and Hybrid Verification for dLLMs: Combine LoPA with lossless draft-and-verify to bound distribution shift while sustaining high TPF.<br>â€¢ Memory-Efficient Branch Parallelism with KV-Cache Compression and Consistency: Develop compressed, shardable KV caches and formal commit protocols to scale branches across heterogeneous accelerators.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">StoryMem: Multi-shot Long Video Storytelling with Memory</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.19539" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.19539" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Multi-shot long video storytelling needs strong cross-shot coherence (characters, environments, style, narrative flow), which single-shot models lack.<br>â€¢ Joint multi-shot training methods (e.g., full attention with long contexts) are computationally expensive, require scarce long-video data, and often degrade visual quality as sequence length grows.<br>â€¢ Keyframe-based decoupled pipelines generate each shot independently, lack temporal awareness, lead to inconsistent identities/scenes, and produce rigid or unnatural transitions.<br>â€¢ Existing approaches struggle to propagate context across shots (e.g., new character introductions, evolving scenes, changing camera viewpoints), hurting narrative continuity.<br>â€¢ There is no standard benchmark for evaluating multi-shot story video generation, hindering fair comparison and progress.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>StoryMem introduces a Memory-to-Video (M2V) framework that conditions each new shot on an explicit, compact visual memory of keyframes from previous shots, injected into a pretrained single-shot I2V diffusion model via latent concatenation and a negative RoPE temporal shift, with only LoRA fine-tuning. A semantic keyframe selector (CLIP) plus aesthetic filtering (HPSv3) and a memory sink + sliding-window update keep memory informative and stable; extensions support smoother transitions (MI2V) and personalization via reference-initialized memory (MR2V).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Entity-Aware Textâ€“Visual Memory for Multi-Character Long-Form Video Generation: Fuse entity tracking and textual metadata with visual keyframe memory to resolve ambiguous retrieval and maintain per-character consistency across scenes.<br>â€¢ Motion-Aware MI2V: Overlapped-Frame Transition Modeling for Smooth Multi-Shot Video Diffusion: Learn speed/trajectory continuity with multi-frame overlaps and motion priors to mitigate non-causal or jerky transitions between adjacent shots.<br>â€¢ Structured Memory-to-Video with MMDiT: Joint Textâ€“Visual Memory Fusion for Minute-Long Narrative Generation: Replace DiT cross-attention with MMDiT-style joint conditioning to integrate temporal memory and prompts in a unified, scalable architecture.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.19432" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.19432" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing mobile-agent benchmarks (e.g., AndroidWorld) are saturatedâ€”SOTA agents exceed 90% successâ€”making it hard to measure real progress and differentiate methods.<br>â€¢ Benchmarks under-represent real smartphone use: tasks are short-horizon, single-app, and miss key app categories (e-commerce, enterprise chat); real use often requires long, cross-app workflows (MobileWorld averages 27.8 steps and 62.2% cross-app tasks; see Figure 1).<br>â€¢ Current setups assume fully specified user instructions; they do not test agentsâ€™ ability to detect ambiguity and engage in clarification with users.<br>â€¢ Benchmarks lack evaluation of hybrid execution where GUI actions are combined with external tool use (e.g., MCP tools) that are increasingly important in practice.<br>â€¢ Many evaluations depend on commercial backends and "MLLM-as-a-judge," leading to non-deterministic, noisy scoring; reproducible, deterministic verification is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce MobileWorld, a reproducible benchmark of 201 tasks across 20 apps that stresses long-horizon, cross-app workflows and adds two new task types: agentâ€“user interaction and MCP-augmented hybrid tool use. A containerized, snapshot-based Android environment with self-hosted backends enables deterministic verification (text matching, backend DB inspection, local storage checks, app callbacks), and a plannerâ€“executor agent framework extends the action space with ask_user and mcp_call to unify GUI control, user clarification, and tool invocation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Clarify-First Mobile Agents: Learning Ambiguity Detection and Efficient User Querying: Train agents to detect underspecification and optimize when/how to issue ask_user to maximize UIQ and success.<br>â€¢ MCP-Aware Context Management for Hybrid GUIâ€“Tool Agents: Design retrieval, summarization, and schema validation to handle long MCP outputs and reduce tool-name/argument errors in mcp_call.<br>â€¢ Persistent Memory for Long-Horizon Mobile Workflows: Develop episodic/state memory to track completed subtasks and device state, preventing repetition and drift across 25â€“50 step tasks.<br>â€¢ Numerically Faithful Reasoning for On-Device Tasks: Integrate verified calculators or neuro-symbolic modules to improve arithmetic and logical reasoning in e-commerce and productivity scenarios.<br>â€¢ Temporalâ€“Spatial Grounding for Mobile Agents: Build mechanisms to read time/location from the device UI and sensors to resolve "tomorrow," local times, and place-aware actions reliably.<br>â€¢ RL with Deterministic MobileWorld Signals: Leverage database- and callback-based ground-truth rewards to train planner policies via reinforcement learning for better planning and tool orchestration.<br>â€¢ Unified Planning over GUI and MCP Tools: Develop decision-theoretic planners that choose between GUI navigation and MCP calls, optimizing cost, latency, and reliability across mixed pipelines.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.18658" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.18658" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Before closing venture capital financing rounds, lawyers conduct diligence that includes tying out the capitalization table: verifying that every security (for example, shares, options, warrants) and issuance term (for example, vesting schedules, acceleration triggers, transfer restrictions) is supported by large sets of underlying legal documentation. While LLMs continue to improve on legal benchmarks, specialized legal workflows, such as capitalization tie-out, remain out of reach even for strong agentic systems. The task requires multi-document reasoning, strict evidence traceability, and deterministic outputs that current approaches fail to reliably deliver. We characterize capitalization tie-out as an instance of a real-world benchmark for legal AI, analyze and compare the performance of existing agentic systems, and propose a world model architecture toward tie-out automation-and more broadly as a foundation for applied legal intelligence.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Cap-table tie-out is a high-stakes, labor-intensive verification workflow that requires reconciling thousands of heterogeneous legal documents with a reference cap table under strict evidence traceability and determinism.<br>â€¢ Existing LLM/RAG agentic systems struggle with multi-document, combinatorial reasoning: they cannot reliably prove negatives (e.g., missing documentation), maintain lineage across amendments and corporate actions, or guarantee reproducible, deterministic outputs at scale.<br>â€¢ Real-world complexity scales super-linearly: securities and shareholders grow faster than document counts; anomalies shift from simple omissions to intricate inconsistencies; verification steps and cognitive load explode as companies mature.<br>â€¢ The fractured data landscape (scanned PDFs, near-duplicates, redlines vs. executed copies, temporal amendments, entity name variants) defeats ad-hoc retrieval, causing compounding errors, poor recall on global checks, and high per-check latency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Equall proposes an eager world-model architecture that first builds a provenance-grounded Event Graph from dataroom documents (via LLM parsers producing low-level facts and inductive event nodes like Issuance, Amendment, Transfer), then executes deterministic neuro-symbolic queries to construct a virtual cap table and compare it to the reference. This decouples extraction from verification, enabling global reasoning, strict evidence traceability, reproducibility, and fast per-check validation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ World-Model-Driven RL for Autonomous Legal Diligence: Train agents over synthetic tie-out curricula using dense, verifiable rewards atop the Event Graph to learn end-to-end policies for multi-step lineage verification.<br>â€¢ A Unified Event Graph for Corporate Law: Generalize the world-model to adjacent domains (M&A, debt, governance, compliance) and study transfer learning and cross-domain robustness of neuro-symbolic verification.<br>â€¢ Uncertainty-Aware Human-in-the-Loop Tie-Out: Add confidence estimation and active review triage over graph-derived proofs to optimize attorney time, with guarantees on negative-evidence (missing-doc) assertions.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.19402" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.19402" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Collecting diverse, spatially varied real-world manipulation demonstrations is costly and time-consuming; policies struggle with spatial generalization when data is scarce (noted in the Introduction and validated by Table 1/8, where 1â€“5 source demos normally underperform 50 real demos).<br>â€¢ Simulation- or renderer-based demo generation (e.g., MimicGen, Real2Render2Real, RoboSplat) introduces visual and physics gaps, requires assets or dense captures, and limits scalability and realism (Sec. 2.1; Table 4 in Supplement).<br>â€¢ Point-cloud editing methods relying on depth sensors (e.g., DemoGen) are incompatible with common multi-view RGB 2D/VLA pipelines and cannot directly augment existing RGB demos (Sec. 2.1; Table 4).<br>â€¢ No prior method bridges 3D editability with 2D video while preserving multi-view consistency, realistic appearance, correct robotâ€“object interactions, and camera motion (Sec. 1, 3).<br>â€¢ Need a unified, scalable pipeline to turn few real demos into many multi-view RGB demonstrations with novel placements and trajectories, robust to textures/heights and directly usable for VLA training (Fig. 1; Sec. 4.2â€“4.3).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Real2Edit2Real uses depth as a 3D control interface to connect 3D editing with 2D multi-view video generation: it first reconstructs metric-scale geometry and poses from RGB (Metric-VGGT fine-tuned on hybrid real+sim data), then performs depth-reliable spatial editing with motion planning and robot pose correction to produce kinematically consistent depth/action sequences, and finally generates multi-view videos via a transformer model conditioned on depth (primary), edges, actions, and ray maps with dual intra-/cross-view attention.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ ArticuEdit2Real: Depth-Controlled Demonstration Generation for Articulated and Deformable Objects: Extend pose correction and editing to articulated parts with learned kinematic priors and contact-aware constraints to improve physical realism on doors, drawers, cloth, etc.<br>â€¢ Active Real2Edit2Real: Uncertainty-Aware Data Generation and Source Demo Selection: Quantify uncertainty in reconstruction/editing/generation to actively decide where to generate vs. collect new real demos for maximal policy gains.<br>â€¢ Language-Conditioned Real2Edit2Real for Task and Scene Recomposition: Add language prompts to control object sets, spatial layouts, textures, and goals, enabling compositional demo synthesis for VLA pretraining.<br>â€¢ Physics-Grounded Real2Edit2Real via Differentiable Dynamics Checks: Integrate lightweight physics validators or differentiable simulators during spatial editing and first-frame relocation to enforce contact stability and feasibility.<br>â€¢ Cross-Embodiment Real2Edit2Real: Transferable 3D Control Across Robots and Camera Rigs: Adapt Metric-VGGT and the 3D control interface to new kinematics and camera configurations for embodiment-agnostic generation.<br>â€¢ Closed-Loop Gen2Act: On-Policy Counterfactual Demo Generation with Real2Edit2Real: Interleave policy rollouts with targeted generation around failure states to improve robustness and spatial generalization.<br>â€¢ World-Model Pretraining from Real2Edit2Real: Train multi-view, depth-consistent video world models using generated data to enhance planning and model-based control.<br>â€¢ Scaling Laws for 3D-Controlled Demo Generation in VLA Training: Systematically study how the number of source and generated demonstrations affects downstream success across tasks, embodiments, and textures.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Name That Part: 3D Part Segmentation and Naming</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.18003" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.18003" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We address semantic 3D part segmentation: decomposing objects into parts with meaningful names. While datasets exist with part annotations, their definitions are inconsistent across datasets, limiting robust training. Previous methods produce unlabeled decompositions or retrieve single parts without complete shape annotations. We propose ALIGN-Parts, which formulates part naming as a direct set alignment task. Our method decomposes shapes into partlets - implicit 3D part representations - matched to part descriptions via bipartite assignment. We combine geometric cues from 3D part fields, appearance from multi-view vision features, and semantic knowledge from language-model-generated affordance descriptions. Text-alignment loss ensures partlets share embedding space with text, enabling a theoretically open-vocabulary matching setup, given sufficient data. Our efficient and novel, one-shot, 3D part segmentation and naming method finds applications in several downstream tasks, including serving as a scalable annotation engine. As our model supports zero-shot matching to arbitrary descriptions and confidence-calibrated predictions for known categories, with human verification, we create a unified ontology that aligns PartNet, 3DCoMPaT++, and Find3D, consisting of 1,794 unique 3D parts. We also show examples from our newly created Tex-Parts dataset. We also introduce 2 novel metrics appropriate for the named 3D part segmentation task.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Simultaneously segmenting and naming 3D parts is under-served; existing datasets use inconsistent part taxonomies, limiting robust training and cross-dataset generalization.<br>â€¢ Prior methods either do class-agnostic segmentation (produce unlabeled parts) or text-to-part retrieval (return single queried parts), failing to output complete, non-overlapping, named decompositions in one pass.<br>â€¢ Methods often need prompts, per-category fine-tuning, or ground-truth part counts (e.g., K-means postprocessing), leading to brittleness and inefficiency.<br>â€¢ Lack of open-vocabulary grounding and semantic disambiguation: short part names are ambiguous across objects; there is a need for affordance-aware text grounding.<br>â€¢ A scalable annotation engine is missing; inconsistent labels across PartNet/3DCoMPaT++/Find3D create a data bottleneck for named part segmentation.<br>â€¢ Evaluation metrics for named 3D part segmentation are inadequate; current practice ignores semantic correctness, hindering fair assessment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>ALIGN-Parts formulates named 3D part segmentation as set alignment: it learns a small set of part-level â€œPartletsâ€ that aggregate fused geometry (PartField) and appearance (multi-view DINOv2) features, then bipartite-matches these Partlets to MPNet embeddings of affordance-aware text descriptions via optimal transport (Sinkhorn in training, Hungarian/JV in inference). End-to-end losses (mask, partness, contrastive text alignment, coverage/overlap regularizers, and global shapeâ€“text alignment) yield permutation-consistent, dynamic-count, open-vocabulary, feed-forward part segmentation and naming.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ PartletFormer: End-to-End Geometric Backbones for Named 3D Part Segmentation: Replace frozen geometric features with a fully trainable backbone to jointly learn geometryâ€“semantics for improved accuracy and data efficiency.<br>â€¢ Weakly-Supervised ALIGN-Parts: Learning Named 3D Parts from Web-Scale Text and Sparse Cues: Reduce reliance on dense labels using self/weak supervision, imageâ€“text priors, and limited human verification.<br>â€¢ Affordance-Driven Open-Vocabulary Part Ontologies at Scale: Automatically grow and refine unified part taxonomies with LLM verification, cross-dataset mapping, and continual learning.<br>â€¢ Robust Part Naming in the Wild: Domain Adaptation and Confidence Calibration for Noisy Scans: Improve robustness to real-world scans and distribution shift via uncertainty modeling, Mahalanobis/energy-based calibration, and test-time adaptation.<br>â€¢ Articulated ALIGN-Parts: Joint Part Segmentation, Naming, and Kinematic Reasoning: Extend partlets to infer joints and motion constraints, enabling manipulation-aware semantics for robots and editors.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.19535" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.19535" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Token insertion achieves strong VLM performance but scales poorly: it inflates compute/memory and KV cache with image tokens, making highâ€‘resolution images, long dialogs, and streaming videos impractical (pages 1â€“4, Table 1 on page 4, Figure 6 on page 9).<br>â€¢ Cross-attention is efficient but underperforms on fineâ€‘grained visual tasks (charts, documents, OCR); explicit gating and architectural tweaks havenâ€™t closed the gap (Figure 1 on page 1; Table 2 on page 6; discussion pages 1â€“3, 6â€“8).<br>â€¢ Root cause identified: cross-attention lacks local textâ€‘toâ€‘text interaction during visual injection, causing destructive updates and weak implicit gating (pages 3â€“4; Table 8 on page 7; Figure 7 on page 12).<br>â€¢ Token compression mitigates cost but degrades highâ€‘detail understanding and does not stop KV cache growth in streaming (Table 6b on page 7; Figures 5â€“6 on page 9).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>CASA injects vision via crossâ€‘attention where each text token attends causally to both image tokens and the local span of text tokens since the last image (textâ†’{image + local text}), yielding implicit gating and avoiding pushing image tokens through FFNs. It is implemented with efficient blockwise attention (FlashAttention2), offered in three plugâ€‘andâ€‘play variants (CASAâ†’, CASAâŠ•, CASAâˆ¨) that preserve KV cache size and deliver crossâ€‘attentionâ€‘like scalability while closing much of the tokenâ€‘insertion performance gap.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive CASA: Dynamic Local Windows for Multimodal Streams: Learn window boundaries/lengths and cross-window routing so CASA can fuse multiple images and long videos with adaptive receptive fields under latency constraints.<br>â€¢ CASA-Compress: Hierarchical KV Compression for Efficient Vision Keys/Values: Combine CASA with lightweight visual token pooling/merging and query sparsification to further reduce attention cost without sacrificing fine-grained detail.<br>â€¢ Selective Visual Updates for CASA via Low-Rank or Token-Wise FFNs: Add budgeted, selective FFN updates (e.g., low-rank adapters or token gating) to visual keys/values to narrow the residual gap to full token insertion at minimal extra compute.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">MatSpray: Fusing 2D Material World Knowledge on 3D Geometry</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.18314" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.18314" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Manual modeling of material parameters and 3D geometry is a time consuming yet essential task in the gaming and film industries. While recent advances in 3D reconstruction have enabled accurate approximations of scene geometry and appearance, these methods often fall short in relighting scenarios due to the lack of precise, spatially varying material parameters. At the same time, diffusion models operating on 2D images have shown strong performance in predicting physically based rendering (PBR) properties such as albedo, roughness, and metallicity. However, transferring these 2D material maps onto reconstructed 3D geometry remains a significant challenge. We propose a framework for fusing 2D material data into 3D geometry using a combination of novel learning-based and projection-based approaches. We begin by reconstructing scene geometry via Gaussian Splatting. From the input images, a diffusion model generates 2D maps for albedo, roughness, and metallic parameters. Any existing diffusion model that can convert images or videos to PBR materials can be applied. The predictions are further integrated into the 3D representation either by optimizing an image-based loss or by directly projecting the material parameters onto the Gaussians using Gaussian ray tracing. To enhance fine-scale accuracy and multi-view consistency, we further introduce a light-weight neural refinement step (Neural Merger), which takes ray-traced material features as input and produces detailed adjustments. Our results demonstrate that the proposed methods outperform existing techniques in both quantitative metrics and perceived visual realism. This enables more accurate, relightable, and photorealistic renderings from reconstructed scenes, significantly improving the realism and efficiency of asset creation workflows in content production pipelines.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Relightability gap: Modern 3D reconstructions (NeRF/Gaussians) entangle lighting with appearance, yielding textures not physically meaningful for relighting, especially when materials vary spatially (pp. 2â€“3).<br>â€¢ 2Dâ†’3D mismatch: Diffusion models predict plausible PBR maps per view, but are inconsistent across views and not attached to 3D, so direct projection causes blur, baked-in lighting, and view-dependent artifacts (pp. 2â€“4).<br>â€¢ Specular/metallic failure modes: Existing Gaussian inverse-rendering methods (e.g., R3DGS, IRGS) struggle with specular/metallic surfaces, produce artifacts (floaters, flatness), and IRGS lacks metallic support, hurting realism (pp. 5â€“7; Fig. 3â€“5).<br>â€¢ Optimization cost and instability: Per-scene inverse rendering is slow and can make environment map estimation unstable when materials are unconstrained; a faster, stable pipeline is needed for production workflows (pp. 2, 7â€“8; Table 2).<br>â€¢ Practical importance: Accurate, spatially varying PBR materials are essential for high-quality relighting and efficient asset creation in games/film; leveraging diffusion â€œworld material knowledgeâ€ in 3D is an open challenge (pp. 1â€“2).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MatSpray fuses per-view diffusion-based PBR predictions with a 3D Gaussian Splatting reconstruction by lifting 2D material maps to 3D via Gaussian ray tracing and then merging them per-Gaussian using a lightweight, softmax-weighted Neural Merger; training is supervised by an L1 loss to 2D material maps and a deferred PBR photometric loss with joint environment map optimization to enforce multi-view consistency and suppress baked-in lighting.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ MatSpray++: End-to-End Co-Training of 2D Diffusion and 3D Gaussians for Linear-Space PBR Materials: Jointly finetune the 2D predictor with 3D supervision to output tone-linear, cross-view-consistent PBR maps and reduce mismatch from tone mapping.<br>â€¢ ProjFormer: Transformer-Based 2D-to-3D Material Assignment for Gaussians: Replace median/ray-trace aggregation with a learnable transformer that handles occlusion, missing Gaussians, and view selection for more robust material lifting.<br>â€¢ Seg-Mat: Language-Grounded 3D Part Segmentation via Geometryâ€“Material Fusion: Exploit the tight geometryâ€“material association to perform fine-grained, text-guided 3D segmentation and constrained reconstruction/editing.<br>â€¢ U-Merger: Uncertainty- and Dirichlet-Prior Fusion for Multi-View PBR Estimation: Extend the Neural Merger with uncertainty-aware weighting and Dirichlet priors to better handle noisy/inconsistent 2D material predictions.<br>â€¢ Beyond Disney: Relightable svBRDFs with Subsurface and Anisotropy in Gaussian Splatting: Generalize MatSpray to richer reflectance models (e.g., SSS, anisotropy) while retaining efficiency and multi-view consistency.<br>â€¢ EM-SLAM: Multi-Sequence Joint Environment Map and Material Estimation: Leverage multiple environments and sequences to stabilize and improve lighting/material disentanglement, especially for highly specular objects.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.12620" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.12620" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Disentangle formal logical validity from natural-language believability to fairly assess LLM reasoning; prior work often conflates these dimensions (dual ground truth introduced; Table 2 on p.3; Figure 3 on p.5 shows a 25.5 pp syntaxâ€“NLU gap)<br>â€¢ Measure belief bias in LLMs with psychometrically sound metrics; traditional human indices have known flaws, requiring an accuracy-based congruent vs. incongruent analysis (Section 3.4; Figure 2 on p.3 shows bias in 12/14 models)<br>â€¢ Provide a systematic, model-agnostic evaluation across prompting strategies, temperatures, and content/order variants to test assumptions (e.g., few-shot helps) and isolate architectural effects (Figure 1 on p.3 shows few-shot degradation; Section 4.1 notes architecture > size)<br>â€¢ Assess robustness and consistency of reasoning under content and premise-order changesâ€”dimensions underexplored in prior benchmarks (consistency metrics Call, CNâ†”X, COâ†”OX in Table 3 on p.6)</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Build a dual-ground-truth syllogism benchmark (160 items across normal, nonsense, order-switched, and combined variants) and evaluate 14 LLMs under four prompting strategies and three temperatures using a temperature-adaptive self-consistency procedure that outputs a one-word correctness judgment. Quantify belief bias as the accuracy gap between congruent and incongruent cases and assess robustness via content/order consistency metrics, reporting comprehensive statistics across 26,880 evaluations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Beyond Categorical Syllogisms: Generalizing Dual-Ground-Truth Evaluation to Modal and Quantified Logics: Test whether the logicâ€“NLU gap and belief bias persist in richer logical systems (modal, conditional, nested quantifiers).<br>â€¢ Causally Probing Bias Resistance in LLMs: Does Logical Training Reduce Belief Bias?: Use controlled fine-tuning and mechanistic interpretability to identify whether improved formal reasoning causally reduces belief bias (or vice versa).<br>â€¢ When Do Demonstrations Help? A Systematic Study of Few-Shot Prompting for Formal Reasoning: Map the conditions and prompt designs under which few-shot examples help or harm deductive performance across tasks and model families.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Brain-Grounded Axes for Reading and Steering LLM States</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.19399" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.19399" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Text-derived interpretability and steering directions lack external grounding in human cognition, making them hard to validate and trust.<br>â€¢ There is no stable, neurophysiology-based coordinate system for reading and steering LLM states, limiting interpretability and control.<br>â€¢ Existing approaches risk circularity (recovering superficial text statistics like frequency) and can degrade fluency when steering.<br>â€¢ Most brainâ€“LM work focuses on encoding/decoding comparisons rather than actionable, controllable steering interfaces.<br>â€¢ Steering typically requires fine-tuning or heavy supervision; a lightweight, model-agnostic method is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Build a MEG-derived, word-level brain atlas of PLV connectivity and extract latent axes via ICA; then train a lightweight linear adapter that maps LLM hidden states into this brain-axis space without fine-tuning, and steer by adding the normalized adapter weight vector at selected layers. Validate with independent lexica/labels, perplexity-matched controls, and cross-model tests to confirm interpretable, controllable shifts in generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Source-Resolved, Leakage-Robust Brain Axes for LLM Steering: Use source-reconstructed MEG and leakage-robust connectivity (e.g., wPLI) across frequency bands to derive more stable, semantically precise axes and improve layer consistency.<br>â€¢ Disentangling Lexical and Semantic Subspaces in Neuro-Grounded Steering: Causally separate frequency/length/surprisal from semantic axes (animacy, concreteness, affect) via orthogonalization and controlled generation to increase semantic purity.<br>â€¢ Personalized and Real-Time Neuro-Interfaces for Language Models: Develop subject-specific adapters and closed-loop controllers that steer models on-the-fly while maintaining or improving fluency and task performance.<br>â€¢ Scaling Neuro-Grounded Axes Across Models, Languages, and Tasks: Evaluate transfer to larger LLMs and multilingual corpora, and test gains on safety, style, and controllability benchmarks relative to text-only probes.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.18542" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.18542" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>AI assistants produce vulnerable code in 45% of security-relevant scenarios, introducing flaws into production systems at scale. Yet existing secure coding datasets fall short. They lack incident grounding, don't provide the scale modern training requires, and miss the operational security context developers need for production deployments. We present SecureCode v2.0, a production-grade dataset of 1,215 security-focused coding examples that passed structural validation and expert security review. Every example ties to actual documented security incidents with CVE references, provides vulnerable and secure implementations, demonstrates concrete attacks, and includes defense-in-depth operational guidance. The dataset covers 11 vulnerability categories (complete OWASP Top 10:2025 plus AI/ML Security Threats) across 11 languages (Python, JavaScript, Java, Go, PHP, C#, TypeScript, Ruby, Rust, Kotlin, and YAML for infrastructure-as-code). Our quality assurance framework ensures complete incident grounding. Each example includes SIEM integration strategies, infrastructure hardening recommendations (Docker, AppArmor, WAF configurations), and testing approaches using language-appropriate frameworks. The dataset uses a 4-turn conversational structure mirroring actual developer-AI interactions, escalating from basic implementations to advanced security considerations and defense-in-depth guidance. Our contributions: (1) 1,215 rigorously validated examples split into 989 training, 122 validation, and 104 test sets, (2) an automated validation framework ensuring dataset consistency, (3) a 4-turn conversational structure capturing realistic security workflows, (4) comprehensive operational security guidance with SIEM integration strategies, (5) complete language-specific implementation fidelity, and (6) open-source release of data, validation tools, and benchmarking protocols.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ AI coding assistants generate insecure code in ~45% of security-relevant cases, creating systematic, large-scale risk as vulnerable patterns silently enter production.<br>â€¢ Models learn from public code that includes insecure patterns and often lack the security context to distinguish â€œfunctionalâ€ from â€œexploitable.â€<br>â€¢ Existing datasets are largely synthetic and rarely incident-grounded (few CVE links), so they fail to capture real exploitation paths and production context.<br>â€¢ Code-only formats miss realistic multi-turn developerâ€“AI workflows, preventing models from maintaining security context across iterations.<br>â€¢ Prior datasets lack operational guidance (logging, SIEM detections, infra hardening), limiting production applicability beyond code fixes.<br>â€¢ Coverage gaps (languages, OWASP categories) and weak quality controls hinder training production-grade, security-aware models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Construct SecureCode v2.0: an incident-grounded, multi-language, 4-turn conversational dataset (1,215 examples) pairing vulnerable and secure implementations with concrete attacks, plus defense-in-depth operational guidance (SIEM, infrastructure hardening, testing). Ensure production quality via an automated validation and CVE-aware splitting pipeline (deduplication, structural checks), with examples synthesized by multi-LLM + expert review from documented incidents.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Closed-Loop Secure Code Generation via SIEM-Guided Reinforcement Learning: Use SIEM detections and runtime signals as rewards to fine-tune LLMs toward secure-by-default outputs.<br>â€¢ Multi-Lingual SecureCode: Cross-Language Transfer for Security-Aware Code Models: Localize and expand the dataset beyond English to study cross-lingual security generalization.<br>â€¢ Mobile and Embedded SecureCode: Incident-Grounded Datasets for iOS/Android and IoT: Extend coverage to mobile and embedded domains with verified exploits and defenses.<br>â€¢ Do Conversational Structures Improve Code Security? An Ablation Study on 4-Turn Training: Quantify the impact of multi-turn conversations versus single-shot data on security outcomes.<br>â€¢ Preventing Data Leakage in Security Benchmarks: CVE-Aware Splitting and Validation Standards: Formalize leakage-resistant splits and validators for security datasets and benchmarks.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 8</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-12-23 23:11:23 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 8;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>