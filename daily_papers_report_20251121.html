<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 21, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 21, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16668" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16668" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Absence of a standardized, reproducible benchmark to quantify emergent reasoning abilities in generative video models.<br>â€¢ Evaluation of reasoning via intermediate frames is costly, noisy, and ambiguous; need scalable last-frame assessment under the Chain-of-Frame paradigm.<br>â€¢ Vision-language models are unreliable as sole automatic judges for dense, grid-structured, and fine-grained visual outputs.<br>â€¢ Lack of unified coverage across key reasoning dimensions (structured problem-solving, spatial cognition, pattern-based inference, physical dynamics) impedes fair comparison and diagnosis of model strengths/weaknesses.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>V-ReasonBench introduces a unified Chain-of-Frames benchmark using deterministic last-frame scoring with a hybrid evaluation stack (mask-based, grid-based, and lightweight VLM-based) across four reasoning dimensions, converting numeric scores to pass/fail via task-specific thresholds for scalable pass@k metrics on synthetic and real sequences. It evaluates six state-of-the-art video generators to reveal dimension-wise performance, hallucination patterns, and duration effects.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Grid-aware Visual Judges for Video Reasoning: Designing specialized evaluators that robustly score fine-grained, dense layouts and geometric relations to reduce reliance on generic VLMs.<br>â€¢ Chain-of-Frames Supervision for Reliable Video Reasoning: Training video models with explicit intermediate reasoning signals and outcome-consistency losses to strengthen structured, spatial, and physical inference.<br>â€¢ Temporal Scaling Laws and Hallucination Mitigation in Video Generation: Systematically characterizing how video length affects reasoning stability and developing principled methods to diagnose and curb CoF-specific hallucinations.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Step-Audio-R1 Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.15848" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.15848" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Audio language models degrade with longer chain-of-thought, exhibiting inverted test-time compute scaling compared to text/vision.<br>â€¢ Models rely on textual surrogate reasoning (transcripts/captions) instead of acoustic properties, causing modality mismatch and poor performance.<br>â€¢ Existing fixes (e.g., RL with LM judges) enforce answer consistency but do not address the lack of acoustic grounding in reasoning.<br>â€¢ Standard SFT on text-derived CoT transfers linguistic grounding to audio tasks, undermining reasoning as length increases.<br>â€¢ Real-time audio dialogue requires both reasoning quality and low latency, which current systems struggle to balance.<br>â€¢ Pretraining on large text corpora induces self-cognition errors ("I cannot process audio"), harming usability of audio LLMs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce Modality-Grounded Reasoning Distillation (MGRD), an iterative self-distillation and RL framework that transitions reasoning from text-based to acoustically grounded by curating perception-heavy audio tasks, filtering chains for explicit acoustic references, and rewarding both correctness and the presence of reasoning. Built on a frozen Qwen2 audio encoder with an adaptor and a Qwen2.5-32B decoder, the pipeline combines tri-modal SFT, RLVR, and composite audio rewards to unlock test-time compute scaling in audio.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Unified Multimodal Grounding: Transferable Reasoning Across Audio, Vision, and Text: Develop a shared grounding framework that enforces modality-specific CoT anchors and evaluates cross-modal compute scaling.<br>â€¢ Signal-Attribution-Guided Audio CoT: Interpretable and Causally Grounded Acoustic Reasoning: Integrate timeâ€“frequency attribution and pitch/timbre detectors to align reasoning tokens with signal-level evidence.<br>â€¢ Curriculum RL for Audio Intelligence: Difficulty-Adaptive Data Selection and Reward Shaping: Formalize and automate selection of challenging-but-solvable audio tasks and dynamic reward schedules to sustain extended reasoning without collapse.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">First Frame Is the Place to Go for Video Content Customization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.15700" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.15700" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>What role does the first frame play in video generation models? Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation. In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation. Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning. This unveils a powerful, overlooked capability of video generation models for reference-based video customization.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Achieving coherent, reference-based video content customization with multiple subjects and interactions remains challenging across diverse applications.<br>â€¢ Existing video models typically treat the first frame only as a temporal seed, lacking mechanisms to retain and reuse entities for compositional subject mixing throughout the video.<br>â€¢ Current customization approaches often require large-scale finetuning or architectural modifications, making them data- and compute-intensive and limiting generalization from few examples.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>FFGo is a lightweight add-on that leverages pre-trained video generators by treating the first frame as a conceptual memory buffer/compositional blueprint, enabling subject mixing and guided interactions from a single input image plus text. With only 20â€“50 LoRA fine-tuning examples and no architectural changes, it biases the model to reuse first-frame entities coherently across time.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ First-Frame Memory Mechanisms in Diffusion Video Models: Analyze and quantify how video models store and reuse first-frame entities, with interpretability tools (e.g., attention tracing) to reveal the internal memory dynamics.<br>â€¢ Multi-Frame Compositional Blueprints for Long-Horizon Video Customization: Extend the approach to multiple reference frames to manage scene evolution, complex interactions, and longer temporal coherence.<br>â€¢ Cross-Model Generalization of First-Frame Customization: Systematically evaluate and adapt first-frame guidance across diverse video architectures to assess robustness, portability, and minimal fine-tuning requirements.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">Scaling Spatial Intelligence with Multimodal Foundation Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13719" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13719" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Multimodal foundation models exhibit weak spatial intelligence (e.g., perspective-taking, metric measurement, 3D relational reasoning), creating a critical gap for embodied AI despite strong general VLM performance.<br>â€¢ Spatial data is scarce, fragmented, and imbalancedâ€”over-representing metric/relations while under-covering perspective-taking and mental reconstructionâ€”hindering comprehensive learning and fair evaluation.<br>â€¢ Existing methods either bolt on 3D experts or train on narrow/synthetic datasets, often leading to poor generalization, susceptibility to language shortcuts, and limited understanding of how spatial capability scales.<br>â€¢ There is little clarity on scaling laws, emergent generalization, and robustness in spatial intelligence, and how to improve SI without sacrificing general multimodal competence.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SenseNova-SI uses a data-centric scaling approach: it curates and synthesizes SenseNova-SI-8M (â‰ˆ8.5M QA) under a rigorous spatial taxonomy with heavy emphasis on perspective-taking and mental reconstruction, then continues training strong base MLLMs (Qwen3-VL, InternVL3, Bagel) without architectural changes. The work demonstrates SoTA results across spatial benchmarks, analyzes scaling behavior, emergent transfer and extrapolation, robustness to shortcuts, preliminary spatial chain-of-thought, and validates zero-shot gains in embodied manipulation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Beyond Textual CoT: Diagrammatic and 3D-State Chain-of-Thought for Spatial Reasoning: Develop compact, verifiable reasoning traces using world-coordinate maps, causal scene graphs, or sketches to surpass text-only CoT.<br>â€¢ Equivariant Multimodal Architectures for Viewpoint Transformation: Introduce SE(3)-aware inductive biases or modules to natively support perspective-taking and cross-view correspondence.<br>â€¢ Active and Continual Spatial Data Scaling for MLLMs: Close the loop with active sampling from 3D corpora/embodied interaction and curriculum learning to target underlearned spatial skills without forgetting.<br>â€¢ Verifiable, Debiased Spatial Intelligence Benchmarks: Build large-scale tests with circular-choice protocols, no-vision controls, counterfactual scenes, and proof-style outputs to measure true spatial grounding.<br>â€¢ Long-Context Spatial Understanding Beyond 128 Frames: Train and evaluate models on very long multi-view/video contexts with hierarchical spatial memory to robustly integrate sparse observations over time.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SAM 3D: 3Dfy Anything in Images</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16624" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16624" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D "data barrier". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Robust single-image 3D reconstruction in natural, cluttered, and occluded scenes, where existing single-image methods trained on isolated objects fail to infer full shape, texture, pose, and multi-object layout.<br>â€¢ Overcoming the 3D data barrier: severe scarcity of large-scale, visually grounded imageâ€“3D pairs, and the difficulty for generalist annotators to produce 3D ground truth for real images.<br>â€¢ Lack of standardized, in-the-wild benchmarks and human-aligned evaluation for 3D reconstruction, limiting progress and fair comparison across methods.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SAM 3D employs a two-stage latent flow matching architecture: a Geometry model predicts pose and coarse shape from DINOv2 features of both cropped object and full-image context, followed by a Texture & Refinement model that adds high-resolution detail and textures. Training follows an LLM-style pipeline (synthetic pretraining, semi-synthetic mid-training via pasted renders, and model-in-the-loop + human post-training) to align with real images and overcome the 3D data gap.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Active MITL-3D: Scaling Visually Grounded 3D Annotation with Active Learning and Retrieval â€” Enhance the model-in-the-loop pipeline with active selection of hard cases and proposal generation to reduce human effort while increasing dataset diversity and quality.<br>â€¢ Contextual Scene Graphs for Single-Image 3D: Reasoning About Occlusion and Inter-Object Relations â€” Integrate scene-graph reasoning into SAM 3D to better infer hidden geometry and coherent multi-object layout under heavy occlusion.<br>â€¢ VideoSAM 3D: Temporally Consistent 3D Reconstruction from Monocular Video â€” Extend the two-stage architecture to multi-frame inputs to produce temporally consistent 3D assets and full scenes using motion and temporal priors.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16669" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16669" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Text-only Next-Event Prediction (NEP) fails to convey spatial layout, motion, and temporal ordering; a video answer modality (VNEP) is needed for clearer, personalized guidance.<br>â€¢ VNEP demands instruction-conditioned reasoning and generation of videos that are both visually coherent and semantically faithfulâ€”capabilities not well handled by existing models.<br>â€¢ Cascaded VLMâ†’VDM pipelines suffer semantic-to-visual misalignment, while unified models trade off understanding versus generation and struggle to excel at both.<br>â€¢ Existing NEP datasets lack video quality and diverse instructional/predictive questions, hindering training and evaluation for VNEP.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>VANS introduces Joint-GRPO, an RL post-training framework that jointly optimizes a Vision-Language Model and a Video Diffusion Model via a shared reward, aligning textual reasoning with visual generation for Video-Next-Event Prediction. It leverages a two-stage optimization (SFT on VANS-Data-100K followed by RL) to produce visualize-friendly captions and videos faithful to both captions and input context.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ End-to-End Co-Training of Reasoning and Generation for Video-as-Answer: Build a unified architecture with a shared latent bridge to reduce misalignment between VLM and VDM while preserving specialization.<br>â€¢ Human-in-the-Loop Joint Reward Learning for VNEP: Learn multi-component rewards from human preferences to better capture semantic faithfulness, executability, and visual coherence.<br>â€¢ Benchmarking VNEP: Unified Metrics for Semantic Faithfulness and Visual Coherence: Develop standardized datasets and multimodal evaluation metrics tailored to event-conditioned video answers.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">MiMo-Embodied: X-Embodied Foundation Model Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16518" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16518" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Fragmented progress: existing models typically specialize in either Embodied AI (task planning, affordance prediction, spatial understanding) or Autonomous Driving (environmental perception, status prediction, driving planning), but rarely excel across both.<br>â€¢ Limited generalization and transfer: current open- and closed-source VLMs struggle to achieve state-of-the-art performance uniformly across heterogeneous benchmarks and fail to leverage positive cross-domain transfer.<br>â€¢ Inadequate training recipes: a lack of multi-stage learning, curated multimodal data pipelines, and CoT/RL fine-tuning strategies prevents mutual reinforcement between embodied and driving tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MiMo-Embodied is a cross-embodied foundation model trained via multi-stage learning on curated multimodal datasets, combined with Chain-of-Thought and reinforcement learning fine-tuning, to unify perception, status prediction, and planning across both autonomous driving and embodied AI tasks. This design exploits positive transfer between domains to achieve state-of-the-art results on 17 embodied AI and 12 driving benchmarks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Scaling Cross-Embodied Foundation Models with Unified CoT/RL Fine-Tuning: Scale model size and diverse datasets to further amplify positive transfer between driving and embodied AI tasks.<br>â€¢ Safe and Robust Cross-Embodied Planning under Distribution Shifts: Develop risk-aware training and evaluation protocols to ensure reliability across varied environments and task conditions.<br>â€¢ Interpretable Affordance and Spatial Reasoning for Unified Robots and Vehicles: Create methods to visualize, audit, and validate CoT and decision pathways for accountability and debugging across both domains.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16664" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16664" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Training multi-size reasoning LLM families is prohibitively expensive because each size is trained or distilled separately on hundreds of billions to trillions of tokens.<br>â€¢ Existing compression (structured pruning + KD) still incurs large token cost per target model and yields multiple checkpoints, inflating deployment memory and operational complexity.<br>â€¢ Prior elastic/nested methods rarely handle hybrid Mambaâ€“Attention models and ignore SSM group/structure constraints, limiting applicability to modern efficient hybrids.<br>â€¢ Reasoning LLMs need extended-context optimization and flexible compute for multi-step inference; existing methods decouple architecture search from training and donâ€™t support zero-shot, multi-budget deployment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Nemotron Elastic trains a hybrid Mambaâ€“Attention LLM with nested submodels via importance-ranked width/depth masking and an end-to-end Gumbel-Softmax router, optimized with a two-stage curriculum (short context â†’ 49k-token extended context) and frozen-teacher knowledge distillation. The learned router enables zero-shot slicing of 6B/9B variants from a 12B parent with constant deployment memory, including group-aware SSM elastification, normalized-MSE depth ranking, and heterogeneous layer-wise configurations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Test-Time Adaptive Elastic Reasoning via Input-Conditioned Routing: Make routing conditioned on problem difficulty/context to select budgets per request or per step, improving latencyâ€“accuracy trade-offs at inference.<br>â€¢ Multi-Modal Nemotron Elastic: Nested Hybrid Visionâ€“Language SSMâ€“Attention Models: Extend group-aware elastification and extended-context curriculum to multimodal LLMs, jointly optimizing long-context textual and visual reasoning.<br>â€¢ Co-Distilled Elastic Families with Multi-Teacher Supervisors: Use budget-aware ensembles or stronger reasoning teachers to co-distill all sub-networks, boosting small-budget faithfulness and stability across long contexts.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16671" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16671" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Visual generators lack long-horizon compositionality and nuanced adherence to complex prompts, especially with multi-entity relations.<br>â€¢ Chain-of-thought methods either pre-plan (fixed, non-adaptive) or post-refine (loosely coupled, costly), providing no on-the-fly, fine-grained guidance during synthesis.<br>â€¢ There is no truly interleaved reasoning mechanism that decides when to think, what to say, and how to correct within a single generative trajectory; existing 'interleaving' attempts remain block-wise and limit controllability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>TWIG interleaves textual reasoning throughout visual generation by scheduling when to think, producing localized thoughts to guide each region, and performing region-level reflection/correction within a single autoregressive ULM trajectory. It is realized via zero-shot prompting, supervised fine-tuning on TWIG-50K, and reinforcement learning (TWIG-GRPO) to optimize when to think, what to say, and how to refine.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Adaptive Interleave Schedules for Content-Aware Visual Generation: Train models to infer variable, content-dependent when/where to think to improve controllability and efficiency.<br>â€¢ Extending TWIG to Diffusion, Video, and 3D Synthesis: Interleave thoughts at diffusion timesteps and across spatiotemporal/3D domains to achieve on-the-fly guidance beyond text-to-image.<br>â€¢ Reward Design and Policy Optimization for Interleaved Reasoning: Develop richer multimodal rewards and RL strategies to better align thought quality, correction efficacy, and visual outcomes.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16528" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16528" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600times smaller than the 600M turkish-e5-large dense encoder while preserving over 71\% of its average mAP. Late-interaction models that are 3--5times smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33times faster than PLAID and offers +1.7\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets (leq50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of a systematic, reproducible benchmark comparing dense bi-encoders and late-interaction models for Turkish IR, despite Turkishâ€™s agglutinative morphology benefiting from token-level matching.<br>â€¢ Dense single-vector encoders create an information bottleneck and multilingual models trained on high-resource languages under-capture Turkish morphology and token-level semantics.<br>â€¢ Late-interaction architectures (e.g., ColBERT) have been scarcely evaluated for Turkish; prior work lacks uniform training protocols and comprehensive cross-domain baselines.<br>â€¢ Production constraints (latency, storage) for multi-vector retrieval are underexplored in Turkish; indexing strategies like MUVERA/PLAID have not been systematically assessed.<br>â€¢ Need for parameter-efficient retrievers suitable for resource-constrained deployment while retaining strong effectiveness in Turkish domains.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce TurkColBERT, a two-stage adaptation pipeline that first fine-tunes multilingual/English encoders on Turkish NLI/STS, then converts them into ColBERT-style late-interaction retrievers via PyLate trained on MS MARCO-TR, followed by MUVERA integration for fixed-dimensional, low-latency indexing with optional reranking. Evaluate 10 models across five Turkish BEIR datasets and compare indexing backends (PLAID, MUVERA, MUVERA+Rerank) for qualityâ€“efficiency trade-offs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Web-Scale Turkish Late-Interaction Retrieval with MUVERA: A Billion-Document Evaluation: Evaluate MUVERA(+Rerank) at web scale to quantify latency, throughput, and effectiveness trade-offs for Turkish.<br>â€¢ Morphology-Aware Tokenization for Late-Interaction in Agglutinative Languages: Incorporate morpheme-level segmentation and analyzers into ColBERT to enhance Turkish token-level matching.<br>â€¢ Hybrid Sparseâ€“Late-Interaction Architectures for Turkish IR: Combine SPLADE/BM25 with MUVERA-backed ColBERT for recall-efficient, low-latency two-stage retrieval.<br>â€¢ Building Native Turkish IR Benchmarks Beyond Translation: Construct large, domain-diverse Turkish corpora with human relevance judgments to mitigate translation artifacts.<br>â€¢ Parameter-Efficient Late-Interaction via Hashing, Distillation, and Quantization: Fuse BERT-Hash projections with knowledge distillation and low-bit quantization to produce sub-5M Turkish ColBERT models with minimal quality loss.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Generalist Foundation Models Are Not Clinical Enough for Hospital Operations</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13703" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13703" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Hospital operations require accurate predictions (readmission, mortality, length of stay, comorbidity, insurance denial) from complex EHR text, which generalist LLMs do not reliably provide in real-world settings.<br>â€¢ Existing evaluations rely on proxy/knowledge benchmarks and diagnostic dialogues that poorly reflect operational workflows, temporal drift, and data scarcity constraints.<br>â€¢ General-purpose models underperform in zero-shot on most operational tasks (except mortality), indicating a gap between medical knowledge benchmarks and operational utility.<br>â€¢ Privacy and data access limitations leave most clinical LLMs pretrained only on public corpora (e.g., MIMIC, PubMed), missing large-scale in-house EHR signals crucial for generalization.<br>â€¢ Unclear trade-offs between smaller specialist models and larger generalist LLMs for hospital operations; lack of rigorous, deployment-grounded comparisons.<br>â€¢ Need for benchmarks and methods that ensure temporal robustness and cross-institution transfer beyond single-site, static evaluations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce Lang1 (100Mâ€“7B), pretrained via next-token prediction on a mixed corpus of 80B EHR tokens and 627B web tokens, then instruction-finetuned (multiple-choice format) on five real-world hospital tasks. Build the ReMedE benchmark (668,331 EHR notes) with time-based splits and data-efficiency protocols; demonstrate that supervised finetuning is necessary, and that in-domain pretraining enables smaller models (e.g., Lang1-1B) to outperform much larger generalist models and transfer across tasks and an external health system.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Federated Clinical Pretraining for Hospital Operations: Privacy-preserving, multi-institution EHR pretraining to improve cross-system generalization without sharing raw data.<br>â€¢ Continual and Temporally-Robust Finetuning for Clinical LLMs: Methods that adapt Lang1 online to temporal shifts in practice patterns, policies, and patient mix.<br>â€¢ Multimodal Operational Models Integrating Structured EHR and Notes: Combine text with labs, vitals, orders, and bed management signals to enhance operational predictions.<br>â€¢ Causality-Aware LLMs for Operational Decision Support: Incorporate causal inference to estimate intervention effects (e.g., early discharge planning) rather than correlations.<br>â€¢ Human-in-the-Loop Deployment and Safety Evaluation of Operational LLMs: Prospective studies quantifying workflow impact, fairness, and error modes in live hospital settings.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.15605" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.15605" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ VLA models overly depend on expert demonstrations, causing demonstration bias and limiting performance and generalization.<br>â€¢ Existing RL finetuning (e.g., GRPO) suffers from severe reward sparsity (binary success), wasting information in failed trajectories and yielding low training efficiency.<br>â€¢ Hand-crafted process reward modeling (PRM) requires external demonstrations and task-specific engineering, resulting in poor scalability and high cost.<br>â€¢ Lack of task-agnostic, dense, progress-wise rewards and robust behavior similarity measures that transfer beyond pixel space to real-world settings.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SRPO uses in-batch successful trajectories as self-referential anchors and computes progress-wise rewards for failures via distances in latent world representations, enabling trajectory-level advantages. This removes reliance on external demonstrations and manual reward engineering, improving efficiency, generalization, and real-world performance in VLA RL.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ World-Representation-Guided SRPO for Long-Horizon Manipulation: Extend self-referential progress to hierarchical and temporally aligned references for complex multi-step tasks.<br>â€¢ Adaptive Latent Encoders for Task-Agnostic Policy Optimization: Jointly learn or adapt the world representation encoder with the policy to improve robustness across domains and sensors.<br>â€¢ Self-Referential Curriculum RL for VLA Agents: Build curricula from evolving in-batch successes to guide exploration and accelerate learning without external reward models.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16618" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16618" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing SAM2 for Surgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average J\&F over vanilla SAM2. SAM2S further advances performance to 80.42 average J\&F, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Surgical video segmentation is essential for computer-assisted surgery, but interactive models struggle with the domain gap (lighting, smoke/blood occlusions, camera motion/zoom, hour-long procedures).<br>â€¢ Existing datasets lack instance-level, temporally consistent masklets across diverse procedures, limiting development and evaluation of long-term tracking and zero-shot generalization.<br>â€¢ SAM2â€™s greedy short-term memory leads to viewpoint overfitting and tracking failures when objects disappear/reappear over long durations.<br>â€¢ Class-agnostic segmentation cannot leverage stable instrument semantics, weakening consistent tracking across procedures.<br>â€¢ Multi-source annotation inconsistencies and inherently ambiguous tissue boundaries cause overconfident, poorly calibrated predictions in safety-critical settings.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SAM2S augments SAM2 with DiveMem (trainable diverse memory via hybrid temporal sampling and diversity-based long-term frame selection), Temporal Semantic Learning (visionâ€“language contrastive learning with CLIP for instrument categories), and Ambiguity-Resilient Learning (Gaussian label softening with focal loss), trained on the new SA-SV surgical iVOS benchmark to deliver robust long-term tracking and zero-shot generalization at real-time speed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Memory Policies for Hour-Long Surgical Videos via Reinforcement Learning: Learn task-aware memory selection policies that optimize long-term tracking beyond heuristic diversity criteria.<br>â€¢ Open-Vocabulary Surgical Tissue and Instrument Segmentation with Medical Ontologies: Extend TSL to tissues using curated medical terminology and domain-specific language models for semantic-aware, zero-shot segmentation.<br>â€¢ Uncertainty-Calibrated Interactive VOS for Surgery with Bayesian SAM2S: Integrate probabilistic modeling to quantify spatiotemporal uncertainty, guiding prompts and memory updates for safer, more reliable segmentation in ambiguous regions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16595" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16595" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Long-context inefficiency: Transformer backbones have quadratic attention cost and large KV-cache, making long-video processing computationally prohibitive.<br>â€¢ Vision-token explosion and redundancy: Hour-long videos yield millions of vision tokens; even after projector-level compression, the LLM remains the primary bottleneck due to redundant tokens.<br>â€¢ Limits of existing compression: Most methods compress before the LLM or drop tokens via attention scores, risking irreversible information loss; in-LLM compression for hybrid (Mamba-Transformer) models is largely unexplored.<br>â€¢ Hybrid modeling gap: Linearized models (e.g., Mamba) offer O(n) efficiency but often underperform on complex multimodal tasks; combining efficiency of SSMs with attentionâ€™s expressivity remains challenging.<br>â€¢ Lack of interpretability: Little understanding of information flow in hybrid MLLMs; how and where vision information aggregates into text tokens is unclear, hindering principled compression.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>TimeViper combines a hybrid Mamba-Transformer LLM backbone with TransV, a gated cross-attention token-transfer module that progressively transfers and compresses redundant vision tokens into instruction tokens inside the LLM (uniform dropping in shallow layers, attention-guided in deep layers). Together with ViT+ToMe pre-LLM compression, this design enables processing of >10k frames while maintaining competitive performance and improving throughput.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Scaling Laws for Hybrid Mamba-Transformer MLLMs: Systematically study data/model/context scaling to close performance gaps with SOTA while retaining efficiency.<br>â€¢ TransV++: Task-Aware Token Transfer for Multimodal Compression: Learn adaptive layer placement and transfer rates conditioned on task type and input redundancy.<br>â€¢ Curriculum Long-Context Training for Hour-Long Video Understanding: Train end-to-end on >10k-frame sequences using progressive context expansion and stabilization techniques.<br>â€¢ Timestamp-Aware Hybrid Layers for Temporal Precision: Integrate explicit temporal encodings (e.g., MRoPE-like) and temporal tokenization to enhance grounding and sequence reasoning.<br>â€¢ Bidirectional Information Routing in Hybrid MLLMs: Generalize TransV to bidirectionally transfer between modalities or into shared compressed tokens for improved multimodal fusion.<br>â€¢ Interpretable Probes for Attentionâ€“State Dynamics in Hybrid Models: Develop probes and diagnostics to visualize information aggregation paths and guide compression policies.<br>â€¢ Audio-Video-Language TimeViper for Long-Form Multimodal Understanding: Extend the framework to incorporate audio and study cross-modal compression within the hybrid LLM.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">NaTex: Seamless Texture Generation as Latent Color Diffusion</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16317" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16317" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Multi-view diffusion (MVD) texturing suffers from occlusions that require inpainting, leading to artifacts and incomplete coverage.<br>â€¢ Precise textureâ€“geometry alignment is difficult due to projection-induced information loss and latent diffusion errors; 2D normal/depth controls are insufficient for fine details.<br>â€¢ Cross-view content and color coherence is hard to maintain and computationally costly, with cascading errors from 2D-to-3D baking and reliance on UV/proxy representations limiting scalability and data efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>NaTex models textures natively as a 3D color field using a geometry-aware color point cloud VAE to compress RGB+position+normal into latent tokens, and a multi-control DiT to generate texture latents conditioned on native geometry tokens (RoPE + geometry latents), image features, and optional color control. A dual-branch VAE (color + geometry), near-surface color regression, and truncated UDF loss enable precise alignment, occlusion-free synthesis, and efficient (>80Ã—) latent diffusion.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Joint Geometryâ€“Texture Diffusion: End-to-End Asset Generation with Native 3D Controls: Extend NaTex to simultaneously generate mesh geometry and texture via coupled VAEs/DiT, improving global consistency and topology-aware appearance.<br>â€¢ Physically-Based Material Field Diffusion: Unified Generation of Albedo, Metalness, Roughness in 3D Space: Generalize the color field to multi-channel PBR fields with render-aware losses and material-aware conditioning for faithful material synthesis.<br>â€¢ Part-Aware Texture Diffusion: Semantic Tokens for Controllable Region-Level Texturing: Integrate segmentation-derived semantic tokens into the DiT to enable region-specific styles and editing without UV dependence.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.15248" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.15248" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Long-term RL training of LLMs needs stable exploration; uncontrolled entropy causes premature convergence (too low) or noisy, unstable outputs (too high).<br>â€¢ Existing entropy regulation methods (token masking/down-weighting, entropy bonuses/weights) cannot keep entropy near a desired target, often losing useful gradients, adding complexity, and showing high sensitivity to hyperparameters; most focus on off-policy settings while on-policy entropy control remains underexplored.<br>â€¢ Pretrained LLMs start with low entropy, and mixed positive/negative samples affect entropy in opposite ways across steps, making fixed regularization ineffective and causing early-stage entropy fluctuations in large-scale training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>EntroPIC applies proportional-integral control to stabilize policy entropy by dynamically tuning the loss coefficients of positive and negative samples to track a target entropy. It provides theoretical analysis for both on-policy and off-policy settings, enabling robust entropy control during large-scale LLM RL training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ PID-Enhanced Entropy Stabilization for LLM RL: Extend PI to full PID by adding a derivative term to damp rapid entropy oscillations under non-stationary data.<br>â€¢ Curriculum-Adaptive Entropy Targets in RLVR: Learn or schedule target entropy over training phases to align exploration with task difficulty and model capability.<br>â€¢ Covariance-Aware Entropy Control for LLMs: Incorporate online estimates of log-probâ€“advantage covariance to refine control signals and improve entropy regulation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.14865" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.14865" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ FS platforms generate long-range, heterogeneous multi-channel sequences (digital and physical), yet must serve recommendations under strict real-time latency.<br>â€¢ Multiple interrelated products and surfaces (feeds vs placements) require coordinated modeling that balances CTR, CVR, and shifting business priorities.<br>â€¢ Tree-based models dominate for explainability, but require heavy feature engineering, flatten temporal histories, miss long-range dependencies, and struggle with high-dimensional embeddings.<br>â€¢ Siloed product-specific models increase maintenance cost and technical debt, and waste cross-product signal sharing.<br>â€¢ Prior sequential rec work largely ignores non-digital financial signals (transactions, payments), complex conversion attribution (exposure without click, variable windows), and opt-out constraints.<br>â€¢ Single-objective optimization (CTR-only or CVR-only) yields clickbait or poor UX; FS needs multi-objective ranking with monetary value and urgency controls.<br>â€¢ Regulatory environments demand visit-level interpretability and guardrail monitoring, which many deep models lack.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>FinTRec is a unified transformer framework that ingests raw clickstream plus static/dynamic context and 768-d financial foundation-model embeddings, using a timestamp-masked decoder for pCTR and a bi-directional encoder for pCVR with explicit temporal encodings, then ranks via a multi-objective score combining urgency, pCTR, and pCVRÃ—PV. It supports efficient cross-product adaptation through token-embedding extension and LoRA/LP/full fine-tuning, provides visit-level attributions (attention and Grad-SAM), and meets sub-120 ms p99 serving with point-in-time FM aggregation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Unified Dual-Objective Transformers for Joint Click and Conversion Modeling in Financial Services: A single architecture that jointly learns pCTR and pCVR with shared backbones and task-specific heads to reduce technical debt and improve end-to-end optimization.<br>â€¢ Streaming Financial Foundation Embeddings for Fresh Real-Time Personalization: Incremental, low-latency updates to FM embeddings to capture same-day interactions without batch staleness, with on-device or edge aggregation.<br>â€¢ Regulatory-Grade Explainability for Sequential Recommenders in Banking: A compliant visit-level attribution framework unifying attention-, gradient-, and counterfactual-based methods with fairness auditing and policy constraints.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PartUV: Part-Based UV Unwrapping of 3D Meshes</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16659" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16659" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing artifacts and hindering downstream tasks. We introduce PartUV, a part-based UV unwrapping pipeline that generates significantly fewer, part-aligned charts while maintaining low distortion. Built on top of a recent learning-based part decomposition method PartField, PartUV combines high-level semantic part decomposition with novel geometric heuristics in a top-down recursive framework. It ensures each chart's distortion remains below a user-specified threshold while minimizing the total number of charts. The pipeline integrates and extends parameterization and packing algorithms, incorporates dedicated handling of non-manifold and degenerate meshes, and is extensively parallelized for efficiency. Evaluated across four diverse datasets, including man-made, CAD, AI-generated, and Common Shapes, PartUV outperforms existing tools and recent neural methods in chart count and seam length, achieves comparable distortion, exhibits high success rates on challenging meshes, and enables new applications like part-specific multi-tiles packing. Our project page is at https://www.zhaoningwang.com/PartUV.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing UV unwrapping methods on noisy, AI-generated meshes (bumpy surfaces, tiny/irregular triangles) often fail or return extremely fragmented charts.<br>â€¢ Heuristics based solely on local geometry produce unintuitive seams that cut across semantic parts, leading to long seams, artifacts, and poor downstream usability (painting, baking, editing).<br>â€¢ Optimization-based/neural per-shape methods (e.g., neural fields, joint cut optimization) are slow (tens of minutes per mesh) and can still yield noticeable distortion.<br>â€¢ Lack of robustness and efficiency on non-manifold and multi-component meshes limits practical deployment.<br>â€¢ Absence of semantic priors prevents part-aligned charts and part-aware atlas packing, which are valuable for organization and editing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>PartUV couples a learned hierarchical part decomposition (PartField) with a top-down, budgeted recursive search that interleaves two geometric heuristicsâ€”Normal (face-normal clustering into candidate charts) and Merge (OBB-guided component merging)â€”selecting the fewest charts whose ABF++ parameterizations satisfy a user distortion threshold and no-overlap constraint. The pipeline accelerates evaluation via parallel recursion and GPU simplification for surrogate distortion, includes non-manifold/connected-component handling, and supports semantic-aware UV packing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ End-to-End Semantic UV Unwrapping: Jointly learn part decomposition, seam placement, and parameterization in a differentiable model that optimizes distortion, chart count, and overlap.<br>â€¢ Perception- and Texture-Aware Seam Placement for Part-Based UV: Integrate saliency, material boundaries, and view-dependent visibility to position seams where they are least perceptible under distortion constraints.<br>â€¢ Semantic UDIM Packing via Multi-Objective Optimization: Optimize multi-tile atlas layouts that respect part hierarchies, texel density targets, and baking/editing constraints for improved workflows.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">BioBench: A Blueprint to Move Beyond ImageNet for Scientific ML Benchmarks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16315" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16315" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>ImageNet-1K linear-probe transfer accuracy remains the default proxy for visual representation quality, yet it no longer predicts performance on scientific imagery. Across 46 modern vision model checkpoints, ImageNet top-1 accuracy explains only 34% of variance on ecology tasks and mis-ranks 30% of models above 75% accuracy. We present BioBench, an open ecology vision benchmark that captures what ImageNet misses. BioBench unifies 9 publicly released, application-driven tasks, 4 taxonomic kingdoms, and 6 acquisition modalities (drone RGB, web video, micrographs, in-situ and specimen photos, camera-trap frames), totaling 3.1M images. A single Python API downloads data, fits lightweight classifiers to frozen backbones, and reports class-balanced macro-F1 (plus domain metrics for FishNet and FungiCLEF); ViT-L models evaluate in 6 hours on an A6000 GPU. BioBench provides new signal for computer vision in ecology and a template recipe for building reliable AI-for-science benchmarks in any domain. Code and predictions are available at https://github.com/samuelstevens/biobench and results at https://samuelstevens.me/biobench.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ ImageNet-1K accuracy no longer predicts performance on scientific/ecological imagery; across 46 checkpoints, ImageNet explains only ~34% of BioBench variance and rank concordance drops (Ïâ‰ˆ0.55 overall; â‰ˆ0.42 above 75% ImageNet), mis-ranking ~30% of top models.<br>â€¢ Severe distribution mismatch and task structure gap: scientific images differ in spectrum, noise, and acquisition (IR camera traps, drones, micrographs), and tasks are fine-grained and long-tailedâ€”conditions underrepresented in ImageNet/COCO/ADE.<br>â€¢ Fragmented ecology datasets with heterogeneous repos, splits, and metrics prevent fair, reproducible, cross-task comparisons; evolving evaluation scripts and hyperparameters create a â€œbenchmark lottery.â€<br>â€¢ Existing benchmarks (ImageNet, iNat2021, VTAB, WILDS) lack ecological task diversity, mission-driven objectives (e.g., behavior, traits), and statistical rigor (CIs, rank stability), making them insufficient proxies for ecological workflows.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>BioBench: a unified, application-driven ecology vision benchmark consolidating 9 tasks across 4 kingdoms and 6 image regimes (3.1M images) with a single Python API and a standardized frozen-embedding linear/logistic probing protocol, reporting class-balanced macro-F1 (plus domain metrics) with bootstrap confidence intervals. The authors evaluate 46 modern checkpoints and quantify the breakdown of ImageNetâ€™s predictive validity, providing a reproducible, scalable recipe (single-GPU, hours) for assessing representation quality under real distribution shift.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ MedBench: A Blueprint for Reliable Scientific ML Benchmarks in Medical Imaging â€” Extend the BioBench recipe to radiology/pathology with detection/segmentation tasks, calibration and operating-point metrics, and domain-specific shifts.<br>â€¢ When to Fine-Tune: A Systematic Study of Frozen Probes vs Task-Specific Adaptation on BioBench â€” Quantify performance gains, rank stability, and costâ€“benefit trade-offs of fine-tuning across modalities and long-tailed taxa.<br>â€¢ Ecology-Aligned Pretraining: Objectives and Data Pipelines for Long-Tail, Multi-Modal Transfer â€” Design and evaluate pretraining (e.g., IR/multispectral, video, taxon-aware losses) tailored to ecological imagery to improve BioBench transfer.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Boosting Medical Visual Understanding From Multi-Granular Language Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.15943" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.15943" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at https://github.com/HUANGLIZI/MGLL{https://github.com/HUANGLIZI/MGLL}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ CLIPâ€™s single-label, single-granularity alignment fails in medical imaging where one image often has multiple labels and hierarchical annotations (e.g., disease, severity, clinical findings).<br>â€¢ Existing multi-label contrastive methods struggle to capture cross-granularity semantics and generalize poorly across datasets with heterogeneous annotations.<br>â€¢ Medical domains face data scarcity, privacy constraints, and annotation noise, making robust, efficient, and plug-and-play alignment methods crucial for practical deployment.<br>â€¢ Standard imageâ€“text pretraining often treats granularities in a single space, limiting fine-grained discrimination needed for clinical tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MGLL introduces a contrastive framework that jointly optimizes multi-label and cross-granularity alignment using a soft CLIP loss (co-occurrenceâ€“weighted multi-label positives), a point-wise binary cross-entropy for fine-grained imageâ€“text pairs, and a smooth KL divergence to enforce distributional consistency across granularities, all with standard encoders and no extra compute. Trained on constructed multi-granular datasets (fundus, X-ray), MGLL serves as a plug-and-play module for visionâ€“language models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Federated Multi-Granular Medical VLP: Privacy-Preserving MGLL Across Institutions: Extend MGLL with federated learning to leverage multi-center data without sharing patient information, maintaining cross-granularity consistency under domain shift.<br>â€¢ Noise-Robust MGLL: Probabilistic Label Smoothing for Heterogeneous Clinical Annotations: Model annotation uncertainty across granularities with probabilistic soft labels and robust objectives to handle noisy, partial, or conflicting supervision.<br>â€¢ Adaptive Granularity-Conditioned Encoders for Medical VLMs: Learn encoders that dynamically condition on granularity level to further boost fine-grained discrimination while retaining MGLLâ€™s cross-granularity consistency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Draft and Refine with Visual Experts</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11005" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11005" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While recent Large Vision-Language Models (LVLMs) exhibit strong multimodal reasoning abilities, they often produce ungrounded or hallucinated responses because they rely too heavily on linguistic priors instead of visual evidence. This limitation highlights the absence of a quantitative measure of how much these models actually use visual information during reasoning. We propose Draft and Refine (DnR), an agent framework driven by a question-conditioned utilization metric. The metric quantifies the model's reliance on visual evidence by first constructing a query-conditioned relevance map to localize question-specific cues and then measuring dependence through relevance-guided probabilistic masking. Guided by this metric, the DnR agent refines its initial draft using targeted feedback from external visual experts. Each expert's output (such as boxes or masks) is rendered as visual cues on the image, and the model is re-queried to select the response that yields the largest improvement in utilization. This process strengthens visual grounding without retraining or architectural changes. Experiments across VQA and captioning benchmarks show consistent accuracy gains and reduced hallucination, demonstrating that measuring visual utilization provides a principled path toward more interpretable and evidence-driven multimodal agent systems.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LVLMs often produce ungrounded, hallucinated responses by over-relying on linguistic priors instead of visual evidence.<br>â€¢ There is no quantitative, query-conditioned metric to measure how much a model actually uses relevant visual inputs during reasoning.<br>â€¢ Existing tool-calling agents invoke visual experts based on language confidence or heuristics, inheriting LLM biases and lacking principled visual criteria.<br>â€¢ Learning-based multi-expert coordination is costly and inflexible, requiring joint optimization across tasks and experts.<br>â€¢ Prior hallucination mitigation typically applies global or post-hoc corrections without query-conditioned relevance, risking over-attention to irrelevant regions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DnR introduces a question-conditioned visual utilization metric built from an LLM-decomposed query set and CLIP-based relevance maps, then uses relevance-weighted top/bottom-k probabilistic masking and semantic deviation to quantify evidence dependence. Guided by this metric, DnR renders visual expertsâ€™ outputs onto the image and re-queries the LVLM, selecting the refinement that maximizes utilization gain (optionally via a learned selector), all without retraining.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Rendering Policies for LVLMs: Learn input-specific policies to adjust masking density, rendering style, and expert combinations using utilization gains as the optimization signal.<br>â€¢ Calibrated Utilization Across Models and Domains: Develop normalization and calibration schemes for Uq to enable cross-architecture comparability and robust thresholds for expert invocation.<br>â€¢ Temporal DnR: Draft-and-Refine with Video Experts: Extend query-conditioned relevance and utilization to video, integrating tracking, motion, and temporal grounding experts to reduce time-dependent hallucination.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 7</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-21 23:06:31 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 7;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>