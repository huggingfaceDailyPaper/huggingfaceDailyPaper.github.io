<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - January 13, 2026</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">January 13, 2026</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06943" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06943" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing video reasoning benchmarks are closed-evidence and do not evaluate open-web verification; real-world video QA requires cross-frame cue extraction plus iterative web search and multi-hop evidence synthesis.<br>‚Ä¢ Deep research benchmarks are predominantly text-initiated and treat visual content as auxiliary, failing to make video-derived anchors first-class signals that must be tracked and propagated through retrieval chains.<br>‚Ä¢ The capability boundaries of Agentic versus Workflow paradigms for video deep research are unclear; issues like goal drift and long-horizon consistency, especially the maintenance of initial video anchors over extended search, remain under-characterized.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces VideoDR, a video-conditioned open-domain QA benchmark that explicitly integrates browser-based search and multi-hop reasoning anchored to cross-frame visual cues, with rigorous video/web dependency checks and human difficulty stratification. It evaluates leading MLLMs under Workflow versus Agentic settings using standardized tools (search, think) and LLM-as-judge scoring to reveal bottlenecks such as goal drift and long-horizon consistency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Anchored Memory for Agentic Video Research: Preventing Goal Drift via Persistent Visual Cue State: Design persistent, externalized memory of visual anchors with re-anchoring and correction mechanisms to stabilize multi-round web retrieval.<br>‚Ä¢ Adaptive Rewatch-and-Search Agents for Long Videos: A Closed-loop Planner for Video-conditioned Web Retrieval: Develop agents that strategically rewatch key segments, refine anchors, and adapt query plans to improve long-horizon consistency on lengthy videos.<br>‚Ä¢ VideoDR-XL: Scaling Open-Web Video Deep Research with Live-Web Dynamics and Automated Evidence Auditing: Expand the benchmark across domains and durations, incorporate live-web evaluations, and add automated evidence archiving/verification to stress robustness and reproducibility.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">BabyVision: Visual Reasoning Beyond Language</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06521" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06521" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ MLLMs excel on knowledge-heavy benchmarks yet fail at foundational, pre-linguistic visual skills (discrimination, tracking, spatial perception, pattern recognition), with SOTA models far below human (e.g., 49.7% vs adult 94.1%).<br>‚Ä¢ Existing evaluations over-rely on linguistic priors and suffer language leakage, rarely isolating beyond-language visual reasoning; many tasks are naturally solved by drawing rather than text.<br>‚Ä¢ There is no rigorous, developmentally grounded, low-text benchmark (and tooling) to systematically measure early-vision competencies and human‚Äìmodel gaps across fine-grained subskills.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce BABYVISION, a 388-item benchmark spanning 22 subtypes across four early-vision domains, curated via a multi-stage pipeline (developmental taxonomy, data expansion/filtering, annotation, double-blind validation) to minimize linguistic dependence and paired with child/adult baselines. Extend with BABYVISION-GEN to assess drawing-based visual reasoning and an automatic evaluation toolkit (~96% human agreement), enabling comprehensive assessment of both MLLMs and generators.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Curriculum Learning for Visual Primitives on BABYVISION: Stage training to mirror developmental progression from basic discrimination to tracking and spatial imagination, targeting measured gaps.<br>‚Ä¢ Vision-Only Reinforcement Learning on BABYVISION-GEN: Optimize models using visual feedback from the automatic evaluator to improve drawing-based reasoning while reducing reliance on text priors.<br>‚Ä¢ Augmenting MLLMs with Differentiable Tracking and Spatial Imagination Modules: Integrate specialized components for line-following, occlusion reasoning, and mental rotation, validated on BABYVISION subtypes.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05593" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05593" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LMs struggle to scale test-time compute beyond sequential chain-of-thought due to fixed context windows, limiting effective exploration depth and breadth.<br>‚Ä¢ Existing methods lack a mechanism to coordinate many parallel reasoning trajectories and synthesize dispersed evidence into compact, bounded context, yielding little benefit from increased TTC.<br>‚Ä¢ Models are not trained for outcome-based synthesis and compression; standard supervision/RL optimizes token generation rather than multi-round message aggregation, causing added compute to be underutilized.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>PaCoRe scales TTC via multi-round parallel reasoning coordinated by a message-passing architecture: each round launches many trajectories, compacts their findings into bounded messages, and synthesizes them to guide subsequent rounds and the final answer. It is trained end-to-end with large-scale outcome-based reinforcement learning, enabling multi-million-token effective TTC without exceeding context limits.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Budgeting for PaCoRe: Learning to Allocate Parallel Trajectories and Rounds Under Compute Constraints: Develop policies that dynamically choose the number of trajectories and coordination rounds per instance to optimize accuracy‚Äìcost trade-offs.<br>‚Ä¢ Hierarchical Message-Passing PaCoRe: Multi-Level Synthesis for Long-Horizon Mathematical and Code Reasoning: Introduce hierarchical coordinators that aggregate messages at multiple scales to enable deeper search while respecting context bounds.<br>‚Ä¢ PaCoRe-Toolformer: Coordinated Parallel Reasoning with External Tools and Verifiers: Integrate program execution, retrieval, and formal verification into PaCoRe‚Äôs parallel trajectories with learned synthesis of heterogeneous tool outputs.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06953" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06953" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Competitive programming needs deep reasoning, but existing post-training relies on scarce real-world datasets (APPS, CodeContests, TACO), limiting scale, diversity, and difficulty.<br>‚Ä¢ Prior synthetic approaches mostly rewrite/evolve seed tasks, capping novelty and complexity; they also lack accurate, comprehensive test cases needed for reliable SFT and RL.<br>‚Ä¢ RL for code requires large-scale, robust feedback, yet practical infrastructures for high-concurrency code validation and resilience to noisy rewards are underdeveloped.<br>‚Ä¢ It is unclear how to best scale synthetic SFT data (more unique tasks vs. more solutions per task) and how code-centric RL behaves under imperfect supervision; guidance on effective scaling and staged training is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>X-Coder trains code reasoning models entirely on synthetic data produced by SynthSmith‚Äîa feature-based pipeline that generates multi-style competitive tasks, tool/prompt-based test inputs, and candidate solutions, then selects ‚Äúgolden‚Äù tests/solutions via dual verification (consensus voting plus weighted evaluation with hold-out validation), followed by GRPO-style RL on a high-concurrency execution infrastructure.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Curriculum-SynthSmith: Difficulty-Calibrated Synthetic Curriculum for Code RL: Design adaptive curricula and data-selection policies that progressively increase task difficulty and reasoning length, optimizing RL stability and gains.<br>‚Ä¢ Adversarial TestSmith: Fuzzing- and Symbolic-Execution-Driven Test Generation for Robust Code RL: Integrate fuzzing, SMT/symbolic execution, and structural coverage metrics to auto-generate harder, diverse tests that curb reward hacking and improve generalization.<br>‚Ä¢ Polyglot X-Coder: Multilingual Synthetic Competitive Programming with Cross-Language Consistency: Extend SynthSmith to multiple languages and enforce transpilation consistency, studying transfer, robustness, and evaluation across language ecosystems.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.07832" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.07832" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\% improvement on ImageNet classification, a 6.3\% gain on NLP, a 12.6\% improvement on image generation, and a 41\% enhancement on video generation under the same time complexity.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Quadratic cost of softmax self-attention limits scalability to long sequences and high-resolution generative tasks.<br>‚Ä¢ Linear attention compresses all keys/values into a single global summary, causing global context collapse, low-rank/near-uniform attention, and loss of query-dependent diversity.<br>‚Ä¢ Existing fixes (e.g., depthwise convolutions, gating) reintroduce computational overhead and still degrade as sequence length grows, undermining the efficiency benefits of linear attention.<br>‚Ä¢ Need a method that restores expressivity and query-specific context while preserving O(N) complexity and streaming/stateful compatibility.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MHLA splits tokens into multiple non-overlapping token-level heads, computes local KV summaries per head, and lets each query block form a query-conditioned mixture over these summaries with additional query-dependent reweighting within selected blocks. Implemented with standard GEMMs, it preserves O(N) complexity and streaming compatibility while recovering much of softmax attention‚Äôs expressivity (higher-rank behaviors).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Token-Head Partitioning for Linear Attention: Learn or dynamically adjust token block boundaries and head counts to match content and sequence length.<br>‚Ä¢ Hierarchical Multi-Scale MHLA for Ultra-Long Contexts: Stack MHLA across scales to aggregate local-to-global context efficiently for images, videos, and long texts.<br>‚Ä¢ Tight Expressivity and Rank Bounds for Token-Level Multi-Head Linear Attention: Formalize and prove bounds connecting head partitioning, mixture mechanisms, and achievable attention rank.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05110" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05110" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost. Collaborative inference offers a promising solution by selectively allocating work between lightweight and large models, yet a fundamental challenge remains: determining when a reasoning step requires the capacity of a large model or the efficiency of a small model. Existing routing strategies either rely on local token probabilities or post-hoc verification, introducing significant inference overhead. In this work, we propose a novel perspective on step-wise collaboration: the difficulty of a reasoning step can be inferred from its very first token. Inspired by the "Aha Moment" phenomenon in LRMs, we show that the entropy of the initial token serves as a strong predictor of step difficulty. Building on this insight, we introduce GlimpRouter, a training-free step-wise collaboration framework. GlimpRouter employs a lightweight model to generate only the first token of each reasoning step and routes the step to a larger model only when the initial token entropy exceeds a threshold. Experiments on multiple benchmarks demonstrate that our approach significantly reduces inference latency while preserving accuracy. For instance, GlimpRouter attains a substantial 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to a standalone large model on AIME25. These results suggest a simple yet effective mechanism for reasoning: allocating computation based on a glimpse of thought rather than full-step evaluation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Large Reasoning Models (LRMs) incur high inference latency and computational cost due to long, multi-step chain-of-thought generation.<br>‚Ä¢ Collaborative inference needs accurate, low-overhead routing to decide when to use small vs. large models at the step level.<br>‚Ä¢ Existing methods rely on local token probabilities or post-hoc validation (e.g., LLM-as-a-judge), introducing significant overhead; averaged uncertainty metrics (entropy/perplexity) dilute informative signals and poorly capture step difficulty.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>GlimpRouter is a training-free step-wise collaboration: a lightweight model emits the first token of each reasoning step, its entropy is computed, and the step is routed to a large model only if that entropy exceeds a threshold. This 'glimpse of thought' captures step difficulty to reduce latency while preserving accuracy and works orthogonally with speculative decoding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Glimpse Thresholds for Step-wise Routing: Learn task- and context-aware entropy thresholds to optimize accuracy‚Äìlatency trade-offs.<br>‚Ä¢ Predictive Aha-Score Beyond Entropy: Train lightweight predictors from context and logits to estimate step difficulty and enhance routing decisions.<br>‚Ä¢ Unified Token- and Step-level Collaboration: Integrate GlimpRouter with speculative decoding into a single framework for compound speedups and stability.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.07779" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.07779" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of granular control over historical visual context in memory causes visual context loss, prevents trajectory-level self-correction (e.g., intent drift, loops), and undermines robustness in long-horizon tasks.<br>‚Ä¢ Absence of visual-aware tutorial retrieval: existing RAG pipelines are largely text-only or rely on costly, static local knowledge bases, leading to poor generalization and fidelity in unseen/OOD software scenarios.<br>‚Ä¢ Single-agent and API-dependent approaches struggle with complex workflows and cross-software/version variability; modular orchestration with visual grounding is needed for reliable generalist CUAs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>OS-SYMPHONY is a modular CUA framework with an Orchestrator that coordinates a Reflection-Memory Agent using milestone-driven long-term memory and structured trajectory-level reflections, and Versatile Tool Agents including a Multimodal Searcher that follows a See-Act paradigm to browse in a sandbox and synthesize visually aligned tutorials for OOD tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Milestone Selection for Long-Horizon GUI Memory: Learn to identify and compress milestone screenshots with visual-semantic fidelity to optimize auditing and self-correction.<br>‚Ä¢ Web-Scale Multimodal RAG for GUI Agents: Integrate screenshots, DOM structure, and spatial layouts to automatically retrieve and synthesize high-fidelity, version-aware tutorials across OS/software.<br>‚Ä¢ Uncertainty-Aware Orchestrators for Robust Computer Use: Incorporate risk estimation, loop detection, and recovery policies into trajectory-level reflection to provide formal robustness guarantees in complex workflows.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Lost in the Noise: How Reasoning Models Fail with Contextual Distractors</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.07226" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.07226" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Real-world agentic AI must operate on noisy inputs (faulty tool outputs, incorrect retrievals, irrelevant chat history), but sanitized benchmarks mask true robustness.<br>‚Ä¢ State-of-the-art reasoning models suffer catastrophic drops (up to ~80%) under contextual distractors, with emergent misalignment and error amplification in agentic workflows.<br>‚Ä¢ Existing mitigations‚Äîprompting, context engineering, supervised fine-tuning, and outcome-only RL‚Äîfail to ensure robustness against diverse noise.<br>‚Ä¢ Models exhibit inverse scaling (more test-time reasoning worsens performance) and over-attend to distractor tokens, revealing fundamental weaknesses in grounding and filtering.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce NoisyBench, a benchmark that injects diverse distractors (random documents, irrelevant chat history, hard negatives) across 11 datasets in RAG, reasoning, alignment, and tool-use tasks, and propose NoisyInstruct plus a Rationale-Aware Reward (RARE) that explicitly rewards using helpful context to filter noise. RARE strengthens resilience by increasing distractor filtering in chain-of-thought and improving final accuracy beyond outcome-only RL.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Noise-Aware Agent Planning: Calibrating Tool Trust and Memory under Contextual Distractors: Develop adaptive agent frameworks that downweight unreliable tools and irrelevant history to prevent error propagation across steps.<br>‚Ä¢ Inverse-Compute Schedules: Test-Time Reasoning Strategies for Robustness in Noisy Contexts: Design dynamic stopping, compute allocation, and trajectory pruning to counter inverse scaling when distractors are present.<br>‚Ä¢ Attention-Guided Rationale Rewards: Learning to Reweight Context for Distractor Suppression: Integrate attention diagnostics into reward shaping to penalize focus on distractors and reinforce evidence-grounded reasoning.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.07351" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.07351" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion Language Models (DLMs) offer a promising alternative for language modeling by enabling parallel decoding through iterative refinement. However, most DLMs rely on hard binary masking and discrete token assignments, which hinder the revision of early decisions and underutilize intermediate probabilistic representations. In this paper, we propose EvoToken-DLM, a novel diffusion-based language modeling approach that replaces hard binary masks with evolving soft token distributions. EvoToken-DLM enables a progressive transition from masked states to discrete outputs, supporting revisable decoding. To effectively support this evolution, we introduce continuous trajectory supervision, which aligns training objectives with iterative probabilistic updates. Extensive experiments across multiple benchmarks show that EvoToken-DLM consistently achieves superior performance, outperforming strong diffusion-based and masked DLM baselines. Project webpage: https://aim-uofa.github.io/EvoTokenDLM.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Hard binary masking in MDLMs makes token decisions irreversible, limiting the ability to revise early errors and undermining the iterative refinement paradigm.<br>‚Ä¢ Inefficient utilization of computed token distributions: models predict probabilities for all positions each step but only update a small subset, discarding rich intermediate probabilistic information and wasting computation.<br>‚Ä¢ Training‚Äìinference mismatch: standard single-step denoising objectives do not align with the multi-step probabilistic evolution during inference, reducing effectiveness of diffusion-based decoding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>EvoToken-DLM models each token as an evolving soft probability distribution that transitions through four states ([MASK] ‚Üí Soft([MASK]‚à™V) ‚Üí Soft(V) ‚Üí [Decode]) with top-K mixed embeddings and blockwise progressive inference, enabling smooth, revisable decoding. It introduces continuous trajectory supervision by simulating multiple refinement steps during training and applying supervision at each step to align optimization with iterative probabilistic updates.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Token-State Scheduling for Diffusion Language Models: Learn policies that dynamically decide when tokens transition between states and which positions to advance, optimizing speed‚Äìquality trade-offs via uncertainty and context.<br>‚Ä¢ Uncertainty-Guided Reversible Finalization in Progressive Token Evolution: Enable decoded tokens to revert to soft states based on calibrated uncertainty or inconsistency signals, integrating error-correction loops for robustness.<br>‚Ä¢ Multimodal EvoToken: Extending Progressive Soft Tokens to Vision‚ÄìLanguage Diffusion Models: Generalize soft token evolution and trajectory supervision to multimodal tokens, designing cross-modal blockwise policies and continuous relaxations across modalities.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05107" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05107" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to Memory Anchoring, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose Steerable Memory Agent, SteeM, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLM agents often use an ‚Äúall-or-nothing‚Äù approach to memory that either over-anchors outputs to past interactions or ignores valuable history entirely.<br>‚Ä¢ Real-world user needs for memory usage are dynamic: users may want both faithful continuity and disruptive innovation at different times.<br>‚Ä¢ Existing methods rely on static memory injection, experience-following tendencies, and rigid masking/prompting, lacking nuanced, user-controllable regulation of memory influence.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces a behavioral metric to quantify memory dependence and proposes SteeM (Steerable Memory Agent), which exposes a user-controllable parameter to continuously modulate how strongly retrieved history influences current outputs, enabling modes from fresh-start to high-fidelity and outperforming static prompting/masking baselines.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Personalized Memory Reliance Profiles for Long-Term Agents: Train agents to infer and adapt user-specific memory dependence preferences from interaction signals.<br>‚Ä¢ Benchmarking Memory Anchoring and Innovation Trade-offs in Long-Horizon Tasks: Create standardized tasks and metrics to evaluate controllable memory usage across domains and time scales.<br>‚Ä¢ Policy-Guided Memory Control via Preference Optimization: Use preference modeling or RL to optimize the memory-reliance parameter toward task-specific outcomes (e.g., creativity vs. fidelity).</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.01528" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.01528" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing evaluations of generative driving videos rely on generic metrics (e.g., FVD, VLM-based scores) that miss safety-critical imaging factors specific to autonomous driving.<br>‚Ä¢ Trajectory plausibility is under-evaluated: generated ego and agent motions must be natural, dynamically feasible, interaction-aware, and safe, not just visually realistic.<br>‚Ä¢ Temporal and agent-level consistency is neglected, leading to issues like abrupt appearance changes or disappearing agents that undermine reliability.<br>‚Ä¢ Motion controllability with respect to ego conditioning is largely ignored, risking misalignment between planned actions and generated motion.<br>‚Ä¢ Benchmarks lack diversity across weather/time of day, geographic regions, and complex maneuvers/interactions, limiting real-world deployment readiness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DrivingGen introduces a comprehensive benchmark combining a diverse evaluation dataset curated from driving datasets and internet-scale videos with new metrics evaluating visual realism, trajectory plausibility, temporal and agent-level coherence, and ego-conditioned controllability. It provides a unified evaluation protocol and benchmarks 14 state-of-the-art models to reveal trade-offs between visual quality and physical realism.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Physics-Consistent Generative Driving World Models: Integrate explicit dynamics and interaction constraints into video generation to reduce trajectory violations while preserving visual fidelity, validated on DrivingGen metrics.<br>‚Ä¢ Agent-Level Consistency Regularization for Driving Videos: Develop identity- and appearance-preserving mechanisms that enforce persistent agents over time to improve temporal coherence and reduce disappearances.<br>‚Ä¢ Controllable Ego-Conditioned Video Generation for Safe Planning: Design methods that tightly align generated motion to given trajectories/actions with formal controllability guarantees and closed-loop evaluation on DrivingGen.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.07526" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.07526" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapid development of interactive and autonomous AI systems signals our entry into the agentic era. Training and evaluating agents on complex agentic tasks such as software engineering and computer use requires not only efficient model computation but also sophisticated infrastructure capable of coordinating vast agent-environment interactions. However, no open-source infrastructure can effectively support large-scale training and evaluation on such complex agentic tasks. To address this challenge, we present MegaFlow, a large-scale distributed orchestration system that enables efficient scheduling, resource allocation, and fine-grained task management for agent-environment workloads. MegaFlow abstracts agent training infrastructure into three independent services (Model Service, Agent Service, and Environment Service) that interact through unified interfaces, enabling independent scaling and flexible resource allocation across diverse agent-environment configurations. In our agent training deployments, MegaFlow successfully orchestrates tens of thousands of concurrent agent tasks while maintaining high system stability and achieving efficient resource utilization. By enabling such large-scale agent training, MegaFlow addresses a critical infrastructure gap in the emerging agentic AI landscape.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of infrastructure to orchestrate large-scale, complex agent-environment interactions for agent training and evaluation, despite growing needs in the agentic era<br>‚Ä¢ Security and isolation constraints in typical training clusters that forbid arbitrary container execution, blocking realistic agent environments<br>‚Ä¢ Storage scalability issues from massive container image footprints (e.g., >25TB for popular SE benchmarks), making local storage costly and unmanageable<br>‚Ä¢ Computational throughput bottlenecks and resource contention in centralized, high-spec machines (bandwidth, initialization, limited concurrency), while existing distributed ML frameworks focus on model compute rather than agent-environment coordination<br>‚Ä¢ Monolithic training pipelines without unified interfaces, hindering independent scaling/optimization of model inference/training, agent coordination, and environment provisioning</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MegaFlow decomposes agent training into three independently scalable services‚ÄîModel, Agent, and Environment‚Äîinterconnected via unified APIs and orchestrated through an event-driven scheduler on elastic, many-small cloud instances with hybrid ephemeral/persistent execution and on-demand container provisioning. It enforces standardized resource allocation with distributed semaphores/quotas, delegates container lifecycle to agent frameworks, and separates operational metadata from artifacts for robust, scalable persistence.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning-Based Adaptive Orchestration for Agent-Environments: Develop reinforcement learning or contextual bandit schedulers that optimize mode selection (ephemeral vs persistent), placement, and rate limits based on live workload signals.<br>‚Ä¢ Federated Multi-Cloud MegaFlow: Design a cross-cloud, provider-agnostic orchestration layer with policy-driven federation, failover, and cost-aware instance selection while preserving the three-service abstraction.<br>‚Ä¢ Verifiable Isolation for Multi-Tenant Agent Training: Integrate formal methods and sandbox attestation (e.g., confidential computing, eBPF policies) to provide provable isolation and compliance in large-scale agentic workloads.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">Boosting Latent Diffusion Models via Disentangled Representation Alignment</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05823" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05823" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Latent Diffusion Models (LDMs) generate high-quality images by operating in a compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Tokenizer‚Äìgenerator mismatch: Conventional VAEs are trained for pixel-level reconstruction, not for the representational needs of latent diffusion models (LDMs), leading to suboptimal downstream training and sample quality.<br>‚Ä¢ Misaligned alignment targets: Prior works align both VAEs and LDMs to the same vision foundation model (VFM) features, ignoring that VAEs need attribute-level semantic disentanglement while LDMs benefit from high-level semantics.<br>‚Ä¢ Weak latent-space diagnostics: Existing VAE latent metrics (uniformity, discrimination, ImageNet linear probing) do not reliably predict diffusion performance; a more intrinsic metric is needed.<br>‚Ä¢ Training inefficiency: Poorly structured VAE latents slow diffusion model convergence; there is a need to accelerate training while improving FID on ImageNet 256√ó256.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Send-VAE fine-tunes a VAE by aligning its noisy latent codes, transformed through a non-linear ViT-based mapper, to patch-wise VFM features via a cosine loss, while retaining standard VAE reconstruction (MSE/LPIPS/GAN) and KL losses to encourage semantic disentanglement. This disentanglement is quantified via linear probing on attribute prediction, and, when paired with SiT, yields faster convergence and state-of-the-art ImageNet 256√ó256 FID (1.21 with CFG, 1.75 without).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Mapper Networks for Disentangled Tokenizers: Learn data- and stage-adaptive mapper depth/architecture to optimally bridge VAE‚ÄìVFM representation gaps.<br>‚Ä¢ Multi-Scale and Multi-Target Alignment for Send-VAE: Jointly align latents to features from multiple VFMs and pyramid scales to enrich attribute-level disentanglement.<br>‚Ä¢ Joint Training of VAE, Mapper, and Diffusion with Disentanglement Regularizers: End-to-end optimization that couples REPA-style alignment with explicit disentanglement constraints.<br>‚Ä¢ Beyond Linear Probing: Information-Theoretic and Causal Metrics for Disentanglement in Diffusion Tokenizers: Develop metrics that better predict generative performance than linear probes.<br>‚Ä¢ Send-VAE for Video and 3D Generative Modeling: Extend disentangled alignment to spatiotemporal and 3D tokenizers for video and neural rendering.<br>‚Ä¢ Attribute-Controllable Diffusion via Disentangled Latent Modulation: Exploit disentangled VAE latents for fine-grained, label-free attribute control during sampling.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06165" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06165" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current vision-language benchmarks predominantly feature well-structured questions with clear, explicit prompts. However, real user queries are often informal and underspecified. Users naturally leave much unsaid, relying on images to convey context. We introduce HAERAE-Vision, a benchmark of 653 real-world visual questions from Korean online communities (0.76% survival from 86K candidates), each paired with an explicit rewrite, yielding 1,306 query variants in total. Evaluating 39 VLMs, we find that even state-of-the-art models (GPT-5, Gemini 2.5 Pro) achieve under 50% on the original queries. Crucially, query explicitation alone yields 8 to 22 point improvements, with smaller models benefiting most. We further show that even with web search, under-specified queries underperform explicit queries without search, revealing that current retrieval cannot compensate for what users leave unsaid. Our findings demonstrate that a substantial portion of VLM difficulty stem from natural query under-specification instead of model capability, highlighting a critical gap between benchmark evaluation and real-world deployment.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Current VLM benchmarks rely on clean, explicit prompts, failing to reflect real user queries that are informal and under-specified.<br>‚Ä¢ Under-specified visual questions cause models to miss user intent and context, leading to substantial performance drops despite strong capabilities.<br>‚Ä¢ Retrieval/web search cannot compensate for missing, implicit intent; this misattributes evaluation difficulty to model limitations and widens the gap between benchmarks and real-world deployment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce HAERAE-Vision, a rigorously filtered benchmark of 653 real-world, under-specified visual questions from Korean communities, paired with explicit rewrites (HAERAE-Vision-Explicit), and evaluate 45 VLMs with/without web search to isolate the impact of query explicitation. The six-stage pipeline (from 86K candidates across 13 domains) reveals consistent gains from explicitation and shows retrieval is ineffective when intent is unspecified.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning to Explicate: Training Vision-Language Models to Auto-Rewrite Underspecified Queries: Develop models that detect missing intent and generate explicit clarifications or rewrites before answering, improving robustness without external search.<br>‚Ä¢ Interactive Clarification for Multimodal Assistants: Uncertainty-Aware Dialogue to Resolve Under-Specification: Design multi-turn VLMs that estimate ambiguity and ask targeted follow-up questions to elicit necessary context prior to retrieval or reasoning.<br>‚Ä¢ Retrieval That Understands What‚Äôs Missing: Intent-Informed Multimodal RAG for Underspecified Queries: Build retrieval systems that jointly infer user intent from images and text, condition search on inferred goals, and integrate results with explicit reformulations.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06860" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06860" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) can extend their parameter knowledge limits by adopting the Tool-Integrated Reasoning (TIR) paradigm. However, existing LLM-based agent training framework often focuses on answers' accuracy, overlooking specific alignment for behavior patterns. Consequently, agent often exhibits ineffective actions during TIR tasks, such as redundant and insufficient tool calls. How to calibrate erroneous behavioral patterns when executing TIR tasks, thereby exploring effective trajectories, remains an open-ended problem. In this paper, we propose ET-Agent, a training framework for calibrating agent's tool-use behavior through two synergistic perspectives: Self-evolving Data Flywheel and Behavior Calibration Training. Specifically, we introduce a self-evolutionary data flywheel to generate enhanced data, used to fine-tune LLM to improve its exploration ability. Based on this, we implement an two-phases behavior-calibration training framework. It is designed to progressively calibrate erroneous behavioral patterns to optimal behaviors. Further in-depth experiments confirm the superiority of across multiple dimensions, including correctness, efficiency, reasoning conciseness, and tool execution accuracy. Our ET-Agent framework provides practical insights for research in the TIR field. Codes can be found in https://github.com/asilverlight/ET-Agent</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLM agents in Tool-Integrated Reasoning (TIR) often exhibit improper tool-use (redundant/insufficient calls, aborted executions) and flawed reasoning logic, yielding inefficiency and errors.<br>‚Ä¢ Existing frameworks prioritize final-answer accuracy while neglecting behavioral alignment (efficiency, conciseness, execution robustness) critical for practical deployment.<br>‚Ä¢ Prior data-driven (SFT/DPO) and RL approaches mainly mitigate redundancy, overlook other error patterns, and suffer from limited action-space exploration; DPO‚Äôs binary optimization can collapse outputs into narrow behaviors.<br>‚Ä¢ The vast TIR action space lacks effective on-policy exploration and calibration strategies to discover and converge to optimal tool-use trajectories.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ET-Agent couples a self-evolving data flywheel that iteratively augments and refines tool-use trajectories to expand action-space coverage with a two-phase behavior calibration training: action space exploration fine-tuning, followed by iterative RL alternating group-wise Pareto sampling and curriculum RL to progressively correct improper tool-use and flawed reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Multi-Objective Reward Shaping for Tool-Integrated Agents: Design dynamic rewards balancing accuracy, efficiency, conciseness, and execution reliability to further align agent behavior.<br>‚Ä¢ Cross-Tool Generalization in ET-Agent via Meta-Learning: Develop meta-learning mechanisms to transfer calibrated behaviors across diverse tools and domains with minimal fine-tuning.<br>‚Ä¢ Proactive Tool-Use Planning with Uncertainty-Aware Calibration: Integrate uncertainty estimation to decide when and how to call tools, reducing both redundant and insufficient tool invocations.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">Dr. Zero: Self-Evolving Search Agents without Training Data</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.07055" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.07055" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As high-quality data becomes increasingly difficult to obtain, data-free self-evolution has emerged as a promising paradigm. This approach allows large language models (LLMs) to autonomously generate and solve complex problems, thereby improving their reasoning capabilities. However, multi-turn search agents struggle in data-free self-evolution due to the limited question diversity and the substantial compute required for multi-step reasoning and tool using. In this work, we introduce Dr. Zero, a framework enabling search agents to effectively self-evolve without any training data. In particular, we design a self-evolution feedback loop where a proposer generates diverse questions to train a solver initialized from the same base model. As the solver evolves, it incentivizes the proposer to produce increasingly difficult yet solvable tasks, thus establishing an automated curriculum to refine both agents. To enhance training efficiency, we also introduce hop-grouped relative policy optimization (HRPO). This method clusters structurally similar questions to construct group-level baselines, effectively minimizing the sampling overhead in evaluating each query's individual difficulty and solvability. Consequently, HRPO significantly reduces the compute requirements for solver training without compromising performance or stability. Extensive experiment results demonstrate that the data-free Dr. Zero matches or surpasses fully supervised search agents, proving that complex reasoning and search capabilities can emerge solely through self-evolution.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ High-quality human-curated training data is costly and scarce; open-domain search agents need a data-free way to self-improve.<br>‚Ä¢ Prior self-evolving methods focus on narrow domains (math/coding) and underperform on open-domain, multi-hop reasoning tasks.<br>‚Ä¢ Existing proposers generate low-diversity, mostly one-hop questions and fail to provide progressively harder curricula as solvers improve.<br>‚Ä¢ Standard GRPO/nested sampling makes training multi-turn search agents computationally prohibitive due to many rollouts per prompt.<br>‚Ä¢ There is a need for verifiable, label-free rewards that leverage only external search signals to guide both proposer and solver.<br>‚Ä¢ Single-sample baselines suffer high variance and instability when query structures are heterogeneous, limiting reliable optimization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Dr.Zero is a data-free proposer‚Äìsolver co-evolution framework for search-augmented LLMs that uses a multi-turn tool-use pipeline and a difficulty-guided reward (via solver pass rates) to synthesize verifiable, progressively challenging questions. Training efficiency and stability come from Hop-grouped Relative Policy Optimization (HRPO), which clusters questions by hop complexity to form group-level baselines without nested sampling, while the solver is refined via GRPO.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Stabilizing Data-Free Self-Evolution at Scale: Adaptive HRPO and Entropy Controls: Develop adaptive hop clustering, entropy regularization, and curriculum policies to overcome performance plateaus and entropy collapse in larger models.<br>‚Ä¢ Reward-Safe Self-Evolving Search Agents: Robust Verifiability and Bias Mitigation: Design reward mechanisms and audits that detect and prevent reward hacking and bias amplification, using causal checks, adversarial evaluation, and fairness constraints.<br>‚Ä¢ Multi-Tool Self-Evolution: Extending Dr.Zero Beyond Search to Code, Math, and Web Browsing: Integrate additional tools and tool-selection policies to broaden self-evolution to diverse reasoning modalities and more complex multi-hop interactions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Forest Before Trees: Latent Superposition for Efficient Visual Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06803" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06803" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While Chain-of-Thought empowers Large Vision-Language Models with multi-step reasoning, explicit textual rationales suffer from an information bandwidth bottleneck, where continuous visual details are discarded during discrete tokenization. Recent latent reasoning methods attempt to address this challenge, but often fall prey to premature semantic collapse due to rigid autoregressive objectives. In this paper, we propose Laser, a novel paradigm that reformulates visual deduction via Dynamic Windowed Alignment Learning (DWAL). Instead of forcing a point-wise prediction, Laser aligns the latent state with a dynamic validity window of future semantics. This mechanism enforces a "Forest-before-Trees" cognitive hierarchy, enabling the model to maintain a probabilistic superposition of global features before narrowing down to local details. Crucially, Laser maintains interpretability via decodable trajectories while stabilizing unconstrained learning via Self-Refined Superposition. Extensive experiments on 6 benchmarks demonstrate that Laser achieves state-of-the-art performance among latent reasoning methods, surpassing the strong baseline Monet by 5.03% on average. Notably, it achieves these gains with extreme efficiency, reducing inference tokens by more than 97%, while demonstrating robust generalization to out-of-distribution domains.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Explicit Chain-of-Thought (CoT) in VLMs suffers an information bandwidth bottleneck, discarding continuous visual detail during discrete tokenization and weakening visual reasoning fidelity.<br>‚Ä¢ Existing latent reasoning still uses rigid autoregressive next-token objectives, causing premature semantic collapse ('tunnel vision') that conflicts with hierarchical global-to-local visual perception.<br>‚Ä¢ Current approaches trade off interpretability, efficiency, and robustness; they require verbose token chains, risk collapse, and generalize poorly‚Äîhighlighting the need for interpretable, efficient latent reasoning that preserves global context and scales to OOD settings.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Laser introduces Dynamic Windowed Alignment Learning (DWAL) that aligns each latent state to a dynamic validity window over future semantics, maintaining a probabilistic superposition that progressively shifts from global to local. It stabilizes and grounds this process via Self-Refined Superposition (model-estimated soft targets) and Entropy-Regularized Intervention (adaptive ground-truth injection), preserving decodable trajectories while reducing inference tokens by over 97%.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Dynamic Windowed Alignment for Temporal Visual Reasoning: Extend DWAL to videos with time-aware semantic windows for long-horizon, multi-frame causal understanding.<br>‚Ä¢ Cognitive-Guided Latent Superposition in Vision-Language Models: Infuse human perceptual priors (e.g., gaze, global-to-local schedules) to adapt window sizing and entropy control for better alignment with visual cognition.<br>‚Ä¢ Formal Guarantees for Windowed Manifold Alignment in Latent Reasoning: Establish theoretical bounds on convergence, stability, and information retention for DWAL with entropy regularization, offering diagnostics for reliable deployment.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.04698" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.04698" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Travel planning is a sophisticated decision-making process that requires synthesizing multifaceted information to construct itineraries. However, existing travel planning approaches face several challenges: (1) Pruning candidate points of interest (POIs) while maintaining a high recall rate; (2) A single reasoning path restricts the exploration capability within the feasible solution space for travel planning; (3) Simultaneously optimizing hard constraints and soft constraints remains a significant difficulty. To address these challenges, we propose TourPlanner, a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Massive POI candidate sets exceed LLM context windows, requiring pruning that preserves high recall and spatial coherence to avoid degrading itinerary quality.<br>‚Ä¢ Single-path reasoning in existing agents limits exploration of the solution space, often yielding low-feasibility itineraries and poor conflict resolution among objectives.<br>‚Ä¢ Current methods lack effective mechanisms to jointly optimize hard constraints (e.g., opening hours, non-repetition, transportation feasibility) and soft constraints (e.g., route efficiency, personalization), leading to suboptimal plans.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TourPlanner combines a Personalized Recall and Spatial Optimization (PReSO) workflow to build a compact, spatially-aware POI set, a Competitive consensus Chain-of-Thought (CCoT) that instantiates persona-based agents to generate and arbitrate parallel proposals, and a constraint-gated reinforcement learning stage that uses a sigmoid gate to prioritize soft-constraint rewards only after hard constraints are satisfied.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Real-Time TourPlanner: Integrating Live Signals into Constraint-Gated RL: Incorporate real-time data (e.g., weather, crowding, transit delays) to dynamically adjust gating and consensus weights for robust itineraries.<br>‚Ä¢ Learning Personas from Interaction Data for Competitive Consensus Planning: Automatically derive and update agent personas and arbitration weights from user feedback and historical success, improving proposal diversity and consensus quality.<br>‚Ä¢ Hierarchical CCoT for Multi-City, Multi-Day Itineraries: Extend competitive consensus to a hierarchical framework that coordinates cross-day and cross-city constraints, enabling scalable planning over longer trips.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">OpenTinker: Separating Concerns in Agentic Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.07376" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.07376" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce OpenTinker, an infrastructure for reinforcement learning (RL) of large language model (LLM) agents built around a separation of concerns across algorithm design, execution, and agent-environment interaction. Rather than relying on monolithic, end-to-end RL pipelines, OpenTinker decomposes agentic learning systems into lightweight, composable components with clearly defined abstraction boundaries. Users specify agents, environments, and interaction protocols, while inference and training are delegated to a managed execution runtime. OpenTinker introduces a centralized scheduler for managing training and inference workloads, including LoRA-based and full-parameter RL, supervised fine-tuning, and inference, over shared resources. We further discuss design principles for extending OpenTinker to multi-agent training. Finally, we present a set of RL use cases that demonstrate the effectiveness of the framework in practical agentic learning scenarios.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Applying RL to LLM agents involves long-horizon rollouts, heterogeneous execution patterns, and tight coordination of inference and training at scale, creating significant systems complexity.<br>‚Ä¢ Existing RL frameworks largely retain monolithic pipelines and do not separate agent programming from execution, requiring repeated infrastructure deployment and limiting reuse and accessibility.<br>‚Ä¢ Agent environments and interaction protocols are not treated as reusable, first-class abstractions, hindering portability across algorithms and backends.<br>‚Ä¢ There is no open, reproducible RL-as-a-service platform for agentic workflows; proprietary solutions (e.g., Tinker) are not accessible to the research community.<br>‚Ä¢ Lack of unified execution semantics across training and inference for multi-turn agent workflows and limited support for coordinated multi-agent training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>OpenTinker implements RL-as-a-service with a modular client‚Äìscheduler‚Äìserver architecture, using a Ray-based centralized scheduler to manage shared GPU resources while treating environments and interaction protocols as first-class, reusable abstractions. It unifies multi-turn training and inference via a finite state machine with precise token masking and introduces an environment-level Agent Protocol Coordinator to synchronize multi-agent interactions without sharing parameters.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ RLaaS at Scale: Multi-Node Orchestration and Elastic Scheduling for Agentic RL: Extend OpenTinker with distributed control planes and elastic resource management for multi-tenant, cross-node clusters.<br>‚Ä¢ Decoupled Engines: Separating Training and Inference Backends for LLM Agents: Design and evaluate independent server-side engines for training and inference to improve throughput, isolation, and maintainability.<br>‚Ä¢ Batch-Aware LoRA RL: Scheduling Strategies for High-Throughput Parameter-Efficient Optimization: Develop batch-level scheduling and packing policies tailored to LoRA-based RL to maximize GPU utilization and minimize latency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Are LLM Decisions Faithful to Verbal Confidence?</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.07767" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.07767" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) can produce surprisingly sophisticated estimates of their own uncertainty. However, it remains unclear to what extent this expressed confidence is tied to the reasoning, knowledge, or decision making of the model. To test this, we introduce RiskEval: a framework designed to evaluate whether models adjust their abstention policies in response to varying error penalties. Our evaluation of several frontier models reveals a critical dissociation: models are neither cost-aware when articulating their verbal confidence, nor strategically responsive when deciding whether to engage or abstain under high-penalty conditions. Even when extreme penalties render frequent abstention the mathematically optimal strategy, models almost never abstain, resulting in utility collapse. This indicates that calibrated verbal confidence scores may not be sufficient to create trustworthy and interpretable AI systems, as current models lack the strategic agency to convert uncertainty signals into optimal and risk-sensitive decisions.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLMs can verbalize calibrated confidence, but it is unclear whether this signal is faithfully translated into actions (answer vs. abstain), especially under varying error costs.<br>‚Ä¢ Existing evaluations emphasize calibration (e.g., ECE, AUARC) and accuracy, not decision-theoretic, cost-sensitive behavior or strategic reliability.<br>‚Ä¢ In high-stakes settings, models should adapt policies to penalties; current models appear risk-insensitive and biased toward always answering, leading to utility collapse.<br>‚Ä¢ There is no standardized framework to jointly assess confidence‚Äìpolicy alignment, cost-aware abstention, and recoverable utility from post-hoc optimal policies.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RiskEval: an evaluation framework that varies error penalties in the prompt and lets models answer or abstain while eliciting verbal confidence; it defines the Bayes-optimal threshold œÑ=Œª/(1+Œª) and compares model actions to this policy using policy consistency, normalized regret/utility, and AUARC, including a post-hoc œÄ* scaffold to quantify recoverable utility.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Training LLMs for Risk-Aware Abstention via Penalty-Aware RLHF: Incorporate cost-sensitive rewards and selective prediction losses so models learn to abstain when confidence falls below œÑ(Œª).<br>‚Ä¢ From Verbal Confidence to Action: Learning Decision Heads for Cost-Sensitive Policies: Add a lightweight decision module that maps internal confidence to answer/abstain with explicit optimal-threshold supervision.<br>‚Ä¢ Strategic Reliability Benchmarks Beyond Accuracy: Build multi-domain suites with controllable penalties to evaluate calibration‚Äìpolicy faithfulness and utility under risk.<br>‚Ä¢ Post-hoc Risk Controllers for LLMs: Optimal Thresholding with Calibrated Confidence: Deploy external controllers that enforce œÄ* using verbal confidence and penalties, analyzing safety‚Äìcoverage trade-offs.<br>‚Ä¢ Debiasing the Always-Answer Prior in Instruction-Tuned Models: Identify and mitigate training-time pressures that discourage abstention via counterfactual data, penalties, and selective losses.<br>‚Ä¢ Faithfulness of Confidence Under Distribution Shift: Test whether confidence remains calibrated and actionable under shifts/adversarial prompts; develop robust calibration for decision use.<br>‚Ä¢ Human-in-the-Loop Triage with Budgeted Abstention: Jointly optimize model utility and human workload by routing low-confidence items given cost and throughput constraints.<br>‚Ä¢ Inferring the Stakes: Learning to Elicit or Estimate Penalties for Risk-Aware Decisions: Enable models to query or infer Œª when unspecified, then adapt policies accordingly.<br>‚Ä¢ Mechanistic Interpretability of Uncertainty-to-Action Pathways: Probe circuits that generate confidence and drive answering bias to design interventions that couple them.<br>‚Ä¢ Compute- and Tool-Aware Risk Management at Test Time: Allocate extra reasoning/tools when confidence is near œÑ(Œª) to cross the optimal decision boundary.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Structured Episodic Event Memory</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06411" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06411" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLMs lack stable long-term memory; finite context windows cause degradation in reasoning over extended interactions.<br>‚Ä¢ Flat, vector-based RAG leads to scattered retrieval and fails to capture structural dependencies needed for multi-hop and temporal reasoning.<br>‚Ä¢ Existing graph/summarization methods are rigid (schema-bound), entangle high-level summaries with fine-grained facts, and weakly encode episodic dimensions (time, causality, participants), resulting in incoherent narratives and poor provenance grounding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SEEM is a hierarchical memory framework combining an Episodic Memory Layer (structured Episodic Event Frames with semantic roles and provenance) and a Graph Memory Layer (schema-agnostic relational quadruples), integrated via hybrid retrieval with relational propagation and Reverse Provenance Expansion to reconstruct coherent, provenance-grounded contexts.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Schema Induction for SEEM: Learn dynamic, task-specific memory schemas that evolve with interaction streams to better capture emerging relations and roles.<br>‚Ä¢ Multimodal SEEM: Extend episodic event frames and graph memory to vision/audio channels to support cross-modal narrative coherence and grounding.<br>‚Ä¢ End-to-End Trainable Provenance-Aware Memory: Jointly optimize extraction, fusion, and RPE with supervision and reinforcement signals to reduce LLM-induced noise and improve consistency.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03666" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03666" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Modern information systems often involve different types of items, e.g., a text query, an image, a video clip, or an audio segment. This motivates omni-modal embedding models that map heterogeneous modalities into a shared space for direct comparison. However, most recent omni-modal embeddings still rely heavily on implicit alignment inherited from pretrained vision-language model (VLM) backbones. In practice, this causes three common issues: (i) similarity logits have modality-dependent sharpness, so scores are not on a consistent scale; (ii) in-batch negatives become less effective over time because mixed-modality batches create an imbalanced hardness distribution; as a result, many negatives quickly become trivial and contribute little gradient; and (iii) embeddings across modalities show mismatched first- and second-order statistics, which makes rankings less stable. To tackle these problems, we propose e5-omni, a lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. e5-omni combines three simple components: (1) modality-aware temperature calibration to align similarity scales, (2) a controllable negative curriculum with debiasing to focus on confusing negatives while reducing the impact of false negatives, and (3) batch whitening with covariance regularization to better match cross-modal geometry in the shared embedding space. Experiments on MMEB-V2 and AudioCaps show consistent gains over strong bi-modal and omni-modal baselines, and the same recipe also transfers well to other VLM backbones. We release our model checkpoint at https://huggingface.co/Haon-Chen/e5-omni-7B.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Modality-dependent similarity scale mismatch: scores from different modalities (text, image, audio, video) have inconsistent sharpness, making cross-modal comparisons unreliable; existing omni-modal fine-tuning with a single global temperature fails to calibrate heterogeneous modalities.<br>‚Ä¢ Negative hardness imbalance in mixed-modality batches: many in-batch negatives become trivial over time, while naive hard-negative mining introduces false-negative bias; uniform negative treatment in baselines yields weak late-stage gradients and unstable optimization.<br>‚Ä¢ Cross-modal geometric drift causing unstable rankings: embeddings exhibit mismatched first- and second-order statistics across modalities; standard contrastive training lacks explicit second-order alignment, making retrieval sensitive to small score changes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>e5-omni introduces a lightweight explicit alignment recipe for VLM-based embeddings that requires no architectural changes and calibrates similarity scales, negative selection, and geometry. It combines modality-aware temperature calibration, a quantile-scheduled masked hard-negative curriculum with debiased contrastive learning, and batch whitening with CORAL-style covariance regularization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Content-Adaptive Temperature Calibration for Omni-Modal Embeddings: Learn instance-level, uncertainty-aware temperatures to dynamically calibrate logits beyond fixed per-modality vectors.<br>‚Ä¢ Learned Negative Mining Policies for Mixed-Modality Contrastive Training: Optimize negative selection and debiasing via reinforcement/meta-learning to maximize informative gradients while mitigating false negatives.<br>‚Ä¢ Second-Order Geometry Alignment at Scale: Exploring Alternatives to Whitening and CORAL: Investigate and theorize advanced cross-modal geometry alignment (e.g., CCA, optimal transport, batch-norm variants) for large-scale omni-modal retrieval.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">"TODO: Fix the Mess Gemini Created": Towards Understanding GenAI-Induced Self-Admitted Technical Debt</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.07786" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.07786" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of empirical understanding of how self-admitted technical debt (SATD) manifests when developers use LLMs (e.g., ChatGPT, Copilot, Gemini) in real codebases.<br>‚Ä¢ Existing SATD taxonomies and detectors largely predate generative AI and do not capture AI-specific patterns or how developers attribute responsibility to AI in creating/mitigating debt.<br>‚Ä¢ Absence of a conceptual lens for uncertainty-driven debt arising from integrating AI-generated code without full comprehension or validation, hindering maintainability and accountability.<br>‚Ä¢ Need for data and evidence on whether AI assistance shifts the distribution of SATD types (e.g., toward requirement/test debt) to inform SDLC practices and tooling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Mine GitHub for Python/JavaScript comments that reference LLMs, filter by SATD markers (TODO/FIXME/HACK/XXX), then manually label 81 comments using a standard SATD taxonomy and inductively code AI‚Äôs role (Source/Catalyst/Mitigator/Neutral). Analyze distributions and introduce the GIST concept to characterize uncertainty-driven SATD linked to AI-generated code.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Toward Automatic Detection of GIST in the Wild: Build NLP models to detect AI-referencing SATD and classify AI roles across code, commits, PRs, and issues beyond keyword search.<br>‚Ä¢ Longitudinal Dynamics of AI-Related Technical Debt: Track the creation, evolution, and resolution of GIST over time and quantify impacts on defects, lead time, and maintenance cost.<br>‚Ä¢ Provenance- and Explainability-Aware Tooling for AI-Assisted Development: Integrate prompt/model provenance and rationale capture with verification gates to mitigate deferred QA and reduce GIST.<br>‚Ä¢ Nudging Against Automation Bias in AI Coding Assistants: Controlled experiments evaluating uncertainty warnings, confidence displays, and review prompts on SATD incidence and code quality.<br>‚Ä¢ Cross-Language and Industrial Validation of GIST: Replicate and extend the study to additional languages and proprietary codebases to assess generalizability and contextual factors.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ShowUI-Aloha: Human-Taught GUI Agent</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.07181" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.07181" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Graphical User Interfaces (GUIs) are central to human-computer interaction, yet automating complex GUI tasks remains a major challenge for autonomous agents, largely due to a lack of scalable, high-quality training data. While recordings of human demonstrations offer a rich data source, they are typically long, unstructured, and lack annotations, making them difficult for agents to learn from.To address this, we introduce ShowUI-Aloha, a comprehensive pipeline that transforms unstructured, in-the-wild human screen recordings from desktop environments into structured, actionable tasks. Our framework includes four key components: A recorder that captures screen video along with precise user interactions like mouse clicks, keystrokes, and scrolls. A learner that semantically interprets these raw interactions and the surrounding visual context, translating them into descriptive natural language captions. A planner that reads the parsed demonstrations, maintains task states, and dynamically formulates the next high-level action plan based on contextual reasoning. An executor that faithfully carries out these action plans at the OS level, performing precise clicks, drags, text inputs, and window operations with safety checks and real-time feedback. Together, these components provide a scalable solution for collecting and parsing real-world human data, demonstrating a viable path toward building general-purpose GUI agents that can learn effectively from simply observing humans.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Scarcity of scalable, high-quality training data for complex desktop GUI automation, unlike web/mobile where metadata and tools are abundant.<br>‚Ä¢ Human screen recordings are long, noisy, unstructured, and unannotated, making it hard to extract user intent, task boundaries, and actionable supervision.<br>‚Ä¢ Existing LLM/VLM agents trained in zero-shot or on curated labels struggle with software logic and robust planning, leading to context drift, unsupported actions, and stuck states.<br>‚Ä¢ Lack of standardized cross-platform tools to capture synchronized screen video and precise user interactions at scale.<br>‚Ä¢ Difficulty in grounding low-level inputs (pixels, coordinates) to high-level, semantically meaningful actions and expectations that generalize across UI drift and layout changes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ShowUI-Aloha proposes a record‚Äìparse‚Äìlearn pipeline that captures real desktop demonstrations (screen + input events), cleans and marks actions, and uses a VLM to convert them into semantic, four-field JSON traces, which a planner‚Äìactor‚Äìexecutor then leverages to generate trace- and prompt-guided plans and robustly execute tasks at the OS level.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Scaling Demonstration-Driven GUI Agents with Privacy-Preserving Collection: Build large, diverse, in-the-wild desktop corpora using privacy filters and anonymization to train unified vision‚Äìlanguage‚Äìaction models.<br>‚Ä¢ Learning Software Logic from Semantic Traces: Weakly-Supervised Program-State Modeling for GUI Agents: Infer hidden application states, preconditions, and postconditions from Observation‚ÄìThink‚ÄìAction‚ÄìExpectation traces to improve planning and recovery.<br>‚Ä¢ Interactive Human-in-the-Loop Coaching for Robust Desktop Agents: Integrate real-time human feedback, critique, and corrections to adapt plans online, reduce drift, and enable continual improvement across changing interfaces.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">Codified Foreshadowing-Payoff Text Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.07033" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.07033" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Foreshadowing and payoff are ubiquitous narrative devices through which authors introduce commitments early in a story and resolve them through concrete, observable outcomes. However, despite advances in story generation, large language models (LLMs) frequently fail to bridge these long-range narrative dependencies, often leaving "Chekhov's guns" unfired even when the necessary context is present. Existing evaluations largely overlook this structural failure, focusing on surface-level coherence rather than the logical fulfillment of narrative setups. In this paper, we introduce Codified Foreshadowing-Payoff Generation (CFPG), a novel framework that reframes narrative quality through the lens of payoff realization. Recognizing that LLMs struggle to intuitively grasp the "triggering mechanism" of a foreshadowed event, CFPG transforms narrative continuity into a set of executable causal predicates. By mining and encoding Foreshadow-Trigger-Payoff triples from the BookSum corpus, we provide structured supervision that ensures foreshadowed commitments are not only mentioned but also temporally and logically fulfilled. Experiments demonstrate that CFPG significantly outperforms standard prompting baselines in payoff accuracy and narrative alignment. Our findings suggest that explicitly codifying narrative mechanics is essential for moving LLMs from surface-level fluency to genuine narrative competence.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Large language models often fail to realize long-range foreshadowed commitments, leaving narrative setups unpaid-off ("Chekhov‚Äôs guns" unfired).<br>‚Ä¢ Existing story-generation evaluations focus on local coherence and style, lacking causal verification of whether setups are logically and temporally fulfilled.<br>‚Ä¢ Prior controllable generation and planning methods model static attributes but do not codify or enforce discrete foreshadow‚Äìtrigger‚Äìpayoff events, leading to premature or missing payoffs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>CFPG codifies foreshadow‚Äìtrigger‚Äìpayoff triples and maintains a dynamic foreshadow pool, selecting commitments whose triggers are satisfied, conditioning the LM to realize their payoffs, and updating the pool by resolving satisfied commitments and mining new ones. A BookSum-derived dataset of sentence-anchored foreshadow‚Äìpayoff pairs is built via GPT-based candidate mining, symbolic verification, and rubric filtering to provide structured supervision and evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning End-to-End Trigger Induction for CFPG: Train models to infer F‚ÄìT‚ÄìP triples and triggering conditions from raw narratives using differentiable logic constraints and reinforcement learning.<br>‚Ä¢ Causal Narrative Benchmarks: Measuring Payoff Realization at Scale: Construct long-form datasets and metrics that track commitments and payoffs over extended contexts to evaluate narrative competence beyond surface coherence.<br>‚Ä¢ Multimodal CFPG: Codifying Foreshadow‚ÄìPayoff Across Text, Audio, and Visual Storytelling: Extend the codified framework to scripts, comics, and films by integrating visual and acoustic cues into trigger detection and payoff enforcement.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Sci-Reasoning: A Dataset Decoding AI Innovation Patterns</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.04577" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.04577" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While AI innovation accelerates rapidly, the intellectual process behind breakthroughs -- how researchers identify gaps, synthesize prior work, and generate insights -- remains poorly understood. The lack of structured data on scientific reasoning hinders systematic analysis and development of AI research agents. We introduce Sci-Reasoning, the first dataset capturing the intellectual synthesis behind high-quality AI research. Using community-validated quality signals and an LLM-accelerated, human-verified pipeline, we trace Oral and Spotlight papers across NeurIPS, ICML, and ICLR (2023-2025) to its key predecessors, articulating specific reasoning links in a structured format. Our analysis identifies 15 distinct thinking patterns, with three dominant strategies accounting for 52.7%: Gap-Driven Reframing (24.2%), Cross-Domain Synthesis (18.0%), and Representation Shift (10.5%). The most powerful innovation recipes combine multiple patterns: Gap-Driven Reframing + Representation Shift, Cross-Domain Synthesis + Representation Shift, and Gap-Driven Reframing + Cross-Domain Synthesis. This dataset enables quantitative studies of scientific progress and provides structured reasoning trajectories for training the next generation AI research agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of structured, large-scale data capturing how AI researchers synthesize prior work into breakthroughs; current understanding is anecdotal, hindering systematic analysis and automation.<br>‚Ä¢ Citation networks and prior lineage methods capture only that a citation exists‚Äîor a single progenitor‚Äînot the roles, reasoning links, or multi-source synthesis that actually drive innovation.<br>‚Ä¢ Absence of training/evaluation data for AI research agents to learn expert reasoning patterns; need community-validated, contemporaneous signals to reliably identify high-quality research.<br>‚Ä¢ Existing datasets (e.g., peer reviews, QA, source tracing) do not model the intellectual synthesis process or multi-paper reasoning trajectories required to study and emulate innovation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Select high-quality papers via Oral/Spotlight status at NeurIPS/ICML/ICLR (2023‚Äì2025), then use an LLM-accelerated, human-verified pipeline to extract key predecessors and generate structured ‚ÄúLineage Links‚Äù encoding roles, relationship types, and natural-language reasoning. Release a 3,819-paper dataset with annotated thinking patterns and validate it through ground-truth tests, multi-model cross-validation, and model ablations for cost‚Äìquality trade-offs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Innovation Recipes: Training Research Agents on Structured Lineage Links: Fine-tune and evaluate LLM agents to imitate identified thinking patterns and their combinations to generate novel, valid research ideas from predecessor sets.<br>‚Ä¢ Forecasting Breakthroughs from Predecessor Graphs: Model temporal lineage structures to predict likely next-step contributions and high-impact (Oral/Spotlight) outcomes given existing intellectual predecessors.<br>‚Ä¢ Beyond AI: Cross-Disciplinary Sci-Reasoning for Comparative Innovation Science: Extend the pipeline to other fields (e.g., biology, physics) to compare pattern distributions and assess transferability of ‚Äúinnovation recipes.‚Äù</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06993" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06993" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multi-modal large language models (MLLMs) exhibit strong general-purpose capabilities, yet still struggle on Fine-Grained Visual Classification (FGVC), a core perception task that requires subtle visual discrimination and is crucial for many real-world applications. A widely adopted strategy for boosting performance on challenging tasks such as math and coding is Chain-of-Thought (CoT) reasoning. However, several prior works have reported that CoT can actually harm performance on visual perception tasks. These studies, though, examine the issue from relatively narrow angles and leave open why CoT degrades perception-heavy performance. We systematically re-examine the role of CoT in FGVC through the lenses of zero-shot evaluation and multiple training paradigms. Across these settings, we uncover a central paradox: the degradation induced by CoT is largely driven by the reasoning length, in which longer textual reasoning consistently lowers classification accuracy. We term this phenomenon the ``Cost of Thinking''. Building on this finding, we make two key contributions: (1) \alg, a simple and general plug-and-play normalization method for multi-reward optimization that balances heterogeneous reward signals, and (2) ReFine-RFT, a framework that combines ensemble rewards with \alg to constrain reasoning length while providing dense accuracy-oriented feedback. Extensive experiments demonstrate the effectiveness of our findings and the proposed ReFine-RFT, achieving state-of-the-art performance across FGVC benchmarks. Code and models are available at https://github.com/jiezhu23/ReFine-RFT{Project Link}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ MLLMs underperform on Fine-grained Visual Classification (FGVC), which demands subtle visual discrimination crucial for real-world applications.<br>‚Ä¢ Chain-of-Thought (CoT) prompting, effective in math/coding, paradoxically degrades FGVC accuracy; its causes and scope are insufficiently explored.<br>‚Ä¢ Existing RFT pipelines rely on sparse, binary rewards and naively aggregate heterogeneous signals, causing instability and unintended "reasoning collapse" without explicit control.<br>‚Ä¢ There is a need to quantify how reasoning length affects perception and to design training that balances multi-reward signals while constraining excessive reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ReFine-RFT combines an ensemble, accuracy-centric reward (format, classification, thinking-length, MLLM-based grading, embedding similarity) with Multi-reward Normalization (MRN), which normalizes each reward independently within GRPO before aggregation. This constrains reasoning length and stabilizes optimization, improving FGVC accuracy with concise, decision-focused outputs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond the Cost of Thinking: Mechanistic Analyses of Why Lengthy Textual Reasoning Hurts Visual Perception: Disentangle attention dilution, modality interference, and hallucination to causally explain performance drops.<br>‚Ä¢ Adaptive CoT Gating for Multimodal Tasks: Learn a policy that dynamically decides whether and how much to think based on uncertainty, visual ambiguity, and reward signals.<br>‚Ä¢ Length-Aware Reinforcement Fine-Tuning for Medical Imaging and Industrial Inspection: Extend ReFine-RFT with domain-specific semantic rewards and strict length control to high-stakes perception tasks.<br>‚Ä¢ Reward Model Debiasing for Multimodal RLHF/RFT: Calibrate and ensemble reward models to mitigate length, lexical, and format biases while preserving semantic grading fidelity.<br>‚Ä¢ Visual-Grounded Reasoning: Coupling Region Attribution with CoT to Reduce Hallucinations: Integrate region-level grounding (boxes/segments) and evidence-alignment rewards to anchor reasoning to image content.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06944" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06944" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual understanding, they often struggle when faced with the unstructured and ambiguous nature of human-generated sketches. This limitation is particularly pronounced in the underexplored task of visual grading, where models should not only solve a problem but also diagnose errors in hand-drawn diagrams. Such diagnostic capabilities depend on complex structural, semantic, and metacognitive reasoning. To bridge this gap, we introduce SketchJudge, a novel benchmark tailored for evaluating MLLMs as graders of hand-drawn STEM diagrams. SketchJudge encompasses 1,015 hand-drawn student responses across four domains: geometry, physics, charts, and flowcharts, featuring diverse stylistic variations and distinct error types. Evaluations on SketchJudge demonstrate that even advanced MLLMs lag significantly behind humans, validating the benchmark's effectiveness in exposing the fragility of current vision-language alignment in symbolic and noisy contexts. All data, code, and evaluation scripts are publicly available at https://github.com/yuhangsu82/SketchJudge.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ MLLMs struggle with unstructured, ambiguous, and stylistically diverse hand-drawn STEM diagrams, while most benchmarks assume clean, standardized visuals.<br>‚Ä¢ Existing evaluations focus on the "solver" paradigm and rarely assess models as graders that must diagnose partial errors with structural, semantic, and metacognitive reasoning.<br>‚Ä¢ There is a lack of realistic educational datasets of student-style sketches with fine-grained error annotations, impeding progress in robust vision-language alignment and grading-oriented capabilities.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce SketchJudge, a diagnostic benchmark of 1,015 hand-drawn student responses to 300 questions across geometry, physics, charts, and flowcharts, with binary correctness labels and 5‚Äì7 domain-specific error categories per domain to enable model-as-grader evaluation. Provide data, code, and evaluation protocols, and conduct systematic experiments showing state-of-the-art MLLMs lag behind humans on diagram-level reasoning and error diagnosis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Structure-Aware Multimodal Graders for Hand-drawn STEM Diagrams: Combine graph-based sketch parsing with symbolic reasoning modules to improve spatial/logical understanding and fine-grained error diagnosis.<br>‚Ä¢ Metacognitive Calibration for MLLMs in Noisy Diagram Grading: Develop uncertainty estimation and self-critique mechanisms tailored to ambiguous sketches to enhance reliability and fairness in grading.<br>‚Ä¢ Learning to Tutor from SketchJudge: Generating Targeted Feedback for Student Diagrams: Train models to produce actionable, taxonomy-aligned feedback that bridges grading with pedagogical guidance.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Artificial Entanglement in the Fine-Tuning of Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06788" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06788" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) can be adapted to new tasks using parameter-efficient fine-tuning (PEFT) methods that modify only a small number of trainable parameters, often through low-rank updates. In this work, we adopt a quantum-information-inspired perspective to understand their effectiveness. From this perspective, low-rank parameterizations naturally correspond to low-dimensional Matrix Product States (MPS) representations, which enable entanglement-based characterizations of parameter structure. Thereby, we term and measure "Artificial Entanglement", defined as the entanglement entropy of the parameters in artificial neural networks (in particular the LLMs). We first study the representative low-rank adaptation (LoRA) PEFT method, alongside full fine-tuning (FFT), using LLaMA models at the 1B and 8B scales trained on the Tulu3 and OpenThoughts3 datasets, and uncover: (i) Internal artificial entanglement in the updates of query and value projection matrices in LoRA follows a volume law with a central suppression (termed as the "Entanglement Valley"), which is sensitive to hyper-parameters and is distinct from that in FFT; (ii) External artificial entanglement in attention matrices, corresponding to token-token correlations in representation space, follows an area law with logarithmic corrections and remains robust to LoRA hyper-parameters and training steps. Drawing a parallel to the No-Hair Theorem in black hole physics, we propose that although LoRA and FFT induce distinct internal entanglement signatures, such differences do not manifest in the attention outputs, suggesting a "no-hair" property that results in the effectiveness of low rank updates. We further provide theoretical support based on random matrix theory, and extend our analysis to an MPS Adaptation PEFT method, which exhibits qualitatively similar behaviors.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of principled tools to quantify and compare the internal correlation structures induced by full fine-tuning (FFT) versus low-rank PEFT (e.g., LoRA), beyond aggregate performance metrics.<br>‚Ä¢ Unclear whether complex internal parameter correlations propagate to attention-level representations; explaining LoRA‚Äôs effectiveness despite restricted updates is important for reliable and efficient adaptation.<br>‚Ä¢ Absence of theoretical scaling laws for attention matrix entropy with sequence length and limited understanding of robustness to hyperparameters; prior PEFT analyses largely ignore internal/external correlation structure and hyperparameter sensitivity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>An artificial entanglement framework that reshapes parameter updates and attention matrices into tensors, factorizes them as Matrix Product States, and computes bond entanglement entropies to profile internal (parameters) versus external (attention) correlations. A random-matrix ‚ÄòAttention Cardy Formula‚Äô explains area-law scaling with logarithmic corrections; results are validated on LLaMA with LoRA and an MPS Adaptation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Entanglement-Guided PEFT: Adaptive Rank and Scaling Selection for LoRA: Use artificial entanglement profiles to dynamically choose layer-wise rank and scaling, targeting performance, stability, and robustness.<br>‚Ä¢ Provable No-Hair Coarse-Graining in Transformer Attention: Establish theoretical conditions under which attention outputs remain invariant to internal entanglement differences, extending random matrix analysis to trained regimes.<br>‚Ä¢ Artificial Entanglement Regularization for Stable Fine-Tuning: Introduce loss terms or constraints to shape internal entanglement (e.g., controlling the Entanglement Valley) to improve generalization and reduce overfitting.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">FinForge: Semi-Synthetic Financial Benchmark Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06747" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06747" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Evaluating Language Models (LMs) in specialized, high-stakes domains such as finance remains a significant challenge due to the scarcity of open, high-quality, and domain-specific datasets. Existing general-purpose benchmarks provide broad coverage but lack the depth and domain fidelity needed to assess LMs' capabilities for real-world financial reasoning, which requires both conceptual understanding and quantitative rigor. To address this gap, we introduce FinForge, a scalable, semi-synthetic pipeline for constructing finance-specific evaluation benchmarks through a hybrid of expert-guided data curation and controlled LM-based synthesis. FinForge combines manual and programmatic corpus construction from authoritative financial sources with structured question generation and validation using Gemini 2.5 Flash. To demonstrate the pipeline's efficacy, we produce FinForge-5k, a snapshot benchmark comprising over 5,000 human-validated question-answer pairs across 11 finance subdomains, derived from a curated corpus of 100,000 verified documents totaling 143M tokens. Evaluation of state-of-the-art open-source and closed-source models on FinForge-5k reveals significant differences in financial reasoning, with leading models achieving accuracy levels near 80%. These findings underscore the framework's utility for diagnosing current model limitations and guiding future improvements in financial domain competence. All code and data are available at https://github.com/gtfintechlab/FinForge.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Reliable evaluation of LMs in finance is hindered by a lack of open, high-quality, domain-specific datasets; general benchmarks lack the depth and domain fidelity needed for real-world financial reasoning.<br>‚Ä¢ Static test suites are prone to training-data contamination and rapidly go stale in a dynamic domain, inflating scores via memorization and failing to assess up-to-date knowledge.<br>‚Ä¢ Existing finance QA sets are narrow and small (often focused on numeric/table reasoning), missing broad conceptual, counterfactual, and multi-constraint tasks needed to diagnose both conceptual and quantitative reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>FinForge is a two-stage, semi-synthetic pipeline that (1) curates an authoritative finance corpus across 11 subdomains via a hybrid manual‚Äìprogrammatic process, and (2) uses an LM-driven five-stage workflow (document analysis, answer planning, self-contained MCQ synthesis with distractors, labeling, and rubric-based LM-as-judge validation) to produce high-quality, contamination-resistant benchmarks. A snapshot, FinForge-5k, yields 5,000 validated questions generated with Gemini 2.5 Flash from a 143M-token corpus.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Temporal-FinForge: Time-Aware Dynamic Financial Benchmarking for Recency and Temporal Reasoning ‚Äî Integrate time-stamped sources and rolling updates to evaluate models on freshness, regime changes, and time-sensitive causal reasoning.<br>‚Ä¢ JudgeBench: Transparent and Calibrated LM-as-Judge for Financial Reasoning ‚Äî Develop auditable validators with explicit rubrics, rationales, and calibration to align with expert judgments and reduce over-approval in complex finance items.<br>‚Ä¢ Tool-Augmented FinForge: Separating Conceptual and Arithmetic Failures in Quantitative Finance QA ‚Äî Embed calculator/code tools and program annotations to evaluate tool use, isolate arithmetic vs. conceptual errors, and improve quantitative rigor.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-11">

    <div class="paper">
        <h2 class="paper-title">Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06463" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06463" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Designing a unified neural network to efficiently and inherently process sequential data with arbitrary lengths is a central and challenging problem in sequence modeling. The design choices in Transformer, including quadratic complexity and weak length extrapolation, have limited their ability to scale to long sequences. In this work, we propose Gecko, a neural architecture that inherits the design of Mega and Megalodon (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability to capture long range dependencies, including timestep decay normalization, sliding chunk attention mechanism, and adaptive working memory. In a controlled pretraining comparison with Llama2 and Megalodon in the scale of 7 billion parameters and 2 trillion training tokens, Gecko achieves better efficiency and long-context scalability. Gecko reaches a training loss of 1.68, significantly outperforming Llama2-7B (1.75) and Megalodon-7B (1.70), and landing close to Llama2-13B (1.67). Notably, without relying on any context-extension techniques, Gecko exhibits inherent long-context processing and retrieval capabilities, stably handling sequences of up to 4 million tokens and retrieving information from contexts up to 4times longer than its attention window. Code: https://github.com/XuezheMax/gecko-llm</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Transformers have quadratic attention cost and weak length extrapolation, making long-sequence processing inefficient and unreliable.<br>‚Ä¢ Existing alternatives (sparse attention, state-space models, linear Transformers) underperform on in-context retrieval and practical long-context tasks.<br>‚Ä¢ Megalodon's timestep normalization diminishes the influence of current statistics as sequence length grows, limiting inherent scalability.<br>‚Ä¢ Chunk-wise normalized gated attention incurs boundary losses and fails to capture information beyond chunk windows.<br>‚Ä¢ There is a need for a unified architecture that inherently handles arbitrary-length sequences with efficient training, inference, and distributed scalability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Gecko builds on EMA-based gated attention (Mega/Megalodon) and introduces timestep decay normalization, sliding chunk attention (attending to current and previous chunks), and an adaptive working memory via linear attention with position-aware online softmax, enabling efficient, stable, and inherently long-context processing and retrieval.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Scaling Gecko's Adaptive Memory: Capacity Scheduling and Collision Mitigation for Million-Token Contexts: Develop dynamic memory sizing and collision-avoidance strategies to preserve information fidelity over extremely long sequences.<br>‚Ä¢ Gecko-Multimodal: Extending Sliding Chunk Attention and Adaptive Memory to Video, Audio, and Code Sequences: Adapt Gecko‚Äôs components to heterogeneous modalities and evaluate long-range cross-modal reasoning.<br>‚Ä¢ Theory and Optimization of Timestep Decay Normalization: Convergence, Bias Correction, and Length Extrapolation Guarantees: Provide formal analysis of decay parameters, bias correction, and their impact on stability and generalization to arbitrary lengths.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06423" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06423" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Self-consistency has emerged as a popular technique for improving large language model accuracy on reasoning tasks. The approach is straightforward: generate multiple reasoning paths and select the most common answer through majority voting. While this reliably boosts accuracy, it remains unclear whether these gains reflect genuine improvements in reasoning quality. We investigate a fundamental question that has not been studied before: does inference scaling improve reasoning faithfulness? We conduct a comprehensive empirical study across four frontier models (GPT-5.2, Claude Opus 4.5, Gemini-3-flash-preview, and DeepSeek-v3.2) on 100 GSM8K mathematical reasoning problems. Our analysis employs bootstrap confidence intervals, McNemar's tests for paired comparisons, and Cohen's d effect sizes to quantify the effects rigorously. The results reveal striking differences across models that challenge common assumptions about self-consistency. GPT-5.2 shows the expected pattern: accuracy improves from 78% to 90% at N=5, with faithfulness remaining relatively stable (0.540 to 0.510). Claude Opus 4.5 tells a completely different story. Its accuracy actually drops from 78% to 74.3% while faithfulness jumps dramatically from 0.270 to 0.891 at N=5. DeepSeek-v3.2, already at 98% accuracy, shows ceiling effects with modest faithfulness gains (0.440 to 0.541). Gemini-3-flash improves from 81% to 86% accuracy with a slight faithfulness decrease (0.260 to 0.212). Problem difficulty analysis reveals that GPT-5.2 solves 82% of hard problems while breaking only 13% of easy ones. Claude, in contrast, breaks 23% of easy problems, explaining its accuracy decrease. These findings matter for practitioners: self-consistency is not universally beneficial, and teams should test their specific models before deployment. We release our code and provide practical recommendations for navigating these tradeoffs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Accuracy gains from self-consistency (inference scaling) are well-documented, but it is unknown whether these gains reflect more faithful reasoning rather than better answer aggregation.<br>‚Ä¢ Existing inference scaling work focuses on accuracy; faithfulness studies largely examine single-sample CoT, leaving a gap on how aggregation affects faithfulness.<br>‚Ä¢ Unfaithful reasoning undermines interpretability, oversight, and safety (models may be right for the wrong reasons); practitioners lack quantitative guidance on accuracy‚Äìfaithfulness tradeoffs.<br>‚Ä¢ Assumptions of universal benefit from self-consistency ignore model-dependent behavior; rigorous, cross-model analysis is needed.<br>‚Ä¢ Aggregate accuracy can hide dynamics (solving hard vs. breaking easy problems); difficulty-aware evaluation is missing.<br>‚Ä¢ Deployment decisions need cost-efficiency insights (when and how much to scale) and practical recommendations grounded in faithfulness metrics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Conduct a multi-model empirical study on 100 GSM8K problems, sampling N‚àà{1,5,20} chain-of-thought paths (T=0.7), majority-voting answers, and measuring per-path faithfulness via the early-answering probe (T=0). Analyze accuracy and faithfulness using bootstrap CIs, McNemar‚Äôs and paired t-tests, Cohen‚Äôs d, difficulty stratification (easy vs. hard), and cost-efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond Early Answering: A Multi-Probe Evaluation of Reasoning Faithfulness under Inference Scaling: Combine perturbation, paraphrase, filler-token, and causal probes to triangulate faithfulness changes with N.<br>‚Ä¢ Adaptive Self-Consistency: Difficulty- and Uncertainty-Aware Inference Scaling: Learn policies that choose N per problem to jointly optimize accuracy, faithfulness, and cost.<br>‚Ä¢ Mechanistic Origins of Overthinking in LLMs under Self-Consistency: A Causal Analysis: Use interpretability tools to explain Claude-like accuracy drops with faithfulness gains.<br>‚Ä¢ Faithfulness Scaling Laws: Joint Effects of Model Size and Inference Compute on Reasoning Faithfulness: Map how faithfulness varies across model families/sizes and N.<br>‚Ä¢ Cross-Domain Faithfulness under Inference Scaling: From Math to Commonsense, Science, and Law: Evaluate whether model-dependent tradeoffs generalize beyond GSM8K.<br>‚Ä¢ Cost-Aware Deployment of Self-Consistency: Optimizing Compute for Accuracy‚ÄìFaithfulness Tradeoffs: Derive practical guidelines and budget-aware strategies for production systems.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">FlyPose: Towards Robust Human Pose Estimation From Aerial Views</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.05747" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.05747" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasibile models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ~20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Robust, real-time human pose estimation from aerial viewpoints is hard due to steep top-down angles, foreshortening, self-occlusion, and low-resolution humans at safe UAV altitudes.<br>‚Ä¢ UAVs impose strict onboard compute, weight, and power constraints, making many accurate ground-level methods impractical for real-time deployment.<br>‚Ä¢ Existing person detectors and pose estimators trained on frontal/ground views generalize poorly to aerial imagery, often missing or mislocalizing keypoints for small-scale persons.<br>‚Ä¢ Tiny-object detection typically relies on complex two-stage architectures that hurt latency; state-of-the-art one-stage detectors still underperform on aerial datasets like VisDrone.<br>‚Ä¢ Scarcity and fragmentation of aerial pose datasets (RGB and thermal) limit training and benchmarking, further degrading performance in diverse real-world UAV scenarios.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>FlyPose is a lightweight top-down pipeline combining an RT-DETRv2-S person detector (ResNet-18, multi-dataset RGB+thermal training with optional NWDL) and a ViTPose-S heatmap-based pose head finetuned on UAV-Human with resolution-aware downscaling augmentation. Optimized to TensorRT on Jetson Orin, it achieves ~20 ms end-to-end latency and significant accuracy gains (+6.8 mAP in detection across aerial test sets; +16.3 mAP in pose on UAV-Human).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Motion-Aware Aerial Person Detection for Tiny Scales: Integrate lightweight temporal models and motion cues to improve small, distant person detection without sacrificing real-time performance.<br>‚Ä¢ Multi-Modal RGB‚ÄìThermal Fusion for Robust Aerial Pose Estimation: Design modality-specific encoders with shared pose heads to fuse RGB and thermal inputs, enhancing nighttime and occlusion-robust keypoint prediction.<br>‚Ä¢ Aerial PoseNet: Leveraging 3D Priors and Altitude Metadata for Top-Down 2D/3D Human Pose: Use camera altitude/orientation priors and learned 3D human models with synthetic-to-real adaptation to constrain and refine 2D heatmaps and infer 3D pose from single UAV views.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-12">

    <div class="paper">
        <h2 class="paper-title">Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.07790" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.07790" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>System logs are crucial for monitoring and diagnosing modern computing infrastructure, but their scale and complexity require reliable and efficient automated interpretation. Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value, revealing little about its underlying ability to interpret system logs. We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task. Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs) under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting. The results reveal strong stratification. Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval. In contrast, several SRLMs, including Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B, degrade substantially when paired with RAG. Efficiency measurements further separate models: most Gemma and Llama variants complete inference in under 1.2 seconds per log, whereas Phi-4-Mini-Reasoning exceeds 228 seconds per log while achieving <10% accuracy. These findings suggest that (1) architectural design, (2) training objectives, and (3) the ability to integrate retrieved context under strict output constraints jointly determine performance. By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability, with implications for root cause analysis (RCA) and broader DT integration.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Real-world gap: absence of a benchmark on operational Linux journalctl logs; prior work focuses on code-embedded logs, security/telecom datasets, or preprocessed HPC corpora that miss real runtime noise and variability.<br>‚Ä¢ Deployability need: lack of evaluation of small, locally deployable models under real-time latency constraints required by digital twin (DT) monitoring pipelines.<br>‚Ä¢ Retrieval uncertainty: limited understanding of how RAG affects small (reasoning) models in constrained classification, including cases of retrieval-induced degradation.<br>‚Ä¢ Limits of ML/DL baselines: traditional and deep models often require known event sets, struggle with unseen patterns and temporal dependencies, and demand heavy tuning, limiting practicality across evolving systems.<br>‚Ä¢ Label noisiness: Syslog severity is weakly standardized and noisy; using it as a probing task requires methods robust to ambiguity while enforcing strict output control.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Construct a semi-balanced benchmark of 46,774 real journalctl logs and evaluate nine SLMs/SRLMs on severity classification under zero-shot, few-shot, and FAISS-based RAG (nomic-embed-text-v1.5, k=5), enforcing a single-digit (0‚Äì7) output. Measure accuracy and per-log latency, analyze retrieval depth effects and architectural/training factors behind performance and RAG sensitivity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Retrieval-Aware Distillation for Small Log-Analysis Models: Transfer non-thinking modes and retrieval integration skills from large teachers to SLMs/SRLMs to improve RAG robustness under strict output constraints.<br>‚Ä¢ Noise-Tolerant RAG for System Logs via Hybrid Retrieval and Reranking: Combine dense+lexical retrievers with learned rerankers and adaptive k to mitigate retrieval errors and stabilize performance across models.<br>‚Ä¢ Latency-Constrained Co-Design for DT Pipelines with SLMs: Jointly optimize quantization, batching, constrained decoding, and GPU scheduling to meet sub-second per-log latency without sacrificing accuracy.<br>‚Ä¢ Beyond Severity: A Multi-Task Probe Suite for Log Comprehension and RCA: Introduce tasks for causal linkage, component attribution, and temporal anomaly chains to better reflect operational reasoning.<br>‚Ä¢ Probabilistic Ground Truth for Syslog Labels using Weak Supervision: Build denoised, confidence-weighted labels from heuristics and historical patterns to counteract inconsistent administrator-defined severities.<br>‚Ä¢ Embedding and Index Choices for Log RAG: An Ablation across Encoders and FAISS Configurations: Systematically study domain-tuned embeddings, hybrid retrieval, and index hyperparameters on accuracy‚Äìlatency trade-offs.<br>‚Ä¢ Output-Control Mechanisms for Reasoning Models under Constraints: Evaluate constrained decoding, logit biasing, and instruction shaping to suppress verbose CoT and enforce single-token answers reliably.<br>‚Ä¢ Cross-System and Temporal Generalization in Operational Logs: Assess transfer, continual learning, and drift adaptation across hosts, services, and software versions for durable real-world deployment.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.07239" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.07239" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Deterministic inference is a comforting ideal in classical software: the same program on the same input should always produce the same output. As large language models move into real-world deployment, this ideal has been imported wholesale into inference stacks. Recent work from the Thinking Machines Lab has presented a detailed analysis of nondeterminism in LLM inference, showing how batch-invariant kernels and deterministic attention can enforce bitwise-identical outputs, positioning deterministic inference as a prerequisite for reproducibility and enterprise reliability. In this paper, we take the opposite stance. We argue that, for LLMs, deterministic inference kills. It kills the ability to model uncertainty, suppresses emergent abilities, collapses reasoning into a single brittle path, and weakens safety alignment by hiding tail risks. LLMs implement conditional distributions over outputs, not fixed functions. Collapsing these distributions to a single canonical completion may appear reassuring, but it systematically conceals properties central to artificial cognition. We instead advocate Stochastic CHAOS, treating distributional variability as a signal to be measured and controlled. Empirically, we show that deterministic inference is systematically misleading. Single-sample deterministic evaluation underestimates both capability and fragility, masking failure probability under paraphrases and noise. Phase-like transitions associated with emergent abilities disappear under greedy decoding. Multi-path reasoning degrades when forced onto deterministic backbones, reducing accuracy and diagnostic insight. Finally, deterministic evaluation underestimates safety risk by hiding rare but dangerous behaviors that appear only under multi-sample evaluation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Deterministic inference collapses LLMs‚Äô conditional distributions to single outputs, misrepresenting uncertainty and competence.<br>‚Ä¢ Single-sample, greedy decoding systematically underestimates capability and overestimates robustness, masking failures under paraphrases, reorderings, and noise.<br>‚Ä¢ Emergent abilities that rely on exploration (self-consistency, tree-of-thought) vanish under deterministic decoding, and multiple valid reasoning paths collapse to brittle traces.<br>‚Ä¢ Deterministic safety evaluation underestimates tail risks; low-probability harmful generations surface only under multi-sample, multi-perturbation testing, while determinism makes exploits reliably reproducible.<br>‚Ä¢ Existing pushes for bitwise determinism (batch-invariant kernels, deterministic attention) optimize for clean traces over faithful characterization of pŒ∏(y|x), and incur performance/complexity costs.<br>‚Ä¢ System-level nondeterminism (floating-point non-associativity, dynamic batching, kernel selection, hardware/software drift) persists even at T=0, yet current practice lacks a clear taxonomy separating bitwise, distributional, and semantic stability.<br>‚Ä¢ GLUE-style single-score, single-output evaluation encourages benchmark memorization; sequence-level determinism risks repeating this brittleness at the generative level.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes Stochastic CHAOS: prioritize distributional reproducibility and semantic stability over bitwise determinism via a stability taxonomy (bitwise determinism, distributional reproducibility, semantic stability) and a distributional evaluation protocol using multi-sample stochastic decoding, majority-vote label extraction, and robustness ratios across paraphrased/perturbed/adversarial variants to stress-test deterministic inference.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Distributional Reproducibility Benchmarks for LLMs: Standardize multi-sample protocols, metrics, and leaderboards that report mean/variance, calibration, and robustness across seeds, hardware, and batching.<br>‚Ä¢ Stochastic Safety Auditing for Tail Risk in LLMs: Develop frameworks that estimate and mitigate low-probability harmful behaviors via multi-sample, multi-perturbation evaluation and probabilistic risk models.<br>‚Ä¢ Adaptive Multi-Path Decoding Controllers for Robust Reasoning: Design algorithms that dynamically allocate sampling and aggregate diverse reasoning trajectories (self-consistency/ToT) to maximize accuracy and semantic stability under constraints.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06966" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06966" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As Large Language Models (LLMs) evolve from static dialogue interfaces to autonomous general agents, effective memory is paramount to ensuring long-term consistency. However, existing benchmarks primarily focus on casual conversation or task-oriented dialogue, failing to capture **"long-term project-oriented"** interactions where agents must track evolving goals. To bridge this gap, we introduce **RealMem**, the first benchmark grounded in realistic project scenarios. RealMem comprises over 2,000 cross-session dialogues across eleven scenarios, utilizing natural user queries for evaluation. We propose a synthesis pipeline that integrates Project Foundation Construction, Multi-Agent Dialogue Generation, and Memory and Schedule Management to simulate the dynamic evolution of memory. Experiments reveal that current memory systems face significant challenges in managing the long-term project states and dynamic context dependencies inherent in real-world projects. Our code and datasets are available at [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench).</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing benchmarks focus on casual or task-oriented dialogues and isolated fact retrieval, failing to capture long-term, project-oriented interactions with evolving goals and interleaved sessions.<br>‚Ä¢ Real-world memory-driven interaction requires endogenous queries, dynamic state evolution, and proactive contextual alignment (including schedule-aware reasoning), which current evaluations and systems do not adequately test.<br>‚Ä¢ Current memory systems struggle to maintain coherent project threads, track fine-grained state updates, and manage complex temporal dependencies, resulting in large performance gaps from oracle retrieval and limited generalization in realistic scenarios.<br>‚Ä¢ Prior datasets rely on external QA after sessions and lack project state memory and proactive alignment, preventing assessment of continuous, natural dialogue progression.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RealMem is a benchmark built via a three-stage synthesis pipeline‚ÄîProject Foundation Construction, Multi-Agent Dialogue Generation, and Memory & Schedule Management‚Äîto produce 2,000+ cross-session, interleaved project dialogues with dynamic memory points and schedules, evaluated with both classical (Recall/NDCG) and LLM-based metrics (Mem Recall, Mem Helpful, QA Score) across four query types (temporal reasoning, static retrieval, dynamic updating, proactive alignment).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Proactive Alignment as Decision Policy: Learning Schedule-Aware Intent Inference for Long-Term Project Agents: Train agents (e.g., via RL or supervised policies) to proactively resolve ambiguous intents and schedule conflicts using RealMem‚Äôs natural queries.<br>‚Ä¢ State Graphs Over Sessions: Adaptive Graph Memory for Tracking Evolving Project States: Build and update project state graphs over interleaved sessions to improve retrieval ranking and downstream QA consistency in dynamic contexts.<br>‚Ä¢ From Oracle to Practice: Training Memory Retrievers with Temporal Consistency Objectives: Develop retrievers optimized with temporal and state-consistency losses to close the gap to oracle recall while preserving useful context for generation.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-13">

    <div class="paper">
        <h2 class="paper-title">3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06496" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06496" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming a foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it remains challenging due to the sparsity and irregularity of point clouds and, more critically, the weak grounding and limited out-of-distribution (OOD) generalization of existing captioners across drastically different environments, including indoor and outdoor 3D scenes. To address this challenge, we propose 3D CoCa v2, a generalizable 3D captioning framework that unifies contrastive vision-language learning with 3D caption generation and further improves robustness via test-time search (TTS) without updating the captioner parameters. 3D CoCa v2 builds on a frozen CLIP-based semantic prior, a spatially-aware 3D scene encoder for geometry, and a multimodal decoder jointly optimized with contrastive and captioning objectives, avoiding external detectors or handcrafted proposals. At inference, TTS produces diverse caption candidates and performs reward-guided selection using a compact scene summary. Experiments show improvements over 3D CoCa of +1.50 CIDEr@0.5IoU on ScanRefer and +1.61 CIDEr@0.5IoU on Nr3D, and +3.8 CIDEr@0.25 in zero-shot OOD evaluation on TOD3Cap. Code will be released at https://github.com/AIGeeksGroup/3DCoCav2.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ 3D captioning is hard due to sparse, irregular point clouds and the need to reason about object attributes and spatial relations across diverse indoor/outdoor environments.<br>‚Ä¢ Existing captioners exhibit weak grounding and poor out-of-distribution (OOD) generalization, often hallucinating when deployed beyond the training domain.<br>‚Ä¢ Two-stage detect-then-describe pipelines suffer from proposal noise and error propagation, making caption quality tightly coupled to detection accuracy.<br>‚Ä¢ Current methods lack strong, transferable 3D‚Äìlanguage alignment; standard single-shot decoding is brittle under domain shift and ignores alternative hypotheses.<br>‚Ä¢ There is no practical mechanism to leverage language-only judges at inference without a reliable way to summarize 3D scene content for selection.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>3D CoCa v2 unifies a CLIP-based semantic prior, a spatially-aware 3D scene encoder, and a CoCa-style multimodal decoder jointly trained with contrastive and captioning losses, avoiding external detectors. At inference, a test-time search samples diverse captions and uses an LLM judge conditioned on a compact contrastive-retrieval scene summary to select the best caption without updating model parameters.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Structured 3D Evidence Graphs for Judge-Guided Caption Selection: Replace text-descriptor summaries with object‚Äìrelation scene graphs to provide richer, verifiable evidence for LLM judges and reduce hallucinations.<br>‚Ä¢ Real-Time 3D Captioning via Learned Lightweight Judges and Adaptive Test-Time Search: Train small on-device judges and use adaptive candidate generation/early exit to preserve robustness while cutting latency and cost.<br>‚Ä¢ Domain-Robust 3D Captioning for LiDAR and Dynamic Embodied Scenes: Extend the framework to outdoor LiDAR and time-varying environments, integrating motion cues and agent context for stronger cross-environment generalization.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06329" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06329" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Generative spoken language models pretrained on large-scale raw audio can continue a speech prompt with appropriate content while preserving attributes like speaker and emotion, serving as foundation models for spoken dialogue. In prior literature, these models are often evaluated using ``global token perplexity'', which directly applies the text perplexity formulation to speech tokens. However, this practice overlooks fundamental differences between speech and text modalities, possibly leading to an underestimation of the speech characteristics. In this work, we propose a variety of likelihood- and generative-based evaluation methods that serve in place of naive global token perplexity. We demonstrate that the proposed evaluations more faithfully reflect perceived generation quality, as evidenced by stronger correlations with human-rated mean opinion scores (MOS). When assessed under the new metrics, the relative performance landscape of spoken language models is reshaped, revealing a significantly reduced gap between the best-performing model and the human topline. Together, these results suggest that appropriate evaluation is critical for accurately assessing progress in spoken language modeling.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Global token perplexity, inherited from text modeling, fails to capture localized acoustic consistency in speech and is dominated by long-range and semantic factors.<br>‚Ä¢ Flattening multi-codebook/multi-channel speech tokens into a single stream treats all tokens equally, masking short-span likelihood spikes around acoustic transitions.<br>‚Ä¢ Misalignment with human perception yields weak correlations with MOS and distorts comparative evaluations (e.g., on SALMon), underestimating true generation quality.<br>‚Ä¢ Existing benchmarks reduce semantic volatility but cannot guarantee attribute-wise independence; improved, speech-aware evaluation protocols are needed to reflect short-span conditioning and perceptual fidelity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes speech-aware evaluation protocols: (1) localized and normalized likelihood-based metrics that focus on a short window after the prompt and normalize conditional probabilities to mitigate global/semantic noise, and (2) generation-based evaluation using human MOS and an embedding-based model-as-a-judge that scores continuations by distance to positive/negative references, achieving stronger alignment with human ratings.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Localized Likelihood Metrics for Prosody and Emotion Across Languages: Extend localized/normalized perplexity to diverse paralinguistic attributes and languages, studying window size selection and calibration strategies for robust cross-lingual evaluation.<br>‚Ä¢ Training Speech Embedding Judges Aligned with Human Perception: Develop and fine-tune embedding-based judges to predict MOS and contrastive correctness, analyzing robustness, domain transfer, and bias mitigation in model-as-a-judge frameworks.<br>‚Ä¢ Evaluation-Aware Pretraining Objectives for Spoken Language Models: Incorporate localized likelihood and perceptual constraints into pretraining/fine-tuning objectives to optimize for generation quality, validated against SALMon and large-scale human MOS benchmarks.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">A Rising Tide Lifts All Boats: MTQE Rewards for Idioms Improve General Translation Quality</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06307" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06307" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Non-compositional expressions (e.g., idioms, proverbs, and metaphors) pose significant challenges for neural machine translation systems because their meanings cannot be derived from individual words alone. These expressions encode rich, cultural meaning, and have both figurative and literal meanings, making accurate translation difficult. Because models are fairly good at translating compositional text, we investigate GRPO-style fine-tuning using Machine Translation Quality Estimation (MTQE) models as reward functions to train models to better translate idioms. Using Chinese and Hindi idiom datasets, we find that idiom translation abilities improve by ~14 points, general, non-idiomatic translation implicitly improves by ~8 points, and cross-lingual translation abilities (trained on one language, evaluated on another) improves by ~6 points. Overall, our work quantifies the non-compositional translation gap and offers insights for developing LLMs with stronger cross-cultural and figurative language understanding.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ NMT/LLMs struggle with non-compositional expressions (idioms, proverbs, metaphors) whose meanings aren‚Äôt derivable from constituent words, cannot be literally translated, and shift with context.<br>‚Ä¢ Weak idiom translation undermines multilingual accessibility and cross-cultural communication; improving idiom handling can also lift general and cross-lingual translation quality.<br>‚Ä¢ Existing methods prioritize compositional text with token-level objectives and n-gram metrics, lacking targeted reward signals and quality estimation tailored to idioms, leading to misalignment and figurative meaning errors.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>GRPO-style reinforcement learning fine-tuning uses machine translation quality estimation (MTQE) models as reward functions to train models specifically on idiom translation (Chinese and Hindi), yielding gains that also transfer to general and cross-lingual translation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Idiom-Aware Reinforcement Learning at Scale for Multilingual NMT: Extend MTQE-rewarded GRPO to dozens of languages and figurative forms, with culturally grounded reward shaping.<br>‚Ä¢ Contextual MTQE: Discourse- and Pragmatics-Informed Reward Models for Figurative Translation: Build QE models that incorporate context, sense disambiguation, and figurative vs. literal cues to guide RL.<br>‚Ä¢ Knowledge-Augmented MT for Idioms via Retrieval and Reward Shaping: Integrate idiom dictionaries and cultural knowledge graphs into the RL pipeline to propose equivalents and penalize literal mistranslations.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-14">

    <div class="paper">
        <h2 class="paper-title">SPINAL -- Scaling-law and Preference Integration in Neural Alignment Layers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06238" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06238" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Direct Preference Optimization (DPO) is a principled, scalable alternative to RLHF for aligning large language models from pairwise preferences, but its internal geometric footprint remains undercharacterized, limiting audits, checkpoint comparisons, and failure prediction. We introduce SPINAL (Scaling-law and Preference Integration in Neural Alignment Layers), a diagnostic that measures how alignment reshapes representations across depth by tracing localized structural change layer by layer. Across model families, DPO produces a layerwise calibration effect concentrated in the final decoder blocks (often layers 21-30), where preference gradients most directly affect the next-token distribution. SPINAL encodes each checkpoint as a depth trace over (layer index, contraction score, transport score). The contraction score summarizes how quickly the tail of a layer's spectrum decays (how fast small modes vanish); higher values indicate stronger contraction into fewer effective directions. The transport score summarizes how much the token distribution shifts between adjacent layers using a bounded overlap measure; lower values indicate shorter, smoother steps through representation space. Aligned checkpoints show a late-layer ramp-up in contraction and a smooth reduction in transport, consistent with tightened and stabilized policy mass, while unaligned models trace higher-curvature, more entropic, and geometrically incoherent depth paths. Overall, alignment is geometrically localized: the final layers encode the dominant preference-induced corrections. SPINAL turns this localization into a practical audit signal, quantifying where alignment concentrates, how strongly it manifests, and when it begins to destabilize during training.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Alignment is mostly evaluated at the output level; we lack a measurable, layer-resolved view of how DPO reshapes internal model geometry, limiting audits, comparisons, and failure prediction.<br>‚Ä¢ Existing analyses (e.g., safety directions, latent separability) do not provide a depth-indexed geometric diagnostic; they miss whether alignment is localized vs. diffuse across layers.<br>‚Ä¢ Practitioners need a reproducible, post-hoc, cross-model metric to quantify where alignment concentrates (typically terminal decoder layers), how strongly it manifests, and when it disappears under ablations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SPINAL is a post-hoc, geometry-first diagnostic that computes per-layer spectral tail exponent (Œ±‚Ñì) from activation singular-value tails and Fisher‚ÄìRao belief-transport length (L‚Ñì) between adjacent logit-lens distributions, then aggregates terminal-layer sharpening‚Äìcontraction (‚àÜalign), trajectory coherence, and gradient footprint (Gterm) into a single SPINALScore to quantify localized alignment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Causal Tests of Terminal Alignment via Activation Patching and Layer Surgery: Evaluate whether SPINAL‚Äôs terminal signature is causally necessary/sufficient by swapping or ablating final blocks and measuring co-transfer of behavior and geometry.<br>‚Ä¢ Real-Time Alignment Auditing with SPINAL during DPO Training: Compute g‚Ñì, ‚àÜalign, S_coh, and Gterm online to predict failure modes, guide selective layer updates, and enable early stopping.<br>‚Ä¢ Extending SPINAL to RLHF and Instruction-Tuned Regimes: Compare localization and geometric signatures across alignment algorithms, scales, and architectures to test generality.<br>‚Ä¢ Predicting Safety Failures from SPINAL Geometry: Train predictors that map SPINAL trajectories to harmful compliance risk and refusal quality, enabling proactive triage.<br>‚Ä¢ Unifying Directional Safety Metrics and SPINAL Geometry: Integrate residual-space safety directions with SPINAL‚Äôs layer-wise fingerprints to link subspace modulation and geometric calibration.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 14</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button><button class="page-btn" onclick="goToPage(11)">11</button><button class="page-btn" onclick="goToPage(12)">12</button><button class="page-btn" onclick="goToPage(13)">13</button><button class="page-btn" onclick="goToPage(14)">14</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2026-01-13 23:11:05 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 14;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>