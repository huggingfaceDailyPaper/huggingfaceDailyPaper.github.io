<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - January 07, 2026</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">January 07, 2026</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03252" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03252" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Discrete grid-based depth representations fix prediction to training resolution, leading to poor scalability to arbitrary output resolutions and loss of high-frequency geometric details due to smoothing or coarse linear projections.<br>â€¢ Per-pixel depth unprojection creates severe 3D point density imbalance (depth-squared scaling and surface orientation effects), causing holes and artifacts in novel view synthesis under large viewpoint shifts.<br>â€¢ Existing benchmarks and evaluations are low-resolution and sparse, lacking targeted measures for fine-detail regions, making it difficult to assess modelsâ€™ ability to recover high-resolution, fine-grained geometry.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>InfiniDepth models depth as a neural implicit field conditioned on the image: a ViT encoder builds a multi-scale feature pyramid, local features are bilinearly queried at any continuous coordinate and hierarchically fused via gated residual blocks, then an MLP outputs depth; an adaptive sub-pixel query strategy (weighted by depth-squared and normalâ€“view alignment) yields uniformly distributed surface points to improve novel view synthesis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Real-to-Implicit: Robust Training of Depth Implicit Fields on Noisy Real-World Data: Integrate self-supervision, confidence weighting, and denoising priors to learn continuous depth fields from imperfect real-world annotations.<br>â€¢ Temporal InfiniDepth: Continuous Depth Fields for Video with Motion-Aware Consistency: Extend the implicit decoder to sequences, enforcing temporal smoothness via optical flow/scene motion and consistent multi-frame query fusion.<br>â€¢ End-to-End Implicit Geometry for Novel View Synthesis: Jointly learn implicit depth and view-dependent appearance with adaptive querying and Gaussian splatting, optimizing rendering losses for large viewpoint changes.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.01554" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.01554" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Absence of end-to-end Speaker-Attributed, Time-Stamped Transcription (SATS) systems leads to brittle multi-stage pipelines and suboptimal joint optimization for meeting transcription.<br>â€¢ Limited context windows and weak long-range speaker memory undermine accurate diarization and attribution in long recordings, especially for 60â€“90 minute meetings.<br>â€¢ Inability to produce precise, aligned timestamps per speaker reduces the utility of transcripts for search, analytics, and downstream workflows.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A unified multimodal large language model that jointly performs speaker-attributed, time-stamped transcription in an end-to-end paradigm, trained on extensive real-world data. It is equipped with a 128k context window to robustly process up to 90-minute inputs while maintaining long-range speaker memory.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Streaming End-to-End SATS with Low Latency: Extend the model to real-time streaming with incremental diarization and timestamping for live meetings.<br>â€¢ Cross-Lingual SATS for Code-Switching Meetings: Enhance robustness to multilingual speech and code-switching, optionally leveraging multimodal cues to improve speaker attribution.<br>â€¢ Memory-Augmented SATS with Persistent Speaker Profiles: Integrate long-term speaker representations to maintain consistent identities across sessions and organizations.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LTX-2: Efficient Joint Audio-Visual Foundation Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03233" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03233" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Text-to-video models are "silent," lacking synchronized audio that carries semantic, emotional, and environmental cues, making outputs feel incomplete.<br>â€¢ Decoupled pipelines (V2A/A2V) fail to learn the joint audio-visual distribution and bidirectional dependencies, hurting lip-sync, foley timing, and environmental acoustics.<br>â€¢ There is no open, efficient, high-fidelity unified T2A+V framework; existing open solutions are computationally heavy with limited cross-modal synergy, while proprietary systems are closed.<br>â€¢ Current text conditioning often lacks deep multilingual and phonetic grounding, limiting speech accuracy, prompt adherence, and controllability.<br>â€¢ Efficient training/inference requires modality-aware capacity allocation and synchronization mechanisms to scale video fidelity without over-parameterizing audio.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>LTX-2 is a unified text-to-audio+video diffusion transformer with asymmetric dual streams (14B video, 5B audio) that jointly denoise modality-specific latents via bidirectional audio-video cross-attention with temporal RoPE and cross-modality AdaLN for shared timestep conditioning. It combines separate causal VAEs for video and audio, a multilingual text encoder with refined "thinking token" conditioning, and modality-aware (bimodal) classifier-free guidance to improve alignment, controllability, and efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Cross-Modality Guidance Schedules for Dynamic Scenes: Learn or infer per-timestep bimodal CFG schedules that adapt to scene changes to further improve alignment, prosody, and foley timing.<br>â€¢ Hierarchical Long-Form Audiovisual Generation with Multi-Scale Latents: Extend LTX-2 with hierarchical temporal transformers and coarse-to-fine latents to produce minute-scale synchronized content efficiently.<br>â€¢ Phoneme-Aware Multilingual Speech Control in Joint AV Generation: Integrate phoneme-level conditioning and accent/style tokens to enhance lip-sync precision, prosody control, and multilingual speech fidelity.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22334" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22334" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing general-purpose evaluation platforms do not capture core competencies of scientific intelligence, leading to poor alignment with authentic scientific tasks.<br>â€¢ Lack of expert-grade, domain-specific benchmarks prevents rigorous assessment across physics, chemistry, astronomy, materials science, life science, and earth science.<br>â€¢ Current evaluations underrepresent key capabilities (multimodal perception/reasoning/understanding, symbolic reasoning, code and hypothesis generation, knowledge understanding).<br>â€¢ Fragmented and inflexible pipelines hinder batch, reproducible, and comparable evaluations across models and datasets.<br>â€¢ Limited extensibility makes it difficult to integrate custom models and datasets and to bridge capability-based evaluation with disciplinary diversity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SciEvalKit provides a unified, open-source benchmarking toolkit with expert-curated, real-world datasets covering six scientific domains and seven capability categories, coupled with a flexible, extensible pipeline for batch evaluation, custom integration, and transparent, reproducible metrics. It standardizes yet customizes evaluation to compare models across disciplines and scientific competencies.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Agentic Scientific Workflow Evaluation: Extend SciEvalKit to benchmark end-to-end, tool-using agents on literature review, experimental design, data processing, result interpretation, and paper writing.<br>â€¢ Measuring Scientific Hypothesis Generation Quality: Develop standardized metrics and human-in-the-loop protocols to assess novelty, feasibility, and impact of AI-generated research hypotheses.<br>â€¢ Cross-Domain Robustness and Fairness in AI4Science: Analyze model bias and robustness across modalities and disciplines; propose fairness-aware metrics and stress tests within SciEvalKit.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03193" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03193" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ UMMs exhibit a comprehensionâ€“generation gap (Conduction Aphasia): they accurately understand multimodal inputs but fail to translate that understanding into faithful, controllable image generation.<br>â€¢ Existing self-improvement pipelines rely on external data, teacher models, or heuristic/task-specific rewards, which limit scalability, generalization, and robustness across tasks and OOD settings.<br>â€¢ Current evaluations often isolate understanding and generation, lacking a unified, training-free metric of multimodal coherence; a cycle-consistency assessment is needed to measure conceptual alignment across modality transitions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>UniCorn partitions a single UMM into Proposerâ€“Solverâ€“Judge roles to self-generate prompts, images, and evaluative rewards, then reconstructs these interactions into explicit training signals via Cognitive Pattern Reconstruction (caption, judgement, reflection) for fully self-supervised post-training. It also introduces UniCycle, a Textâ†’Imageâ†’Text cycle-consistency benchmark to assess multimodal coherence without external supervision.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Rubrics from Feedback: Auto-Evolving Judges for Self-Improving UMMs: Automatically induce task-specific rubrics and scoring criteria from self-play traces to reduce hand-crafted evaluation and improve generalization.<br>â€¢ Multi-Modal UniCycle: Cycle-Consistency Across Text, Image, Audio, and Video: Extend cycle-consistency evaluation and training to additional modalities to probe and strengthen cognitive symmetry beyond Tâ†”I.<br>â€¢ UniCorn-RLHF: Combining Self-Generated Supervision with Preference Optimization for Controlled Generation: Integrate CPR signals with RLHF/DPO using self-judged preferences to enhance controllability, fidelity, and safety in unified multimodal generation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">NitroGen: An Open Foundation Model for Generalist Gaming Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02427" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02427" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of large, diverse, action-labeled datasets for embodied AI, hindering generalist agent training across many games<br>â€¢ Existing LLM-based agents depend on hand-crafted APIs or complex perception pipelines, limiting scalability and portability to arbitrary commercial titles<br>â€¢ RL agents achieve superhuman results only in narrow, simulator-specific settings, with high training cost and limited availability of suitable environments<br>â€¢ Behavior cloning from pixels has relied on expensive, manually collected demonstrations, constraining coverage to a few games<br>â€¢ Absence of open, standardized frameworks and multi-game benchmarks to measure cross-game generalization and enable reproducible progress</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>NitroGen automatically constructs an internet-scale videoâ€“action dataset by extracting frame-level gamepad inputs from public gameplay videos with input overlays via template matching (SIFT/XFeat) and a SegFormer-based hybrid classificationâ€“segmentation parser, then pre-trains a unified visionâ€“action transformer through large-scale behavior cloning. A universal Gymnasium-style simulator wraps commercial games for evaluation across 30 tasks in 10 titles, enabling zero/few-shot transfer and fine-tuning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Beyond Overlays: Self-Supervised Action Inference from Raw Gameplay Videos: Learn to recover player actions without input overlays using multimodal alignment and self-supervised objectives, broadening data sources beyond annotated overlays.<br>â€¢ Closed-Loop Fine-Tuning of Visionâ€“Action Foundation Models with On-Game Reinforcement: Combine NitroGen pre-training with RL or offline RL in the universal simulator to adapt policies to new games and tasks efficiently.<br>â€¢ Generalist Controllers: Automatic Cross-Device Keybinding Translation for Foundation Gaming Agents: Learn mappings across gamepad, keyboardâ€“mouse, and touch interfaces to generalize control policies across heterogeneous input devices and titles.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">SOP: A Scalable Online Post-Training System for Vision-Language-Action Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03044" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03044" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Real-world deployment demands high-performance generalists; pretrained VLA models generalize broadly but lack expert-level proficiency in specific settings.<br>â€¢ Existing post-training is largely offline, single-robot, or task-specific, decoupling execution from learning and causing distribution shift and compounding errors over long horizons.<br>â€¢ Batch/DAgger-style update latency prevents timely on-policy corrections in real-time sequential decision making, limiting effectiveness.<br>â€¢ Current systems do not support scalable, fleet-wide online learning that maintains a single generalist policy across tasks; single-robot data collection limits throughput and diversity, and task-specific finetuning often sacrifices generality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SOP is a closed-loop, distributed actorâ€“learner system where a robot fleet streams on-policy trajectories and human intervention signals to a centralized cloud learner that mixes online and offline buffers with task-balanced sampling and asynchronously broadcasts updated policies to all actors, enabling low-latency, multi-task post-training of a single generalist VLA model. The framework is algorithm-agnostic and demonstrated with HG-DAgger (interactive imitation learning) and RECAP (reinforcement learning), yielding prompt correction and near-linear scaling with fleet size.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Continual Fleet-Scale Post-Training for Generalist VLA Models: Algorithms and sampling strategies to prevent catastrophic forgetting and handle non-stationary tasks and objects during long deployments while preserving generality.<br>â€¢ Federated SOP: Privacy-Preserving Cross-Fleet Knowledge Sharing and Personalization: Federated actorâ€“learner protocols that share a generalist backbone across organizations and learn site-specific adapters under heterogeneous embodiments and data distributions.<br>â€¢ Uncertainty- and Safety-Aware SOP Interventions: Integrate online uncertainty estimation, risk-sensitive control, and formal safety constraints to automate intervention triggering and reduce human supervision while guaranteeing safe operation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DreamStyle: A Unified Framework for Video Stylization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02785" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02785" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing methods are tied to a single style condition (text-only or image-only), limiting flexibility, user control, and generalization to novel or abstract styles.<br>â€¢ Lack of high-quality, modality-aligned paired stylized/raw video data leads to style inconsistency, temporal flicker, and motion mismatch.<br>â€¢ Current synthetic-pair approaches (e.g., T2V + ControlNet inversion) are constrained by T2V quality and strict control alignment, struggling with styles involving geometric deformation.<br>â€¢ Limited exploration of extended applications such as multi-style fusion and long-video stylization reduces practical utility.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DreamStyle is a unified V2V stylization framework built on a vanilla I2V model that injects diverse style conditions (text, style image, stylized first frame) via lightweight channel/frame concatenation and cross-attention, and trains with a token-specific LoRA (shared down, token-specific up matrices) to reduce inter-condition interference. A two-step data curation pipeline (stylize first frame with SOTA image stylizers, then generate full videos with I2V + depth/pose ControlNets and hybrid filtering) yields large CT and high-quality SFT paired datasets for two-stage training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ StyleFusion-Video: Compositional Multi-Style and Multi-Condition Control for Unified Video Stylization: Learn adaptive weighting and routing to blend multiple texts, reference images, and first-frame cues with fine-grained, user-controllable fusion.<br>â€¢ LongDreamStyle: Hierarchical and Memory-Augmented Conditioning for Hour-Long Video Stylization: Introduce temporal memory, chunked inference, and cross-chunk consistency losses to maintain style over long durations without drift.<br>â€¢ Beyond Depth and Pose: Learning Geometry- and Motion-Aware Priors for Robust Video Stylization: Replace hand-crafted controls with learned 3D/flow/parsing priors and joint motion-style adapters to handle large deformations while preserving content.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MiMo-V2-Flash Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02780" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02780" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Large LLMs suffer from slow inference and high compute costs, especially for long-context reasoning and agentic tasks<br>â€¢ Existing attention designs trade off efficiency and global context: global attention is quadratic/expensive, while local windows miss long-range dependencies<br>â€¢ Post-training pipelines struggle to fuse diverse domain expertise efficiently; single-teacher or trajectory-level RL/distillation yields limited transfer and poor scalability<br>â€¢ Speculative decoding lacks efficient draft models and high acceptance rates; integrating MTP for speed without quality loss is underexplored<br>â€¢ Mixture-of-Experts models often have massive total parameters; achieving top-tier performance with low active parameter budgets remains challenging</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MiMo-V2-Flash is a 309B-parameter MoE with 15B active parameters that combines hybrid sliding-window plus global attention (128-token SWA at a 5:1 ratio) and lightweight Multi-Token Prediction, pre-trained on 27T tokens with a native 32k context extended to 256k. It introduces Multi-Teacher On-Policy Distillation (MOPD) using domain-specialized teachers to provide dense token-level rewards, and repurposes MTP for speculative decoding to reach up to 3.6 acceptance length and 2.6Ã— speedups.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Hybrid Attention for Ultra-Long Contexts: Learnable window sizes and global-attention ratios that dynamically adjust to content and task demands for 256k+ contexts<br>â€¢ Generalized Multi-Teacher On-Policy Distillation: Automated teacher routing, uncertainty-aware blending, and token-level reward shaping to unify cross-domain expertise transfer<br>â€¢ Multi-Token Predictive Speculative Decoding: Formal analysis of acceptance length and error bounds, with architectural designs for robust MTP heads achieving â‰¥3Ã— decoding speedups without quality loss</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.01874" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.01874" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perceptionRightarrowinternalizationRightarrowreasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Multimodal LLMs underperform on visual mathematical problems due to perception errors and shortcut-prone, unstructured reasoning.<br>â€¢ Decoupled pipelines improve extraction but often suffer reasoning drift, failing to faithfully integrate visual cues into subsequent reasoning.<br>â€¢ Existing RL for multimodal reasoning lacks explicit visual-grounded rewards and controls to prevent visually ungrounded chains.<br>â€¢ Perception supervision is fragmented (pixel-level or task-specific), missing both geometric parametric fidelity and global semantic/layout consistency.<br>â€¢ There is a shortage of datasets with aligned perceptionâ€“internalizationâ€“reasoning annotations to train and evaluate faithful integration.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>CogFlow is a cognitive-inspired three-stage framework (perceptionâ†’internalizationâ†’reasoning) trained with Visual-Gated Policy Optimization that fuses Synergistic Visual Rewards (parametric VPR + semantic VSR), a Knowledge Internalization Reward (IntlzR) learned via Softmax-DPO to penalize reasoning drift, and an outcome Inference Reward, using the MATHCOG dataset with aligned annotations. The visual gate filters low-quality perception trajectories so that reasoning is anchored to reliable visual knowledge.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Uncertainty-Calibrated Visual Gates for Reliable Multimodal Reasoning: Integrate calibrated uncertainty and selective regeneration into VGPO, with theoretical guarantees on coverage and risk.<br>â€¢ Programmatic Internalization to Formal Proofs in Geometry: Convert internalized knowledge into formal programs/proof graphs and verify with geometry provers for end-to-end verifiable reasoning.<br>â€¢ Learning Internalization Rewards from Human Preferences at Scale: Train IntlzR with large-scale human preference data and hard-negative mining to generalize beyond handcrafted error types.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.01321" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.01321" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ The field lacks a unified, AI-centered framework that connects modeling, mirroring, intervention, and autonomous management across the digital twin lifecycle.<br>â€¢ Traditional numerical solvers are computationally expensive and slow for repeated or real-time simulations, while pure data-driven models often lack physical consistency, interpretability, and reliability.<br>â€¢ Real-world data integration is challenging due to heterogeneous sensors/IoT/logs, misaligned timestamps, noise, and uncertainty; existing pipelines struggle with robust acquisition, alignment, and data assimilation at scale.<br>â€¢ Current digital twins are largely passive simulators; there is a need to enable proactive, reasoning-capable, agentic systems using LLMs and world models for closed-loop autonomy.<br>â€¢ Cross-domain challengesâ€”scalability, explainability, trustworthiness, and interoperabilityâ€”remain unsynthesized in prior domain-specific reviews, hindering generalizable best practices.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes a unified four-stage lifecycle framework for AI-driven digital twinsâ€”modeling, mirroring, intervening, and autonomous managementâ€”synthesizing physics-informed AI (PINNs, neural operators), robust data acquisition/alignment and assimilation (KF/EnKF, 3D/4D-Var, hybrid ML), generative models (VAE/flow/GAN/diffusion, NeRF/3DGS), and agentic LLM/foundation models for closed-loop decision and control.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Neuro-Symbolic Data Assimilation for Real-Time Digital Twins: Integrate PINNs/neural operators with ensembleâ€“variational filters via differentiable programming to jointly update states, parameters, and uncertainties at streaming scale.<br>â€¢ Physics-Constrained Foundation World Models for Digital Twin Simulation: Train multimodal world models that embed PDE priors and conservation laws to generate controllable, high-fidelity, and generalizable simulations across domains.<br>â€¢ Safety-Aware Agentic LLMs for Closed-Loop Digital Twin Management: Develop explainable, verifiable LLM-based agents that plan, monitor, and actuate with formal safety guarantees and uncertainty-aware decision-making.<br>â€¢ Edge-to-Cloud Federated Pipelines for Scalable Digital Twins: Design standardized, low-latency acquisition/alignment and federated learning frameworks to harmonize heterogeneous IoT, sensor, and log data for autonomous twins.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02989" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02989" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs), despite strong performance on complex mathematical problems, exhibit systematic limitations in counting tasks. This issue arises from architectural limits of transformers, where counting is performed across layers, leading to degraded precision for larger counting problems due to depth constraints. To address this limitation, we propose a simple test-time strategy inspired by System-2 cognitive processes that decomposes large counting tasks into smaller, independent sub-problems that the model can reliably solve. We evaluate this approach using observational and causal mediation analyses to understand the underlying mechanism of this System-2-like strategy. Our mechanistic analysis identifies key components: latent counts are computed and stored in the final item representations of each part, transferred to intermediate steps via dedicated attention heads, and aggregated in the final stage to produce the total count. Experimental results demonstrate that this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting tasks. This work provides mechanistic insight into System-2 counting in LLMs and presents a generalizable approach for improving and understanding their reasoning behavior.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLMs exhibit systematic failures on large-scale counting because implicit, layerwise Systemâ€‘1 counters saturate with depth, causing accuracy collapse beyond ~30 items.<br>â€¢ Numerical magnitudes are encoded in compressed, sublinear representations (log-like), reducing precision for two- and three-digit counts and making large-number counting brittle.<br>â€¢ Neither Chain-of-Thought alone nor structured input alone reliably fixes large-context counting; models cannot simultaneously extract partial counts and aggregate them without loss.<br>â€¢ Existing interpretability tools (e.g., logit/tuned lens) poorly decode numerical information, limiting mechanistic understanding of where and how counts are stored and routed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A test-time Systemâ€‘2 strategy partitions long item lists into small, marked segments using a '|' separator, elicits intermediate partition-level counts, and then aggregates them to keep each sub-task within the modelâ€™s reliable range. Mechanistic analyses (attention mapping, CountScope probing, activation patching, and attention knockout) localize storage at partition-final tokens, identify middle-to-late layer heads that transfer counts to reasoning tokens, and reveal distinct heads that aggregate the final sum.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Systemâ€‘2 Decomposition for Numerical Reasoning Beyond Counting: Learn to dynamically choose segment sizes and decomposition plans based on model confidence, extending partition-and-aggregate to sums, products, and algorithmic tasks.<br>â€¢ Trainingâ€‘Time Alignment of Structured CoT with Aggregation Circuits: Supervise intermediate subcounts and aggregation during fine-tuning to shape attention heads and residual pathways for robust large-number reasoning across prompts and tokenizations.<br>â€¢ Systemâ€‘2 Counting in Multimodal Models: Partitionâ€‘andâ€‘aggregate strategies for LVLMs and images, with mechanistic circuit localization of visual-to-text count transfer and aggregation under cluttered, naturalistic inputs.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03256" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03256" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Muses, the first training-free method for fantastic 3D creature generation in a feed-forward paradigm. Previous methods, which rely on part-aware optimization, manual assembly, or 2D image generation, often produce unrealistic or incoherent 3D assets due to the challenges of intricate part-level manipulation and limited out-of-domain generation. In contrast, Muses leverages the 3D skeleton, a fundamental representation of biological forms, to explicitly and rationally compose diverse elements. This skeletal foundation formalizes 3D content creation as a structure-aware pipeline of design, composition, and generation. Muses begins by constructing a creatively composed 3D skeleton with coherent layout and scale through graph-constrained reasoning. This skeleton then guides a voxel-based assembly process within a structured latent space, integrating regions from different objects. Finally, image-guided appearance modeling under skeletal conditions is applied to generate a style-consistent and harmonious texture for the assembled shape. Extensive experiments establish Muses' state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing. Project page: https://luhexiao.github.io/Muses.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing 3D generators struggle to create out-of-distribution, highly compositional creatures (e.g., mixing animal, robot, and mythical parts) with coherent geometry and style.<br>â€¢ Part-aware pipelines are hard to control (granularity, interfaces), often yielding artifacts and incoherent joints when assembling pieces.<br>â€¢ 2D creative image pipelines lifted to 3D are sensitive to image quality and typically fail to maintain geometric realism and stylistic harmony in 3D.<br>â€¢ Optimization-based text-to-3D (e.g., SDS) is slow and unstable (Janus problem), while feed-forward models lack structure-aware control for complex compositions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Muses is a training-free, skeleton-driven feed-forward pipeline: it designs a composite skeleton via graph-based heuristics and LLM-guided assembly, maps skeleton regions to Trellis structured latent space using skinning to compose parts with voxel-space interpolation, and performs geometry-invariant, image-guided texture harmonization to produce a coherent, high-fidelity 3D creature.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Skeleton-Conditioned End-to-End 3D Generation: Train a feed-forward backbone to natively condition on skeleton graphs, replacing heuristic parsing/LLM planning and improving robustness across categories.<br>â€¢ Multi-View 3D-Aware Texture Harmonization via Diffusion: Develop a 3D-consistent diffusion module that jointly optimizes textures across views, removing reliance on single-image editing while preserving geometry.<br>â€¢ Motion-Aware Fantasy Creature Synthesis: Extend skeleton-centric generation to animation by integrating motion priors and dynamic texture models for physically plausible, rig-ready fantastic characters.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02439" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02439" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent's own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Visual web RL is hard to scale: browser-based rollouts are slow, rewards are sparse/ambiguous, and prior systems use mostly synchronous pipelines that bottleneck data collection.<br>â€¢ Existing task sets are too small/simplistic (e.g., TTI) and often artificial, failing to capture the diversity, non-stationarity, and long-horizon nature of live websites, which hurts generalization.<br>â€¢ Lack of large, diverse, rubric-evaluated training environments limits learning meaningful policies and prevents systematic scaling along breadth, depth, and size.<br>â€¢ Prior environments/benchmarks emphasize evaluation over training, provide limited task hardness/diversity, and do not offer high-throughput rollout infrastructure for online RL.<br>â€¢ Agents trained on limited data underperform on out-of-distribution (OOD) live-website tasks and lag behind what could be achieved with scaled RL and richer environments.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>WebGym pairs a nearly 300k-task, rubric-evaluated live-website environment with a fully asynchronous, high-throughput rollout system (4â€“5Ã— faster) to enable scalable online RL for visual web agents. Using a simple REINFORCE-like recipe, the authors fine-tune Qwen-3-VL-8B-Instruct on its own rollouts with task rewards while systematically scaling task breadth, depth, and size to improve OOD generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Curriculum-Guided RL in WebGym: Difficulty-Aware Scheduling for Long-Horizon Web Tasks: Exploit explicit difficulty levels to adaptively allocate rollout budget and learning rates, improving sample efficiency and robustness on hard, compositional tasks.<br>â€¢ Learning Dense Reward Models for Live-Web Agents: From Rubrics and Human Feedback to Step-Level Credit: Train reward models using rubric-derived fact groups and limited human feedback to provide intermediate, explainable rewards and reduce reliance on sparse success signals.<br>â€¢ Hierarchical Memory-Augmented Web Agents for Non-Stationary Interfaces: Combine hierarchical planning, persistent memory, and tool-use to handle changing UIs, long contexts, and repeated subgoals across diverse, dynamic websites.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.01592" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.01592" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapid integration of Multimodal Large Language Models (MLLMs) into critical applications is increasingly hindered by persistent safety vulnerabilities. However, existing red-teaming benchmarks are often fragmented, limited to single-turn text interactions, and lack the scalability required for systematic evaluation. To address this, we introduce OpenRT, a unified, modular, and high-throughput red-teaming framework designed for comprehensive MLLM safety evaluation. At its core, OpenRT architects a paradigm shift in automated red-teaming by introducing an adversarial kernel that enables modular separation across five critical dimensions: model integration, dataset management, attack strategies, judging methods, and evaluation metrics. By standardizing attack interfaces, it decouples adversarial logic from a high-throughput asynchronous runtime, enabling systematic scaling across diverse models. Our framework integrates 37 diverse attack methodologies, spanning white-box gradients, multi-modal perturbations, and sophisticated multi-agent evolutionary strategies. Through an extensive empirical study on 20 advanced models (including GPT-5.2, Claude 4.5, and Gemini 3 Pro), we expose critical safety gaps: even frontier models fail to generalize across attack paradigms, with leading models exhibiting average Attack Success Rates as high as 49.14%. Notably, our findings reveal that reasoning models do not inherently possess superior robustness against complex, multi-turn jailbreaks. By open-sourcing OpenRT, we provide a sustainable, extensible, and continuously maintained infrastructure that accelerates the development and standardization of AI safety.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing red-teaming benchmarks are fragmented, largely single-turn, text-only, and do not scale, leaving MLLMsâ€™ safety insufficiently tested.<br>â€¢ Advanced MLLMs retain significant vulnerabilities to adaptive, multi-turn, and multi-agent attacks; current defenses overfit static templates and fail to generalize.<br>â€¢ Models show polarized robustnessâ€”strong against some attack families but highly vulnerable to othersâ€”indicating patch-based, non-holistic defenses.<br>â€¢ Enhanced reasoning and multimodal capabilities introduce new exploitation vectors (e.g., modality gap where visual inputs bypass text-based filters).<br>â€¢ Proprietary models are not inherently safer; a standardized, continuously maintained evaluation infrastructure is needed to avoid safety-through-obscurity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>OpenRT introduces a unified, modular, high-throughput red-teaming framework with an adversarial kernel that standardizes attack interfaces and decouples adversarial logic from an asynchronous runtime, enabling scalable evaluation across diverse models. It integrates 37 attack strategies (white-box gradients, multimodal perturbations, multi-agent evolutionary methods), flexible judging modules, and metrics to comprehensively assess MLLM safety.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Defense-in-Depth for Multimodal LLMs: Integrating Architectural Safety with Runtime Risk Estimation: Design layered defenses combining intrinsic safeguards, real-time risk scoring, and adversarial training on multi-turn, multimodal interactions.<br>â€¢ Closing the Modality Gap: Cross-Modal Safety Alignment and Certification for MLLMs: Develop unified safety mechanisms and certification protocols that prevent visual inputs from bypassing text-based filters and ensure consistent cross-modal robustness.<br>â€¢ Generalizing Robustness to Adaptive Multi-Agent Jailbreaks via Continual Red Teaming: Create training curricula and co-evolution pipelines that leverage automated frameworks (e.g., OpenRT) to continually generate diverse, adaptive attacks and improve generalization.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.01720" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.01720" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Current first-frame propagation (FFP) video editing methods depend on cumbersome run-time guidance (e.g., flow, tracking, masks), hindering usability, speed, and generalization.<br>â€¢ Existing training datasets are inadequateâ€”too short, low-resolution, with limited edit types and incomplete paired sourceâ€“target dataâ€”failing to teach robust temporal priors for propagation.<br>â€¢ State-of-the-art models often struggle to maintain original motion and deliver high visual quality and temporal consistency, especially on long (81+ frames), high-resolution (720p) videos and diverse edits.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce FFP-300Kâ€”a 300K-pair, 720p, 81-frame, high-fidelity dataset built via a principled two-track pipeline for diverse local and global editsâ€”and train a guidance-free FFP framework that learns robust temporal priors to propagate an edited first frame across the video without any run-time auxiliary guidance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ FFP-4K: High-Resolution First-Frame Propagation for Long-Horizon Video Editing: Scale FFP to 4K resolution and longer sequences to study fidelityâ€“coherence trade-offs and efficient architectures.<br>â€¢ Self-Supervised Temporal Priors for Guidance-Free Video Edit Propagation: Leverage large unlabeled video corpora to learn propagation-consistency priors that reduce dependence on paired edit datasets.<br>â€¢ Multimodal First-Frame Propagation: Unified Guidance-Free Editing with Text, Audio, and Motion Cues: Train with multimodal conditions to enhance edit adherence and temporal coherence while keeping inference guidance-free.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03227" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03227" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs' reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ No established benchmark or public dataset for audio geo-localization with reliable location annotations, hindering systematic evaluation and progress.<br>â€¢ Absence of a quantitative notion of "audio localizability" to filter geographically informative recordings from crowd-sourced audio.<br>â€¢ Need to assess and advance ALMs' geospatial compositional reasoning, fine-grained acoustic perception, and robustness, given audio geo-localizationâ€™s relevance to public safety and misinformation detection.<br>â€¢ Existing (especially open-source) ALMs often misidentify languages and subtle acoustic clues, show regional prediction biases, and over-rely on single cues, unlike vision where large geo-tagged datasets enabled progress.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce AGL1K, a curated benchmark of 1,444 crowd-sourced audio clips spanning 72 countries, filtered via an Audio Localizability metric that aggregates evidence from positive and negative sound categories to quantify geographic informativeness. Benchmark 16 ALMs and analyze reasoning traces, regional bias, and error causes to expose capability gaps and guide improvements.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Fine-Grained Acoustic Perception for Geo-Localization in ALMs: Develop models and training regimes to improve language ID, environmental sound recognition, and subtle cue detection for reliable geographic inference.<br>â€¢ Debiasing Audio Geo-Localization: Methods to measure, mitigate, and calibrate regional prediction biases in ALMs while maintaining accuracy across continents and cultures.<br>â€¢ Compositional Reasoning with Multi-Cue Integration for Audio Geo-Localization: Architectures and training strategies that fuse heterogeneous weak clues (linguistic, environmental, cultural) with uncertainty-aware inference to avoid over-commitment to single signals.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03194" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03194" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on https://github.com/ziarehman30/X-MuTeST</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Scarcity of human-annotated, token-level rationales for hate speech in under-resourced Indic languages (Hindi, Telugu), limiting explainable detection.<br>â€¢ Misalignment between machine-provided rationales (LLMs, attention) and human judgments due to cultural and societal nuances, leading to low plausibility.<br>â€¢ Existing explainability methods (attention, LIME, prompt-based LLMs) often lack faithfulness to model decisions and underperform in multilingual/code-mixed settings.<br>â€¢ Need for training frameworks that jointly improve classification accuracy and explanation quality by balancing human supervision with model-driven saliency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>X-MuTeST is a two-stage explainable hate speech detection framework: Stage-1 aligns model attention with human token-level rationales via an attention alignment loss; Stage-2 derives saliency masks using a novel n-gram contribution method based on logit drops, and final explanations are the union of these with LLaMA-3.1 rationales.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Cross-Lingual Cultural Rationale Alignment for Hate Speech Detection: Integrate cultural knowledge bases and LLM distillation to better capture implicit offensiveness across diverse, low-resource languages.<br>â€¢ Multimodal X-MuTeST: Explainable Hate Speech Detection in Text, Images, and Videos: Extend the two-stage training and rationale fusion to memes and videos, with faithfulness metrics tailored for multimodal content.<br>â€¢ Adaptive Teacher-Student Rationale Fusion for Low-Resource Toxicity: Learn dynamic weights between human, n-gram, and LLM rationales via meta-learning to optimize plausibility and faithfulness under data scarcity.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">Parallel Latent Reasoning for Sequential Recommendation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03153" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03153" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Capturing complex user preferences from sparse behavioral sequences remains a fundamental challenge in sequential recommendation. Recent latent reasoning methods have shown promise by extending test-time computation through multi-step reasoning, yet they exclusively rely on depth-level scaling along a single trajectory, suffering from diminishing returns as reasoning depth increases. To address this limitation, we propose Parallel Latent Reasoning (PLR), a novel framework that pioneers width-level computational scaling by exploring multiple diverse reasoning trajectories simultaneously. PLR constructs parallel reasoning streams through learnable trigger tokens in continuous latent space, preserves diversity across streams via global reasoning regularization, and adaptively synthesizes multi-stream outputs through mixture-of-reasoning-streams aggregation. Extensive experiments on three real-world datasets demonstrate that PLR substantially outperforms state-of-the-art baselines while maintaining real-time inference efficiency. Theoretical analysis further validates the effectiveness of parallel reasoning in improving generalization capability. Our work opens new avenues for enhancing reasoning capacity in sequential recommendation beyond existing depth scaling.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Capturing complex, evolving user preferences from sparse behavioral sequences remains difficult for sequential recommenders, limiting their ability to perform robust logical inference.<br>â€¢ Explicit reasoning approaches suffer from high inference latency and lack well-defined, verifiable reasoning chains in recommendation scenarios, making them impractical for real-time serving.<br>â€¢ Existing latent reasoning methods rely on depth-only scaling along a single trajectory, leading to diminishing returns with increased reasoning depth and insufficient diversity.<br>â€¢ There is a need to enhance reasoning capacity while preserving real-time inference efficiency and improving generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Parallel Latent Reasoning (PLR) widens test-time computation by running multiple latent reasoning streams in parallel, each initialized via learnable trigger tokens, regularized to maintain diversity, and adaptively combined through a mixture-of-reasoning-streams aggregator to improve generalization and efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Width-Depth Latent Reasoning for Real-Time Sequential Recommendation: Jointly optimize the number of parallel streams and per-stream depth with a latency-aware controller to maximize accuracy under tight serving budgets.<br>â€¢ Explaining Parallel Latent Reasoning via Semantic Trigger Alignment: Map learnable trigger tokens to interpretable user factors using side-information and contrastive alignment, yielding self-explaining parallel reasoning in recommendation.<br>â€¢ Multi-Modal Parallel Latent Reasoning for Cross-Domain Recommendation: Extend PLR to fuse text/image/audio signals and transfer reasoning streams across domains via meta-learning for robust generalization.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Unified Thinker: A General Reasoning Modular Core for Image Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03127" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03127" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Generative models exhibit a reasoningâ€“execution gap: they struggle to decompose logic-intensive instructions into executable visual plans, leading to failures in complex tasks.<br>â€¢ Open-source image generators lag behind closed-source systems in reasoning-driven generation, highlighting limited instruction-following under implicit or multi-step constraints.<br>â€¢ Built-in reasoning tightly entangles understanding with rendering, reducing modularity, destabilizing training, and often degrading visual fidelity.<br>â€¢ External planner-driven approaches produce text-space plans not grounded in generator capabilities, causing visual mismatches and high compute due to iterative planning; there is no principled, executable reasoning paradigm aligned to pixel outcomes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Unified Thinker decouples a standalone multimodal Thinker from a diffusion Generator in a think-then-execute architecture, where the Thinker outputs structured, generator-friendly plans (intent, constraints, ordered sub-goals) to condition the Generator. It is trained via joint supervised fine-tuning on the HieraReason-40K dataset and dual-phase GRPO-based reinforcement learning that grounds planning in pixel-level rewards and improves execution fidelity through stochastic diffusion rollouts.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Reasoning-to-Rendering Interfaces for Diverse Generators: Learnable adapters that translate Thinker plans into conditioning spaces of different generators to maximize portability and alignment.<br>â€¢ Visual Constraint Critics for RL-Grounded Planning: Multimodal reward models that assess fine-grained constraint satisfaction (counts, spatial relations, attributes) to provide stronger, low-noise signals for Thinker/Generator RL.<br>â€¢ Hierarchical Spatial Reasoning with Scene Graphs in Unified Thinker: Augment the planning interface with explicit layouts and scene graphs to boost compositional consistency and complex spatial edits.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02996" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02996" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large reasoning models (LRMs) achieve strong performance on mathematical reasoning tasks, often attributed to their capability to generate explicit chain-of-thought (CoT) explanations. However, recent work shows that LRMs often arrive at the correct answer before completing these textual reasoning steps, indicating the presence of latent reasoning -- internal, non-verbal computation encoded in hidden states. While this phenomenon has been explored in English, its multilingual behavior remains largely unknown. In this paper, we conduct a systematic investigation of multilingual latent reasoning in LRMs across 11 languages. Using a truncation-based strategy, we examine how the correct answer emerges as the model is given only partial reasoning traces, allowing us to measure stepwise latent prediction formation. Our results reveal clear evidence of multilingual latent reasoning, though unevenly: strong in resource-rich languages, weaker in low-resource ones, and broadly less observable on harder benchmarks. To understand whether these differences reflect distinct internal mechanisms, we further perform representational analyses. Despite surface-level disparities, we find that the internal evolution of predictions is highly consistent across languages and broadly aligns with English -- a pattern suggesting an English-centered latent reasoning pathway.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of systematic understanding of latent (non-verbal) reasoning in multilingual settings; prior work focuses almost exclusively on English.<br>â€¢ Need to quantify when correct answers emerge before explicit chain-of-thought and disentangle latent reasoning from explicit answer articulation and memorization/contamination.<br>â€¢ Unclear whether internal latent reasoning mechanisms are language-specific or shared; and how task difficulty and language resource levels modulate latent reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce a truncation-based evaluation across 11 languages that measures pass@k under partial reasoning, gold-in-trace, and aggregates them via AUTC, AUGC, and the Latent Reasoning Score (LRS) to quantify latent reasoning. Complement with representational analyses (logit lens rank trajectories, cross-lingual hidden-state cosine similarity) and controlled perturbation tests (numeric edits vs paraphrases) to separate memorization from genuine reasoning, using prompt-hacking to enforce trace language.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Reducing English-Centered Latent Pathways via Cross-Lingual Representation Regularization: Train with alignment losses and multilingual contrastive objectives to lessen reliance on English-aligned latent trajectories, boosting low-resource language LRS.<br>â€¢ LRS-Driven Curriculum Learning for Multilingual Latent Reasoners: Use the Latent Reasoning Score as a signal to schedule data and training steps, emphasizing examples that improve early latent answer formation across under-resourced languages.<br>â€¢ Building Anti-Contamination Multilingual Math Benchmarks: Design difficulty-calibrated, contamination-audited datasets and perturbation protocols to reliably evaluate latent reasoning without memorization confounds.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02359" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02359" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Poor generalization of deepfake detectors to unseen manipulations due to reliance on supervised or pseudo-fake training that overfits to known artifacts.<br>â€¢ Existing self-supervised methods lack discriminative power; audio-visual consistency objectives face a vast correspondence space and fail on highly synchronized high-fidelity fakes.<br>â€¢ Reference-based approaches often require identity labels or lengthy per-subject training, lack a scalable general prior, and are constrained by limited identity-labeled datasets.<br>â€¢ Practical need for person-of-interest (POI) authentication that is zero-shot across manipulation types and robust to real-world corruptions (e.g., blur, heavy compression) and novel generators (e.g., Sora2).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>ExposeAnyone trains a self-supervised audio-to-expression diffusion model that predicts FLAME expression/jaw trajectories from speech, then personalizes it to a subject via lightweight adapter tokens. Forgery detection is performed by a content-agnostic identity test that compares diffusion reconstruction distances with and without the subjectâ€™s adapter.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ End-to-End Audio-to-Expression Forgery Detection Without 3DMMs: Replace 3DMM extraction with end-to-end pixel/landmark-space diffusion to reduce domain gaps and capture subtler identity dynamics.<br>â€¢ Meta-Personalization for One-Clip POI Deepfake Detection: Develop meta-learned or prompt-based adapters enabling reliable personalization from a few seconds of reference video/audio.<br>â€¢ Multimodal Diffusion Consistency Beyond Faces: Jointly model audio, facial expressions, head pose, gaze, and body gestures to detect forgeries in full-body or multi-person videos and in modality-missing scenarios.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.00581" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.00581" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce AceFF, a pre-trained machine learning interatomic potential (MLIP) optimized for small molecule drug discovery. While MLIPs have emerged as efficient alternatives to Density Functional Theory (DFT), generalizability across diverse chemical spaces remains difficult. AceFF addresses this via a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds. This approach yields a force field that balances high-throughput inference speed with DFT-level accuracy. AceFF fully supports the essential medicinal chemistry elements (H, B, C, N, O, F, Si, P, S, Cl, Br, I) and is explicitly trained to handle charged states. Validation against rigorous benchmarks, including complex torsional energy scans, molecular dynamics trajectories, batched minimizations, and forces and anergy accuracy demonstrates that AceFF establishes a new state-of-the-art for organic molecules. The AceFF-2 model weights and inference code are available at https://huggingface.co/Acellera/AceFF-2.0.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Achieve near-DFT accuracy for diverse, drug-like molecules at speeds suitable for routine MD and RBFE, where classical MM lacks fidelity and DFT is prohibitively expensive<br>â€¢ Overcome limited element and charge coverage in popular MLIPs (e.g., ANI-2x supports fewer elements and only neutral molecules), and the speed-accuracy trade-off of more accurate but slower models (e.g., MACE/OrbMol)<br>â€¢ Address inadequate long-range electrostatics and weak extrapolation to charged species in prior TensorNet charge handling<br>â€¢ Provide smooth, stable potentials and CUDA-graphâ€“friendly implementations for efficient MD, MLIP/MM embedding, and batched workflows<br>â€¢ Demonstrate rigorous, broad generalization across torsions, strained conformers, larger ligands, forces/energies, and geometry minimization benchmarks</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>AceFF-2 introduces TensorNet2, an equivariant GNN that augments TensorNet with AIMNet2-style neutral charge equilibration across multiple feature channels and an explicit Coulomb term, integrating predicted charges into message passing and energy computation. Trained on DFT energies and forces for drug-like molecules with 12 medicinal-chemistry elements and charged states, it is optimized for fast inference (CUDA graphs, Triton neighbor lists) while maintaining high accuracy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Linear-Scaling Electrostatics for TensorNet2 via Learned Multipoles or FMM: Replace N^2 Coulomb with multipole/FMM to enable larger MLIP/MM regions without sacrificing accuracy<br>â€¢ Active Learning AceFF-2.5 for Highly Charged and Larger Drug-like Molecules: Expand training to extreme charges and 50â€“100 atom ligands to improve robustness and extrapolation<br>â€¢ Multi-Task TensorNet2 with Charges, Dipoles, and Polarizabilities: Jointly learn electrostatic observables to enhance physical fidelity and MLIP/MM polarizable embedding<br>â€¢ Delta-Learning AceFF-CC: Correct AceFF-2 from DFT to CCSD(T)/CBS to close the remaining accuracy gap on torsions and strained conformers<br>â€¢ Polarizable MLIP/MM Embedding with Environment-Dependent Charges: Couple TensorNet2â€™s charge channels to MM polarization for more accurate proteinâ€“ligand simulations<br>â€¢ Uncertainty-Aware AceFF for Production RBFE: Calibrate uncertainties, detect out-of-domain geometries (e.g., high charge, rare chemistries), and trigger on-the-fly QM data acquisition</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">U-Net-Like Spiking Neural Networks for Single Image Dehazing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23950" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23950" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Image dehazing is a critical challenge in computer vision, essential for enhancing image clarity in hazy conditions. Traditional methods often rely on atmospheric scattering models, while recent deep learning techniques, specifically Convolutional Neural Networks (CNNs) and Transformers, have improved performance by effectively analyzing image features. However, CNNs struggle with long-range dependencies, and Transformers demand significant computational resources. To address these limitations, we propose DehazeSNN, an innovative architecture that integrates a U-Net-like design with Spiking Neural Networks (SNNs). DehazeSNN captures multi-scale image features while efficiently managing local and long-range dependencies. The introduction of the Orthogonal Leaky-Integrate-and-Fire Block (OLIFBlock) enhances cross-channel communication, resulting in superior dehazing performance with reduced computational burden. Our extensive experiments show that DehazeSNN is highly competitive to state-of-the-art methods on benchmark datasets, delivering high-quality haze-free images with a smaller model size and less multiply-accumulate operations. The proposed dehazing method is publicly available at https://github.com/HaoranLiu507/DehazeSNN.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ CNN-based dehazing struggles to capture long-range dependencies and global context due to limited receptive fields.<br>â€¢ Transformer-based dehazing achieves strong global modeling but at the cost of large parameter counts and high MACs, limiting deployment on resource-constrained devices.<br>â€¢ Real-world and remote-sensing haze is highly non-uniform, requiring lightweight models that robustly extract multi-scale and cross-channel features.<br>â€¢ Standard U-Net skip concatenation can overlook branch biases; more selective, low-cost fusion is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DehazeSNN integrates a 5-stage U-Net-like encoderâ€“decoder with a Spiking Neural Network Block featuring the OLIFBlock, which uses spatial, orthogonal (horizontal/vertical) LIF-based iterative grouping with full-precision outputs to capture local and global dependencies efficiently; SKfusion provides selective, low-cost skip fusion. This design achieves competitive dehazing quality with markedly fewer parameters and MACs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Video Dehazing with Temporal Spiking U-Nets: Extend OLIFBlocks to true temporal LIF dynamics and recurrent spiking modules to exploit frame-to-frame consistency for real-time video dehazing.<br>â€¢ Neuromorphic Dehazing on Event-Driven Hardware: Map OLIFBlock operations to neuromorphic accelerators and integrate event-camera inputs for ultra-low-power, on-device dehazing.<br>â€¢ Physics-Aware Spiking Dehazing via Joint Depth and Transmission Estimation: Fuse atmospheric scattering priors with OLIF-based SNNs in a multi-task framework to improve robustness under non-uniform haze.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">Steerability of Instrumental-Convergence Tendencies in LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.01584" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.01584" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). A central question is whether capability growth reduces steerability and risks control collapse. We also distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma of AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability for malicious actors to elicit harmful behaviors. This tension presents a significant challenge for open-weight models, which currently exhibit high steerability via common techniques like fine-tuning or adversarial attacks. Using Qwen3 and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces the measured convergence rate (e.g., shutdown avoidance, self-replication). For Qwen3-30B Instruct, the convergence rate drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models show lower convergence rates than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Clarify the empirical relationship between capability and steerability, challenging the common assumption that increasing capability implies control collapse.<br>â€¢ Distinguish and quantify authorized vs. unauthorized steerability to expose the safetyâ€“security dilemma, especially acute for open-weight models.<br>â€¢ Provide a protocol-relative, non-mentalistic measurement of instrumental-convergence behaviors and their suppressibility via feasible interventions.<br>â€¢ Assess whether minimal deployment-time steering (prompt suffixes) can reliably amplify or suppress instrumental behaviors across scales and post-training variants.<br>â€¢ Highlight limitations of existing safeguards (e.g., refusal training) that are brittle to jailbreaks/fine-tuning and lack mechanisms to selectively reduce unauthorized steerability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Evaluate Qwen3 (4B/30B; Base/Instruct/Thinking) on InstrumentalEvalâ€™s 76 scenarios under two short prompt suffixes (pro- vs. anti-instrumental), and use an external judge to label outputs (convergence/no-convergence/safety-refusal/nonsensical) to compute Conv% and the steerability gap Î” = Conv(pro) âˆ’ Conv(anti). Analyze how post-training and scale affect convergence/refusal rates and interpret results through the authorizedâ€“unauthorized steerability trade-off.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Beyond Prompting: Train-Time and Representation-Level Steering of Instrumental Tendencies: Systematically compare SFT/RL, activation steering, and concept editing for amplifying/suppressing instrumental markers, and test durability against jailbreaks/fine-tuning.<br>â€¢ Differential Steerability for Open-Weights: Mechanisms to Preserve Authorized Control While Limiting Adversarial Elicitation: Formalize metrics and evaluate capability unlearning, tamper-resistant checkpoints, and practical encrypted/tethered execution.<br>â€¢ Robust and Scalable InstrumentalEval: Multi-Domain, Multi-Judge, and Long-Horizon Assessments: Expand scenarios, add human+model adjudication, and test tool-using/agentic settings with real-time oversight to measure cross-protocol robustness.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 9</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2026-01-08 00:10:28 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 9;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>