<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - October 31, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">October 31, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">The End of Manual Decoding: Towards Truly End-to-End Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26697" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26697" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The "end-to-end" label for LLMs is a misnomer. In practice, they depend on a non-differentiable decoding process that requires laborious, hand-tuning of hyperparameters like temperature and top-p. This paper introduces AutoDeco, a novel architecture that enables truly "end-to-end" generation by learning to control its own decoding strategy. We augment the standard transformer with lightweight heads that, at each step, dynamically predict context-specific temperature and top-p values alongside the next-token logits. This approach transforms decoding into a parametric, token-level process, allowing the model to self-regulate its sampling strategy within a single forward pass. Through extensive experiments on eight benchmarks, we demonstrate that AutoDeco not only significantly outperforms default decoding strategies but also achieves performance comparable to an oracle-tuned baseline derived from "hacking the test set"-a practical upper bound for any static method. Crucially, we uncover an emergent capability for instruction-based decoding control: the model learns to interpret natural language commands (e.g., "generate with low randomness") and adjusts its predicted temperature and top-p on a token-by-token basis, opening a new paradigm for steerable and interactive LLM decoding.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Manual, static decoding hyperparameters (temperature, top‑p, top‑k) require costly, task-specific tuning and are inherently suboptimal both across tasks and within a single generation where the ideal stochasticity changes over time (Introduction, pp.1–2; Figure 1 on p.2).<br>• Standard decoding pipelines are not end-to-end differentiable (e.g., hard top‑p cutoff), blocking gradient flow and preventing the model from learning decoding strategies directly from task loss (Section 2.1; Figure 2a on p.3–4).<br>• The optimal hyperparameters vary drastically by dataset and task, making a single static setting impractical in real deployments; developers cannot feasibly switch configs per query type (Figure 3 on p.8).<br>• Existing methods lack native, intuitive user control to steer decoding style (e.g., “high/low diversity”) via natural language; any emergent effects are inconsistent without targeted training (Figure 5 on p.10; Table 4 on p.10–11).<br>• Suboptimal decoding materially harms accuracy and quality across domains, showing the importance of better decoding: AutoDeco outperforms greedy/default baselines on math and general tasks (Tables 1–2 on pp.6–7) while adding negligible latency (Table 3 on p.9).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>AutoDeco augments a transformer with lightweight MLP heads that predict per-token temperature and top‑p from the hidden state (with top‑p conditioned on the predicted temperature), then internally rescales and truncates logits in a single forward pass. A differentiable “soft” top‑p mask enables end‑to‑end training of these heads via cross‑entropy (Section 2.1; Figure 2), with the backbone frozen and minor debiasing (easy-token masking, dynamic fine-tuning).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Joint End-to-End Training of LLMs and AutoDeco for Precise Instruction-Guided Decoding: Co-train the backbone and decoding heads to achieve fine-grained, absolute control over temperature/top‑p from natural-language commands and reduce data biases (noted limitation in Section 5).<br>• Multi-Knob, Multi-Objective AutoDeco: Extend beyond temperature/top‑p to learn joint control over top‑k, repetition penalties, contrastive search weights, and safety knobs, optimizing jointly for accuracy, diversity, factuality, safety, and latency.<br>• Self-Verification–Aware Decoding Policies: Integrate verifiers/uncertainty estimators to adapt decoding parameters on-the-fly (e.g., lower temperature under high uncertainty) to reduce hallucinations and improve reliability across domains.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Emu3.5: Native Multimodal Models are World Learners</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26583" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26583" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily derived from sequential frames and transcripts of internet videos. The model naturally accepts interleaved vision-language inputs and generates interleaved vision-language outputs. Emu3.5 is further post-trained with large-scale reinforcement learning to enhance multimodal reasoning and generation. To improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA), which converts token-by-token decoding into bidirectional parallel prediction, accelerating per-image inference by about 20x without sacrificing performance. Emu3.5 exhibits strong native multimodal capabilities, including long-horizon vision-language generation, any-to-image (X2I) generation, and complex text-rich image generation. It also exhibits generalizable world-modeling abilities, enabling spatiotemporally consistent world exploration and open-world embodied manipulation across diverse scenarios and tasks. For comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image (Nano Banana) on image generation and editing tasks and demonstrates superior results on a suite of interleaved generation tasks. We open-source Emu3.5 at https://github.com/baaivision/Emu3.5 to support community research.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Long-horizon multimodal learning is under-served: existing models mostly handle short, independent clips/pairs and struggle to learn or generate interleaved vision–language sequences with temporal consistency, causality, and extended context (p.4, Fig. 6).<br>• Lack of a native, unified interface: current systems rarely accept and produce interleaved VL inputs/outputs natively, limiting general-purpose multimodal reasoning, visual narrative/guidance, and world modeling (p.4, Fig. 2).<br>• Scaling challenges: it is unclear how to effectively pre-train on web-scale interleaved video+transcript data (10–13T tokens), post-train across tasks, and share capabilities via a single model (p.4, Fig. 4).<br>• Inference inefficiency: autoregressive decoding over tens of thousands of visual tokens is slow; efficient inference without quality loss is needed (p.4–5, Fig. 3).<br>• Gaps vs SOTA generation: diffusion models lead on image editing/text rendering, but are not unified world models; there is a need to match speed/quality while enabling interleaved VL generation and open-world editing (p.4, Fig. 1–2).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Emu3.5 is a 34B decoder-only transformer trained with a unified next-token prediction objective on >10T interleaved video-frame and transcript tokens to natively accept and generate interleaved vision–language sequences. It is post-trained with large-scale SFT and multimodal RL, and accelerated at inference by Discrete Diffusion Adaptation (DiDA) that parallelizes visual token prediction (~20× per-image) while keeping text decoding sequential (Fig. 3–4).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Adaptive Interleaved Tokenization for Long-Horizon World Models: Develop dynamic, content-aware token allocation (variable-rate visual/text tokens) to improve efficiency and fidelity across long contexts.<br>• Multimodal Reward Modeling for Interleaved Generation and Control: Learn scalable, unified vision–language reward models for RL that better capture temporal coherence, text rendering accuracy, and editing controllability.<br>• DiDA-XL: Generalized Bidirectional Parallel Inference for Multimodal AR Models: Extend DiDA to span-level and mixed-modality parallel prediction for real-time video generation, editing, and interactive agents.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Exploring Conditions for Diffusion models in Robotic Control</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.15510" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.15510" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While pre-trained visual representations have significantly advanced imitation learning, they are often task-agnostic as they remain frozen during policy learning. In this work, we explore leveraging pre-trained text-to-image diffusion models to obtain task-adaptive visual representations for robotic control, without fine-tuning the model itself. However, we find that naively applying textual conditions - a successful strategy in other vision domains - yields minimal or even negative gains in control tasks. We attribute this to the domain gap between the diffusion model's training data and robotic control environments, leading us to argue for conditions that consider the specific, dynamic visual information required for control. To this end, we propose ORCA, which introduces learnable task prompts that adapt to the control environment and visual prompts that capture fine-grained, frame-specific details. Through facilitating task-adaptive representations with our newly devised conditions, our approach achieves state-of-the-art performance on various robotic control benchmarks, significantly surpassing prior methods.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Frozen pre-trained visual encoders used in robotic control are task-agnostic; picking the best one is task-by-task and fine-tuning often overfits and hurts generalization.<br>• Text prompts that boost diffusion-based perception do not transfer to control due to domain gaps (web images vs. robot simulators) and poor text–image grounding in cross-attention, yielding negligible or negative gains versus a null prompt.<br>• Control demands frame-wise, fine-grained, action-relevant visual cues; existing conditioning methods are global/static or require extra optimization, making them ill-suited for dynamic policy learning without adapting the diffusion model.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>ORCA conditions a frozen text-to-image diffusion model with learnable task prompts (shared, task-specific text tokens) and frame-wise visual prompts (dense DINOv2 features projected as tokens), then extracts early/mid U-Net features as the state for a policy. Both prompts are learned end-to-end via a behavior cloning objective, enabling task-adaptive representations without fine-tuning the diffusion model.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• VideoDiff-Prompt: Temporal Prompting of Video Diffusion Models for Long-Horizon Robot Control: Replace image diffusion with video diffusion and learn sequence-level temporal prompts to better capture dynamics in long-horizon tasks.<br>• RL-in-the-Loop Prompt Adaptation for Diffusion-Conditioned Policies: Use reinforcement learning or online adaptation to update task/visual prompts during deployment, improving robustness to distribution shifts and sim-to-real transfer.<br>• Dynamic Timestep-and-Layer Gating for Diffusion Hyperfeatures in Control: Learn to select or weight diffusion timesteps and U-Net layers per state to maximize control-relevant information while minimizing latency.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">Kimi Linear: An Expressive, Efficient Attention Architecture</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26692" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26692" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule. We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths. To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Softmax attention has quadratic time and linearly growing KV cache, bottlenecking long-context inference and RL test-time scaling.<br>• Prior linear attention underperforms full attention on short contexts and struggles with precise long-range retrieval due to limited finite-state capacity.<br>• Existing gated/delta variants rely on coarse head-wise forgetting; general DPLR formulations are numerically fragile and hardware-inefficient.<br>• Previous hybrids (linear + full attention) were limited in scale or lacked fair, matched-training comparisons; a drop-in, superior alternative to full attention is missing.<br>• Fixed-frequency positional encodings (e.g., RoPE) complicate length extrapolation; models need a learnable, position-aware mechanism compatible with NoPE global layers.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Kimi Linear interleaves a new linear module—Kimi Delta Attention (KDA), a delta-rule fast-weight update with channel-wise fine-grained gating and a bespoke chunkwise algorithm—with periodic NoPE global attention layers (3:1), yielding softmax-level quality while shrinking KV cache and boosting decoding speed. KDA ties a specialized DPLR-style transition to keys for numerical stability and efficiency, enabling hardware-friendly kernels and vLLM integration.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• KDA-Sparse Hybrid: Marrying Linear Compression with Learnable Sparse Retrieval: Combine KDA’s fixed-state compression with chunk/token-level sparse attention to boost exact retrieval while keeping memory low.<br>• State-Expanded KDA: Mixture-of-Memories for Exact Long-Range Recall: Augment KDA with state expansion or mixture-of-memories to improve copying and multi-entity recall without KV caches.<br>• Position Learning in Hybrids: NoPE–KDA Co-Design and Length Extrapolation: Systematically study positional-bias allocation across depth and propose adaptive transition matrices for robust >1M-token generalization.<br>• System Co-Design for KDA: FP8 Kernels, Quantization, and vLLM Scheduling: Develop fused FP8/INT kernels, quantization-aware training, and batching/scheduling policies to maximize KDA throughput under tight memory budgets.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26298" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26298" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>OpenAI's ChatGPT Atlas introduces new capabilities for web interaction, enabling the model to analyze webpages, process user intents, and execute cursor and keyboard inputs directly within the browser. While its capacity for information retrieval tasks has been demonstrated, its performance in dynamic, interactive environments remains less explored. In this study, we conduct an early evaluation of Atlas's web interaction capabilities using browser-based games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird, and Stein.world. We employ in-game performance scores as quantitative metrics to assess performance across different task types. Our results show that Atlas performs strongly in logical reasoning tasks like Sudoku, completing puzzles significantly faster than human baselines, but struggles substantially in real-time games requiring precise timing and motor control, often failing to progress beyond initial obstacles. These findings suggest that while Atlas demonstrates capable analytical processing, there remain notable limitations in dynamic web environments requiring real-time interaction. The website of our project can be found at https://atlas-game-eval.github.io.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Evaluating ChatGPT Atlas in dynamic, interactive web environments is underexplored; existing demos center on information retrieval rather than real-time control, timing, and continuous action execution (pp. 1–3).<br>• Current web-agent benchmarks and studies often use static or constrained tasks, offline settings, or screenshot-only paradigms, leading to potentially over-optimistic results and limited insight into motor/timing precision and adaptation (pp. 2–3).<br>• There is a need to assess how agents translate intents into precise cursor/keyboard actions, adapt strategies when stuck, and comprehend narrative, multi-step objectives in text-rich interfaces (pp. 1–2).<br>• Games offer clear, quantitative metrics across diverse interaction types (logic, reflex, exploration), enabling systematic probing of analytical reasoning versus motor control gaps (Figure 1 on page 2).<br>• Prior work highlights persistent gaps in grounding, multi-step reasoning, and functionality preservation on real webpages, indicating limitations of existing methods for dynamic, real-time web tasks (pp. 2–3).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>A mixed-methods, zero-shot evaluation of ChatGPT Atlas (Agent Mode) across five browser games using a standardized protocol: 10 trials per structured game with in-game quantitative metrics (T-Rex Runner, Sudoku, Flappy Bird, 2048) plus a qualitative RPG case study (Stein.world), analyzing four axes—analytical processing, input execution, adaptive behavior, and contextual understanding. The testbed spans puzzle, reflex, and RPG scenarios (Figure 1, p.2) and includes detailed navigation analysis in Stein.world (Figure 2, p.6).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• WebAgentBench++: Online, Real-World Benchmarking of Web Agents Beyond Games: Expand evaluation to dynamic web forms, interactive visualizations, and complex web tools with online, latency-aware metrics.<br>• Disentangling Perception, Cognition, and Motor Execution in Web Interaction Agents: Design protocols and diagnostics that isolate visual grounding, decision-making, and low-latency control to pinpoint failure modes.<br>• A Comparative Study of Multimodal Web Agents Under Unified Game-and-Task Protocols: Systematically compare Atlas with specialized web automation and emerging MLLM agents under standardized online settings.<br>• Training for Real-Time Precision: RL and Imitation Learning to Improve Timing-Control in Browser Agents: Investigate targeted training and architectural improvements (e.g., control loops, action buffering) to enhance reflexes and continuous control.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26802" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26802" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Clarify whether modern video generation models (e.g., Veo-3, Sora-2) perform genuine zero-shot visual reasoning or merely replay surface patterns, especially under the proposed Chain-of-Frame (CoF) paradigm.<br>• Fill the lack of a standardized, multi-dimensional benchmark and protocol to evaluate reasoning (spatial, geometric, physical, temporal, embodied) in video models; existing work emphasizes fidelity or QA on MLLMs rather than generative models.<br>• Disentangle short-horizon coherence from long-horizon causal, geometric, and abstract logic to expose strengths and failure modes that current ad-hoc evaluations miss.<br>• Reduce confounds from prompt ambiguity and camera motion by enforcing a unified prompt style (static shots, fixed layout, explicit constraints) to test reasoning rather than stylistic generation.<br>• Provide comparable, quantitative metrics across models; prior studies lack a robust verifier and category-wise scoring to separate instruction alignment, temporal consistency, stability, fidelity, and focus.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces MME-COF, a compact benchmark and unified evaluation protocol for zero-shot CoF reasoning across 12 categories, with tightly controlled prompts, six generations per case, and Good/Moderate/Bad plus success-rate judgments. It further employs an automatic verifier (Gemini-2.5-Pro) to score videos on five criteria (instruction alignment, temporal consistency, visual stability, content fidelity, focus relevance) for direct comparison among Veo-3, Sora-2, Seedance, and Kling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Chain-of-Frame Reinforcement Learning for Video Reasoning: Train video generators with rule-based rewards and temporal credit assignment to maintain long-horizon causal consistency and state tracking.<br>• Geometry-Aware Video Generators via Differentiable 3D Priors: Integrate 3D structure priors/simulators to enforce rigid-body constraints, eliminate self-intersections, and preserve coordinate frames during multi-step transforms.<br>• Physics-Constrained Video Generation with Differentiable Simulation: Couple generation with physics engines to enforce conservation laws, contact mechanics, and reflection rules for quantitative physical fidelity.<br>• Visual Engine Meets CoT: A Hybrid CoF–CoT Reasoner: Orchestrate an LLM planner/verifier with a video model visualizer to iteratively plan, render, and critique reasoning steps.<br>• MME-COF-XL: A Benchmark for Long-Horizon Causal Video Reasoning: Extend MME-COF with longer sequences, branching tasks, and strict geometric/temporal constraints for stress-testing causal logic.<br>• Agentic GUI and Embodied Interaction for Video Models: Build interactive environments and feedback signals to learn functional GUI behaviors and manipulation affordances beyond imitation.<br>• Domain-Aware CoF for Medical Imaging: Inject medical priors and geometry-preserving operations to avoid distortions and enable precise localization/measurement tasks.<br>• Prompt-Programmed Camera Control DSL for Reliable Spatial Grounding: Design a declarative camera/motion DSL with verifiable constraints to separate camera control errors from reasoning failures.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">AMO-Bench: Large Language Models Still Struggle in High School Math Competitions</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26768" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26768" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original problems to prevent potential performance leakages from data memorization. Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. These results highlight the significant room for improving the mathematical reasoning in current LLMs. We release AMO-Bench to facilitate further research into advancing the reasoning abilities of language models. https://amo-bench.github.io/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Existing math reasoning benchmarks are saturating for top LLMs (e.g., >90% on AIME24/25), making them less discriminative for tracking frontier progress (Figure 1).<br>• Many benchmarks are built from past competitions, introducing contamination and memorization risks; results may reflect data leakage rather than genuine reasoning (Section 1, Originality concerns).<br>• Proof-based Olympiad problems require expert grading, hindering scalable, consistent, and reproducible evaluation; there is a need for automatically gradable, high-difficulty tasks (Section 1).<br>• Lack of rigorous difficulty control means some competition-style problems are too easy for state-of-the-art models; a benchmark must guarantee IMO-level challenge (Section 2.1).<br>• Current evaluations underutilize insights about test-time compute; a benchmark should expose scaling behavior and pass@k headroom to guide method development (Figures 5, 7, 8).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>AMO-Bench is a 50-problem, original IMO-level benchmark built via a four-stage human+LLM pipeline (creation, quality, originality via 10-gram/web/expert checks, and difficulty filtering requiring strong models to fail), with final-answer formatting for automatic grading. It combines parser-based grading for 39 numeric/set/expression problems and LLM majority-vote grading for 11 descriptive problems (99.2% grading accuracy), and evaluates 26 LLMs (AVG@32), revealing low accuracy ceilings and clear test-time scaling trends.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• From Pass@K to Pass@1 on AMO-Bench: Reinforcement Learning and Search to Consolidate Reasoning Paths: Leverage high pass@32 headroom to train policies/verifiers that convert multi-sample success into reliable single-shot solutions.<br>• Token-Budgeted Reasoning on Olympiad Math: Scaling Laws and Compute-Efficient Decoding: Develop decoding, planning, and self-reflection strategies that optimize accuracy per token, exploiting the near-linear gains with log output length.<br>• Verifiers that Scale: Hybrid LLM+Symbolic Graders for Descriptive Olympiad Answers: Build robust graders combining LLM judgments with formal/symbolic checks to reduce bias and cost, enabling fully automatic evaluation of open-format solutions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Surfer 2: The Next Generation of Cross-Platform Computer Use Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.19949" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.19949" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Building agents that generalize across web, desktop, and mobile environments remains an open challenge, as prior systems rely on environment-specific interfaces that limit cross-platform deployment. We introduce Surfer 2, a unified architecture operating purely from visual observations that achieves state-of-the-art performance across all three environments. Surfer 2 integrates hierarchical context management, decoupled planning and execution, and self-verification with adaptive recovery, enabling reliable operation over long task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior systems without task-specific fine-tuning. With multiple attempts, Surfer 2 exceeds human performance on all benchmarks. These results demonstrate that systematic orchestration amplifies foundation model capabilities and enables general-purpose computer control through visual interaction alone, while calling for a next-generation vision language model to achieve Pareto-optimal cost-efficiency.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Cross-platform generalization: Prior agents depend on environment-specific interfaces (web DOM, mobile accessibility trees, desktop APIs), hindering deployment across web, desktop, and mobile.<br>• Visual-only control: Many real tools lack APIs; text/DOM-based methods fail on dynamic, visually rich UIs—agents need to perceive and act from screenshots.<br>• Long-horizon reliability: Existing systems lack explicit planning, persistent shared state, and verification, leading to error cascades and premature termination on multi-step tasks.<br>• Precise UI grounding: Small localization errors translate into action failures; prior approaches often underperform at pixel-level target selection.<br>• Model-centric limitations: Scaling/fine-tuning large models without orchestration is costly, brittle across tasks, and misses gains achievable via system design and self-correction.<br>• Unstable evaluation and cost: LLM-as-judge variance and high inference costs impede reliable benchmarking and practical deployment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Surfer 2 is a unified, purely visual, hierarchical agent that separates high-level planning (optional Orchestrator) from low-level execution (Navigator with a ReAct loop), adds a specialized Localizer (Holo1.5) for pixel-accurate grounding, and uses a Validator for multi-stage self-verification and adaptive recovery. Shared environment state and hierarchical context enable plan–delegate–validate cycles across web, desktop, and mobile without task-specific fine-tuning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Pareto-Optimal Vision-Language Models for GUI Agents: Develop smaller, specialized reasoning and localization models that retain Surfer 2 accuracy while reducing cost, variance, and latency.<br>• Learning to Localize from Interaction Feedback: Train UI localizers using click outcomes and error-driven signals to cut failures on dynamic or ambiguous elements and improve pixel-accurate grounding.<br>• Long-Horizon Memory for Visual Computer Use: Introduce persistent, structured memory/state graphs to sustain 50+ step workflows, reduce context overflow, and mitigate compounding errors.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">The Quest for Generalizable Motion Generation: Data, Model, and Evaluation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26794" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26794" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in modeling human behaviors, highlighting transferable insights that MoGen can leverage. Motivated by this observation, we present a comprehensive framework that systematically transfers knowledge from ViGen to MoGen across three key pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a large-scale dataset comprising 228,000 high-quality motion samples that integrates high-fidelity optical MoCap data with semantically annotated motions from web videos and synthesized samples generated by state-of-the-art ViGen models. The dataset includes both text-motion pairs and text-video-motion triplets, substantially expanding semantic diversity. Second, we propose ViMoGen, a flow-matching-based diffusion transformer that unifies priors from MoCap data and ViGen models through gated multimodal conditioning. To enhance efficiency, we further develop ViMoGen-light, a distilled variant that eliminates video generation dependencies while preserving strong generalization. Finally, we present MBench, a hierarchical benchmark designed for fine-grained evaluation across motion quality, prompt fidelity, and generalization ability. Extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations. The code, data, and benchmark will be made publicly available.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Generalization bottleneck: current 3D motion generation models struggle to follow diverse, long‑tail prompts compared with video generation models<br>• Data scarcity and bias: optical MoCap corpora are small, studio‑bound, and semantically narrow, limiting learned motion priors<br>• Low-fidelity video-derived motions: in-the-wild and T2V-derived motions broaden coverage but introduce jitter, pose/translation errors, and modality gaps<br>• Inefficient/limited knowledge transfer: prior pipelines extract motion knowledge from ViGen via costly optimization and weak text encoders, yielding limited robustness<br>• Inadequate evaluation: prevailing metrics (e.g., FID) and simple indoor prompts poorly reflect human judgment and fail to measure generalization and text alignment</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>ViMoGen: a flow-matching diffusion transformer with adaptive gated dual-branch fusion that unifies MoCap priors (Text-to-Motion branch) and video-derived motion tokens (Motion-to-Motion branch) to balance motion quality and semantic generalization. Trained on the 228K-sample ViMoGen-228K dataset and distilled into ViMoGen-light (no T2V at inference), the framework is evaluated with MBench, a nine-dimension, human-aligned benchmark for quality, prompt fidelity, and generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Towards a Unified Motion Foundation Model via Multimodal Pretraining: Pretrain a single model on text–video–motion–audio–scene data to unify priors and enable broad compositional control<br>• Physics- and Contact-Aware Flow Matching for Realistic Motion: Integrate differentiable physics, foot-ground contact, and collision constraints directly into the flow field for physically consistent outputs<br>• Open-World Multi-Agent and Human–Object Interaction Motion Generation: Extend ViMoGen to coordinated multi-human and HOI scenarios with scalable interaction priors and scene affordances<br>• Self-Refining Motion Generation with VLM-in-the-Loop Feedback: Use vision–language critiques to adaptively select branches and iteratively refine motions during training and inference<br>• Scaling Laws and Data Mixture Optimization for Motion Generation: Systematically study scaling behavior and optimize mixtures of MoCap, in-the-wild, and synthetic T2V data for maximal generalization</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25992" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25992" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, we propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem solving as generating a sequence of logical "actions". SRL trains the model to generate an internal reasoning monologue before committing to each action. It provides smoother rewards based on the similarity between the model's actions and expert actions extracted from the SFT dataset in a step-wise manner. This supervision offers richer learning signals even when all rollouts are incorrect, while encouraging flexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRL before refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRL generalizes effectively to agentic software engineering tasks, establishing it as a robust and versatile training framework for reasoning-oriented LLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Outcome-based RL with verifiable rewards (RLVR) fails on hard, multi-step reasoning problems where pass@k ≈ 0, yielding vanishing advantages and unstable learning when penalizing all incorrect rollouts (pp. 2–4).<br>• Supervised fine-tuning (SFT) overfits token-by-token to long CoT traces, harming generalization for small models and modest, complex datasets; on s1k, SFT underperforms the base model while SRL improves markedly (Figure 1, p. 1).<br>• There is a need for dense, step-wise supervision that provides informative signals even when all rollouts are incorrect, encourages flexible reasoning (not verbatim copying), and remains practical for latency-heavy agentic tasks like software engineering where online RL is costly (pp. 5, 10–11).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Supervised Reinforcement Learning (SRL) decomposes expert solutions into step-level “actions”; at each step the model first generates an internal monologue, then commits an action and receives a dense reward equal to the sequence-similarity between its action and the expert’s action (computed via difflib), optimized with GRPO and variance-based dynamic sampling (Figures 2–3, pp. 3, 6). The reward is applied only to actions (not the monologue), enabling flexible internal reasoning while aligning external decisions with expert strategies.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Automatic Action Discovery for SRL: Learn to segment demonstrations into optimal step-level actions via latent-variable models or alignment with environment transitions, reducing reliance on heuristic parsing.<br>• Semantically Grounded Rewards for SRL: Replace string-similarity rewards with verifier-backed signals (e.g., math solvers, AST diffs, unit tests) to improve robustness and reduce spurious matches.<br>• Curriculum SRL→RLVR at Scale: Study curricula that anneal from fine-grained step rewards to final-outcome rewards, deriving theory and scaling laws for stability, efficiency, and transfer across domains.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">The Era of Agentic Organization: Learning to Organize with Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26658" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26658" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We envision a new era of AI, termed agentic organization, where agents solve complex problems by working collaboratively and concurrently, enabling outcomes beyond individual intelligence. To realize this vision, we introduce asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large language models, which organizes the internal thinking process into concurrently executable structures. Specifically, we propose a thinking protocol where an organizer dynamically assigns sub-queries to workers, merges intermediate knowledge, and produces coherent solutions. More importantly, the thinking structure in this protocol can be further optimized through reinforcement learning. Experiments demonstrate that AsyncThink achieves 28% lower inference latency compared to parallel thinking while improving accuracy on mathematical reasoning. Moreover, AsyncThink generalizes its learned asynchronous thinking capabilities, effectively tackling unseen tasks without additional training.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• LLMs lack an organization mechanism to think collaboratively and concurrently; sequential CoT is purely linear, while parallel thinking runs independent traces and aggregates late, causing unnecessary latency and weak coordination.<br>• Existing parallel methods are constrained by the slowest branch plus aggregation overhead and rely on fixed, handcrafted workflows that cannot adapt the reasoning structure to each query.<br>• There is no learned organization policy for when to fork, join, or answer; manually designing optimal thinking structures per query is intractable, motivating reinforcement learning to discover effective policies.<br>• Evaluation of reasoning often ignores efficiency; beyond accuracy, we need metrics like critical-path latency and explicit incentives for concurrency to capture real inference-time trade-offs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>AsyncThink introduces a text-only organizer–worker protocol where an organizer emits Think/Fork/Join/Answer actions to spawn workers on sub-queries and merge their <RETURN> outputs, enabling adaptive concurrent reasoning without changing model architecture. A two-stage training—cold-start format SFT on synthetic traces followed by RL (GRPO) with accuracy, format-compliance, and concurrency rewards—teaches the model to learn efficient Fork–Join structures that minimize critical-path latency while improving accuracy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Scaling AsyncThink: Laws of Accuracy–Latency Trade-offs with Massive Agent Pools: Derive empirical and theoretical scaling laws as agent-pool size and diversity grow, including system-level overheads and optimal concurrency targets.<br>• Hierarchical AsyncThink: Recursive Organizers and Multi-Level Fork–Join for Deep Problem Decomposition: Allow workers to become sub-organizers that spawn their own teams, enabling flexible, multi-level reasoning hierarchies.<br>• Human-AI Agentic Organizations: Interactive Fork–Join Protocols and Shared Control: Integrate humans as organizers or workers, design interfaces and RLHF/RLVR objectives for mixed-initiative planning, verification, and intervention.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26800" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26800" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance this technique to generate graphics-ready 3D scenes suitable for physically based rendering (PBR), relighting, and simulation. Our key insight is to repurpose 2D generative models for panoramic perception of geometry, textures, and PBR materials. Unlike existing 2D lifting approaches that emphasize appearance generation and ignore the perception of intrinsic properties, we present OmniX, a versatile and unified framework. Based on a lightweight and efficient cross-modal adapter structure, OmniX reuses 2D generative priors for a broad range of panoramic vision tasks, including panoramic perception, generation, and completion. Furthermore, we construct a large-scale synthetic panorama dataset containing high-quality multimodal panoramas from diverse indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness of our model in panoramic visual perception and graphics-ready 3D scene generation, opening new possibilities for immersive and physically realistic virtual world generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Existing 2D lifting and panorama-based scene generation focus on appearance only, neglecting intrinsic properties (albedo, roughness, metallic) and robust geometry, making results incompatible with modern PBR, relighting, and physics pipelines.<br>• No unified framework to reuse powerful 2D generative priors for panoramic generation, RGB→X perception, and masked completion under a single formulation; prior work typically trains separate task-specific models and cannot handle multi-conditional inputs.<br>• Scarcity of panoramic datasets with dense geometry and material annotations across both indoor and outdoor scenes, limiting training and fair evaluation of panoramic inverse rendering.<br>• Adapting 2D models to 360° imagery suffers from ERP seam continuity and cross-view consistency issues, degrading panoramic quality.<br>• Single-pose panoramas do not support exploration; lack of interactive completion and guided perception to progressively extend scenes into explorable 3D environments.<br>• Practical pipelines to turn a single image into graphics-ready, mesh-based 3D assets are missing, especially those that output all PBR maps necessary for downstream graphics engines.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>OmniX repurposes a pre-trained DiT-based flow matching generator (FLUX.1-dev) via lightweight Separate-Adapter LoRAs and a unified multi-conditional flow formulation to perform panoramic generation, RGB→X perception (distance, normal, albedo, roughness, metallic), and masked completion/guided perception. It predicts multimodal 360° maps, reconstructs mesh geometry from the panoramic distance map, and assigns PBR textures via spherical UV unwrapping to yield graphics-ready 3D scenes, trained with the new PanoX dataset and occlusion-aware mask sampling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Spherical OmniX: Native Spherical Positional Encoding for Seamless 360° Flow Transformers: Replace ERP with spherical-aware encodings/attention to eliminate seams and boost panoramic fidelity.<br>• RealPanoX: A Large-Scale Real-World Panoramic PBR Dataset with Dense Annotations: Capture HDR indoor/outdoor panoramas with calibrated geometry and SVBRDF to bridge the synthetic-to-real gap.<br>• OmniX-4D: Unified Panoramic Generation and Perception for Dynamic Scenes: Extend the adapters to video diffusion for time-consistent 4D scenes with relightable, physically plausible dynamics.<br>• Metric-Accurate OmniX: Geometry- and Photometry-Consistent Training for Precise Distance: Add multi-view/self-supervision, LiDAR pseudo-labels, and differentiable rendering losses to improve Euclidean distance accuracy and surface smoothness.<br>• MaterialNet-X: Joint SVBRDF Estimation with Cross-Modal Co-Training: Enhance metallic/roughness generalization via joint training on mixed narrow-FoV/360 data with cross-attention and rendering priors.<br>• OmniX-Edit: Promptable, Physics- and Lighting-Aware Scene Editing on Panoramas: Develop localized, semantics-aware editing while preserving geometry and PBR consistency for interactive scene authoring.<br>• Adapterless OmniX: Parameter-Efficient Universal Cross-Modal Conditioning: Replace per-modality LoRAs with shared conditional tokenizers/routing to further reduce parameters and improve flexibility.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25897" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25897" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current text-to-image generative models are trained on large uncurated datasets to enable diverse generation capabilities. However, this does not align well with user preferences. Recently, reward models have been specifically designed to perform post-hoc selection of generated images and align them to a reward, typically user preference. This discarding of informative data together with the optimizing for a single reward tend to harm diversity, semantic fidelity and efficiency. Instead of this post-processing, we propose to condition the model on multiple reward models during training to let the model learn user preferences directly. We show that this not only dramatically improves the visual quality of the generated images but it also significantly speeds up the training. Our proposed method, called MIRO, achieves state-of-the-art performances on the GenEval compositional benchmark and user-preference scores (PickAScore, ImageReward, HPSv2).</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Post-hoc alignment pipelines (curation + RLHF) discard “low-quality” yet informative data and optimize a single reward, harming diversity/semantic fidelity and adding extra training stages and hyperparameters (see Figure 2, p.2; §1).<br>• Single-reward optimization encourages reward hacking and mode collapse; improving one metric (e.g., AestheticScore) often degrades others and text-image alignment (Figure 4, p.4; Table 1, p.8).<br>• Current models offer limited controllability over trade-offs among aesthetics, preference, and alignment at inference; users cannot dial objectives jointly (Figure 9, p.10).<br>• Training and inference are compute-inefficient; baseline training converges slowly and test-time scaling needs many samples to reach good rewards (Figure 5, p.5; Figure 8, p.9).<br>• Synthetic captioning helps alignment but is compute-heavy; better ways are needed to exploit rich supervision without filtering data (Figure 7, p.7; §3.3).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>MIRO conditions a flow-matching text-to-image model on a vector of binned scores from multiple reward models computed per image–caption pair, embedding these scores as tokens alongside the text during pretraining. At inference it applies multi-reward classifier-free guidance between positive and negative reward targets to steer generation toward jointly high-reward regions with user-controllable trade-offs (Figures 2–3, pp.2–3).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Jointly Learning Generators and Reward Models for Stable Multi-Objective Alignment: Co-train MIRO with learnable reward heads to reduce reward drift, improve robustness across domains, and further mitigate reward hacking.<br>• From Bins to Continuous Reward Fields: Differentiable Multi-Reward Conditioning for T2I: Replace discrete binning with continuous, calibrated reward embeddings and learn adaptive guidance scales to yield smoother control and better sample efficiency.<br>• MIRO-Video: Multi-Reward Conditioned Pretraining for Text-to-Video and 3D Generation: Extend MIRO’s conditioning and guidance to spatio-temporal/3D settings with temporal consistency and geometry rewards.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25628" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25628" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Electronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is critical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical workflows, their ability to analyze EHRs remains limited due to narrow task coverage and lack of EHR-oriented reasoning capabilities. This paper aims to bridge the gap, specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning instruction dataset, comprising 300k high-quality reasoning cases and 4M non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a thinking-graph-driven framework that enables to generate high-quality reasoning data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage training paradigm, including domain adaptation, reasoning enhancement, and reinforcement learning, EHR-R1 systematically acquires domain knowledge and diverse reasoning capabilities, enabling accurate and robust EHR analysis. Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning 42 tasks, to comprehensively assess reasoning and prediction across EHR scenarios. In experiments, we show that the resulting EHR-R1 consistently outperforms state-of-the-art commercial and open-source LLMs (including DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and achieving a 10\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins, EHR-R1, and EHR-Bench have significantly advanced the development for more reliable and clinically relevant EHR analysis.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Narrow task coverage: prior EHR work targets isolated tasks and cannot support holistic, evolving clinical workflows<br>• Lack of EHR-oriented reasoning: models struggle to filter redundancy, integrate multi-source temporal evidence, and build longitudinal disease narratives<br>• General reasoning doesn’t transfer: generic reasoning LLMs underperform on EHR tasks, and test-time reasoning without EHR-specific training offers little-to-negative gains<br>• Missing resources: absence of large, high-quality EHR reasoning datasets and unified benchmarks spanning decision-making and risk prediction<br>• Limited interoperability: current LLMs fail to extract/integrate EHR data reliably, hindering integration with hospital information systems and real-world adoption</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Construct EHR-Ins, a large instruction dataset with thinking-graph–driven rationale synthesis (entity co-occurrence lift analysis + UMLS linking + GPT-4o to produce structured Extraction→Reasoning→Final chains), and train EHR-R1 via a three-stage pipeline (domain adaptation, reasoning SFT, and GRPO-based reinforcement learning). They also introduce EHR-Bench, a 42-task benchmark covering decision-making and risk-prediction for comprehensive evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Reasoning for Risk Prediction in EHRs: Building Explicit Rationales for Binary Clinical Outcomes: Extend the thinking-graph pipeline to mortality/readmission/LOS tasks to generate auditable chains and quantify interpretability–accuracy trade-offs<br>• From Lift Scores to Knowledge Fusion: Robust Medical Thinking-Graph Construction with Multi-KB and Neural Entity Linking: Fuse UMLS with additional knowledge bases and learned entity linking to reduce data attrition and expand reasoning coverage<br>• Beyond Specialization: Multi-Domain Continual Pretraining and Modular Adapters to Preserve General Abilities in Clinical LLMs: Explore curricula, adapters, and MoE routing to balance EHR expertise with general reasoning and cross-domain robustness</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26213" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26213" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Document AI has advanced rapidly and is attracting increasing attention. Yet, while most efforts have focused on document layout analysis (DLA), its generative counterpart, document layout generation, remains underexplored. A major obstacle lies in the scarcity of diverse layouts: academic papers with Manhattan-style structures dominate existing studies, while open-world genres such as newspapers and magazines remain severely underrepresented. To address this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse document layouts, covering six common document types and comprising contemporary layouts collected from multiple sources. Moreover, since existing methods struggle in complex domains and often fail to arrange long sequences coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage Coarse-to-Fine learning paradigm: 1) learning universal layout principles from OmniLayout-1M with coarse category definitions, and 2) transferring the knowledge to a specific domain with fine-grained annotations. Extensive experiments demonstrate that our approach achieves strong performance on multiple domains in M^{6}Doc dataset, substantially surpassing both existing layout generation experts and several latest general-purpose LLMs. Our code, models, and dataset will be publicly released.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Severe scarcity and domain bias of public layout data—large datasets over-index on academic “Manhattan” layouts while complex, contemporary types (newspaper, magazine, textbook, exam) are underrepresented, hindering generalizable generation.<br>• Existing generators struggle with complex, long-sequence layouts—diffusion models are data-hungry and slow to converge; LLM-based methods directly fine-tuned on complex domains often fail or produce incoherent arrangements.<br>• Outdated/noisy sources and costly manual annotation limit data quality and scale; a modern, automated, diverse curation/annotation pipeline is missing.<br>• Lack of a unified, controllable formulation for tasks like conditional placement, completion, and refinement under category/size/position constraints.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>They curate OmniLayout-1M, a million-scale, six-type, contemporary document layout dataset annotated via an automated pipeline, and propose OmniLayout-LLM (0.5B) trained with a two-stage Coarse-to-Fine paradigm that learns universal spatial priors from coarse labels, then adapts to fine-grained domains via label mapping. A unified token serialization and prompt design supports controllable tasks (unconditional, C→S+P, C+S→P, completion, refinement) using normalized, quantized box coordinates and category tokens.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Open-World Layout Generalization via Dynamic Label Mapping: Learn automatic, data-driven mappings between coarse and emergent fine-grained categories to continuously adapt across unseen document types.<br>• Layout-to-Image Rendering with Coarse-to-Fine Structural Guidance: Couple the LLM layout planner with an image generator to render high-fidelity document pages that preserve alignment/overlap aesthetics and semantics.<br>• Multi-Page Narrative Layout Planning with Long-Context LLMs: Extend layout generation to coherent multi-page documents, modeling cross-page dependencies, reading flow, and global constraints.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">Magentic Marketplace: An Open-Source Environment for Studying Agentic Markets</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25779" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25779" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As LLM agents advance, they are increasingly mediating economic decisions, ranging from product discovery to transactions, on behalf of users. Such applications promise benefits but also raise many questions about agent accountability and value for users. Addressing these questions requires understanding how agents behave in realistic market conditions. However, previous research has largely evaluated agents in constrained settings, such as single-task marketplaces (e.g., negotiation) or structured two-agent interactions. Real-world markets are fundamentally different: they require agents to handle diverse economic activities and coordinate within large, dynamic ecosystems where multiple agents with opaque behaviors may engage in open-ended dialogues. To bridge this gap, we investigate two-sided agentic marketplaces where Assistant agents represent consumers and Service agents represent competing businesses. To study these interactions safely, we develop Magentic-Marketplace-- a simulated environment where Assistants and Services can operate. This environment enables us to study key market dynamics: the utility agents achieve, behavioral biases, vulnerability to manipulation, and how search mechanisms shape market outcomes. Our experiments show that frontier models can approach optimal welfare-- but only under ideal search conditions. Performance degrades sharply with scale, and all models exhibit severe first-proposal bias, creating 10-30x advantages for response speed over quality. These findings reveal how behaviors emerge across market conditions, informing the design of fair and efficient agentic marketplaces.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Lack of end-to-end, two-sided agentic market testbeds: prior work mostly evaluates single-task or dyadic interactions (e.g., negotiation) and misses the full lifecycle of discovery → inquiry/negotiation → payment/fulfillment seen in real markets (Figure 1; Sections 1, 3).<br>• Need to quantify market outcomes and risks under realistic conditions: current methods don’t measure welfare, scale effects, search constraints, behavioral biases (position/proposal), or manipulation resistance across many heterogeneous agents (Figures 4–9; Section 4).<br>• Fragmented/rigid agent protocols: existing tool/A2A protocols don’t directly support marketplace-specific economics; researchers need a minimal, extensible protocol that supports runtime capability discovery and safe transactions for heterogeneous agents (Figure 2b–c, Table 1; Section 3).<br>• Safety and experimental control gaps: lack of controllable, reproducible, and safe environments/data to probe agent behaviors and interventions without real-world harm; need synthetic domains extensible to other settings (Section 4.1).<br>• Urgent practical importance: agentic markets are imminent; poor designs can create efficiency loss, inequality (e.g., speed over quality due to first-proposal bias), and vulnerabilities to manipulation, affecting user value and platform fairness (Sections 1, 5.2–5.4).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>They introduce Magentic Marketplace—an open-source, HTTP/REST client–server simulation of two-sided markets with Assistant (consumer) and Service (business) agents, using a minimal 3-endpoint protocol (register, protocol discovery, action) and a rich action set (search, message, proposal, payment, receive) to support the full transaction lifecycle. The platform includes synthetic domains, scalable evaluation, and experiments that measure welfare, biases, and manipulation resilience under varying search and scale conditions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Mitigating First‑Proposal Bias in Agentic Markets: Mechanism Design and Serving Strategies: Design batched decision windows, randomized proposal ordering, and deliberation prompts to reduce the 10–30x first‑mover advantage and realign competition toward quality and price.<br>• Learning in Dynamic Agentic Economies: Repeated Interactions, Reputation, and Shock Resilience: Extend the static setup to repeated games with adaptive agents, endogenous reviews/refunds, and study stability under shocks and coordinated adversaries.<br>• Human‑AI Co‑Decision Protocols for High‑Stakes Transactions in Two‑Sided Markets: Develop and evaluate human‑in‑the‑loop checkpoints, selective autonomy, and explanation/oversight interfaces to balance speed, safety, and accountability.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Remote Labor Index: Measuring AI Automation of Remote Work</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26787" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26787" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• There is no standardized, economically grounded way to measure AI’s ability to automate remote, computer-based work end-to-end; stakeholders lack reliable metrics to track progress and plan policy.<br>• Existing benchmarks over-focus on narrow skills and simplified environments (e.g., coding, basic web tasks), underrepresenting the breadth and complexity of real freelance work (design, audio/video, 3D/CAD, architecture) and diverse file formats.<br>• Prior evaluations rarely tie results to actual economic value or deliverables; they don’t reflect market-priced projects or enable metrics like dollars earned or cost deflation.<br>• Automated grading of rich, multimodal, multi-file deliverables is not yet feasible; rigorous, consistent manual evaluation infrastructure is needed to assess real project completion.<br>• Despite strong scores on knowledge/reasoning tests, current agents underperform on real projects (best automation rate 2.5%; see Figure 2/Table 1), highlighting a gap that must be measured to ground automation debates.<br>• Existing datasets are less complex and shorter than real freelance jobs; RLI matches Upwork-like completion times and diversity (see Figure 6), addressing representativeness limits.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Introduce the Remote Labor Index (RLI): a benchmark of 240 real freelance projects (brief + input files + human gold deliverable) across 23 Upwork categories, with economic metadata. Evaluate agents via a web-based manual review platform using Automation Rate (acceptability vs human), Elo pairwise comparisons, and economic metrics (dollars earned, “autoflation”) to track absolute and relative automation capacity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• RLI-Interact: Measuring AI Automation with Client Feedback Loops: Extend RLI to iterative, client-in-the-loop workflows (revisions, clarifications) to quantify gains from interaction and project management.<br>• AutoEval-RLI: LLM-Assisted, Multimodal Evaluation for Complex Deliverables: Develop reliable, audited LLM+tool pipelines to partially automate evaluation of rich artifacts while matching human-judged acceptance.<br>• TeamRLI: Benchmarking Multi-Agent and Human–AI Teamwork on Remote Projects: Assess coordination, division of labor, and handoffs on team-required tasks beyond single-agent settings.<br>• The Autoflation Index: Longitudinal Pricing of AI-Automated Digital Labor: Track and forecast cost reductions across RLI bundles over time, linking model releases to market-level price effects.<br>• Skill Maps for Automation: Decomposing RLI Outcomes into Cognitive Abilities: Correlate project success/failures with specific skills (verification, multimodal reasoning, memory, tool use) to guide model and scaffold design.<br>• Closing the Computer-Use Gap: Robust Scaffolds and Verification for End-to-End Project Execution: Build and test improved computer-use frameworks with self-checks, file integrity validation, and visual QA to reduce common failure modes.<br>• Training for Acceptance: Finetuning and RL from Human Project Reviews on RLI: Explore supervised finetuning and reinforcement learning from acceptance judgments to improve completion rates on end-to-end projects.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25867" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25867" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Multimodal Models (LMMs) are increasingly capable of answering medical questions that require joint reasoning over images and text, yet training general medical VQA systems is impeded by the lack of large, openly usable, high-quality corpora. We present MedVLSynther, a rubric-guided generator-verifier framework that synthesizes high-quality multiple-choice VQA items directly from open biomedical literature by conditioning on figures, captions, and in-text references. The generator produces self-contained stems and parallel, mutually exclusive options under a machine-checkable JSON schema; a multi-stage verifier enforces essential gates (self-containment, single correct answer, clinical validity, image-text consistency), awards fine-grained positive points, and penalizes common failure modes before acceptance. Applying this pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over 14,803 images spanning 13 imaging modalities and 28 anatomical regions. Training open-weight LMMs with reinforcement learning using verifiable rewards improves accuracy across six medical VQA benchmarks, achieving averages of 55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA, outperforming strong medical LMMs. A Ablations verify that both generation and verification are necessary and that more verified data consistently helps, and a targeted contamination analysis detects no leakage from evaluation suites. By operating entirely on open literature and open-weight models, MedVLSynther offers an auditable, reproducible, and privacy-preserving path to scalable medical VQA training data.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Scarcity of large, open, high-quality, and auditable training corpora for medical VQA, while modern benchmarks emphasize evaluation without providing training splits.<br>• Existing automatic generation is often text-only and visually ungrounded, producing ambiguous stems, non-parallel options, multiple-correct answers, and medically dubious content that harms learning.<br>• Many large datasets are closed due to privacy/licensing constraints, limiting openness, reproducibility, and community progress.<br>• Need for self-contained, clinically valid, single-answer, image–text consistent MC-VQA items with machine-checkable structure to enable scalable filtering and training.<br>• Absence of a transparent, privacy-preserving pipeline that converts open biomedical literature (figures+captions+in-text references) into reliable VQA training data.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>MedVLSynther uses a rubric-guided, context-aware generator–verifier pipeline: an open-weight LMM generates self-contained, 5-option MC-VQA from PubMed figures, captions, and references under a strict JSON schema, and a separate multi-stage verifier applies essential gates, fine-grained bonuses, and penalty checks to filter items by a normalized quality score. The resulting MedSynVQA dataset trains open LMMs via SFT and reinforcement learning with verifiable rewards to improve medical VQA performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• MedVLSynther-2: From Multiple-Choice to Verifiable Free-Form Medical VQA: Extend the generator–verifier rubric to open-ended answers with rationale scoring, uncertainty calibration, and automatic clinical fact-checking.<br>• Cross-Lingual MedSynVQA: Multilingual Generator–Verifier Pipelines over Global Biomedical Literature: Build multilingual, translation-aware generation and verification for non-English PMC articles with cross-lingual grounding.<br>• Video- and 3D-Aware MedVQA: Generator–Verifier Pipelines for Temporal and Volumetric Medical Imaging: Incorporate sequences (ultrasound cine, endoscopy video) and 3D volumes (CT/MRI) with spatiotemporal grounding and localization rubrics.<br>• Human-in-the-Loop Verifiable Medical VQA: Expert Preference Modeling and Active Auditing: Fuse clinician feedback into the verifier via preference learning and active sampling to target difficult modalities and failure modes.<br>• Causality- and Safety-Aware Verification for Clinical Decision Support: Add causal and guideline-based checks (e.g., next-step questions) and safety constraints to ensure clinically appropriate, non-harmful recommendations.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">FullPart: Generating each 3D Part at Full Resolution</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26140" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26140" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Part-based 3D generation holds great potential for various applications. Previous part generators that represent parts using implicit vector-set tokens often suffer from insufficient geometric details. Another line of work adopts an explicit voxel representation but shares a global voxel grid among all parts; this often causes small parts to occupy too few voxels, leading to degraded quality. In this paper, we propose FullPart, a novel framework that combines both implicit and explicit paradigms. It first derives the bounding box layout through an implicit box vector-set diffusion process, a task that implicit diffusion handles effectively since box tokens contain little geometric detail. Then, it generates detailed parts, each within its own fixed full-resolution voxel grid. Instead of sharing a global low-resolution space, each part in our method - even small ones - is generated at full resolution, enabling the synthesis of intricate details. We further introduce a center-point encoding strategy to address the misalignment issue when exchanging information between parts of different actual sizes, thereby maintaining global coherence. Moreover, to tackle the scarcity of reliable part data, we present PartVerse-XL, the largest human-annotated 3D part dataset to date with 40K objects and 320K parts. Extensive experiments demonstrate that FullPart achieves state-of-the-art results in 3D part generation. We will release all code, data, and model to benefit future research in 3D part generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Existing implicit part representations (latent vecsets) lack fine geometric detail and cannot precisely model spatial mappings, hindering texture generation and precise editing.<br>• Explicit voxel methods that share a single global grid starve small parts of voxels, causing low effective resolution and degraded details for thin or small components.<br>• Scale misalignment when exchanging information across parts of different actual sizes leads to artifacts and incoherent boundaries between neighboring parts.<br>• Maintaining global structural coherence while enabling independent part generation and manipulation remains challenging, especially for occluded or intricately connected components.<br>• There is a scarcity of large, high-quality, semantically consistent 3D part datasets; artist metadata is noisy, incomplete, and inconsistent.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>FullPart first samples part layouts via implicit vecset diffusion over bounding-box meshes, then generates each part in its own fixed full-resolution voxel grid and refines them into textured meshes. A center–corner positional encoding (voxel center + eight corners in a unified high-res global coordinate) with hybrid intra-/inter-part attention resolves cross-part scale misalignment and preserves global coherence.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Adaptive Resolution Allocation for Part-Aware 3D Generation: Learn to allocate per-part and per-region voxel resolution dynamically based on predicted complexity, while preserving cross-part alignment via scale-aware positional encodings.<br>• End-to-End Joint Diffusion of Layout and Geometry with Coherent Tokenization: Fuse box-layout and per-part voxel generation into a single diffusion process to reduce error propagation and improve layout–geometry co-optimization.<br>• Physics- and Articulation-Constrained Part Generation for Functional Assets: Integrate kinematic joints and physical constraints into part-aware diffusion to synthesize manipulable, simulation-ready 3D objects with valid motion ranges.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26474" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26474" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Self-improvement has emerged as a mainstream paradigm for advancing the reasoning capabilities of large vision-language models (LVLMs), where models explore and learn from successful trajectories iteratively. However, we identify a critical issue during this process: the model excels at generating high-quality trajectories for simple queries (i.e., head data) but struggles with more complex ones (i.e., tail data). This leads to an imbalanced optimization that drives the model to prioritize simple reasoning skills, while hindering its ability to tackle more complex reasoning tasks. Over iterations, this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew effect"--which ultimately hinders further model improvement and leads to performance bottlenecks. To counteract this challenge, we introduce four efficient strategies from two perspectives: distribution-reshaping and trajectory-resampling, to achieve head-tail re-balancing during the exploration-and-learning self-improvement process. Extensive experiments on Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks demonstrate that our methods consistently improve visual reasoning capabilities, outperforming vanilla self-improvement by 3.86 points on average.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Vanilla self-improvement for LVLMs exhibits performance bottlenecks and even degradation across iterations, despite initial gains.<br>• Self-generated training data becomes increasingly imbalanced (“Matthew effect”): easy head samples dominate while hard tail samples are underexplored.<br>• Reasoning quality collapses over iterations—average response length shrinks, especially for difficult problems, indicating truncated or missing chains-of-thought.<br>• Brute-force scaling of sampling number K is not cost-effective; early gains vanish and later iterations are insensitive to larger K.<br>• Existing approaches often rely on separate critic models or target text-only settings, incurring extra compute and lacking a principled head–tail re-balancing for visual reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Introduce head–tail re-balancing within the self-improvement loop via distribution-reshaping (Threshold Clipping to cap easy successes per query, Repeat-based Padding to equalize query frequency) and trajectory-resampling (Adaptive-weighted Resampling by fail rate, Guided Resampling starting from intermediate reasoning steps). These strategies counter the Matthew effect by reducing head dominance, enriching tail coverage, and improving efficiency without brute-force increases in K.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Tail-Aware RL for Multimodal Self-Improvement: Integrate tail-focused rewards and verifiers/self-correction into RL/DPO pipelines to directly optimize hard-case mastery while maintaining efficiency.<br>• CoT-Length Regularization for Preventing Reasoning Collapse in LVLMs: Learn adaptive length targets conditioned on difficulty to discourage overly short responses and preserve deep chains-of-thought on hard samples.<br>• Curriculum Scheduling of Head–Tail Re-balancing at Scale: Develop theory-driven, model-size–aware schedules that dynamically tune TC/RP/GR across iterations and datasets, with automatic difficulty prediction and active sampling.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26160" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26160" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Wearable devices such as smart glasses are transforming the way people interact with their surroundings, enabling users to seek information regarding entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG) plays a key role in supporting such questions, yet there is still no comprehensive benchmark for this task, especially regarding wearables scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn conversations across 13 domains, including 6.2K egocentric images designed to mimic captures from wearable devices. We carefully constructed the questions to reflect real-world scenarios and challenges, including five types of image-quality issues, six question types, varying entity popularity, differing information dynamism, and different conversation turns. We design three tasks: single-source augmentation, multi-source augmentation, and multi-turn conversations -- each paired with an associated retrieval corpus and APIs for both image-KG retrieval and webpage retrieval. Our evaluation shows that straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM single- and multi-turn QA, respectively, whereas state-of-the-art industry solutions have similar quality (32%/45%), underscoring ample room for improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K participants and 5K submissions, with winning solutions improving baseline performance by 28%, highlighting its early impact on advancing the field.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• There is no comprehensive benchmark to evaluate multi-modal Retrieval-Augmented Generation (MM-RAG) for wearable AI scenarios (egocentric vision, on-device assistance), where answers require external knowledge beyond the image.<br>• Existing VQA datasets mostly test common knowledge or visual reasoning, often Wikipedia-only or template-generated, and rarely cover factual, web-grounded, or multi-source retrieval, nor multi-turn conversations.<br>• Prior datasets use mostly high-quality images; real wearable captures are egocentric and imperfect (low-light, blur, truncation, occlusion, rotation), making entity recognition and retrieval substantially harder (e.g., image-search recall ~52% with full images; Fig. 3).<br>• There is no fair, standardized retrieval setup (shared corpora + APIs) to compare MM-RAG systems component-wise (entity recognition, OCR, query rewrite, retrieval, and grounded generation) under realistic noise.<br>• Real user queries span complex types (multi-hop, comparison, aggregation, reasoning), dynamic information, and long-tail (torso/tail) entities—settings where current systems struggle (straightforward RAG truthfulness ~32% single-turn and ~43% multi-turn; industry SOTA similar at ~32%/45%; Tables 4–5), highlighting the need for a targeted benchmark.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>CRAG-MM introduces a multimodal benchmark (6.5K single-turn QA and 2K multi-turn conversations across 13 domains) built on 7.9K images (79% egocentric), paired with mock image-KG and web-search APIs over controlled corpora (68K KG images, 800K webpages) to evaluate single-source, multi-source, and multi-turn MM-RAG. It standardizes evaluation via truthfulness/hallucination metrics and an LLM-as-judge, enabling fair, component-level assessment under realistic retrieval noise.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Robust MM-RAG for Egocentric and Low-Quality Images: Jointly learn adaptive cropping/zoom, deblurring/low-light enhancement, and fine-tuned visual encoders to boost image-KG recall and downstream grounding under wearable conditions.<br>• Conversation-Aware MM-RAG with Error Recovery and Clarification: Design agents that track dialogue state, detect uncertainty, ask targeted clarifying questions, and perform retrieval repair to reduce early stops and improve multi-turn truthfulness.<br>• Tail-Entity Grounding and Hallucination-Resistant Answering: Combine OCR/logo detection with long-tail entity linking, cross-modal query rewriting, and calibrated abstention/attribution to improve coverage on torso/tail entities and lower hallucinations across multi-source evidence.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">PORTool: Tool-Use LLM Training with Rewarded Tree</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26020" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26020" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current tool-use large language models (LLMs) are trained on static datasets, enabling them to interact with external tools and perform multi-step, tool-integrated reasoning, which produces tool-call trajectories. However, these models imitate how a query is resolved in a generic tool-call routine, thereby failing to explore possible solutions and demonstrating limited performance in an evolved, dynamic tool-call environment. In this work, we propose PORTool, a reinforcement learning (RL) method that encourages a tool-use LLM to explore various trajectories yielding the correct answer. Specifically, this method starts with generating multiple rollouts for a given query, and some of them share the first few tool-call steps, thereby forming a tree-like structure. Next, we assign rewards to each step, based on its ability to produce a correct answer and make successful tool calls. A shared step across different trajectories receives the same reward, while different steps under the same fork receive different rewards. Finally, these step-wise rewards are used to calculate fork-relative advantages, blended with trajectory-relative advantages, to train the LLM for tool use. The experiments utilize 17 tools to address user queries, covering both time-sensitive and time-invariant topics. We conduct ablation studies to systematically justify the necessity and the design robustness of step-wise rewards. Furthermore, we compare the proposed PORTool with other training approaches and demonstrate significant improvements in final accuracy and the number of tool-call steps.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Static, step-labeled tool-use training is costly and inflexible: it assumes ground-truth tool calls at every step, discourages exploration of alternative valid trajectories, and struggles on real-time/dynamic queries (Figure 1 example).<br>• Trajectory-level RL (e.g., GRPO) assigns uniform rewards across tokens/steps, losing step-wise credit assignment: informative intermediate steps in otherwise wrong trajectories are ignored, while redundant steps in correct ones are over-rewarded.<br>• Independent rollout generation prevents leveraging shared prefixes: there’s no mechanism to compare/credit sibling branches at forks or reuse information across partially shared trajectories.<br>• Formatting sensitivity is under-addressed: models fail due to schema/format errors in tool calls; existing rewards rarely balance correctness with formatting compliance, leading to brittle tool integration.<br>• Narrow tool coverage in prior work limits applicability: many methods focus on single or few tools (e.g., search/programming), lacking robustness to diverse, rigid schema tools and time-sensitive tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>PORTool trains a tool-use LLM with tree rollouts and step-wise rewards: multiple trajectories share prefixes and branch at forks, while an evaluation agent assigns outcome (+1/0/−1) and formatting rewards per step with discounting and an adaptive max/avg aggregator. The policy optimizes combined trajectory-relative and fork-relative advantages (with theoretically derived weights) via PPO-style token-level updates (no KL term), yielding efficient, well-formatted, and accurate tool-use across diverse tools.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Joint Evaluator–Policy Training for Tool-Use RL: Co-train the evaluation agent with the policy using calibrated, graded outcome signals to reduce bias/cost and improve reward reliability.<br>• Cost-Aware PORTool: Rewarded Trees under Latency and API Budgets: Incorporate tool latency and monetary cost into rollout branching and rewards to optimize accuracy–efficiency trade-offs.<br>• Uncertainty-Guided Tree Rollouts for Tool-Integrated Reasoning: Allocate branching adaptively using uncertainty/entropy and predicted tool failure likelihoods to focus exploration where it matters.<br>• Causal Credit Assignment over Tool Arguments and Sub-Calls: Extend fork-relative advantages to attribute credit within tool-argument fields and multi-call batches for finer-grained optimization.<br>• Scaling PORTool to Web-Scale, Dynamic Tool Ecosystems: Learn tool schemas on-the-fly, handle thousands of evolving APIs, and generalize to unseen tools with schema induction and retrieval.<br>• Multimodal PORTool: Rewarded Trees with Vision/Audio Tools: Generalize step-wise rewards and formatting checks to multimodal tools (OCR, vision, speech) for end-to-end multimodal agent training.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25132" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25132" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Designing enzyme backbones with substrate-specific functionality is a critical challenge in computational protein engineering. Current generative models excel in protein design but face limitations in binding data, substrate-specific control, and flexibility for de novo enzyme backbone generation. To address this, we introduce EnzyBind, a dataset with 11,100 experimentally validated enzyme-substrate pairs specifically curated from PDBbind. Building on this, we propose EnzyControl, a method that enables functional and substrate-specific control in enzyme backbone generation. Our approach generates enzyme backbones conditioned on MSA-annotated catalytic sites and their corresponding substrates, which are automatically extracted from curated enzyme-substrate data. At the core of EnzyControl is EnzyAdapter, a lightweight, modular component integrated into a pretrained motif-scaffolding model, allowing it to become substrate-aware. A two-stage training paradigm further refines the model's ability to generate accurate and functional enzyme structures. Experiments show that our EnzyControl achieves the best performance across structural and functional metrics on EnzyBind and EnzyBench benchmarks, with particularly notable improvements of 13\% in designability and 13\% in catalytic efficiency compared to the baseline models. The code is released at https://github.com/Vecteur-libre/EnzyControl.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Substrate-specific enzyme backbone design is under-served: enzymes must bind specific small molecules, preserve conserved catalytic sites, and maintain sensitive catalytic conformations that general protein design methods do not capture (pp. 1–2).<br>• Existing generators neglect functional site conservation or pick motifs arbitrarily, leading to poor catalytic function and high false positives; backbone quality and designability remain limiting (pp. 2–3).<br>• Most methods ignore the substrate during generation (use it only post hoc), so scaffolds cannot be tailored for binding or catalysis of the intended substrate (pp. 2–4).<br>• Lack of high-quality, experimentally grounded datasets/benchmarks with precise pockets and EC-aware evaluation hinders meaningful assessment (pp. 3–4).<br>• Need a stable way to inject substrate information into pretrained motif-scaffolders without degrading their strengths (pp. 4–6).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>EnzyControl augments a pretrained SE(3) flow-matching motif-scaffolder (FrameFlow) with an EnzyAdapter that projects Uni-Mol substrate embeddings and conditions each layer via cross-attention on MSA-annotated catalytic motifs, enabling substrate-aware scaffold generation. A two-stage regimen first trains only the projector/adapter, then LoRA-fine-tunes the base model with the adapter, trained on the curated EnzyBind dataset of ~11k enzyme–substrate complexes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• EnzyControl-Complex: Multimeric and Allosteric Enzyme Co-design with Substrate-Aware Conditioning: Extend EnzyControl to multi-chain complexes and allosteric systems, co-designing interfaces while preserving catalytic motifs.<br>• Pose-Control: End-to-End Enzyme–Substrate Co-generation with Differentiable Docking: Jointly generate enzyme backbones and ligand poses using differentiable docking or SE(3)-aware pose constraints for binding-competent conformations.<br>• Multi-Substrate EnzyControl: Cofactor and Multi-Substrate Conditioning via Adapter Aggregation: Aggregate adapter signals for multiple substrates/cofactors to design enzymes for multi-step or cofactor-dependent reactions.<br>• ActiveSite++: Fine-Grained Motif Supervision with EC Level-4 and Experimental Annotations: Replace MSA-only labels with curated catalytic residue annotations and finer EC constraints to improve functional fidelity and robustness to motif noise.<br>• Diversity-by-Design: Controlling the Designability–Novelty Trade-off in Enzyme Generators: Introduce controllable diversity mechanisms (entropy/coverage regularization, curriculum sampling) while maintaining high designability and function.<br>• Wet-Lab-in-the-Loop EnzyControl: Reinforcement Learning with High-Throughput Kinetics and Stability Assays: RL/Bayesian optimization using experimental kcat/KM, stability, and expression data to close the simulation-to-lab gap.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22282" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22282" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Harnessing publicly available, large-scale web data, such as street view and satellite imagery, urban socio-economic sensing is of paramount importance for achieving global sustainable development goals. With the emergence of Large Vision-Language Models (LVLMs), new opportunities have arisen to solve this task by treating it as a multi-modal perception and understanding problem. However, recent studies reveal that LVLMs still struggle with accurate and interpretable socio-economic predictions from visual data. To address these limitations and maximize the potential of LVLMs, we introduce CityRiSE, a novel framework for Reasoning urban Socio-Economic status in LVLMs through pure reinforcement learning (RL). With carefully curated multi-modal data and verifiable reward design, our approach guides the LVLM to focus on semantically meaningful visual cues, enabling structured and goal-oriented reasoning for generalist socio-economic status prediction. Experiments demonstrate that CityRiSE with emergent reasoning process significantly outperforms existing baselines, improving both prediction accuracy and generalization across diverse urban contexts, particularly for prediction on unseen cities and unseen indicators. This work highlights the promise of combining RL and LVLMs for interpretable and generalist urban socio-economic sensing.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Urban socio-economic indicators are vital for planning and SDGs, but traditional surveys are slow, costly, and temporally coarse.<br>• Web imagery (street view, satellite) is abundant, yet LVLMs struggle to map visuals to abstract socio-economic targets with accuracy and interpretability.<br>• Existing models are often city-specific with weak transfer to unseen cities and almost no cross-indicator generalization.<br>• Most approaches output black-box numbers without transparent, step-by-step reasoning or verifiable intermediate signals.<br>• Heterogeneous inputs/targets and misaligned training objectives (classification-style correctness vs. continuous regression) hinder stable, unified learning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>CityRiSE fine-tunes a vision-language model purely via reinforcement learning (GRPO) using verifiable rewards: a Keyword Reward to encourage semantically grounded, location-aware reasoning and a Regression Reward (exponentiated Huber loss) to align numeric accuracy. It couples these with auxiliary perceptual and general visual reasoning datasets to induce emergent, interpretable reasoning and strong generalization across unseen cities and indicators.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• CityRiSE-GeoFusion: Multisource Integration for Hard-to-Observe Indicators — Fuse imagery with POIs, mobility, and facility networks to improve indicators like healthcare accessibility.<br>• Stable-RL for Urban LVLMs: Curriculum and Off-Policy Optimization for GRPO — Develop stabilization and sample-efficiency techniques to mitigate RL training instability in LVLM reasoning.<br>• Counterfactual CityRiSE: Causal and What-if Reasoning for Urban Policy Simulation — Extend rewards and prompts for causal inference, enabling counterfactual analysis of interventions on socio-economic outcomes.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20976" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20976" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models have demonstrated remarkable reasoning capabilities across diverse natural language tasks. However, comparable breakthroughs in scientific discovery are more limited, because understanding complex physical phenomena demands multifaceted representations far beyond language alone. A compelling example is the design of functional materials such as MOFs-critical for a range of impactful applications like carbon capture and hydrogen storage. Navigating their vast and intricate design space in language-based representations interpretable by LLMs is challenging due to the numerous possible three-dimensional atomic arrangements and strict reticular rules of coordination geometry and topology. Despite promising early results in LLM-assisted discovery for simpler materials systems, MOF design remains heavily reliant on tacit human expertise rarely codified in textual information alone. To overcome this barrier, we introduce L2M3OF, the first multimodal LLM for MOFs. L2M3OF integrates crystal representation learning with language understanding to process structural, textual, and knowledge modalities jointly. L2M3OF employs a pre-trained crystal encoder with a lightweight projection layer to compress structural information into a token space, enabling efficient alignment with language instructions. To facilitate training and evaluation, we curate a structure-property-knowledge database of crystalline materials and benchmark L2M3OF against state-of-the-art closed-source LLMs such as GPT-5, Gemini-2.5-Pro and DeepSeek-R1. Experiments show that L2M3OF outperforms leading text-based closed-source LLMs in property prediction and knowledge generation tasks, despite using far fewer parameters. These results highlight the importance of multimodal approaches for porous material understanding and establish L2M3OF as a foundation for next-generation AI systems in materials discovery.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• MOF design requires understanding complex 3D periodic structures, coordination geometry, and topology that are not captured well by text-only representations.<br>• The design space is enormous and current workflows rely heavily on tacit expert knowledge, slowing discovery for critical applications (e.g., CO2 capture, H2 storage, water harvesting, drug delivery).<br>• CIF-based text inputs are verbose and poorly aligned with LLM tokenization, failing to encode symmetry, periodicity, and long-range correlations essential for porous materials.<br>• Existing LLM approaches are largely text-centric, focused on inorganic crystals, and underperform on MOF-specific tasks; MOF generators (e.g., MOFid-based) miss intrinsic 3D structure-function cues.<br>• There is no standardized structure–property–knowledge (SPK) dataset or benchmark coupling crystal structures with literature-derived functional knowledge for MOFs.<br>• Lack of multimodal integration (3D structure + textual knowledge) limits LLMs’ ability to ground structure-to-function reasoning and accurate property prediction.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>L2M3OF is a multimodal LLM that fuses a frozen crystal encoder (PMTransformer) with a lightweight compression–projection bridge to map structural embeddings into the LLM token space of a Qwen2.5 model, enabling joint reasoning over structure and text. It is instruction-tuned on a curated MOF-SPK dataset (structures, computed properties, and literature-derived knowledge) with joint multi-task training and a group-context strategy across property prediction, structure extraction, description generation, and Q&A.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Structurally-Grounded Generative MOF Design with L2M3OF: Extend the framework with a generative head (diffusion/RL) to propose stable MOFs conditioned on target multi-objective properties while enforcing symmetry/topology constraints.<br>• Autonomous Closed-Loop MOF Discovery Powered by Multimodal LLMs: Integrate L2M3OF with simulation toolchains and robotic synthesis for active learning—prioritize candidates, validate via high-throughput calculations/experiments, and iteratively expand SPK data.<br>• Trustworthy and Interpretable Multimodal LLMs for Crystalline Materials: Add uncertainty quantification, unit/normalization guards, and topology-aware attribution to explain structure–function links and reduce hallucinations in property and application predictions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ChartAB: A Benchmark for Chart Grounding & Dense Alignment</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26781" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26781" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Charts play an important role in visualization, reasoning, data analysis, and the exchange of ideas among humans. However, existing vision-language models (VLMs) still lack accurate perception of details and struggle to extract fine-grained structures from charts. Such limitations in chart grounding also hinder their ability to compare multiple charts and reason over them. In this paper, we introduce a novel "ChartAlign Benchmark (ChartAB)" to provide a comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting tabular data, localizing visualization elements, and recognizing various attributes from charts of diverse types and complexities. We design a JSON template to facilitate the calculation of evaluation metrics specifically tailored for each grounding task. By incorporating a novel two-stage inference workflow, the benchmark can further evaluate VLMs' capability to align and compare elements/attributes across two charts. Our analysis of evaluations on several recent VLMs reveals new insights into their perception biases, weaknesses, robustness, and hallucinations in chart understanding. These findings highlight the fine-grained discrepancies among VLMs in chart understanding tasks and point to specific skills that need to be strengthened in current models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• VLMs lack fine-grained grounding of chart components (data and visual attributes), leading to errors in measuring values, mapping encodings, and performing precise comparisons.<br>• Existing chart benchmarks focus largely on single-chart QA, missing dense grounding and multi-chart alignment, thus failing to diagnose perception and alignment failures.<br>• General-purpose VLMs are pretrained on natural images and struggle with structured layouts, numeric scaling, and text-heavy elements in charts, causing hallucinations and misinterpretations.<br>• There is no standardized, structured workflow or metrics to convert charts into machine-comparable representations and evaluate element-wise differences reliably.<br>• Robustness to real-world design variations (color palettes, legend positions, text styles) is under-evaluated despite being critical across plotting tools and styles.<br>• Chart-specialized models often fail general instruction following and cannot produce unified JSON outputs, hindering consistent evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>ChartAB introduces a benchmark and two-stage “ground-then-compare” pipeline: first ground each chart into structured CSV/JSON (data, color, legend, text style), then perform dense alignment by comparing grounded outputs. It provides task-specific templates and metrics (fuzzy key matching plus value precision, 3×3 legend-distance scoring, and robustness via variance across attribute variants) to assess grounding, alignment, and stability across diverse chart types.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Learning to Ground Charts: Pretraining VLMs with Chart-Structured Objectives: Design pretraining tasks that align visual encodings with tables/attributes to improve data and attribute grounding.<br>• ColorSense: Perceptually-Uniform Color Training for VLMs: Use perceptual color spaces and color-contrast curricula to boost fine-grained color recognition and alignment.<br>• FontNet: Typography-Aware Encoders for Text-Style Recognition in Charts: Integrate font/weight/size detectors into VLMs to enhance style grounding and alignment.<br>• Depth-Aware Chart Understanding for 3D and Polar Plots: Introduce spatial-depth priors and geometry-aware encoders to improve reasoning on 3D bars, radar, and rose charts.<br>• AlignFormer: A Differentiable JSON-Alignment Module for End-to-End Ground-Compare Training: Replace post-hoc comparison with a trainable alignment layer to reduce grounding-to-alignment error propagation.<br>• RobustChart: Attribute-Invariant Training for Stable Alignment Across Design Variations: Learn invariances to legend placement, text styles, and color schemes to improve robustness metrics.<br>• ChartToolformer: Tool-Augmented VLMs for Table Extraction, Code Execution, and Plot Verification: Combine OCR, plotting tool use, and self-verification loops to curb hallucinations and improve QA.<br>• Hallucination-Resistant Chart QA with Evidence Attribution: Calibrate answers with uncertainty estimates and require citation of grounded elements to improve reliability and interpretability.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25364" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25364" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This work investigates whether small-scale LMs can benefit from instruction tuning. We compare conversational and question-answering instruction tuning datasets, applied either in a merged or sequential curriculum, using decoder-only models with 100M and 140M parameters. Evaluation spans both fine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and psycholinguistic correlation) settings. Results show that instruction tuning yields small but consistent gains in fine-tuning scenarios, with sequential curricula outperforming merged data; however, improvements do not consistently transfer to zero-shot tasks, suggesting a trade-off between interaction-focused adaptation and broad linguistic generalization. These results highlight both the potential and the constraints of adapting human-inspired learning strategies to low-resource LMs, and point toward hybrid, curriculum-based approaches for enhancing generalization under ecological training limits.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Determine whether instruction tuning benefits BabyLM-scale models trained on ecologically realistic (child-scale) input, addressing the gap where most instruction-tuning successes are shown only for much larger LMs.<br>• Identify which instruction data is more effective under low-resource constraints—conversational (dialogue) vs. open-ended question answering—and whether data order (merged vs. sequential curriculum) matters.<br>• Understand trade-offs between interaction-focused adaptation and broad linguistic generalization, since small models risk losing zero-shot grammatical/world-knowledge competence after instruction tuning.<br>• Overcome limitations of prior BabyLM work that either used encoder-only MLM or RLHF without formal instruction structures, and lacked controlled comparisons of data type/order and comprehensive evaluations.<br>• Address evaluation mismatches: common benchmarks emphasize classification and log-likelihood, potentially missing gains in interactive competence; propose the need for ecologically valid conversational data and interaction-centric evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Train two small decoder-only LLaMA-style models (100M and 140M) on ~91M-word BabyLM pretraining data, then instruction-tune them with two datasets: processed Switchboard conversational prompt–reply pairs and LLaMA-augmented Simple Wikipedia QA, using target-only loss. Compare merged vs. sequential curricula (Switch→Wiki and Wiki→Switch) and evaluate via (Super)GLUE fine-tuning and zero-shot tests (BLiMP, EWoK, WUGs, entity tracking, psycholinguistic correlations), aggregating cross-task performance with z-scores.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Curriculum Instruction Tuning for BabyLMs: A Systematic Study of Task Order, Mixing Ratios, and Retention under Fixed Data Budgets: Explore dynamic/sequential curricula, weighting, and pacing to maximize generalization while preserving interactive skills.<br>• Hybrid Training for Small LMs: Combining Language Modeling, Multi-task Supervision, and Instruction Tuning under Ecological Constraints: Assess whether blending LM objectives with supervised and instruction signals mitigates zero-shot degradation.<br>• Architectures for Low-Resource Generalization: Retrieval, Recurrence, and MoE Variants for BabyLM-scale Instruction Tuning: Test architectural tweaks (retrieval-augmented memory, lightweight MoE, recurrence) that preserve knowledge while enabling interaction.<br>• Beyond Switchboard: Building an Ecologically Diverse Conversational Corpus for Child-Scale Model Adaptation: Collect and evaluate broader, context-rich conversational data to improve interaction and transfer without scaling compute.<br>• Measuring What Matters: Interactive Benchmarks for BabyLMs Aligned with Instruction-Tuned Abilities: Design dialogue- and task-based evaluations (grounded QA, multi-turn reasoning, feedback use) that better capture interaction gains than log-likelihood metrics.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">POWSM: A Phonetic Open Whisper-Style Speech Foundation Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24992" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24992" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks. POWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing. Our model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR. Our training data, code and models are released to foster open science.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Fragmented phonetic tasks and tooling: ASR, phone recognition (PR), grapheme-to-phoneme (G2P) and phoneme-to-grapheme (P2G) are developed in isolation with task-specific architectures and datasets; a unified phonetic foundation model is missing (Figure 1).<br>• Lack of explicit phonetic supervision and openness in prior foundation models: Whisper/OWSM excel at ASR but do not model phones explicitly and rely on closed data, limiting cross-lingual transfer and posing bias/replicability issues; prior PR systems are specialized (often language-specific inventories or single-task) and generalize poorly to unseen languages (Tables 2 and 4).<br>• Text-only G2P cannot capture socio-phonetic variation: enforcing one-to-one orthography→phoneme mappings misses dialectal/L2/varietal realizations; audio-guided G2P/P2G is needed to ground pronunciations in actual acoustics and enable low-resource scenarios.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>POWSM is a unified attention-based encoder–decoder (E-Branchformer encoder + Transformer decoder) trained from scratch on IPAPack++, where every utterance is reformatted into four prompt-conditioned tasks (PR, ASR, audio-guided G2P, P2G) using language and task tokens plus optional text/phone prompts. A hybrid CTC/attention objective aligns the encoder to simplified PanPhon phone tokens (without suprasegmentals) while the decoder generates phones or graphemes, yielding a single 350M-parameter model that seamlessly converts between audio, phones, and text.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• PhonoLM: Probing Phonetic Universals and Phonotactics in a Unified Phonetic Foundation Model: Use POWSM’s decoder as a phoneme-level language model to test hypotheses about universals and phonotactic constraints across languages.<br>• Adapt-on-the-Fly: Test-Time and In-Context Socio-Phonetic Adaptation for Multilingual Phone Models: Develop unsupervised test-time adaptation, in-context learning, and mechanistic interpretability methods to better handle dialectal and L2 variation.<br>• Early-Exit Phonetic Decoding: Preserving Fine-Grained Variation in Multitask Speech Models: Explore early exiting from encoder layers to retain phonetic detail and reduce decoder-driven normalization while speeding inference.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Performance Trade-offs of Optimizing Small Language Models for E-Commerce</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21970" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21970" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) offer state-of-the-art performance in natural language understanding and generation tasks. However, the deployment of leading commercial models for specialized tasks, such as e-commerce, is often hindered by high computational costs, latency, and operational expenses. This paper investigates the viability of smaller, open-weight models as a resource-efficient alternative. We present a methodology for optimizing a one-billion-parameter Llama 3.2 model for multilingual e-commerce intent recognition. The model was fine-tuned using Quantized Low-Rank Adaptation (QLoRA) on a synthetically generated dataset designed to mimic real-world user queries. Subsequently, we applied post-training quantization techniques, creating GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. Our results demonstrate that the specialized 1B model achieves 99% accuracy, matching the performance of the significantly larger GPT-4.1 model. A detailed performance analysis revealed critical, hardware-dependent trade-offs: while 4-bit GPTQ reduced VRAM usage by 41%, it paradoxically slowed inference by 82% on an older GPU architecture (NVIDIA T4) due to dequantization overhead. Conversely, GGUF formats on a CPU achieved a speedup of up to 18x in inference throughput and a reduction of over 90% in RAM consumption compared to the FP16 baseline. We conclude that small, properly optimized open-weight models are not just a viable but a more suitable alternative for domain-specific applications, offering state-of-the-art accuracy at a fraction of the computational cost.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Deploying state-of-the-art commercial LLMs in e-commerce incurs high compute, latency, and ongoing costs, with added environmental footprint (energy/water/carbon) and vendor lock-in/privacy risks (Section 1).<br>• Existing general-purpose models are overkill for narrow, structured intent tasks; there is a need to validate whether small open-weight models can match accuracy with far lower resource use (Table 1 on page 6 shows parity at 0.99 EM).<br>• Robust multilingual, structured JSON extraction datasets are scarce; enforcing strict schema adherence in noisy, code-switched user inputs remains challenging (Section 3.1).<br>• Quantization promises efficiency but exhibits unclear, hardware-dependent trade-offs; e.g., 4-bit GPTQ reduced VRAM by 41% yet slowed inference by 82% on NVIDIA T4 (Figures 2–4, pages 7–9), while GGUF on CPU achieved up to 18× speedups (Figures 5–7, pages 9–10).<br>• Practitioners lack a hardware-aware evaluation of speed, memory, and energy-per-token to guide deployment choices across GPU/CPU and bit-depths (Sections 4.3–4.5).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Build a hardware-aware specialization pipeline: generate a noisy, multilingual synthetic dataset via metaprompting with GPT-4.1; fine-tune Llama 3.2 1B using QLoRA (NF4 4-bit base; LoRA r=8, α=16) focused on JSON exact match, then merge adapters and apply post-training quantization (GPTQ 4-bit for GPU; GGUF 3/4/5-bit for CPU). Evaluate exact-match accuracy and profile throughput, memory, and energy on NVIDIA T4 GPU and Ryzen 7 CPU to surface accuracy–efficiency trade-offs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• From Synthetic to Real: Field-Validating a 1B E‑commerce Intent Model on Production Traffic: Evaluate robustness, schema adherence, and drift on large, anonymized real-world queries; quantify stability of low-bit variants under noisy, code-switched inputs.<br>• Hardware‑Aware Quantization on Modern GPUs: INT4/FP8 Kernels for Small LLMs: Benchmark GPTQ/AWQ versus FP16 on Ampere/Hopper with native low-precision kernels to measure true latency and energy gains and eliminate dequantization overheads seen on T4.<br>• Beyond Intent: A Suite of Specialized Small Models for End‑to‑End E‑commerce Agents: Extend fine-tuning to recommendation, order status, returns, and tool-use; compare PEFT methods (QLoRA vs DoRA) and quantizers (GPTQ/AWQ) on ShoppingBench-style multi-step workflows with strict JSON guarantees.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 10</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>← 上一页</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button></span>
            <button id="next-btn" onclick="changePage(1)">下一页 →</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-10-31 23:11:02 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 10;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>