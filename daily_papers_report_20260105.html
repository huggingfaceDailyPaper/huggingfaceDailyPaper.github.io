<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - January 05, 2026</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">January 05, 2026</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.00393" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.00393" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ 4D world modeling methods lack scalability, relying on expensive multi-view dynamic data or static scenes that limit generalization and versatility.<br>â€¢ Training pipelines typically depend on heavy offline preprocessing (e.g., depth estimation, pre-reconstruction, 3D detection), incurring high compute/storage costs and blocking online augmentations.<br>â€¢ Existing feed-forward 4DGS models often assume known camera poses and uni-directional temporal modeling, making them ill-suited for unposed, in-the-wild monocular videos.<br>â€¢ There is a need to convert low-quality novel-view renderings into high-quality, spatiotemporally coherent videos while preserving precise camera trajectory control.<br>â€¢ Efficiently leveraging cheap, diverse monocular videos at scale (up to millions of clips) for a unified reconstruction+generation pipeline remains an open challenge.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>NeoVerse introduces a pose-free, feed-forward 4D Gaussian Splatting reconstructor built on VGGT with bidirectional motion encoding, predicting 4DGS (including forward/backward linear and angular velocities) from sparse key frames and interpolating Gaussians over time for efficient on-the-fly reconstruction from monocular videos. It pairs simulated degraded novel-view renderings (via visibility-based Gaussian culling and average geometry filtering) with a control-conditioned video diffusion model to learn high-quality, camera-controllable generation in a scalable pipeline.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Degradation Learning for Monocular 4D Generation: Replace handcrafted culling and averaging with a learnable degradation simulator that adapts to scene geometry and motion, improving conditioning realism and generation quality.<br>â€¢ Pose-Free Multi-Clip Aggregation for Complete 4D Scene Reconstruction: Align and fuse multiple monocular clips without pose annotations using robust global motion tracking and scene-centric constraints to build comprehensive 4D scene models.<br>â€¢ Physics-Guided Nonlinear Motion Encoding in 4DGS: Incorporate physical priors and nonlinear dynamics into bidirectional motion modeling to reduce drift, enhance long-horizon consistency, and enable stable slow-motion and long-trajectory generation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24615" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24615" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\%) and GAIA (72.8\%) using open-weight models. Our automated generation pipeline achieves over 81\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\% and +5.4\% respectively. Moreover, our Agent RL training achieves 40\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\% and 21\% on Maths and general/multi-hop QA benchmarks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ High configuration costs: building agents requires manual tool integration, prompt engineering, and custom code/APIs, creating a barrier to scalability<br>â€¢ Static capabilities: deployed agents struggle to adapt to dynamic environments without costly fine-tuning<br>â€¢ Limitations of existing optimization: manual prompt tuning is unreliable; SFT/RL suffer from data scarcity, high compute, concurrency bottlenecks, and training instability (e.g., entropy explosion in long-horizon tasks)<br>â€¢ Lack of modular, reusable, structured configurations to enable automated synthesis and rapid iteration</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Youtu-Agent introduces a modular, YAML-configured framework that decouples Environment, Tools, and Agent, with two automated generation paradigms (Workflow and Meta-Agent) to synthesize tools, prompts, and configurations. It further provides a hybrid optimization stack: Agent Practice for in-context experience accumulation without parameter updates, and Agent RL integrated with distributed training for scalable, stable reinforcement learning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning-to-Optimize Context Managers for Long-Horizon Agents: Train policies that adaptively prune and summarize context to balance token efficiency and task success<br>â€¢ Safety-Aware Automated Tool Synthesis for LLM Agents: Integrate static/dynamic analysis, test generation, and sandboxing into Meta-Agent pipelines to ensure correctness and security of synthesized tools<br>â€¢ Unified Benchmarks and Protocols for Continuous Agent Evolution: Create standardized datasets and evaluation protocols for in-context Practice and end-to-end RL across web, coding, and multi-hop QA domains</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.00664" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.00664" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing talking-head models yield one-way, non-interactive outputs that fail to mirror usersâ€™ verbal and non-verbal cues (e.g., nods, smiles, laughter), reducing engagement.<br>â€¢ High latency from non-causal motion generation that requires future context (seconds of audio), preventing immediate reactions to live inputs.<br>â€¢ Difficulty learning expressive, diverse reactions without labeled data; training corpora are low-variance, leading to stiff, under-expressive motions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Avatar Forcing introduces causal diffusion forcing with keyâ€“value caching to generate head motion latents from real-time multimodal user inputs (audio, motion) for low-latency, reactive avatars. It augments training with label-free direct preference optimization by creating synthetic less-preferred samples via dropping user conditions to encourage expressiveness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Personalized Causal Avatar Forcing for User-Specific Interaction Styles: Learn lightweight style adapters from few-shot user data to mirror individualized nodding, gaze, and expressiveness while preserving sub-500 ms latency.<br>â€¢ Multi-Speaker Causal Diffusion Forcing for Turn-Taking and Gaze Control: Extend to group conversations by attending to multiple audio/motion streams to model turn-taking, gaze shifts, and active listening in multi-party settings.<br>â€¢ Real-Time Full-Body Avatar Forcing: Coordinated Headâ€“Faceâ€“Gesture Generation via Causal Latent Diffusion: Jointly synthesize synchronized facial expressions, head motion, and upper-body gestures for richer, real-time communication.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24330" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24330" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing VLM agents are text-centric and make isolated tool calls, lacking seamless interleaving of planning, reasoning, and multi-tool execution.<br>â€¢ Inability to coordinate web search (text/image) with fine-grained pixel-level operations (e.g., cropping) limits performance on knowledge-intensive and high-resolution visual tasks.<br>â€¢ Current RL for reasoning (e.g., GRPO/GSPO) is unstable on heterogeneous, multi-turn, tool-augmented trajectories with sequence-level rewards.<br>â€¢ Evaluation gap: no benchmark tailored to high-resolution, search-driven multimodal questions; existing datasets favor low-res or single-tool settings.<br>â€¢ RAG-style systems often follow fixed workflows and over-retrieve, underutilizing adaptive reasoning and tool selection.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SenseNova-MARS trains a VLM to interleave text search, image search, and image crop via a two-stage pipeline (small cold-start SFT ~3k samples, then RL) using Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) for stable sequence-level optimization over heterogeneous multi-tool trajectories. The work also introduces HR-MMSearch, a high-resolution, search-oriented benchmark to evaluate agentic multimodal reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ MARS-X: Open-World Tool Discovery and Adaptation for Multimodal Agents: Enable agents to autonomously discover, verify, and integrate new tools (e.g., OCR, code, charts) via meta-learning and plug-in interfaces.<br>â€¢ DenseReward-MARS: Stepwise Reward Modeling for Fine-Grained Multimodal Tool Use: Replace sparse sequence-level judging with calibrated, step-level rewards (reasoning, tool choice, grounding) to improve credit assignment and stability.<br>â€¢ Video-MARS: Temporal Agentic Reasoning with Search and Spatiotemporal Cropping: Extend MARS to video by learning when to search and where/when to crop across frames for knowledge-intensive, temporal visual queries.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24271" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24271" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise ell_1 advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ MLLMs over-rely on language priors, causing visual ungrounded hallucinationsâ€”especially on counterfactual videos that violate commonsense or physical laws.<br>â€¢ Severe modality imbalance (abundant text vs. scarce, diverse video data) weakens visual grounding and temporal reasoning.<br>â€¢ Collecting and annotating counterfactual video data at scale is expensive and difficult; naturally occurring counterfactual events are rare.<br>â€¢ Automation paradox: current MLLMs fail to perceive counterfactual phenomena reliably, hindering automatic data generation and verification.<br>â€¢ Existing remedies are limited: training-free contrastive decoding needs extra negative views, raises inference cost, is hyperparameter-sensitive and unstable for temporal tasks; training-based datasets demand costly manual curation and under-cover rare edited scenarios.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DualityForge uses controllable diffusion-based video editing (visual, semantic, and commonsense anomaly pipelines) with embedded structured context to synthesize counterfactual videos and auto-generate high-quality QA pairs, yielding paired originalâ€“edited data in the DualityVidQA dataset. DNA-Train is a two-stage SFT+RL regime that enforces pair-wise contrastive QA and applies l1-normalized advantages per realâ€“counterfactual pair to stabilize optimization and compel visual grounding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Counterfactual Reward Shaping for Video MLLMs: Learn reward models that score visual grounding and counterfactual consistency (combining human preference and automated metrics) to further reduce hallucinations during RL.<br>â€¢ Cross-Modal Duality: Extending Counterfactual Paired Training to Audio and 3D: Generate paired counterfactuals across audio tracks and 3D/depth modalities to strengthen multimodal grounding and evaluate hallucinations beyond RGB video.<br>â€¢ Counterfactual Temporal Attribution for Grounded Video Reasoning: Use controlled interventions from DualityForge to quantify frame- and segment-level influence on answers, yielding causal attribution tools that diagnose and improve temporal grounding.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.00796" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.00796" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Dynamic 3D scene reconstruction from monocular videos demands both high-frequency texture fidelity and temporally smooth motion; standard Gaussian primitives act as low-pass filters and blur fine details.<br>â€¢ Existing Gabor-like frequency modulation improves detail but introduces energy instability and intensity artifacts due to fixed amplitudes and static-frequency designs; most target static scenes rather than dynamic videos.<br>â€¢ Many prior methods lack explicit temporal continuity constraints, leading to motion discontinuities, oscillations, and geometric tearing during interpolation, especially under fast motion and occlusions.<br>â€¢ Monocular pipelines often rely on camera pose estimation or implicit models, complicating optimization; unstable early-stage initialization yields flicker and poor convergence.<br>â€¢ Achieving simultaneous detail and continuity is critical for VR/AR, film production, and downstream tasks (frame interpolation, depth consistency, video editing, stereo synthesis), directly impacting realism and usability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>AdaGaR uses Adaptive Gabor primitives in an orthographic camera coordinate system, extending Gaussians with learnable frequency weights and an energy compensation term to capture high-frequency textures while remaining energy-stable and able to degrade to standard Gaussians. Temporal evolution is modeled with Cubic Hermite spline trajectories featuring a monotone gate and Temporal Curvature Regularization, trained via RGB/SSIM, optical-flow, and scaleâ€“shift-invariant depth losses and initialized by fusing depth, point tracking, and masks for a temporally coherent point cloud.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Continuous Frequency Banks for Adaptive Gabor Primitives: Replace fixed base frequencies with learned, spatially and temporally varying frequency/phase banks to capture diverse textures and motions while preserving energy stability.<br>â€¢ Perspective-Aware AdaGaR: Joint Camera Pose and Dynamic Scene Reconstruction: Extend the orthographic formulation to perspective cameras and jointly optimize camera motion and Adaptive Gabor primitives for broader real-world deployments.<br>â€¢ Self-Supervised AdaGaR without External Priors: Remove dependence on depth/tracking foundation models by leveraging cycle-consistency, photometric warping, and geometric constraints for end-to-end training from raw monocular videos.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Nested Learning: The Illusion of Deep Learning Architectures</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24695" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24695" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite the recent progresses, particularly in developing Language Models, there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improve, and find effective solutions. In this paper, we present a new learning paradigm, called Nested Learning (NL), that coherently represents a machine learning model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own context flow. Through the lenses of NL, existing deep learning methods learns from data through compressing their own context flow, and in-context learning naturally emerges in large models. NL suggests a philosophy to design more expressive learning algorithms with more levels, resulting in higher-order in-context learning and potentially unlocking effective continual learning capabilities. We advocate for NL by presenting three core contributions: (1) Expressive Optimizers: We show that known gradient-based optimizers, such as Adam, SGD with Momentum, etc., are in fact associative memory modules that aim to compress the gradients' information (by gradient descent). Building on this insight, we present other more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Learning Module: Taking advantage of NL's insights on learning algorithms, we present a sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: We present a new formulation for memory system that generalizes the traditional viewpoint of long/short-term memory. Combining our self-modifying sequence model with the continuum memory system, we present a continual learning module, called Hope, showing promising results in language modeling, knowledge incorporation, and few-shot generalization tasks, continual learning, and long-context reasoning tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Deep architectures (stacking layers) do not necessarily increase computational depth or algorithmic expressivity; training can converge suboptimally and capacity gains saturate for some parameter classes.<br>â€¢ Large Language Models are largely static post-deployment, limited to in-context learning, and struggle with continual learning and catastrophic forgetting; existing fixes are costly, external, or brittle.<br>â€¢ Current models lack multi-time-scale updates and distributed, reusable memory; Transformers effectively operate at extreme update frequencies (fast attention, frozen MLP), impeding online consolidation.<br>â€¢ The conventional separation of architectures and optimizers obscures their interdependence; common optimizers compress gradients with limited memory expressivity and are not tailored to architecture-specific contexts.<br>â€¢ There is no unifying framework connecting meta-learning, in-context learning, recurrent memory, hypernetworks, and learned optimizers, hampering principled continual learner design.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Nested Learning models a system as ordered, multi-level associative-memory optimization problems with explicit update frequencies and knowledge-transfer mechanisms, reframing both architectures and optimizers as context compressors. On this basis, the paper proposes more expressive optimizers (Delta Gradient Descent, Multi-scale Momentum Muon), a self-modifying sequence model, and a Continuum Memory System, combined into the Hope module for continual learning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Architecture-Specific Optimizers via Nested Learning: Tailor optimizer memory and learning rules to the gradient/token statistics of given architectures to improve compression, stability, and generalization.<br>â€¢ Higher-Order In-Context Learning with Multi-Level Continuum Memory: Leverage deeper nested levels and multi-frequency memory systems to achieve robust continual learning, latent computation, and long-context reasoning.<br>â€¢ Unified Theory of Computational Depth in Nested Systems: Formalize expressivity and algorithmic depth gains from stacking levels (update-frequency hierarchies) versus layers, including bounds and equivalences to classical models.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Deep Delta Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.00417" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.00417" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector k(X) and a gating scalar Î²(X). We provide a spectral analysis of this operator, demonstrating that the gate Î²(X) enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ The identity shortcut in residual networks imposes a strictly additive inductive bias with a fixed Jacobian (identity), limiting the ability to model complex, non-monotonic, and oscillatory state transitions.<br>â€¢ Existing methods lack explicit, data-dependent control over the spectrum (eigenvalues and their signs) of layer-wise transition operators, which is needed to capture behaviors like negative eigenvalues.<br>â€¢ There is a need to preserve stable training characteristics while enabling dynamic, step-size-like gating that can erase outdated information and inject new features in a principled way.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Deep Delta Learning introduces a Delta Residual Block that modulates the identity shortcut via a learned rank-1 generalized Householder operator parameterized by a direction k(X) and a scalar gate Î²(X). This gate smoothly interpolates between identity, orthogonal projection, and reflection, reframing the residual update as a synchronous rank-1 injection that controls the layer spectrum and information overwrite/write.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Rank-k Delta Learning: Extending the Delta Operator Beyond Rank-1: Generalize the rank-1 perturbation to rank-k structures to increase expressivity while studying stabilityâ€“expressivity trade-offs.<br>â€¢ Delta Residual Transformers for Sequence Modeling: Integrate Delta Residual Blocks into transformer architectures to evaluate spectral control, memory management, and performance on long-range dependencies.<br>â€¢ Delta Neural ODEs: Continuous-Time Spectral Control for Deep Dynamics: Formulate a continuous-time variant where Delta Operators govern state evolution in Neural ODEs, enabling principled control of dynamics (including oscillatory behavior) with theoretical stability guarantees.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.00747" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.00747" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Post-training pipelines that optimize a single scalar (correctness) cause semantic entropy collapse, yielding reasoning monocultures that harm creative problem-solving.<br>â€¢ Creativity (a diverse portfolio of high-utility reasoning strategies) is essential for OOD generalization; single templates fail under novel conditions.<br>â€¢ Existing heuristics (KL penalties, entropy bonuses, top-k/Boltzmann sampling) either indiscriminately limit drift, trade off quality, or cannot recover strategies whose probabilities vanished.<br>â€¢ There is no unified theoretical framework that predicts algorithm-specific collapse modes (STaR, GRPO, DPO) and provides provable, principled remedies.<br>â€¢ Stochastic mini-batch noise accelerates fixation, and entropy alone is blind to semantic novelty, failing to ensure structured diversity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Distributional Creative Reasoning (DCR) optimizes a variational objective over the full distribution of solution traces, adding a strictly concave diversity energy that combines entropy with a kernel coverage penalty, and models training via Shahshahani gradient flow. This unifies STaR/GRPO/DPO, predicts their collapse modes, and guarantees convergence to a unique, stable, diverse equilibrium with actionable design levers (creativity kernel and weights, gated to correct traces).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Creativity Kernels for Structured Reasoning Diversity: Meta-learn or contrastively train semantic kernels that capture distinct reasoning strategies, with PSD constraints and verifier gating, to tailor diversity pressure per task.<br>â€¢ Curriculum-Scheduled Diversity: Adaptive Î», Î±, Î² for OOD Robustness: Design schedulers that adjust diversity weights based on correctness/diversity signals to avoid early collapse and maximize OOD generalization without sacrificing accuracy.<br>â€¢ Subquadratic DCR: Scalable Kernel Coverage via Sketching and Sparse Gating: Develop NystrÃ¶m/LSH-based estimators and sparse neighborhood kernels to approximate Q[p] with subquadratic cost while preserving unbiased gradients at scale.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Diversity or Precision? A Deep Dive into Next Token Prediction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22955" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22955" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode. To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning. By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision. Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ How the pre-trained token distribution shapes the exploration space and effectiveness of downstream RL for reasoning is poorly understood and left implicit by standard cross-entropy training.<br>â€¢ Cross-entropy treats next-token prediction as pure supervision, uniformly suppressing all negatives and offering no handle to balance diversity vs. precision for RL-friendly initialization.<br>â€¢ Prevailing intuition that higher distribution entropy aids exploration lacks systematic validation in LLM RL settings.<br>â€¢ Existing weighted CE variants (e.g., label smoothing, focal loss) do not provide rank-aware control over negative tokens (head vs. tail) nor a principled on-policy RL framing.<br>â€¢ There is no unified objective that bridges pre-training and on-policy RL to study how reward design during pre-training impacts long chain-of-thought reasoning and RL stability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Reframe next-token prediction as a single-step on-policy RL problem, showing cross-entropy is a special case of policy gradient with an intrinsic reward. Propose a generalized reward-shaping objective with a positive reward scaling factor (Î²) to control global entropy on ground-truth tokens and rank-aware Top-K negative shaping (ËœÎ» for high-ranking negatives, Ë†Î» for tail negatives) to tune local entropy, thereby reshaping the pre-trained distribution to yield better RL exploration and reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Entropy Scheduling for Next-Token RL Pre-Training: Learn curricula that dynamically adjust Î² and rank-aware negative shaping based on uncertainty to optimize RL readiness.<br>â€¢ Learning Headâ€“Tail Negative Shaping via Meta-Optimization: Meta-learn ËœÎ», Ë†Î», and k across tasks to automatically discover distributional priors that maximize downstream RL performance.<br>â€¢ Integrating Verifiable Token-Level Rewards into Pre-Training: Combine intrinsic rewards with sparse external checks (e.g., unit tests, math verifiers) to align precision priors with task-grounded signals.<br>â€¢ Uncertainty-Guided Compute Allocation with Token-Level Rewards: Couple reward shaping with adaptive inference/training compute (loop/latent reasoning) to allocate more steps at high-entropy forks.<br>â€¢ Theoretical Analysis of Precision Priors in Sequence RL: Provide formal guarantees on exploration, entropy dynamics, and sample efficiency when initializing RL from precision-oriented token distributions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Fast-weight Product Key Memory</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.00671" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.00671" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, "fast-weight" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Long-context sequence models face a core trade-off: softmax attention offers unbounded storage but incurs quadratic cost, while linear attention is efficient but limited to fixed storage capacity.<br>â€¢ Product Key Memory (PKM) provides huge sparse memory at low cost but is a slow-weight channel mixer, remaining static at inference and unable to memorize new key-value pairs from current inputs.<br>â€¢ There is no existing token mixer that combines large storage, sub-quadratic compute, and on-the-fly memorization/retrieval (episodic memory) across timesteps.<br>â€¢ Practically, resolving this tension is crucial for long-context modeling, reducing perplexity on extended sequences, and enabling continual/personalized capabilities without retraining.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>FwPKM converts PKM into a fast-weight episodic memory by updating its key and value matrices during the forward pass via local, chunk-level gradient descent on an MSE reconstruction objective, achieving one-step rewriting (lr=1.0, 0.5 loss factor) while retaining efficient product-key top-k retrieval. Gradients are shaped by per-row aggregation (1/N_read) and gating to balance competitive writes, enabling dynamic memorization at both training and inference with sub-quadratic cost.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Episodic Fast-weight Product Key Memory for Multimodal Long-Context Modeling: Extend FwPKM to bind and retrieve cross-modal (text, vision, audio) key-value pairs for long-horizon reasoning.<br>â€¢ Stability, Safety, and Forgetting in Fast-weight Episodic Memories: Analyze interference, robustness, privacy, and gating strategies to ensure safe, stable test-time updates in FwPKM.<br>â€¢ Scaling Hybrid Attentionâ€“FwPKM Architectures to Million-Token Contexts: Integrate FwPKM with sparse/linear attention and study scaling laws, compute-accuracy trade-offs, and retrieval generalization beyond 128K tokens.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">InfoSynth: Information-Guided Benchmark Synthesis for LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.00575" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.00575" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Creating new reasoning/code benchmarks is expensive and slow when done manually, and LLM-judge-based generation can yield erroneous/invalid problems.<br>â€¢ Existing benchmarks often contaminate LLM training data, inflating scores; there is a need for benchmarks that are genuinely novel and diverse to mitigate overfitting and measure true generalization.<br>â€¢ Reliable, efficient metrics to quantify benchmark novelty and diversity are lacking; prior assessments rely on costly test-taker (model) performance and are computationally intensive.<br>â€¢ Ensuring robustness/correctness of generated coding tasks is difficult without executable verification and high-quality test cases.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>InfoSynth introduces information-theoretic metricsâ€”KL-divergence for novelty and Shannon entropy for diversityâ€”estimated from text embeddings via k-NN estimators to cheaply assess benchmark quality without model evaluations. It pairs these metrics with a genetic-algorithm-based synthesis pipeline that uses iterative code-execution feedback to generate verifiably correct Python problems, solutions, and tests with controllable novelty, diversity, and difficulty.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ InfoSynth++: Multimodal and Multi-language Information-Guided Benchmark Synthesis: Extend the framework beyond Python to math, other programming languages, and multimodal tasks with domain-specific executability and verification.<br>â€¢ Learning Density Models for Benchmark Novelty and Diversity: Replace k-NN/UMAP estimators with learned density models (e.g., normalizing flows) to better estimate KL/entropy in high dimensions and calibrate metrics against downstream evaluation outcomes and contamination audits.<br>â€¢ Adaptive, Model-Aware Information-Guided Benchmarking: Close the loop by steering synthesis using target model responses to maximize information gain, expose failure modes, and precisely control difficulty under evaluation cost constraints.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.00204" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.00204" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ 3D morphing struggles to achieve semantically coherent and temporally smooth deformations, especially for cross-category transformations.<br>â€¢ Matching-based 3D morphing depends on dense correspondences, prioritizes geometry while neglecting texture evolution, and generalizes poorly across categories, leading to structural artifacts.<br>â€¢ 2D-first pipelines (2D morphing + 3D lifting) suffer from frame-wise inconsistencies and fail to capture true 3D deformations, harming temporal coherence.<br>â€¢ Direct interpolation of noise/conditions in SLAT-based generators lacks explicit structural and temporal constraints, producing suboptimal morphs.<br>â€¢ Existing SLAT-based generation lacks principled mechanisms to fuse source/target semantics and temporal context within attention, and is prone to pose/orientation jumps mid-morph.<br>â€¢ There is a need for a training-free solution that leverages modern 3D generative priors (SLAT/Trellis) while producing mesh/NeRF/3DGS-compatible, high-fidelity sequences.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MorphAny3D is a training-free SLAT-based 3D morphing framework that injects sourceâ€“target fusion and temporal context directly into Trellisâ€™s attention: Morphing Cross-Attention computes separate cross-attention to source and target and blends the outputs by the morph weight, and Temporal-Fused Self-Attention mixes current and previous-frame self-attention outputs to improve smoothness. A lightweight orientation correction selects a yaw-rotated sparse structure (0/90/180/270Â°) minimizing Chamfer Distance to the previous frame to suppress pose jumps.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning MorphAny3D: Adaptive Attention Fusion for SLAT-based 3D Morphing: Learn content-aware gates/weights for cross-/self-attention fusion (MCA/TFSA) with minimal fine-tuning to optimize the plausibilityâ€“smoothness trade-off.<br>â€¢ SceneMorphAny3D: Training-free Morphing in Complex 3D Scenes with Multi-object Interactions and Camera Motion: Extend SLAT morphing to multi-object scenes by discovering object correspondences, enforcing occlusion-aware temporal fusion, and jointly stabilizing camera trajectories.<br>â€¢ PoseFlow: Continuous SO(3) Orientation Modeling for Temporally Stable 3D Morphing: Replace discrete yaw candidates with continuous pose estimation and optimization (via differentiable rendering and temporal regularization) to prevent orientation jumps under diverse viewpoints.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 5</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2026-01-06 00:23:57 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 5;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>