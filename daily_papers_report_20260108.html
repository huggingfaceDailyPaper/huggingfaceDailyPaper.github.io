<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - January 08, 2026</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">January 08, 2026</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02151" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02151" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as "Confident Conflicts" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Catastrophic forgetting in supervised fine-tuning (SFT): adapting LLMs to specific domains often degrades general capabilities (alignment tax).<br>â€¢ Distributional gap vs. on-policy RL: SFT introduces low-probability, low-entropy "Confident Conflict" tokens that force destructive updates, unlike RL rollouts that align with model beliefs.<br>â€¢ Limitations of probability/KL-based mitigations: they cannot distinguish epistemic uncertainty from knowledge conflicts, risking amplified gradients on harmful tokens.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Entropy-Adaptive Fine-Tuning (EAFT) scales the cross-entropy loss by a token-level, normalized Top-K entropy weight to down-weight low-entropy (confident, conflicting) tokens while preserving supervision on high-entropy (uncertain) tokens, thereby mitigating forgetting without sacrificing target performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Hybrid On-Policy EAFT: Unifying Entropy Gating with RL Rollouts for Forgetting-Free Alignment: Combine entropy-gated SFT with on-policy RL data to eliminate confident conflicts and further stabilize updates.<br>â€¢ Entropy-Calibrated Fine-Tuning: Distinguishing Epistemic and Aleatoric Uncertainty for Safer Updates: Augment entropy gating with calibrated uncertainty estimates (e.g., ensembles or stochastic forward passes) to better separate true uncertainty from conflicts.<br>â€¢ Curriculum EAFT: Scheduling Entropy Thresholds Across Domains for Robust Multi-Task Adaptation: Design automatic curricula that modulate gating intensity and Top-K settings across tasks to optimize the adaptationâ€“retention trade-off.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Evolving Programmatic Skill Networks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03509" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03509" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\footnote{We plan to open-source the code.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Embodied agents in open-ended environments need to continually acquire, refine, and reuse a growing library of skills to handle diverse, evolving tasks.<br>â€¢ Existing approaches store skills as flat libraries or static graphs without principled mechanisms for continual improvement, leading to brittle reuse and slow adaptation.<br>â€¢ There is no unified framework for assigning credit over hierarchical skill compositions, repairing symbolic programs, and reorganizing structure as new tasks arise.<br>â€¢ Current systems struggle with stabilityâ€“plasticity tradeoffs (protecting reliable skills while adapting uncertain ones) and lack compact, maintainable skill networks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Programmatic Skill Networks (PSN) represent each skill as an executable symbolic program with pre/postconditions, forming a compositional, inspectable graph that evolves via experience. A network-aware planner prioritizes backward-chained skill reuse; failures trigger REFLECT for trace-based fault localization and maturity-aware update gating, while successes enable canonical structural refactoring with rollback validation; LLMs instantiate synthesis/repair within this architectural scaffold.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Differentiable-Programmatic Skill Networks: Unifying gradient-based learning with REFLECT-style trace credit to jointly optimize neural components and symbolic programs.<br>â€¢ Safety-Aware PSN Refactoring with Formal Guarantees: Integrating static analysis and formal verification to ensure refactors preserve pre/postconditions and task safety under rollback constraints.<br>â€¢ Meta-REFLECT: Learning Fault Localization Policies for Programmatic Skill Repair: Training a learned critic to guide fault attribution and patch proposal, improving accuracy and sample efficiency over prompt-only REFLECT.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03872" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03872" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Selecting optimal LLMâ€“tool combinations is a high-dimensional optimization problem; it matters because performance varies widely across heterogeneous pairs and the right synergy yields large accuracy and efficiency gains; existing methods either fix tool-calling logic or route only among models, ignoring tools.<br>â€¢ Tool invocation is rigid and planner-driven, lacking adaptability to task/domain or model capabilities; this limits open-domain and multi-modal reasoning where dynamic sequencing is crucial; current tool-use frameworks rely on fixed, preconfigured pipelines.<br>â€¢ Reinforcement learning is applied in isolation (optimizing a single component or model), missing joint optimization of modelâ€“tool synergies; this prevents discovery of superior multi-step execution trajectories.<br>â€¢ Generalization to out-of-distribution tasks is weak in prior routers trained on specific datasets; robust OOD performance is essential for real-world deployment; existing methods degrade without empirical priors or exploratory routing.<br>â€¢ Specialized multi-modal tools are underutilized by model-only routers, leaving visual and cross-modal reasoning gains on the table.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>ATLAS is a dual-path orchestration framework that dynamically selects and sequences heterogeneous LLMâ€“tool pairs via (1) training-free cluster-based routing in a semantic embedding space using historical, domain-specific priors for in-distribution queries and (2) an RL-based multi-step router that explores and optimizes modelâ€“tool invocation trajectories for out-of-distribution generalization. The RL path is reward-guided with regularization to ensure efficient, robust search over high-dimensional modelâ€“tool spaces.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Continual ATLAS: Lifelong Adaptation of Modelâ€“Tool Routing under Distribution Shift: Develop online/continual learning to update clustering priors and RL policies as tasks and tools evolve.<br>â€¢ Constrained Orchestration: Safety- and Cost-Aware RL for Reliable LLMâ€“Tool Synergy: Integrate safety, latency, and budget constraints into the routing objective with uncertainty-aware decision-making.<br>â€¢ Interpretable Synergy Discovery: Causal Explanations for Modelâ€“Tool Selection in Multi-Modal Reasoning: Provide causal/attribution-based explanations and audits for routing decisions to improve transparency and controllability.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">Benchmark^2: Systematic Evaluation of LLM Benchmarks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03986" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03986" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Proliferation of LLM benchmarks without systematic methods to assess benchmark quality, raising the question of which benchmarks to trust.<br>â€¢ Cross-benchmark ranking inconsistency that yields conflicting model orders and undermines comparability and decision-making.<br>â€¢ Low discriminative power where benchmarks fail to separate models, producing small or insignificant performance gaps.<br>â€¢ Prevalence of rank-inconsistent items where weaker models outperform stronger ones within the same family, suggesting noisy or ill-posed test instances.<br>â€¢ Existing practice treats benchmarks as ground truth and lacks quantitative, domain-agnostic metrics for reliability, discriminability, and capability alignment; prior work focuses on contamination, saturation, or holistic evaluation but not benchmark quality per se.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>BENCHMARK2 introduces three complementary metrics to assess benchmark quality: Cross-Benchmark Ranking Consistency (average Kendallâ€™s tau with peer benchmarks), Discriminability Score (normalized score spread combined with the proportion of practically significant pairwise gaps), and Capability Alignment Deviation (instance-level inversion rate within model-family capability hierarchies). These metrics are applied across 15 benchmarks and 11 models to diagnose quality variations and to filter high-quality items, enabling reduced benchmarks (~35% size) that preserve evaluation fidelity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ AutoBench: Learning-to-Curate LLM Benchmarks via BENCHMARK2 Signals: Use CBRC/DS/CAD as optimization objectives to automatically select and refine benchmark items.<br>â€¢ Dynamic BENCHMARK2: Active Construction of Continually Challenging Test Suites: Iteratively add items that maximize discriminability and consistency as models evolve.<br>â€¢ Cross-Modal BENCHMARK2: Extending Benchmark Quality Metrics to Multimodal and Tool-Use Tasks: Generalize CBRC/DS/CAD to visionâ€“language, code, and agentic evaluation.<br>â€¢ From Metrics to Editors: Instance-Level Diagnosis and Repair of Rank-Inconsistent Questions: Use CAD to localize flawed items and apply automated rewrites/validation loops to fix them.<br>â€¢ FairBench: Beyond Parameter Countâ€”Learning Family-Specific Capability Hierarchies for CAD: Infer data-driven within-family orderings to improve inversion detection.<br>â€¢ RobustStat-DS: Bayesian and Bootstrap Methods for Reliable Discriminability Estimates: Incorporate uncertainty, effect sizes, and multiple-comparison control into DS.<br>â€¢ Meta-Eval Leaderboards: Ranking Benchmarks (Not Models) with BENCHMARK2: Publicly score and track benchmarks by reliability, discriminability, and alignment over time.<br>â€¢ Contamination-Aware BENCHMARK2: Integrating Data Leakage Signals into Quality Scores: Combine contamination detection with CBRC/DS/CAD to downweight suspect items.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Klear: Unified Multi-Task Audio-Video Joint Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.04151" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.04151" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Persistent audioâ€“visual asynchrony, weak lipâ€“speech (lipâ€“audio) alignment, and degradation in unimodal quality in joint generation<br>â€¢ Architectural limitations of dual-tower or shallow cross-attention designs that hinder deep cross-modal interaction and alignment<br>â€¢ Single-task training regimes (e.g., only T2AV) causing biased representations, poor generalization, and failure to exploit A/V correlations and world knowledge<br>â€¢ Scarcity of diverse, high-quality, densely captioned, and strictly aligned audioâ€“video datasets; lack of scalable automated annotation pipelines<br>â€¢ Need for a unified framework that handles multiple tasks (T2AV/TI2AV/TI2V/T2V/T2A) with strong instruction following and OOD robustness</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Klear uses a unified single-tower DiT backbone with an Omni-Full Attention module that jointly attends to audio, video, and their captions to achieve deep cross-modal fusion and tight alignment. It is trained via progressive multi-task learning with random modality masking and a performance-adaptive pretrainâ€“post-train curriculum, and scaled with an automated pipeline producing an 81M-sample, densely captioned, strictly aligned AVâ€“text dataset.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Omni-Full Attention at Scale: Memory- and compute-efficient variants for long-context unified audioâ€“video generation<br>â€¢ Self-Supervised AVâ€“Text Alignment at Web Scale: Weakly supervised and self-training pipelines to expand and denoise multimodal datasets<br>â€¢ RLHF for Instruction-Following AV Generators: Real-time, human-in-the-loop optimization to improve synchrony, fidelity, and adherence to complex instructions</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Choreographing a World of Dynamic Objects</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.04194" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.04194" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Generating scene-level 4D motion (deformations and interactions) from static 3D snapshots is hard, and rule-based pipelines with category-specific rigs are labor-intensive and not scalable.<br>â€¢ Existing learning-based 4D generators rely on scarce datasets that mostly cover single-object or humanoid deformations, failing to generalize to diverse multi-object interactions.<br>â€¢ Prior distillation/reconstruction methods struggle with high-dimensional 4D optimization and noisy guidance, often producing minor or unrealistic motion.<br>â€¢ Modern video generative models are rectified flow-based and incompatible with standard SDS, creating an architectural gap that blocks effective 4D motion distillation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>CHORD distills motion guidance from rectified flow-based video generative models into a hierarchical 4D representation over 3D Gaussian Splatting, using coarse-to-fine spatial control points and a Fenwick tree temporal structure with an RF-SDS objective and annealed noise scheduling plus spatial/temporal regularization to produce realistic multi-object 4D dynamics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Physics-Aware CHORD: Integrating Differentiable Physics into RF-SDS for Constraint-Compliant 4D Scene Dynamics: Embed physical constraints during distillation to improve stability, contact reasoning, and realism.<br>â€¢ Language-Guided Multi-Object Choreography: Compositional Text Control for Long-Horizon 4D Scene Interactions: Strengthen text grounding to plan complex, sequential, and multi-agent interactions from natural language.<br>â€¢ CHORD for Robot Policy Learning: From Distilled 4D Trajectories to Real-World Manipulation via Sim-to-Real Transfer: Use generated Lagrangian trajectories to train robust manipulation policies and transfer them to physical robots.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Agentic Rubrics as Contextual Verifiers for SWE Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.04171" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.04171" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ SWE agents need scalable, reliable verification to supply reward signals for RL and enable effective test-time scaling; current execution-based verification (e.g., running tests) is costly per instance and often brittle (sparse signals, limited distinguishability, test toxicity).<br>â€¢ Existing execution-free verifiers (patch classifiers, heuristics, generic LLM judges) are operationally lightweight but typically under-grounded in repository context, less interpretable, and prone to shallow cues, hurting reliability.<br>â€¢ As agents operate on open-ended, long-tail repositories, verifiers must be codebase-specific and provide granular, diagnostic feedback beyond binary pass/fail tests.<br>â€¢ There is a need for an execution-free yet context-grounded verification method that scales without environment setup while remaining interpretable and consistent with intended behavior.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Agentic Rubrics: an expert rubric agent actively explores the repository and issue/PR to synthesize a context-grounded checklist (e.g., File Change, Spec Alignment, Integrity, Runtime). Candidate patches are then graded against this rubric without executing code to produce a reliable verifier score for selection and post-training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning to Generate Rubrics: Reinforcement learning and human-in-the-loop feedback to optimize rubric specificity, coverage, and grading consistency from downstream test outcomes.<br>â€¢ Hybrid Verifiers with Partial Execution: Integrating agentic rubrics with lightweight static analysis and selective, targeted execution to boost reliability while preserving scalability.<br>â€¢ Cross-Domain Agentic Rubrics: Extending context-grounded rubric verification beyond SWE (e.g., data pipelines, ML workflows, configuration changes) and studying transfer and distillation to small open-weight models.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02075" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02075" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Scarcity and construction difficulty of high-quality, domain-specific datasets for MD text-to-code and knowledge Q&A, limiting effective post-training of LLMs<br>â€¢ Absence of dedicated benchmarks for LAMMPS QA and code generation, hindering rigorous, comprehensive evaluation<br>â€¢ Existing LLM methods largely lack closed-loop execution feedback and self-correction, leading to low executability and physical incorrectness of generated MD code<br>â€¢ High inference cost and limited deployability of large SOTA LLMs (closed-source or ultra-large), necessitating lightweight, domain-specialized models<br>â€¢ Complexity and rigidity of LAMMPS scripts make authoring time-consuming and error-prone, creating a bottleneck in MD research workflows</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MDAgent2 introduces a domain data pipeline (MD-Knowledge, MD-InstructQA, MD-CodeGen) and trains Qwen3-based MD-Instruct and MD-Code via CPT, SFT, and MD-GRPOâ€”an execution-aware RL that uses LAMMPS simulation outcomes and recycled low-reward trajectories as rewards. It deploys a multi-agent runtime that generates, executes, evaluates, and self-corrects LAMMPS scripts, and establishes MD-EvalBench for standardized QA and code-generation assessment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Physics-Constrained MD-GRPO: Reward Shaping with Conservation Laws and Ensemble Checks â€” Integrate domain priors (e.g., energy conservation, thermostat/ensemble consistency) into RL rewards to improve safety and physical fidelity of generated scripts<br>â€¢ Cross-Simulator Agent Systems: Generalizing Closed-Loop Code Generation from LAMMPS to GROMACS and OpenFOAM â€” Extend the data pipeline, RL framework, and runtime tools to other scientific simulators for broader AI4Science applicability<br>â€¢ Multi-Modal MD-LMs: Incorporating Structure Files and Analysis Visuals into Execution-Aware RL â€” Use CIF/PDB inputs and evaluation from plots/metrics (MSD, RDF) to guide code generation with richer feedback signals<br>â€¢ Continual Benchmarking for MD Code: A Dynamic, Expert-in-the-Loop MD-EvalBench â€” Build a living benchmark with periodic task updates and expert audits to track progress and prevent overfitting<br>â€¢ Efficient Deployment of MD LLMs: Distillation and Quantization for Edge and On-Premise Labs â€” Develop compression techniques to retain code executability and QA quality while enabling low-cost local deployment<br>â€¢ Formal Verification for LAMMPS Scripts: Static Analysis and Symbolic Checks Before Execution â€” Create tools to verify syntax, parameter ranges, and physical plausibility pre-run to reduce runtime failures</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.00423" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.00423" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ GRPO over many denoising timesteps yields sparse and ambiguous reward signals; low-entropy steps produce undistinguished rollouts that hinder effective alignment.<br>â€¢ Continuous multi-step SDE sampling accumulates stochasticity, blurring per-step credit assignment so beneficial explorations can be penalized by downstream noise.<br>â€¢ Existing efficiency-focused strategies (mixing SDE/ODE, tree-based sampling) do not address entropy-aware exploration or reward attribution across timesteps.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>E-GRPO increases exploration where it matters by merging consecutive low-entropy SDE steps into a single high-entropy SDE step, while using ODE sampling on remaining steps, and computes multi-step group normalized advantages within samples sharing the consolidated SDE step to improve credit assignment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Entropy Scheduling for GRPO in Flow Models: Learn a dynamic policy that selects and consolidates timesteps based on online entropy estimates to maximize exploration and reward signal.<br>â€¢ Entropy-Aware Credit Assignment for Stochastic Denoising: Develop theory and algorithms (e.g., counterfactuals, per-step reward shaping) to disentangle contributions of consolidated and ODE steps under multi-step noise.<br>â€¢ E-GRPO for Long-Horizon Generative Tasks (Video and 3D): Extend entropy-aware step consolidation and group-relative advantages to spatiotemporal generation, studying scalability and entropy profiles over long trajectories.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03471" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03471" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reliable epidemiological reasoning requires synthesizing study evidence to infer disease burden, transmission dynamics, and intervention effects at the population level. Existing medical question answering benchmarks primarily emphasize clinical knowledge or patient-level reasoning, yet few systematically evaluate evidence-grounded epidemiological inference. We present EpiQAL, the first diagnostic benchmark for epidemiological question answering across diverse diseases, comprising three subsets built from open-access literature. The subsets respectively evaluate text-grounded factual recall, multi-step inference linking document evidence with epidemiological principles, and conclusion reconstruction with the Discussion section withheld. Construction combines expert-designed taxonomy guidance, multi-model verification, and retrieval-based difficulty control. Experiments on ten open models reveal that current LLMs show limited performance on epidemiological reasoning, with multi-step inference posing the greatest challenge. Model rankings shift across subsets, and scale alone does not predict success. Chain-of-Thought prompting benefits multi-step inference but yields mixed results elsewhere. EpiQAL provides fine-grained diagnostic signals for evidence grounding, inferential reasoning, and conclusion reconstruction.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Absence of benchmarks targeting population-level, evidence-grounded epidemiological reasoning across diverse diseases and topics<br>â€¢ Prior biomedical QA resources rely on clinical/patient-level tasks, abstracts, or disease-specific extractive formats, enabling shortcut cues and limiting inference depth<br>â€¢ Need for controlled and trustworthy evaluation with document-grounded answers and support for multiple valid conclusions<br>â€¢ Lack of scalable, low-bias verification for multi-answer correctness without costly expert annotation<br>â€¢ Necessity to diagnose LLM capabilities separately across factual retrieval, multi-step epidemiological inference, and conclusion synthesis</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces EpiQAL, a taxonomy-guided, automated benchmark construction pipeline with three subsets (A: text-grounded recall, B: multi-step inference linking evidence to epidemiological principles, C: conclusion reconstruction with the Discussion masked), using multi-LLM verification, retrieval-based difficulty control, and stem refinement to produce controlled, evidence-grounded multi-answer QA.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ EpiQAL-RAG: Retrieval-Augmented Epidemiological Reasoning with Structured Evidence: Integrate knowledge graphs, causal relations, and table/figure extraction to train models that cite spans and produce stepwise justifications, improving performance on multi-step inference and masked conclusion tasks.<br>â€¢ Calibrated Abstention in Epidemiological QA: Learning When Not to Answer: Develop confidence calibration and abstention strategies tailored to multi-answer settings to reduce overconfident errors and improve trustworthiness on EpiQAL subsets.<br>â€¢ Beyond Exact Match: Justification-Centric Evaluation for Epidemiological QA: Design evaluation metrics that score evidence citation, reasoning chains, and partial correctness, extending EpiQAL with rationale annotations and fine-grained diagnostic signals.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03699" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03699" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As large language models (LLMs) become integral to safety-critical applications, ensuring their robustness against adversarial prompts is paramount. However, existing red teaming datasets suffer from inconsistent risk categorizations, limited domain coverage, and outdated evaluations, hindering systematic vulnerability assessments. To address these challenges, we introduce RedBench, a universal dataset aggregating 37 benchmark datasets from leading conferences and repositories, comprising 29,362 samples across attack and refusal prompts. RedBench employs a standardized taxonomy with 22 risk categories and 19 domains, enabling consistent and comprehensive evaluations of LLM vulnerabilities. We provide a detailed analysis of existing datasets, establish baselines for modern LLMs, and open-source the dataset and evaluation code. Our contributions facilitate robust comparisons, foster future research, and promote the development of secure and reliable LLMs for real-world deployment. Code: https://github.com/knoveleng/redeval</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Fragmented red-teaming resources: existing datasets use inconsistent risk definitions, formats, and scopes, preventing standardized, cross-dataset evaluation.<br>â€¢ Limited domain and risk coverage: many benchmarks focus narrowly (e.g., toxicity, bias, jailbreaking) and miss comprehensive, real-world deployment scenarios.<br>â€¢ Outdated evaluations: prior studies primarily benchmark older LLMs, leaving robustness of modern SOTA models (e.g., Qwen2.5, Llama 3.1, Gemma 2) underexplored.<br>â€¢ Missing balance of safety vs. usability: inadequate assessment of over-refusal on benign prompts hampers understanding of safetyâ€“helpfulness trade-offs.<br>â€¢ Safety-critical deployment demands: lack of a universal, standardized dataset impedes reliable vulnerability assessment for real-world LLMs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Aggregate and harmonize 37 public benchmarks (29,362 prompts) into RedBench, dual-labeled by a standardized taxonomy of 22 risk categories and 19 domains across attack and refusal settings. Provide baselines for modern LLMs and release open-source dataset and evaluation code for reproducible, comprehensive red-teaming.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Red Team Generation via RedBench-Guided Attack Synthesis: Train generators that target under-tested riskâ€“domain intersections identified by RedBenchâ€™s taxonomy to systematically probe model weaknesses.<br>â€¢ Refusal Calibration with RedBench: Balancing Safety and Helpfulness: Develop and evaluate algorithms that tune refusal thresholds using the refusal subset (No Risk) to minimize over-refusal without increasing harmful compliance.<br>â€¢ Continual RedBench: Dynamic Taxonomy and Dataset Evolution for Emerging LLM Risks: Propose methods for incremental taxonomy updates and dataset expansion to track new threat vectors while preserving backward-compatible benchmarking.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03315" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03315" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We report a case study of four end-to-end attempts to autonomously generate ML research papers using a pipeline of six LLM agents mapped to stages of the scientific workflow. Of these four, three attempts failed during implementation or evaluation. One completed the pipeline and was accepted to Agents4Science 2025, an experimental inaugural venue that required AI systems as first authors, passing both human and multi-AI review. From these attempts, we document six recurring failure modes: bias toward training data defaults, implementation drift under execution pressure, memory and context degradation across long-horizon tasks, overexcitement that declares success despite obvious failures, insufficient domain intelligence, and weak scientific taste in experimental design. We conclude by discussing four design principles for more robust AI-scientist systems, implications for autonomous scientific discovery, and we release all prompts, artifacts, and outputs at https://github.com/Lossfunk/ai-scientist-artefacts-v1</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Test whether state-of-the-art LLMs can autonomously go from research idea to an accepted ML paper with minimal scaffolding and only basic tools<br>â€¢ Diagnose why autonomous AI-scientist systems fail in practice across long-horizon, multi-stage workflows<br>â€¢ Provide actionable failure taxonomy (defaults bias, implementation drift, memory/context decay, premature success claims, insufficient domain intelligence, weak experimental design taste)<br>â€¢ Address limitations of prior systems that rely on heavy domain-specific predefinition, complex meta-orchestration (e.g., tree search), or human-specified verification metrics<br>â€¢ Inform design principles and tooling needed for robust, reproducible autonomous scientific discovery, and enable community replication via released prompts/artifacts</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A six-module LLM pipeline (idea â†’ hypotheses â†’ experiment planning â†’ execution/output evaluation â†’ revision â†’ paper outlining) operates over a shared repository and simple tools, using Gemini 2.5 Pro for long-context reasoning and Claude Code for implementation/writing on Modal. The authors ran four end-to-end attempts across ML subdomains to surface outcomes and recurring failure modes under minimal scaffolding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Memory-Robust AI Scientists: Long-Horizon Context Architectures for Autonomous Research: Design hierarchical memory, provenance tracking, and state compaction to prevent context degradation across multi-stage research<br>â€¢ Calibrated Autonomy: Detecting and Preventing Overclaiming in LLM-Driven Scientific Workflows: Build uncertainty-aware, statistics-savvy evaluators that block premature success declarations and enforce validity<br>â€¢ From Defaults to Decisions: Reducing Training-Data Bias in Autonomous Experimentation: Develop counterfactual planning and diversity priors to steer agents away from boilerplate defaults toward context-appropriate choices<br>â€¢ Taste Modules for Experiment Design: Distilling Expert Priors into LLM Researchers: Learn reusable priors on baselines, ablations, and metrics to improve scientific taste and study quality<br>â€¢ Light-Scaffold Research Tree Search: Minimal-Orchestration Strategies that Preserve Autonomy: Hybridize lightweight controllers/search with end-to-end agents to boost robustness without heavy meta-orchestration<br>â€¢ Domain Intelligence Injection for AI Scientists: Retrieval, Simulators, and Reviewer Ensembles: Systematically integrate tools (literature retrieval, simulators) and multi-agent review to raise hypothesis/plan quality<br>â€¢ Benchmarking End-to-End AI Scientists: An Acceptance-Centered Suite for Idea-to-Paper Pipelines: Create benchmarks where success is judged by blinded human/multi-AI review and statistical rigor, not proxy metrics<br>â€¢ Learning Revision Policies in Autonomous Research: When to Pivot Ideas, Hypotheses, or Implementations: Train policies that route failures to the right revision action, reducing human intervention and iteration time</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03467" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03467" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Instruction-driven image editing with unified multimodal generative models has advanced rapidly, yet their underlying visual reasoning remains limited, leading to suboptimal performance on reasoning-centric edits. Reinforcement learning (RL) has been investigated for improving the quality of image editing, but it faces three key challenges: (1) limited reasoning exploration confined to denoising stochasticity, (2) biased reward fusion, and (3) unstable VLM-based instruction rewards. In this work, we propose ThinkRL-Edit, a reasoning-centric RL framework that decouples visual reasoning from image synthesis and expands reasoning exploration beyond denoising. To the end, we introduce Chain-of-Thought (CoT)-based reasoning sampling with planning and reflection stages prior to generation in online sampling, compelling the model to explore multiple semantic hypotheses and validate their plausibility before committing to a visual outcome. To avoid the failures of weighted aggregation, we propose an unbiased chain preference grouping strategy across multiple reward dimensions. Moreover, we replace interval-based VLM scores with a binary checklist, yielding more precise, lower-variance, and interpretable rewards for complex reasoning. Experiments show our method significantly outperforms prior work on reasoning-centric image editing, producing instruction-faithful, visually coherent, and semantically grounded edits.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Instruction-driven image editing models lack robust visual reasoning, leading to suboptimal performance on reasoning-centric edits where understanding must precede synthesis.<br>â€¢ Existing RL methods confine exploration to denoising stochasticity and neglect diverse reasoning trajectories, making them ill-suited for reasoning-first editing tasks.<br>â€¢ Simple weighted reward aggregation biases optimization (e.g., favoring high consistency in unchanged images over instruction fidelity), harming balanced editing quality.<br>â€¢ VLM-based discrete instruction scores are high-variance and unstable for complex reasoning, introducing noise into training and evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>ThinkRL-Edit decouples visual reasoning from image synthesis and uses Chain-of-Thought reasoning sampling with planning and reflection before generation to explore and validate multiple semantic hypotheses. It employs unbiased chain preference grouping across reward dimensions and replaces interval VLM scores with a binary checklist to provide precise, low-variance, interpretable instruction rewards.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ CoT-RL for Temporally Consistent Reasoning-Centric Video Editing: Extend decoupled reasoning and checklist rewards to video to handle temporal constraints and cross-frame consistency.<br>â€¢ Human-in-the-Loop Binary-Checklist RL for Fine-Grained Instruction Fidelity: Integrate interactive checklists and preference feedback to further stabilize training and personalize reasoning-centric edits.<br>â€¢ Structured Reasoning Trajectories for Multimodal Scene Editing and Understanding: Learn reusable planningâ€“reflection templates that transfer across image, text, and 3D modalities to improve generalization and robustness.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03448" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03448" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Language models (LMs) are pre-trained on raw text datasets to generate text sequences token-by-token. While this approach facilitates the learning of world knowledge and reasoning, it does not explicitly optimize for linguistic competence. To bridge this gap, we propose L2T, a pre-training framework integrating Language Learning Tasks alongside standard next-token prediction. Inspired by human language acquisition, L2T transforms raw text into structured input-output pairs to provide explicit linguistic stimulation. Pre-training LMs on a mixture of raw text and L2T data not only improves overall performance on linguistic competence benchmarks but accelerates its acquisition, while maintaining competitive performance on general reasoning tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Standard causal language modeling (CLM) on raw text does not explicitly optimize for linguistic competence (morphology, syntax, semantics), leading models to learn co-occurrence rather than structured linguistic rules.<br>â€¢ LMs often behave as â€œstochastic parrots,â€ mimicking surface patterns without understanding generative structuresâ€”an issue that undermines reliability on linguistic competence benchmarks and real-world language understanding.<br>â€¢ Existing approaches like instruction tuning depend on external supervision/annotations, lacking a self-supervised pathway to induce structured linguistic learning directly from raw text.<br>â€¢ Current pre-training lacks explicit linguistic stimulation, resulting in slower and less data-efficient acquisition of linguistic competence, with trade-offs between linguistic knowledge and general reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>L2T pre-training mixes standard next-token prediction with self-supervised language learning tasks generated from raw text by converting it into structured inputâ€“output pairs, providing explicit linguistic stimulation without external supervision. This induces dependencies and information restructuring that accelerates acquisition of morphological, syntactic, and semantic knowledge while retaining general capabilities.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Scaling L2T: Pre-training Large-Scale Language Models with Expanded Language Learning Task Suites: Assess L2T at >10B parameters and richer task repertoires, quantifying gains in linguistic competence and general reasoning.<br>â€¢ Multilingual L2T: Inducing Cross-Lingual Linguistic Competence via Self-Supervised Language Learning Tasks: Extend L2T to diverse languages with universal and language-specific tasks to improve morphology and syntax across typologies.<br>â€¢ Curriculum-L2T: Adaptive Scheduling of Language Learning Tasks for Efficient Linguistic Competence Acquisition: Design curricula that sequence tasks by difficulty or linguistic domain to optimize sample efficiency and learning dynamics.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Pearmut: Human Evaluation of Translation Made Trivial</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02933" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02933" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Human evaluation is the gold standard for multilingual NLP, but is often skipped in practice and substituted with automatic metrics, because it is notoriously complex and slow to set up with existing tools with substantial engineering and operational overhead. We introduce Pearmut, a lightweight yet feature-rich platform that makes end-to-end human evaluation as easy to run as automatic evaluation. Pearmut removes common entry barriers and provides support for evaluating multilingual tasks, with a particular focus on machine translation. The platform implements standard evaluation protocols, including DA, ESA, or MQM, but is also extensible to allow prototyping new protocols. It features document-level context, absolute and contrastive evaluation, attention checks, ESAAI pre-annotations and both static and active learning-based assignment strategies. Pearmut enables reliable human evaluation to become a practical, routine component of model development and diagnosis rather than an occasional effort.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Human evaluation, the gold standard for translation/NLG, is often skipped and replaced by automatic metrics, leading to misleading hillclimbing and false conclusions.<br>â€¢ Existing tools are complex, slow to set up, and require substantial engineering/operational overhead, discouraging even limited human evaluations.<br>â€¢ Ad-hoc, non-standard protocols produce irreproducible, unauditable, and noisy results.<br>â€¢ Limited access to multilingual evaluators and poor support for multilingual tasks in current tooling.<br>â€¢ General-purpose platforms lack translation-specific features (document-level context, absolute/contrastive judgments, span-level error marking, attention checks, pre-annotation).<br>â€¢ Lack of a go-to default workflow prevents human evaluation from becoming a routine part of model development and diagnosis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Pearmut is a lightweight, specialized platform that makes end-to-end human evaluation of multilingual (including multimodal) translation trivial by implementing standard protocols (DA, ESA, MQM), and providing document-level context, absolute/contrastive tasks, attention checks, ESAAI pre-annotations, and static/active-learning assignment with monitoring and review. It lowers setup and operational barriers while remaining extensible to prototype new evaluation protocols.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Active Learning for Human Translation Evaluation in Pearmut: Optimizing Task Assignment to Maximize Annotation Quality and Efficiency: Evaluate and refine Pearmutâ€™s active-learning strategies to improve label quality and reduce annotator load.<br>â€¢ Standardized Multimodal MQM/ESA Protocols: Extending Pearmut to Audio, Video, and Image Translation Evaluation: Design and validate protocol variants tailored to multimodal translation with document/context-aware interfaces.<br>â€¢ Reproducible Human Evaluation at Scale: Benchmarking Automatic Metrics Against Pearmut-Based MQM Across 50 Languages: Systematically compare metric performance to human judgments, establishing best practices and calibration methods for multilingual settings.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03236" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03236" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLMs struggle with long-term context due to finite attention windows, attention dilution, and context-decay (e.g., â€œlost-in-the-middleâ€), leading to poor long-horizon reasoning and unstable memory.<br>â€¢ Existing MAG systems rely on monolithic, semantically indexed stores that entangle temporal, causal, and entity information, causing misalignment between query intent and retrieved evidence and reducing interpretability.<br>â€¢ Current retrieval heuristics (semantic similarity/recency) overlook heterogeneous relations, resulting in suboptimal accuracy, limited transparency of reasoning paths, and inefficient retrieval (higher latency and token usage).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MAGMA introduces a multi-graph memory where each item is represented across orthogonal semantic, temporal, causal, and entity graphs, and retrieval is formulated as an intent-aware, policy-guided traversal that fuses relevant subgraphs into a compact context. A dual-stream evolution mechanism decouples fast event ingestion from asynchronous structural consolidation to maintain responsiveness while refining relational structure.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Traversal Policies for Multi-Graph Agentic Memory: End-to-end optimization (e.g., RL/imitation learning) of query-adaptive traversal policies to maximize task performance and minimize token/latency costs.<br>â€¢ Probabilistic Causal and Temporal Graph Construction for Noisy Agent Histories: Bayesian/uncertainty-aware methods to induce and update causal/temporal edges robustly under noisy, evolving interactions.<br>â€¢ Multi-Modal MAGMA: Extending Multi-Graph Memory to Vision-Language Agents: Incorporate visual/audio streams into the multi-graph substrate with cross-modal entities and events for richer retrieval and reasoning.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.04090" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.04090" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and video diffusion models for scene-level 3D generation. We repurpose the VGGT reconstruction model to produce geometric latents by training an adapter on its tokens, which are regularized to align with the appearance latents of pre-trained video diffusion models. By jointly generating these disentangled yet aligned latents, Gen3R produces both RGB videos and corresponding 3D geometry, including camera poses, depth maps, and global point clouds. Experiments demonstrate that our approach achieves state-of-the-art results in single- and multi-image conditioned 3D scene generation. Additionally, our method can enhance the robustness of reconstruction by leveraging generative priors, demonstrating the mutual benefit of tightly coupling reconstruction and generative models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ 2D-prior-based pipelines (e.g., SDS, outpainting, multi-view synthesis) lack explicit 3D reasoning, yielding inconsistent geometry and high optimization cost.<br>â€¢ Feed-forward latent diffusion methods struggle without large-scale 3D ground truth; geometry-centric VAEs trained from 2D supervision produce suboptimal geometry and limited generation quality.<br>â€¢ Existing uses of reconstruction models overlook their intrinsic compact token manifolds; a unified approach is needed to align reconstruction geometry tokens with video diffusion appearance latents for joint generation and more robust reconstruction.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Gen3R repurposes the VGGT feed-forward reconstruction model as a VAE-like geometric encoder, training an adapter that projects VGGT tokens into geometric latents aligned (distributionally and resolution-wise) with the appearance latents of a pre-trained video diffusion model. Fine-tuning the diffusion model to jointly sample disentangled yet aligned geometry and appearance latents enables feed-forward generation of temporally coherent videos alongside camera poses, depth maps, and globally consistent point clouds under flexible conditioning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Text-Controllable Gen3R: Semantic Conditioning for Joint Video and 3D Scene Synthesis: Integrate text prompts and scene semantics to steer both appearance and geometry latents for content- and layout-aware generation.<br>â€¢ Dynamic Gen3R: Non-Rigid and Topology-Changing Scene Generation via Motion-Aware Geometric Latents: Extend the geometric latent space to capture deformations and dynamic objects, producing time-varying geometry with consistent tracking.<br>â€¢ Adapter-Free Joint Latent Space Learning for 3D Scene Generation: Learn a single shared latent manifold for geometry and appearance end-to-end, removing the adapter and improving alignment and sample efficiency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.03955" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.03955" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring "vision" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing 1D visual tokenizers are isotropic and single-hierarchy, querying only along depth and ignoring vision-specific hierarchical/residual priors, which limits representation capacity.<br>â€¢ Lack of cross-level feature fusion and redundancy among latent tokens leads to high codebook entropy and uniform probabilities, making autoregressive (AR) modeling harder.<br>â€¢ 2D-to-1D flattening breaks AR causality at scan turning points; hand-crafted hierarchy assignments (frequency/resolution) reduce flexibility; diffusion-based decoders introduce dual stochastic processes and training instability; strict next-token sampling is slow.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>ResTok learns hierarchical residuals by progressively merging image tokens into multi-scale representations and initializing latent tokens with semantic residuals, producing concentrated, coarse-to-fine latents aligned across levels. A hierarchical autoregressive generator predicts entire latent hierarchies (with VF-based alignment and causal attention), cutting sampling steps while maintaining high fidelity (e.g., 2.34 gFID on ImageNet-256 with 9 steps).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ ResTok-Video: Hierarchical Residual Tokenization for Autoregressive Video Generation: Extend hierarchical merging and semantic residuals to spatio-temporal tokens and design HAR to predict temporal hierarchies for efficient video synthesis.<br>â€¢ Adaptive Hierarchy Learning in 1D Tokenizers via Meta-Learned Merging and Residual Schedules: Learn data-driven pooling factors, merging depths, and residual strengths to dynamically shape hierarchies for different domains and resolutions.<br>â€¢ Codebook Entropy Shaping for Reliable AR Generation: Theoretical and practical frameworks to regularize codebook distributions (entropy, mutual information) and jointly train tokenizerâ€“generator for improved AR modeling efficiency.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.00705" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.00705" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Residual-driven densification in Gaussian-splatting SLAM yields non-stationary objectives and unstable early convergence, especially in texture-rich or cluttered regions.<br>â€¢ Incremental spawning/merging of Gaussians delays spatial coverage, causing early pose drift and unreliable mapping when topology keeps changing.<br>â€¢ Iterative densification increases computation and memory overhead, limiting real-time throughput.<br>â€¢ Existing pipelines lack a training-free, dense, structure-aware initialization that stabilizes early optimization while remaining compatible with standard GS-SLAM frameworks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>RGS-SLAM replaces residual-driven densification with a keyframe-triggered one-shot dense initialization: confidence-weighted DINOv3 correspondences are triangulated across a short keyframe window to instantiate a uniformly distributed, structure-aware Gaussian set, which is then refined via a differentiable 3DGS renderer with analytic SE(3) pose Jacobians.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Dynamic RGS-SLAM: One-Shot Dense Initialization with Motion Segmentation for Non-Rigid and Moving Objects â€” extend dense seeding and refinement to dynamic scenes using temporal Gaussian primitives and motion models.<br>â€¢ Adaptive Re-seeding for Gaussian Splatting SLAM: Confidence-Guided Topology Updates in Long-Term Mapping â€” introduce occasional, uncertainty-aware re-initialization to complement stationary optimization while preserving stability and efficiency.<br>â€¢ Semantic-Guided Gaussian Initialization: Leveraging Foundation Model Features for Structure-Aware Seeding and Regularization â€” integrate semantic priors to guide covariance/opacity initialization, merging, and pruning for improved fidelity in complex environments.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 7</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2026-01-08 23:07:28 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 7;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>