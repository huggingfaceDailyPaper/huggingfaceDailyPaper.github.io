<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 11, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 11, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">HaluMem: Evaluating Hallucinations in Memory Systems of Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.03506" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.03506" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Memory systems are key components that enable AI systems such as LLMs and AI agents to achieve long-term learning and sustained interaction. However, during memory storage and retrieval, these systems frequently exhibit memory hallucinations, including fabrication, errors, conflicts, and omissions. Existing evaluations of memory hallucinations are primarily end-to-end question answering, which makes it difficult to localize the operational stage within the memory system where hallucinations arise. To address this, we introduce the Hallucination in Memory Benchmark (HaluMem), the first operation level hallucination evaluation benchmark tailored to memory systems. HaluMem defines three evaluation tasks (memory extraction, memory updating, and memory question answering) to comprehensively reveal hallucination behaviors across different operational stages of interaction. To support evaluation, we construct user-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and HaluMem-Long. Both include about 15k memory points and 3.5k multi-type questions. The average dialogue length per user reaches 1.5k and 2.6k turns, with context lengths exceeding 1M tokens, enabling evaluation of hallucinations across different context scales and task complexities. Empirical studies based on HaluMem show that existing memory systems tend to generate and accumulate hallucinations during the extraction and updating stages, which subsequently propagate errors to the question answering stage. Future research should focus on developing interpretable and constrained memory operation mechanisms that systematically suppress hallucinations and improve memory reliability.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Memory systems in LLM agents frequently hallucinate during storage and retrieval (fabrications, errors, conflicts, omissions), undermining reliable long-term personalization and interaction.<br>‚Ä¢ Existing evaluations are primarily end-to-end question answering, making it difficult to localize which memory operation (extraction, updating, retrieval) introduces hallucinations.<br>‚Ä¢ Lack of operation-level benchmarks and large, user-centric datasets hampers systematic assessment of hallucinations across stages and context scales (including million-token dialogues).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>HaluMem introduces the first operation-level hallucination benchmark for agent memory, defining three tasks‚Äîmemory extraction, memory updating, and memory question answering‚Äîto isolate and measure stage-specific errors. It provides two multi-turn, user-centric datasets (HaluMem-Medium and HaluMem-Long) with ~15k memory points, ~3.5k questions, and million-token contexts to evaluate hallucinations across scales.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Interpretable and Constrained Memory Operations for LLM Agents: Design formal constraints, rule-based validators, and provenance tracking to suppress fabrication, conflicts, and omissions during extraction and updating.<br>‚Ä¢ Causal Tracing of Hallucination Propagation in Agent Memory Pipelines: Develop causal models and intervention tools to quantify how errors introduced in early operations accumulate and impact downstream question answering.<br>‚Ä¢ Self-Healing Memory Systems with Auditing and Conflict Resolution: Create auditing, error-correction, and automatic conflict-resolution mechanisms that detect, repair, and reconcile unreliable or inconsistent memory entries over long interactions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07327" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07327" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that reformulates long-horizon research as a Markov Decision Process with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. We further develop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning framework that incentivizes efficient exploration through geometric reward discounting and enables stable distributed training via adaptive downsampling. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5\% to 42.5\%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Mono-contextual accumulation causes context suffocation in long-horizon research: the expanding prompt crowds out reasoning space, degrading solution quality as interactions grow.<br>‚Ä¢ Noise contamination from irrelevant or erroneous early evidence persists in the single context, creating cascading interference that harms later decisions.<br>‚Ä¢ Existing open-source deep-research agents scale poorly (O(t) context growth), struggling beyond dozens of interactions and limiting exploration depth on complex tasks.<br>‚Ä¢ Traditional RAG setups are largely static and ill-suited for dynamic, open-web information gathering required by real-world long-horizon problems.<br>‚Ä¢ Sparse, terminal-only rewards treat all successful trajectories equally, failing to incentivize efficient (short, cost- and latency-aware) exploration.<br>‚Ä¢ Iterative training introduces variable per-round samples, posing stability and throughput challenges for distributed optimization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>IterResearch reframes deep research as an MDP with Markovian workspace reconstruction: each round retains only the question, an evolving synthesized report (compressed memory), and the last tool interaction, and the agent outputs a structured Think‚ÄìReport‚ÄìAction decision. It is trained with Efficiency-Aware Policy Optimization that applies geometric reward discounting to favor shorter successful trajectories and adaptive downsampling for stable distributed training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning to Synthesize: Training Objectives and Guarantees for Report-Based Markovian Memory: Develop objectives (e.g., information bottleneck, attribution consistency) and verifiable criteria to optimize the fidelity, compression, and noise-robustness of the evolving report.<br>‚Ä¢ Cost-Aware Deep Research: Dynamic Budgeting and Risk-Sensitive Reward Shaping for Long-Horizon Agents: Extend geometric discounting to constrained and risk-sensitive RL that adapts to API cost, latency, and reliability constraints during exploration.<br>‚Ä¢ Hierarchical Markovian Agents: Multi-Granular Memory and Planning for Extreme Interaction Scaling: Combine per-round Markovian workspaces with episodic long-term memory and hierarchical controllers to balance local reasoning with global research strategies.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DRIVE: Data Curation Best Practices for Reinforcement Learning with Verifiable Reward in Competitive Code Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.06307" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.06307" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a resurgence of interest in RLVR. Nevertheless, advances are dominated by mathematics (e.g., AIME), with competitive-programming code generation underexplored and data curation receiving less attention than RL algorithm design. We investigate how to construct RLVR datasets (i.e., RL prompts) and present practical training techniques that yield strong performance on competitive-programming code generation. Our pipeline begins with supervised fine-tuning (SFT) distilled from strong open-source models, augmented with general-purpose and reasoning-intensive data. RL then follows a two-stage process with executable, testcase-driven rewards: first, training on a large, uniformly distributed set of competitive-programming problems using Group Relative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively short response-generation window (e.g., 32k during SFT and 24k in this stage) to expand entropy and mitigate repetition and truncation; second, we perform Pre-GRPO: updating on a small, high-quality set of challenging problems with a large rollout budget (64 rollouts per prompt) under a hard-focus curriculum that continuously retains the most difficult instances throughout training. We implement our method on Qwen2.5-32B and evaluate on LeetCode and Codeforces weekly contests to avoid data leakage. The resulting model achieves state-of-the-art performance among models of similar scale and is comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking. We also examine scaling trends and observe strong RL scaling on an internal large-scale MoE model. Our study distills concise best practices for data curation, entropy expansion, and curriculum design in RLVR for competitive-programming code generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Competitive-programming code generation with RLVR is underexplored versus math-centric benchmarks, despite requiring executable, efficient, and edge-case-robust solutions.<br>‚Ä¢ Prior RLVR work focuses on algorithmic tweaks while overlooking data/prompt curation and difficulty-aware curricula, leading to weak performance on the hardest problems.<br>‚Ä¢ Distilled SFT baselines show low entropy, repetitive/truncated generations, and struggle with long outputs needed by hard tasks.<br>‚Ä¢ Practical gaps exist in how to construct RL prompts, allocate rollout budgets efficiently, and stage curricula under compute constraints; scaling behavior on larger (e.g., MoE) models remains unclear.<br>‚Ä¢ Reliable evaluation is hindered by potential data leakage; verifiable, testcase-driven rewards and clean, post-cutoff benchmarks are needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DRIVE couples difficulty-aware SFT (distillation plus hard-problem oversampling) with a two-stage RLVR pipeline using executable, testcase-based rewards: Stage 1 expands exploration entropy via GRPO on a large, uniformly mixed set with moderate rollouts and shorter generation windows to reduce repetition/truncation; Stage 2 (Pre-GRPO) applies a hard-focus curriculum on a small, high-quality set of difficult problems with a large rollout budget and progressive retention of the hardest instances.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Rollout Allocation and Curriculum Schedules for RLVR in Competitive Programming: Learn to dynamically assign rollouts and adjust curricula based on per-problem difficulty and learning progress to optimize compute use and performance.<br>‚Ä¢ Beyond Pass/Fail: Reward Shaping with Efficiency, Robustness, and Coverage for Verifiable Code Generation: Incorporate runtime, memory, edge-case coverage, and partial-credit signals to guide RL toward efficient and resilient solutions.<br>‚Ä¢ Scaling RLVR with MoE and Long-Context Models: Entropy Expansion and Hard-Focus Curricula at Scale: A systematic study of scaling laws, context windows, and curriculum design from 32B dense models to large MoE systems.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">The Station: An Open-World Environment for AI-Driven Discovery</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.06309" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.06309" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce the STATION, an open-world multi-agent environment that models a miniature scientific ecosystem. Leveraging their extended context windows, agents in the Station can engage in long scientific journeys that include reading papers from peers, formulating hypotheses, submitting code, performing analyses, and publishing results. Importantly, there is no centralized system coordinating their activities - agents are free to choose their own actions and develop their own narratives within the Station. Experiments demonstrate that AI agents in the Station achieve new state-of-the-art performance on a wide range of benchmarks, spanning from mathematics to computational biology to machine learning, notably surpassing AlphaEvolve in circle packing. A rich tapestry of narratives emerges as agents pursue independent research, interact with peers, and build upon a cumulative history. From these emergent narratives, novel methods arise organically, such as a new density-adaptive algorithm for scRNA-seq batch integration. The Station marks a first step towards autonomous scientific discovery driven by emergent behavior in an open-world environment, representing a new paradigm that moves beyond rigid optimization.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Centralized, stateless pipelines (evolution/tree search with a fixed metric and single-step improvements) constrain openness and creativity in scientific discovery.<br>‚Ä¢ Existing systems lack a persistent, autonomous, multi-agent environment with memory and social interaction to support long scientific journeys, cumulative knowledge, and independent narratives.<br>‚Ä¢ Current evaluation paradigms emphasize short-term score-chasing, missing emergent behaviors that can yield cross-domain, original methods beyond rigid optimization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The Station is a persistent open-world multi-agent environment where LLM-based agents autonomously choose actions across specialized rooms (e.g., research counter, archive, forums, reflection), maintain public/private memories and lineages, and interact over thousands of ticks to read, hypothesize, code, run experiments, and publish. This design elicits emergent collaboration and method invention, achieving SOTA across diverse benchmarks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Scaling Open-World Scientific Ecosystems: Inter-Station Knowledge Transfer and Multi-Domain Research at Scale: Connect multiple Stations with heterogeneous agents and tools to study transfer, curriculum emergence, and resource allocation across domains.<br>‚Ä¢ Governance and Incentive Design for AI Research Communities: Reputation, Peer Review, and Anti-Goal-Hacking Mechanisms: Develop formal incentive schemes and moderation protocols that foster cooperation, discourage degenerate behaviors, and improve reproducibility.<br>‚Ä¢ Measuring Emergent Discovery Beyond Leaderboards: Novelty, Narrative Complexity, and Cumulative Impact Metrics: Build evaluation frameworks that quantify originality, narrative-driven progress, reproducibility, and long-horizon contributions in open-world AI science.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07250" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07250" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The advent of Multimodal Large Language Models (MLLMs) has expanded AI capabilities to visual modalities, yet existing evaluation benchmarks remain limited to single-video understanding, overlooking the critical need for multi-video understanding in real-world scenarios (e.g., sports analytics and autonomous driving). To address this significant gap, we introduce MVU-Eval, the first comprehensive benchmark for evaluating Multi-Video Understanding for MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies through 1,824 meticulously curated question-answer pairs spanning 4,959 videos from diverse domains, addressing both fundamental perception tasks and high-order reasoning tasks. These capabilities are rigorously aligned with real-world applications such as multi-sensor synthesis in autonomous systems and cross-angle sports analytics. Through extensive evaluation of state-of-the-art open-source and closed-source models, we reveal significant performance discrepancies and limitations in current MLLMs' ability to perform understanding across multiple videos. The benchmark will be made publicly available to foster future research.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing video benchmarks evaluate single-video understanding, leaving the critical capability of multi-video aggregation, correlation, and reasoning unassessed.<br>‚Ä¢ Real-world applications (e.g., multi-camera sports analytics, autonomous driving, multi-retrieval summarization) inherently require multi-video perception and reasoning that current evaluations do not capture.<br>‚Ä¢ There is no standardized, comprehensive task suite spanning both fundamental perception (OR, SU, counting, cross-video comparison) and high-order reasoning (KIR, ICL, RAG, temporal reasoning) across multiple videos.<br>‚Ä¢ The field lacks clear diagnostics of model limitations on multi-video inputs; current MLLMs‚Äô performance, subtask imbalance, scaling behavior, and context-length sensitivity remain underexplored.<br>‚Ä¢ Existing datasets have limited domain diversity; a broader set of domains is needed to stress-test generalization in multi-video scenarios.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MVU-Eval introduces a comprehensive multi-video benchmark with 1,824 curated QA pairs over 4,959 videos, evaluating eight core competencies split into Perception (OR, SU, Counting, Comparison: Replacement/Removal/Addition) and Reasoning (KIR, ICL, RAG, TR). The authors provide standardized multi-video QA protocols and extensively evaluate open- and closed-source MLLMs to surface performance gaps and design insights.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Cross-Video Memory and Alignment for Multimodal LLMs: Design architectures that build persistent entity/track memory and cross-view scene alignment to support OR, SU, counting, and fine-grained comparison across asynchronous videos.<br>‚Ä¢ Efficient Long-Context Multimodal Attention for Multi-Video Understanding: Develop memory-efficient attention, hierarchical encoders, and adaptive frame/resolution selection to scale to more videos, frames, and higher resolutions without sacrificing accuracy.<br>‚Ä¢ Retrieval-Augmented Instruction Tuning for Knowledge-Intensive Multi-Video Reasoning: Combine large-scale multi-video instruction tuning with video-text retrieval and external knowledge grounding to improve KIR, ICL, RAG, and temporal ordering performance.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Routing Manifold Alignment Improves Generalization of Mixture-of-Experts LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07419" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07419" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large language models since it can efficiently scale up the model capability without increasing the inference cost. However, evaluations on broad downstream tasks reveal a consistent suboptimality of the routers in existing MoE LLMs, which results in a severe performance gap (e.g., 10-20% in accuracy) to the optimal routing. In this paper, we show that aligning the manifold of routing weights with that of task embedding can effectively reduce the gap and improve MoE LLMs' generalization performance. Our method, "Routing Manifold Alignment (RoMA)", introduces an additional manifold regularization term in the post-training objective and only requires lightweight finetuning of routers (with other parameters frozen). Specifically, the regularization encourages the routing weights of each sample to be close to those of its successful neighbors (whose routing weights lead to correct answers) in a task embedding space. Consequently, samples targeting similar tasks will share similar expert choices across layers. Building such bindings between tasks and experts over different samples is essential to achieve better generalization. Moreover, RoMA demonstrates the advantage of unifying the task understanding (by embedding models) with solution generation (by MoE LLMs). In experiments, we finetune routers in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse benchmarks and extensive comparisons with baselines show the substantial improvement brought by RoMA.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Pretrained MoE routers are suboptimal, yielding a 10‚Äì20% accuracy gap versus oracle routing across diverse downstream tasks.<br>‚Ä¢ Routing weight manifolds are misaligned with task embedding manifolds, sending semantically similar inputs to disparate experts and undermining knowledge sharing and generalization.<br>‚Ä¢ Prior routing improvements (e.g., load balancing, differentiable top-k, Dense BP) overlook the geometric structure of tasks; test-time methods like C3PO add 6‚Äì7x inference cost; na√Øve router tuning lacks structure-guided signals.<br>‚Ä¢ There is a need for a lightweight post-training method that improves generalization by better routing, updates only a tiny fraction of parameters, and preserves inference cost.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RoMA post-trains only the routers by adding a manifold regularization term that pulls each sample‚Äôs routing weights toward those of its ‚Äúsuccessful neighbors‚Äù in a task-embedding space (k-NN or Œµ-ball), yielding L = cross-entropy + Œª Œ£_j W_ij ||r_i ‚àí r_j||^2. Experts are frozen, only ~0.0095% of parameters are updated (often in the last layers), improving expert consistency for semantically similar inputs without increasing inference cost.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Joint Learning of Task Embeddings and Routing via End-to-End Manifold Alignment: Co-train the embedding model and routers to remove dependence on external embeddings and better couple task understanding with expert selection.<br>‚Ä¢ Online RoMA: Test-Time Adaptive Routing Manifold Alignment with Budgeted Overhead: Develop lightweight, on-the-fly alignment mechanisms that adapt routing at inference under tight latency constraints.<br>‚Ä¢ Stratified Manifold-Aware Expert Specialization for MoE LLMs: Design experts to correspond to strata in the task manifold, jointly learning expert scopes and routing aligned to stratified geometry.<br>‚Ä¢ Causal Manifold Alignment for Robust Routing under Distribution Shift: Incorporate causal/robust objectives so routing aligns with invariant task structure and remains stable across domains and shifts.<br>‚Ä¢ Multi-Modal Routing Manifold Alignment for Unified MoE: Extend RoMA to text‚Äìvision‚Äìaudio settings with shared or coordinated manifolds guiding cross-modal expert selection.<br>‚Ä¢ Theoretical Guarantees for Manifold-Regularized Routing in Sparse MoE: Analyze conditions under which manifold alignment reduces generalization error and approximates oracle routing.<br>‚Ä¢ Self-Supervised Successful Neighbor Discovery for Label-Efficient RoMA: Replace label-based ‚Äúsuccessful neighbor‚Äù filtering with self-consistency or confidence signals to reduce supervision needs.<br>‚Ä¢ Co-Designing Expert Architectures with Routing-Manifold Partitions: Jointly optimize the number, capacity, and connectivity of experts with learned manifold partitions for efficient specialization.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking Services</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07070" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07070" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As a key medium for human interaction and information exchange, social networking services (SNS) pose unique challenges for large language models (LLMs): heterogeneous workloads, fast-shifting norms and slang, and multilingual, culturally diverse corpora that induce sharp distribution shift. Supervised fine-tuning (SFT) can specialize models but often triggers a ``seesaw'' between in-distribution gains and out-of-distribution robustness, especially for smaller models. To address these challenges, we introduce RedOne 2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized post-training paradigm designed for rapid and stable adaptation. The pipeline consist in three stages: (1) Exploratory Learning on curated SNS corpora to establish initial alignment and identify systematic weaknesses; (2) Targeted Fine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a small fraction of general data to mitigate forgetting; and (3) Refinement Learning that re-applies RL with SNS-centric signals to consolidate improvements and harmonize trade-offs across tasks. Across various tasks spanning three categories, our 4B scale model delivers an average improvements about 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves average performance lift about 8.74 from the base model with less than half the data required by SFT-centric method RedOne, evidencing superior data efficiency and stability at compact scales. Overall, RedOne 2.0 establishes a competitive, cost-effective baseline for domain-specific LLMs in SNS scenario, advancing capability without sacrificing robustness.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ SNS workloads are heterogeneous, fast-changing, and multilingual/cross-cultural, causing sharp distribution shifts that degrade general-purpose LLMs.<br>‚Ä¢ SFT-dominated domain adaptation triggers a ‚Äúseesaw‚Äù trade-off: in-distribution gains come at the expense of out-of-distribution robustness and catastrophic forgetting, especially in smaller models.<br>‚Ä¢ Existing mitigations rely on very large data mixtures and complex schedules, inflating data/compute cost and limiting practicality for compact models.<br>‚Ä¢ Prior post-training is often domain-agnostic and underuses specialized signals; RL for SNS is underexplored and lacks structured curricula aligned to diverse task formats and evaluation criteria.<br>‚Ä¢ SNS tasks have varied formats and metrics (classification, translation, code, instruction-following), demanding task-specific supervision signals rather than a single reward.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RedOne 2.0 introduces a progressive, RL-prioritized three-stage post-training pipeline: Exploratory Learning with DAPO RL on curated SNS plus small general data using task-specific rewards to establish alignment and diagnose weaknesses; Targeted Fine-Tuning (SFT) on identified gaps while mixing a small, soft-labeled general set to mitigate forgetting; and Refinement Learning that re-applies DAPO RL with SNS-centric signals (more rationales) to consolidate gains and harmonize trade-offs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive RL Curriculum for Real-Time Drift in Social Platforms: Design online, trend-aware task sampling and reward updates to track evolving slang/norms and maintain stability without the SFT seesaw.<br>‚Ä¢ Faithful Engagement: Safety- and Faithfulness-Constrained RL for SNS Generation: Integrate factuality and safety constraints into the RL objective to prevent over-optimizing engagement at the expense of essential details in creator assistance and moderation.<br>‚Ä¢ Cross-cultural Reward Modeling for Multilingual SNS Alignment: Learn reward models that encode culture-specific pragmatics (emoji, memes, humor) to improve multilingual translation and dialogue beyond BLEU/chrF++ baselines.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.06411" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.06411" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The soft-thinking paradigm for Large Language Model (LLM) reasoning can outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in some scenarios, underscoring its research and application value. However, while the discrete-token CoT reasoning pattern can be reinforced through policy optimization algorithms such as group relative policy optimization (GRPO), extending the soft-thinking pattern with Reinforcement Learning (RL) remains challenging. This difficulty stems from the complexities of injecting stochasticity into soft-thinking tokens and updating soft-thinking policies accordingly. As a result, previous attempts to combine soft-thinking with GRPO typically underperform their discrete-token GRPO counterparts. To fully unlock the potential of soft-thinking, this paper presents a novel policy optimization algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning pattern. SofT-GRPO injects the Gumbel noise into logits, employs the Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained embedding space, and leverages the reparameterization trick in policy gradient. We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes and weights are available on https://github.com/zz1358m/SofT-GRPO-master</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Soft-thinking can outperform discrete-token Chain-of-Thought (CoT) without fine-tuning, but lacks an effective reinforcement learning (RL) policy optimization counterpart.<br>‚Ä¢ Applying RLVR (e.g., GRPO) to soft-thinking is challenging because soft-thinking tokens are deterministic, making controlled stochasticity and effective policy updates non-trivial.<br>‚Ä¢ Prior attempts (e.g., Gaussian noise on inputs) underperform due to off-manifold soft tokens and poor credit assignment; a method that injects valid randomness and enables unbiased gradients within the pre-trained embedding space is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SofT-GRPO injects Gumbel noise into logits and uses Gumbel-Softmax to sample soft-thinking tokens within the pre-trained embedding space, then applies a Gumbel reparameterization-based policy gradient in a GRPO-style group rollout to attribute rewards to output probabilities and update the policy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Temperature-Scheduled SofT-GRPO: Adaptive Gumbel Noise for Efficient Soft-Thinking RL: Explore dynamic temperature and noise shaping to balance exploration and stability, improving sample efficiency and Pass@1.<br>‚Ä¢ Manifold-Constrained Soft-Thinking: Geometry-Aware Policy Optimization Beyond Gumbel-Softmax: Incorporate embedding manifold constraints (projection/Riemannian optimization) to further prevent off-manifold inputs and enhance credit assignment.<br>‚Ä¢ Multi-Group Credit Assignment in Soft-Thinking RLVR: From GRPO to Counterfactual Baselines: Integrate variance-reduced estimators (e.g., RELAX/REBAR) tailored to Gumbel-Softmax within group rollouts to improve gradient quality at high sample rates.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.06209" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.06209" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Solving complex tasks usually requires LLMs to generate long multi-step reasoning chains. Previous work has shown that verifying the correctness of individual reasoning steps can further improve the performance and efficiency of LLMs on such tasks and enhance solution interpretability. However, existing verification approaches, such as Process Reward Models (PRMs), are either computationally expensive, limited to specific domains, or require large-scale human or model-generated annotations. Thus, we propose a lightweight alternative for step-level reasoning verification based on data-driven uncertainty scores. We train transformer-based uncertainty quantification heads (UHeads) that use the internal states of a frozen LLM to estimate the uncertainty of its reasoning steps during generation. The approach is fully automatic: target labels are generated either by another larger LLM (e.g., DeepSeek R1) or in a self-supervised manner by the original model itself. UHeads are both effective and lightweight, containing less than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, they match or even surpass the performance of PRMs that are up to 810x larger. Our findings suggest that the internal states of LLMs encode their uncertainty and can serve as reliable signals for reasoning verification, offering a promising direction toward scalable and generalizable introspective LLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Chain-of-thought (CoT) solutions often contain flawed intermediate steps that propagate errors and undermine trust, especially in high-stakes domains.<br>‚Ä¢ Process Reward Models (PRMs) for step verification are large and costly, requiring Monte Carlo rollouts, external LLM supervisors, and substantial human/model-generated annotations; they also struggle in domains where final answers don‚Äôt reveal step correctness.<br>‚Ä¢ Existing lightweight, unsupervised uncertainty methods (e.g., self-consistency heuristics) miss subtle reasoning flaws and lag behind PRMs, motivating efficient, generalizable, step-level verification using model-internal signals and automatic labels.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Train compact transformer-based uncertainty quantification heads (UHeads) on top of a frozen LLM to predict per-step uncertainty from internal features (attention to preceding tokens and top-K logits), using labels generated by an external LLM judge or self-supervision. Use 1‚àíU as a quality score for online/offline best-of-N test-time scaling to verify and select reasoning steps/chains, achieving PRM-level performance with <10M parameters.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Branch-and-Verify: Online Reasoning Control with Uncertainty Heads: Integrate UHead scores into dynamic branching, backtracking, and compute allocation to steer generation away from uncertain steps in real time.<br>‚Ä¢ Cross-Model and Cross-Domain Transfer of UHeads for Universal Reasoning Verification: Learn transferable UHeads that generalize across base LLM architectures and tasks (math, planning, QA) with minimal adaptation.<br>‚Ä¢ Self-Supervised Calibration of Internal Uncertainty Signals in LLMs: Improve the calibration of UHead outputs via conformal prediction, temperature scaling, and iterative self-training on automatically labeled reasoning traces.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07384" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07384" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in depth-recurrent language models show that recurrence can decouple train-time compute and parameter count from test-time compute. In this work, we study how to convert existing pretrained non-recurrent language models into depth-recurrent models. We find that using a curriculum of recurrences to increase the effective depth of the model over the course of training preserves performance while reducing total computational cost. In our experiments, on mathematics, we observe that converting pretrained models to recurrent ones results in better performance at a given compute budget than simply post-training the original non-recurrent language model.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Enable test-time compute scaling without increasing parameters or context length by retrofitting depth recurrence into pretrained LMs, avoiding the inefficiencies of token-based scaling (e.g., chain-of-thought, self-consistency).<br>‚Ä¢ Reduce the high training cost of recurrent models trained from scratch by leveraging pretrained weights and efficient continued pretraining.<br>‚Ä¢ Address limitations of prior retrofitting methods that rely on auxiliary adapters or degrade with more recurrences, ensuring benefits from additional test-time compute.<br>‚Ä¢ Preserve general language modeling performance after model surgery despite distribution shift, while improving math reasoning under fixed compute budgets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Perform model surgery to partition a pretrained transformer into a prelude, a shared recurrent block, and a coda; remove intermediate layers and add an input-injection adapter that concatenates prelude and recurrent states before each recurrence. Continue pretraining with a curriculum that gradually increases the mean number of recurrences and a brief healing phase before task-specific data, yielding models that scale with test-time recurrences at constant inference memory.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning to Halt: Adaptive Exit Policies for Retrofitted Recurrent Language Models: Add learned halting mechanisms to choose the number of recurrences per input, optimizing accuracy‚Äìcompute trade-offs.<br>‚Ä¢ Auto-Surgeon: Automated Layer Partitioning and Input Injection for Recurrence Retrofitting: Algorithmically select prelude/recurrent/coda layers and adapter configurations to minimize distribution shift and maximize per-FLOP gains.<br>‚Ä¢ Distilling Chain-of-Thought into Latent Recurrence: Transfer supervision from token-level reasoning traces into latent recurrent dynamics to combine CoT benefits with constant-context, parameter-efficient inference.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Robot Learning from a Physical World Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07416" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07416" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit https://pointscoder.github.io/PhysWorld_Web/{the project webpage} for details.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Bridging visually plausible but physics-agnostic motions from generated videos to executable, accurate robot actions is challenging in real-world settings.<br>‚Ä¢ Prior alignment approaches require large-scale real robot demonstrations, while direct pixel-level retargeting (flows/tracks/poses) ignores physical constraints and leads to manipulation errors.<br>‚Ä¢ Existing real-to-sim pipelines assume multi-view, clean captures; monocular generated videos are partial and noisy (blur/hallucinations), making reconstruction and control unreliable.<br>‚Ä¢ Eliminating costly real-world data collection while achieving zero-shot, physically accurate, and generalizable manipulation is critical for scalable robot learning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>PhysWorld couples task-conditioned video generation with a reconstructed physical world model: from a single RGB-D image and text prompt, it generates a video, builds a metric- and geometry-aligned 4D scene, synthesizes textured meshes with physical properties, aligns to gravity, and resolves collisions to create a simulatable digital twin. It then learns object-centric residual RL policies that track video-derived object pose trajectories, refining grasp-and-plan baselines via PPO in the world model to output physically feasible actions for zero-shot real-robot execution.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ PhysWorld-Gen: Training Video Generators with Physics-Based Feedback for Robot Manipulation: Close the loop by conditioning video generators on simulator feedback to produce physically consistent, task-feasible demonstrations.<br>‚Ä¢ Active Multiview PhysWorld: Autonomous View Planning for Robust World Modeling from Generated Videos: Use active perception or synthetic view synthesis to reduce monocular reconstruction errors and improve sim-to-real transfer.<br>‚Ä¢ Differentiable PhysWorld: End-to-End Learning of Physical Parameters and Policies from Generated Videos: Integrate differentiable simulation to jointly infer masses/frictions and learn policies directly from video supervision.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS Modeling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.06194" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.06194" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Generating editable 3D CAD models from natural language remains challenging, as existing text-to-CAD systems either produce meshes or rely on scarce design-history data. We present NURBGen, the first framework to generate high-fidelity 3D CAD models directly from text using Non-Uniform Rational B-Splines (NURBS). To achieve this, we fine-tune a large language model (LLM) to translate free-form texts into JSON representations containing NURBS surface parameters (i.e, control points, knot vectors, degrees, and rational weights) which can be directly converted into BRep format using Python. We further propose a hybrid representation that combines untrimmed NURBS with analytic primitives to handle trimmed surfaces and degenerate regions more robustly, while reducing token complexity. Additionally, we introduce partABC, a curated subset of the ABC dataset consisting of individual CAD components, annotated with detailed captions using an automated annotation pipeline. NURBGen demonstrates strong performance on diverse prompts, surpassing prior methods in geometric fidelity and dimensional accuracy, as confirmed by expert evaluations. Code and dataset will be released publicly.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Absence of methods that generate editable, parametric CAD (BRep/NURBS) directly from text‚Äîmost prior work outputs meshes or depends on scarce design-history data with low geometric complexity.<br>‚Ä¢ Large BRep resources (e.g., ABC) are underused because they lack high-quality captions and NURBS are hard to encode (variable parameters, knot non-differentiability, trimming complexity).<br>‚Ä¢ Untrimmed NURBS alone struggle on trimmed/degenerate regions (holes, thin faces), causing artifacts; a robust yet token-efficient representation is needed for LLMs.<br>‚Ä¢ Serializing CAD geometry for language models risks exceeding context windows; efficient, structured, and compact tokenization is required.<br>‚Ä¢ Existing text-to-CAD baselines often show lower geometric fidelity and higher invalidity rates, limiting practical deployment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>NURBGen fine-tunes an LLM (Qwen3-4B) to translate text into a hybrid symbolic CAD JSON‚Äîuntrimmed NURBS surfaces plus analytic primitives‚Äîenabling direct BRep reconstruction. It is trained on partABC, a 300k-part dataset built from ABC via a PythonOCC-based NURBS/primitive extractor and a metadata-guided multi-view captioning pipeline, with token-efficiency tricks and CD-based quality gating for NURBS-to-primitive fallback.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Long-Context NURBS-LMs for Assembly-Level Text-to-CAD: Scale context and hierarchical serialization to generate full multi-part assemblies from text.<br>‚Ä¢ Trim-Aware Differentiable NURBS Generation: Learn knots, trims, and weights end-to-end with differentiable trimming and constraint enforcement.<br>‚Ä¢ Multimodal Text‚ÄìImage‚ÄìSketch to NURBS CAD Co-Generation: Fuse textual prompts with images and 2D sketches to improve fidelity and disambiguation.<br>‚Ä¢ Constraint- and Tolerance-Aware Text-to-CAD Synthesis: Integrate geometric constraints, GD&T, and manufacturability checks during generation.<br>‚Ä¢ Self-Verification and Repair for LLM-Generated BReps: Add a verifier/repair module to detect and fix self-intersections, invalid topology, and dimensional drift.<br>‚Ä¢ Tokenization and Compression for Symbolic CAD Sequences: Develop CAD-specific tokenizers and codecs to compress NURBS/primitives while preserving editability.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">DigiData: Training and Evaluating General-Purpose Mobile Control Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07413" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07413" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing mobile control datasets are not curated to cover advanced app functionalities and thus lack the depth, diversity, and scale required for training general-purpose agents.<br>‚Ä¢ Common evaluation practices (e.g., step-accuracy and single-step action matching) are unreliable and narrow, failing to measure true task success on complex, real-world goals.<br>‚Ä¢ There is a need for a high-quality, multi-modal training corpus and robust, dynamic evaluation protocols that reflect human-relevant outcomes and enable rapid, meaningful progress.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces DigiData, a large-scale dataset built via exhaustive feature enumeration per app, human demonstrations captured through a custom Android system, and hybrid LLM+human trajectory verification; it also presents DigiData-Bench with human- and AI-assisted dynamic evaluation protocols to rigorously assess mobile control agents on complex goals.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ LLM Judges for Mobile UI Control: Reliability, Bias, and Calibration: Systematically assess and calibrate LLM-based evaluators against human judgments across diverse tasks, including adversarial and ambiguous cases.<br>‚Ä¢ From Demonstrations to Autonomy: Reinforcement Learning on DigiData: Leverage DigiData for offline-to-online RL, reward modeling, and interactive fine-tuning to surpass supervised performance on complex mobile tasks.<br>‚Ä¢ Cross-App Generalization via Feature-Enumerated Goals: Investigate representation learning and transfer methods that exploit exhaustive feature coverage to improve zero-shot and few-shot generalization across unseen apps.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MPJudge: Towards Perceptual Assessment of Music-Induced Paintings</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07137" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07137" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Music induced painting is a unique artistic practice, where visual artworks are created under the influence of music. Evaluating whether a painting faithfully reflects the music that inspired it poses a challenging perceptual assessment task. Existing methods primarily rely on emotion recognition models to assess the similarity between music and painting, but such models introduce considerable noise and overlook broader perceptual cues beyond emotion. To address these limitations, we propose a novel framework for music induced painting assessment that directly models perceptual coherence between music and visual art. We introduce MPD, the first large scale dataset of music painting pairs annotated by domain experts based on perceptual coherence. To better handle ambiguous cases, we further collect pairwise preference annotations. Building on this dataset, we present MPJudge, a model that integrates music features into a visual encoder via a modulation based fusion mechanism. To effectively learn from ambiguous cases, we adopt Direct Preference Optimization for training. Extensive experiments demonstrate that our method outperforms existing approaches. Qualitative results further show that our model more accurately identifies music relevant regions in paintings.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of human-aligned, perceptual assessment for music-induced paintings beyond emotion matching.<br>‚Ä¢ Existing methods depend on noisy emotion recognition proxies (DES/CES), ignoring rhythm, timbre, texture, composition, and other perceptual cues.<br>‚Ä¢ Dual-encoder late-fusion architectures fail to capture fine-grained, continuous audio‚Äìvisual interactions needed for multi-level coherence.<br>‚Ä¢ Absence of high-quality, expert-annotated datasets; prior labels are often auto-generated rather than grounded in human judgments.<br>‚Ä¢ Difficulty learning from ambiguous scalar labels (scores near 0.5), requiring preference-based supervision to resolve nuanced perceptual choices.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MPJudge integrates music features into a Transformer-based painting encoder via Modality-Adaptive Normalization (MAN) and trains with a hybrid objective combining MSE on expert scalar coherence scores and Direct Preference Optimization (DPO) on pairwise preferences, leveraging the new MPD dataset of over 50,000 annotated music‚Äìpainting pairs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ SynGen: Generative Co-Creation of Music-Induced Paintings Guided by Perceptual Coherence ‚Äî Use MPJudge as a guidance or discriminator to train diffusion/GAN models that generate paintings aligned with input music.<br>‚Ä¢ Personalized MPJudge: Modeling Individual Synesthetic Preferences for Music‚ÄìPainting Alignment ‚Äî Learn user-specific perceptual profiles and adapt the assessor to personal taste and cultural background via meta-learning or conditional adapters.<br>‚Ä¢ TempoPaint: Temporal-Coherent Assessment of Painting Processes Under Music ‚Äî Extend assessment to sequences of painting steps, aligning brushstroke dynamics and composition evolution with musical rhythm and structure.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07025" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07025" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce llama-embed-nemotron-8b, an open-weights text embedding model that achieves state-of-the-art performance on the Multilingual Massive Text Embedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent models show strong performance, their training data or methodologies are often not fully disclosed. We aim to address this by developing a fully open-source model, publicly releasing its weights and detailed ablation studies, and planning to share the curated training datasets. Our model demonstrates superior performance across all major embedding tasks -- including retrieval, classification and semantic textual similarity (STS) -- and excels in challenging multilingual scenarios, such as low-resource languages and cross-lingual setups. This state-of-the-art performance is driven by a novel data mix of 16.1 million query-document pairs, split between 7.7 million samples from public datasets and 8.4 million synthetically generated examples from various open-weight LLMs. One of our key contributions is a detailed ablation study analyzing core design choices, including a comparison of contrastive loss implementations, an evaluation of synthetic data generation (SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b is an instruction-aware model, supporting user-defined instructions to enhance performance for specific use-cases. This combination of top-tier performance, broad applicability, and user-driven flexibility enables it to serve as a universal text embedding solution.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of a truly universal, text-only embedding model that performs robustly across retrieval, classification, and STS tasks in multilingual, cross-lingual, and low-resource settings<br>‚Ä¢ Opaqueness of recent SOTA models (undisclosed data/methods, non-open weights), hindering reproducibility, auditability, and deployment<br>‚Ä¢ Need for instruction-aware embeddings that adapt to diverse task requirements without maintaining multiple specialized models<br>‚Ä¢ Suboptimal use of decoder-only causal LLMs for embeddings; absence of bi-directional context limits representation quality<br>‚Ä¢ Missing transparent, effective training recipes (contrastive objectives, hard negative mining, synthetic data generation) tailored for multilingual embeddings and cross-lingual alignment</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Convert Llama-3.1-8B into a bi-directional encoder, then fine-tune end-to-end with instruction-aware inputs and global average pooling, training via InfoNCE contrastive loss with task-specific triplet construction and mined hard negatives over a 16.1M multilingual mix of public and synthetic pairs. Use a bi-encoder for retrieval and a uni-encoder for STS/classification, with instructions steering the embedding space for each task.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Auto-Instruct: Learning Task-Specific Prompts for Universal Multilingual Embeddings: Automatically induce or optimize instructions per task/domain/language to maximize embedding accuracy without manual prompt engineering<br>‚Ä¢ Synthetic Data Curriculum for Cross-Lingual Embedding Models: Systematically study and schedule synthetic data generation strategies and curricula to improve low-resource and cross-lingual performance<br>‚Ä¢ Distilling 8B Universal Embeddings into Compact Models for Edge Retrieval: Compress the instruction-aware multilingual encoder into small, efficient models while preserving cross-lingual and task-general performance</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.04285" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.04285" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for training large reasoning models, its training dynamics harbor a critical challenge: RL overfitting, where models gain training rewards but lose generalization. Our analysis reveals this is driven by policy over-specialization and catastrophic forgetting of diverse solutions generated during training. Standard optimization discards this valuable inter-step policy diversity. To address this, we introduce RLoop, a self-improving framework built on iterative policy initialization. RLoop transforms the standard training process into a virtuous cycle: it first uses RL to explore the solution space from a given policy, then filters the successful trajectories to create an expert dataset. This dataset is used via Rejection-sampling Fine-Tuning (RFT) to refine the initial policy, creating a superior starting point for the next iteration. This loop of exploration and exploitation via iterative re-initialization effectively converts transient policy variations into robust performance gains. Our experiments show RLoop mitigates forgetting and substantially improves generalization, boosting average accuracy by 9% and pass@32 by over 15% compared to vanilla RL.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ RL overfitting in RLVR: training rewards keep rising while out-of-distribution accuracy and pass@k stagnate or degrade, indicating brittle, overspecialized policies.<br>‚Ä¢ Catastrophic forgetting across RL steps: the model discards a substantial portion of previously learned solutions, and standard training ignores rich inter-step policy diversity.<br>‚Ä¢ Limitations of existing methods: prior RL/SFT hybrids often rely on external expert data or focus on single-policy diversity, providing limited gains in generalization and pass@k and yielding unstable training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RLoop alternates RL exploration (to generate diverse trajectories across steps) with exploitation via Rejection-sampling Fine-Tuning on filtered successful trajectories, re-initializing the policy each iteration to consolidate knowledge. This self-bootstrapping loop, augmented with active learning to focus on hard problems, converts transient inter-step diversity into durable generalization without external expert data.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ RLoop++: Adaptive Iteration Scheduling with Forgetting-Aware Early Stopping: Trigger RL‚ÜîRFT switches using online learning/forgetting estimators and reweight samples by estimated stability.<br>‚Ä¢ Multi-Task RLoop: Cross-Domain Iterative Policy Initialization for General-Purpose Reasoning: Share expert buffers and iterative re-initialization across tasks to study transfer and interference.<br>‚Ä¢ Diversity-Guided RLoop: Explicit Inter-Step Diversity Regularization and Selection: Use trajectory diversity metrics to guide exploration and curate expert datasets that maximize complementary coverage.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.00710" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.00710" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While Vision-Language Models (VLMs) post-trained with Reinforcement Learning (RL) show impressive general reasoning, their evaluation is often confined to language-dominant tasks (e.g., math). This raises a critical question: can RL post-training truly extend the inherent capability boundary of a base VLM, particularly for visual-centric spatial tasks where it initially fails? To investigate this, we introduce Ariadne, a framework utilizing synthetic mazes for multi-step spatial reasoning where task difficulty (e.g., path length, turns) is precisely controlled. We leverage this controllable environment to train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves over 50% accuracy on a problem set where the base model scored 0%, demonstrating that our approach expands the model's initial capability boundary. To assess real-world viability, we evaluate out-of-distribution (OOD) generalization on practical benchmarks. Despite training only on synthetic maze samples, Ariadne achieves significant zero-shot improvements, averaging 16% on MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer tasks). These results confirm that our method not only broadens the model's fundamental limits but also enhances its generalization to real-world spatial reasoning. We acknowledge our study is limited to the post-training phase, given the opaqueness of pre-training data, and hope our research motivates further work on specialized, capability-extending alignment.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ RL-post-trained VLMs are mostly assessed on language-dominant tasks (e.g., math), leaving it unclear whether RLVR can extend a base VLM‚Äôs inherent capability boundary in visual-centric spatial reasoning.<br>‚Ä¢ Existing evaluations lack controllable difficulty and verifiable rewards for multi-step spatial tasks, making systematic probing of reasoning boundaries and failure modes difficult.<br>‚Ä¢ Prior approaches (CoT/SFT or RL with reward models) often rely on large synthetic data or are architecture-specific, and pretraining data opacity hinders analysis of intrinsic limits and targeted boundary expansion.<br>‚Ä¢ Baseline VLMs collapse at modest path lengths/turns; there is a need for a principled framework to extend and analyze reasoning boundaries and to test real-world transfer.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Ariadne trains a VLM on synthetic mazes with tunable complexity using GRPO-based RLVR and a verified, stepwise, turn-scaled reward under a difficulty-aware curriculum, then evaluates zero-shot transfer to real-world map benchmarks (MapBench, ReasonMap).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Pretrain-Ariadne: Integrating Controllable RLVR into VLM Pretraining to Expand Fundamental Spatial Reasoning Capacity: Embed verified, curriculum-based spatial tasks into pretraining to create capabilities beyond post-training alignment.<br>‚Ä¢ Dimensional Curriculum RL for Spatial Reasoning: Decoupling Step-Length and Turn-Complexity to Achieve Uniform Boundary Expansion: Design curricula that independently control steps and turns to eliminate divergent generalization.<br>‚Ä¢ Noise-Aware Transfer from Synthetic to Real Maps: Leveraging Redundancy to Enhance Long-Horizon Planning in VLMs: Model and exploit real-world noise/redundancy to improve out-of-distribution step-count generalization on navigation tasks.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07317" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07317" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Reinforcement Learning (RL) with Adaptive Verifiable Environments (RLVE), an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). RLVE enables each verifiable environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses. In contrast, static data distributions often lead to vanishing learning signals when problems are either too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a large-scale suite of 400 verifiable environments carefully developed through manual environment engineering. Using RLVE-Gym, we show that environment scaling, i.e., expanding the collection of training environments, consistently improves generalizable reasoning capabilities. RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks, starting from one of the strongest 1.5B reasoning LMs. By comparison, continuing this LM's original RL training yields only a 0.49% average absolute gain despite using over 3x more compute. We release our code publicly.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ RL on static, finite datasets saturates as the model improves, causing vanishing or misleading learning signals because task difficulty does not adapt to policy capability.<br>‚Ä¢ Collecting large-scale problems with ground-truth answers for verifiable rewards is costly and unscalable, limiting RL data growth.<br>‚Ä¢ Training stalls when tasks are too easy (no gradient signal) or too hard (uniformly poor rewards), underscoring the need for difficulty that tracks model competence.<br>‚Ä¢ Existing RLVR pipelines often focus on narrow domains, limiting transfer; scalable, diverse, and adaptive environments are needed to improve generalizable reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RLVE trains language models in procedurally generated, verifiable environments whose difficulty adapts online by tracking accuracy at the current upper difficulty level and sliding a capped window to keep tasks appropriately challenging. Implemented via RLVE-GYM (400 hand-engineered environments with algorithmically verifiable rewards) and joint multi-environment training, it scales RL data and improves generalization over static RLVR datasets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ AutoEnv: Automated Synthesis of Verifiable Environments for Language Model RL ‚Äî Methods to automatically generate problem generators and verifiers from formal specifications or demonstrations, reducing manual environment engineering cost.<br>‚Ä¢ Curriculum Schedules for RLVE: Theory and Practice of Adaptive Difficulty for LMs ‚Äî Principled thresholding and windowing policies with convergence and sample-efficiency analysis, plus empirical ablations across tasks and models.<br>‚Ä¢ Scaling Laws for Environment Diversity: Quantifying How Environment Scaling Drives Reasoning Generalization ‚Äî Empirical and theoretical study linking the number/diversity of environments and difficulty distribution to downstream benchmark gains and compute efficiency.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.06090" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.06090" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Optimizing the performance of large-scale software repositories demands expertise in code reasoning and software engineering (SWE) to reduce runtime while preserving program correctness. However, most benchmarks emphasize what to fix rather than how to fix code. We introduce SWE-fficiency, a benchmark for evaluating repository-level performance optimization on real workloads. Our suite contains 498 tasks across nine widely used data-science, machine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a complete codebase and a slow workload, an agent must investigate code semantics, localize bottlenecks and relevant tests, and produce a patch that matches or exceeds expert speedup while passing the same unit tests. To enable this how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests for performance-improving edits, combining keyword filtering, static analysis, coverage tooling, and execution validation to both confirm expert speedup baselines and identify relevant repository unit tests. Empirical evaluation of state-of-the-art agents reveals significant underperformance. On average, agents achieve less than 0.15x the expert speedup: agents struggle in localizing optimization opportunities, reasoning about execution across functions, and maintaining correctness in proposed edits. We release the benchmark and accompanying data pipeline to facilitate research on automated performance engineering and long-horizon software reasoning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of benchmarks that evaluate how to optimize code performance in real repositories and workloads, rather than just what to fix at the function level.<br>‚Ä¢ Existing setups (e.g., GSO, SWE-Perf) rely on oracles, conflate correctness and performance tests, or point directly to target functions, failing to assess investigation, test localization, and cross-function reasoning.<br>‚Ä¢ Performance engineering has high practical impact (significant cost and throughput gains) but is underrepresented in LM evaluations.<br>‚Ä¢ Current LM agents severely underperform on repository-level performance optimization, struggling with bottleneck localization, multi-function reasoning, and maintaining correctness; existing metrics do not measure parity with expert speedups.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce SWE-FFICIENCY, a benchmark of 498 real optimization tasks across 9 popular repositories, built via an automated pipeline that mines performance-improving PRs using keyword filtering, static analysis, coverage, and execution validation to confirm expert speedups and identify relevant tests. Evaluate agents using a speedup ratio metric comparing model patches to expert edits, with an open-source harness enabling end-to-end, oracle-free performance optimization and test localization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Agentic Performance Engineers: Integrating Profilers and Static Analysis with LMs for Repository-Level Optimization: Combine dynamic profiling, call-graph/static analysis, and LM planning to improve bottleneck localization and principled optimizations.<br>‚Ä¢ RL from Speedups: Training Language Models with Speedup Ratio Rewards for Code Optimization: Use SR-based reward signals and offline PR datasets to fine-tune LMs for generating performance-improving patches while preserving correctness.<br>‚Ä¢ Learning to Localize: Autonomous Discovery of Relevant Tests and Workloads in Large Repositories: Develop methods for test/workload identification and correctness inference without oracles, enabling scalable, real-world performance engineering agents.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">10 Open Challenges Steering the Future of Vision-Language-Action Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.05936" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.05936" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Due to their ability of follow natural language instructions, vision-language-action (VLA) models are increasingly prevalent in the embodied AI arena, following the widespread success of their precursors -- LLMs and VLMs. In this paper, we discuss 10 principal milestones in the ongoing development of VLA models -- multimodality, reasoning, data, evaluation, cross-robot action generalization, efficiency, whole-body coordination, safety, agents, and coordination with humans. Furthermore, we discuss the emerging trends of using spatial understanding, modeling world dynamics, post training, and data synthesis -- all aiming to reach these milestones. Through these discussions, we hope to bring attention to the research avenues that may accelerate the development of VLA models into wider acceptability.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ VLAs struggle to generalize robustly from demonstrations across diverse tasks, environments, and embodiments; existing RL and policy models often overfit narrowly-defined scenarios.<br>‚Ä¢ There is a gap between VLMs‚Äô high-level reasoning and robot-executable low-level control; discrete action tokenization introduces quantization and slow autoregression, while continuous diffusion policies demand high compute.<br>‚Ä¢ Perception is limited: most VLAs lack explicit depth, audio, and tactile sensing and are brittle to environmental noise, reflections, and artifacts.<br>‚Ä¢ Reasoning remains fragile for long-horizon tasks and tool use; action-grounded chain-of-thought and planning traces still yield imperfect execution.<br>‚Ä¢ Training data is noisy, heterogeneous, and expensive to collect; sim-to-real transfer is weak due to low-fidelity simulators and domain gaps.<br>‚Ä¢ Evaluation is narrow and poorly correlated with real-world performance; benchmarks lack modality diversity (audio/touch) and realism in dynamics and visuals.<br>‚Ä¢ Cross-robot action heterogeneity prevents zero-shot action transfer across embodiments.<br>‚Ä¢ Resource constraints hinder on-board deployment; thin-client setups suffer from latency and connectivity issues.<br>‚Ä¢ Whole-body coordination (coupling locomotion and manipulation) under uncertainty remains a hard high-dimensional control problem.<br>‚Ä¢ Safety alignment and verifiable guardrails are needed to prevent harmful embodied actions.<br>‚Ä¢ Multi-agent agentic frameworks, trust, and verified workflows are underexplored; human‚Äìrobot communication is mostly unidirectional and lacks interpretability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes a high-level, hierarchical multi-agent VLA framework that uses an LLM/VLM planner as orchestrator, explicit reasoning-before-action traces, specialist low-level action experts (discrete or diffusion), safety guardrails, and world-model-based simulation, alongside technical directions including depth-aware spatial perception, universal action representations, world dynamics modeling, video-driven data synthesis of latent actions, and RL-style post-training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Depth-Aware Multimodal VLA: RGB-D, LIDAR, and Tactile Fusion for Robust Perception: Fine-tune VLM backbones with synthetic and real RGB-D/tactile data (e.g., Locate3D, depth-aware tokens) to achieve 3D spatial understanding and noise robustness.<br>‚Ä¢ Universal Atomic Actions: Codebook and In-Context Adaptation for Cross-Robot Transfer: Learn a shared action codebook and enable few-shot prompt-based adaptation to new embodiments for near zero-shot transfer.<br>‚Ä¢ Reasoning Before Action: Structured CoT Grounding for Long-Horizon Manipulation: Generate explicit intermediate reasoning traces that condition action experts, improving tool use and long-horizon success.<br>‚Ä¢ World Models as Reward: Scalable Post-Training for VLAs Without Simulators: Use embedding-predictive or generative world models (e.g., V-JEPA-like) to evaluate rollouts and provide reward signals for RL-style post-training.<br>‚Ä¢ Video-Driven Latent Actions: Self-Supervised Pretraining of Action Spaces from Generated Data: Extract and align latent actions from synthetic videos with real control signals to pretrain action experts at scale.<br>‚Ä¢ Hybrid Whole-Body Control: Constraint-Aware Planners Coupled with Learned Policies: Combine MPC-based safety constraints with learned policies for coordinated base‚Äìarm actions in mobile manipulation.<br>‚Ä¢ Trustworthy Agentic VLAs: Verified Multi-Agent Workflow Generation and Safety Guardrails: Design verifiable orchestration and inter-agent communication for collaborative embodied tasks.<br>‚Ä¢ On-Device Efficient VLAs: Distillation, Speculative Decoding, and Model Compression for Real-Time Control: Develop resource-efficient policies that retain performance under tight compute and energy budgets.<br>‚Ä¢ Benchmarking Beyond Vision: Realistic Multi-Modality, Noise, and Dynamics for VLA Evaluation: Build evaluation suites with audio/touch, realistic visuals/dynamics, and environmental artifacts to close sim-to-real gaps.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.05705" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.05705" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent progress in multimodal reasoning has been driven largely by undisclosed datasets and proprietary data synthesis recipes, leaving open questions about how to systematically build large-scale, vision-centric reasoning datasets, particularly for tasks that go beyond visual math. In this work, we introduce a new reasoning data generation framework spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions. The dataset also includes preference data and instruction prompts supporting both offline and online RL. Our synthesis framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning traces are then synthesized through a two-stage process that leverages VLMs and reasoning LLMs, producing CoT traces for VLMs that capture the richness and diverse cognitive behaviors found in frontier reasoning models. Remarkably, we show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data baselines across all evaluated vision-centric benchmarks, and even surpasses strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning (MMAU), demonstrating its effectiveness. Similarly, despite not containing videos or embodied visual data, we observe notable gains when evaluating on a single-evidence embodied QA benchmark (NiEH). Finally, we use our data to analyze the entire VLM post-training pipeline. Our empirical analysis highlights that (i) SFT on high-quality data with non-linear reasoning traces is essential for effective online RL, (ii) staged offline RL matches online RL's performance while reducing compute demands, and (iii) careful SFT on high quality data can substantially improve out-of-domain, cross-modality transfer.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Open multimodal reasoning lacks transparent, large-scale, vision-centric datasets with rich chain-of-thought (CoT) structures; most progress depends on undisclosed data and recipes.<br>‚Ä¢ Caption-only synthesis saturates diversity and tends to produce easy, poorly grounded questions; existing open datasets are small or skewed to visual math, limiting general vision reasoning.<br>‚Ä¢ CoT traces generated solely by reasoning LLMs can be off-distribution for VLMs, degrading fine-tuning; need VLM-aligned, verifiable traces and preference data to support both offline (DPO) and online (GRPO) RL.<br>‚Ä¢ Online RL without prior ‚Äúskill teaching‚Äù (verification, backtracking, subgoal setting) scales poorly and underperforms, motivating high-quality SFT that instills non-linear cognitive behaviors and enables cross-modality transfer.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A two-stage synthesis pipeline: (1) scale/diversify grounded MCQs using dense captions plus object metadata (bounding boxes/tags) and rigorous filtering; (2) increase complexity via a composition-hardening LLM that merges multiple MCQs per image into multi-hop problems. Reasoning traces are distilled by sampling in-distribution VLM CoTs and expanding them with a reasoning LLM via thought-expansion and guided decoding, then used in staged post-training (SFT ‚Üí DPO/GRPO).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Grounded Temporal Thoughts: Composition-Hardened CoTs for Video and Embodied Reasoning: Extend the framework to temporal grounding and action-conditioned multi-hop reasoning across video and embodied tasks.<br>‚Ä¢ Trace-Consistent Self-Distillation for VLMs Without External LLMs: Enable VLMs to generate and expand their own CoTs with consistency regularization and curriculum learning, reducing dependence on external reasoning LLMs.<br>‚Ä¢ From Bounding Boxes to Causal Scene Graphs: Verifiable Compositional QA with Counterfactuals: Replace object boxes with causal scene graphs to synthesize counterfactual, intervention-based visual questions that test causal and abductive reasoning.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.03317" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.03317" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Text-to-image diffusion models deliver high-quality images, yet aligning them with human preferences remains challenging. We revisit diffusion-based Direct Preference Optimization (DPO) for these models and identify a critical pathology: enlarging the preference margin does not necessarily improve generation quality. In particular, the standard Diffusion-DPO objective can increase the reconstruction error of both winner and loser branches. Consequently, degradation of the less-preferred outputs can become sufficiently severe that the preferred branch is also adversely affected even as the margin grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule that preserves the winner by adaptively scaling the loser gradient according to its alignment with the winner gradient. A first-order analysis yields a closed-form scaling coefficient that guarantees the error of the preferred output is non-increasing at each optimization step. Our method is simple, model-agnostic, broadly compatible with existing DPO-style alignment frameworks and adds only marginal computational overhead. Across standard text-to-image benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning baselines on automated preference, aesthetic, and prompt alignment metrics. Code is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Standard Diffusion-DPO can widen the preference margin by making the loser worse, yet simultaneously increase the denoising losses of both winner and loser, degrading absolute generation quality.<br>‚Ä¢ There is a gap between relative alignment (larger winner‚Äìloser margin) and absolute error control (preserving/improving the preferred output), causing unstable training and potential collapse.<br>‚Ä¢ Existing DPO-style methods for diffusion lack a safeguard to prevent winner loss increases when winner and loser gradients are misaligned, limiting reliability and alignment with human preferences.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Diffusion-SDPO introduces a winner-preserving safeguard that adaptively scales the loser branch‚Äôs gradient using a closed-form coefficient derived from the output-space gradient inner product, guaranteeing the preferred output‚Äôs loss is non-increasing at each step while expanding the preference margin. It is model-agnostic, plugs into existing diffusion DPO frameworks, and adds negligible computational overhead.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Second-Order SDPO: Curvature-Aware Safeguards for Preference Optimization in Diffusion Models: Extend SDPO with second-order (Hessian) information to improve guarantees under nonlinearity and mini-batch noise.<br>‚Ä¢ Reference-Free SDPO: Winner-Preserving Alignment without a Frozen Baseline: Develop a safe DPO objective that obviates the need for a reference model while maintaining winner-loss monotonicity.<br>‚Ä¢ Temporal SDPO: Step-wise Scheduling and Slack Adaptation Across Diffusion Timesteps: Learn timestep-dependent scaling and safety slack to better handle multi-step denoising dynamics and stability.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and Curriculum Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07061" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07061" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Emotion Recognition in Conversation (ERC) is a crucial task for understanding human emotions and enabling natural human-computer interaction. Although Large Language Models (LLMs) have recently shown great potential in this field, their ability to capture the intrinsic connections between explicit and implicit emotions remains limited. We propose a novel ERC training framework, PRC-Emo, which integrates Prompt engineering, demonstration Retrieval, and Curriculum learning, with the goal of exploring whether LLMs can effectively perceive emotions in conversational contexts. Specifically, we design emotion-sensitive prompt templates based on both explicit and implicit emotional cues to better guide the model in understanding the speaker's psychological states. We construct the first dedicated demonstration retrieval repository for ERC, which includes training samples from widely used datasets, as well as high-quality dialogue examples generated by LLMs and manually verified. Moreover, we introduce a curriculum learning strategy into the LoRA fine-tuning process, incorporating weighted emotional shifts between same-speaker and different-speaker utterances to assign difficulty levels to dialogue samples, which are then organized in an easy-to-hard training sequence. Experimental results on two benchmark datasets-- IEMOCAP and MELD --show that our method achieves new state-of-the-art (SOTA) performance, demonstrating the effectiveness and generalizability of our approach in improving LLM-based emotional understanding.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing ERC prompts largely ignore the interplay of explicit (expressed) and implicit (felt but unspoken) emotions, limiting nuanced understanding of speaker states.<br>‚Ä¢ Demonstration retrieval repositories are typically built only from training sets of standard datasets, reducing diversity and hindering generalization to real-world conversational domains.<br>‚Ä¢ Training strategies overlook curriculum design and class imbalance, leading to resource-heavy processes with limited gains and poor learning of low-frequency emotion classes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>PRC-Emo integrates emotion-sensitive prompts that generate and fuse explicit and implicit emotion interpretations, an ERC-specific demonstration retrieval repository combining multi-dataset and LLM-generated human-verified samples, and an enhanced curriculum learning scheme that uses weighted emotional shifts (same- vs. cross-speaker) to bucket dialogues from easy to hard during LoRA fine-tuning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond Words: Multimodal PRC-Emo for Audio-Visual Emotion Recognition in Conversation: Extend the framework with acoustic and visual cues to jointly model explicit/implicit emotions across modalities and improve robustness.<br>‚Ä¢ Personalized Curriculum ERC: Speaker-Adaptive Difficulty Metrics for Fine-Tuning LLMs: Incorporate speaker traits and historical behavior into the difficulty metric to tailor curricula and boost performance on individualized emotional patterns.<br>‚Ä¢ Cross-Domain and Cross-Lingual ERC via Synthetic Demonstration Expansion: Use controlled LLM generation and alignment to build multilingual, domain-balanced repositories that enhance generalization and transfer across languages and settings.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Generating an Image From 1,000 Words: Enhancing Text-to-Image With Structured Captions</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.06876" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.06876" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Text-to-image models have rapidly evolved from casual creative tools to professional-grade systems, achieving unprecedented levels of image quality and realism. Yet, most models are trained to map short prompts into detailed images, creating a gap between sparse textual input and rich visual outputs. This mismatch reduces controllability, as models often fill in missing details arbitrarily, biasing toward average user preferences and limiting precision for professional use. We address this limitation by training the first open-source text-to-image model on long structured captions, where every training sample is annotated with the same set of fine-grained attributes. This design maximizes expressive coverage and enables disentangled control over visual factors. To process long captions efficiently, we propose DimFusion, a fusion mechanism that integrates intermediate tokens from a lightweight LLM without increasing token length. We also introduce the Text-as-a-Bottleneck Reconstruction (TaBR) evaluation protocol. By assessing how well real images can be reconstructed through a captioning-generation loop, TaBR directly measures controllability and expressiveness, even for very long captions where existing evaluation methods fail. Finally, we demonstrate our contributions by training the large-scale model FIBO, achieving state-of-the-art prompt alignment among open-source models. Model weights are publicly available at https://huggingface.co/briaai/FIBO</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Text-to-image models are trained on short, unstructured prompts, creating a mismatch between sparse textual input and richly detailed visual outputs.<br>‚Ä¢ This mismatch reduces controllability: models fill missing details arbitrarily, biasing toward average preferences and limiting precision for professional, production use.<br>‚Ä¢ Existing datasets lack consistent, fine-grained attribute schemas, leading to entangled visual factors and weak disentangled control (position, size, texture, lighting, relations).<br>‚Ä¢ Processing very long prompts is constrained by token length and compute; naive handling increases sequence length and degrades efficiency.<br>‚Ä¢ Current evaluation metrics struggle with very long captions and do not directly measure controllability or expressive coverage.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Train an open-source text-to-image model on long, schema-aligned structured captions to maximize expressive coverage and disentangled control, using DimFusion to fuse intermediate tokens from a lightweight LLM without increasing token length. Evaluate controllability and expressiveness via the Text-as-a-Bottleneck Reconstruction (TaBR) protocol, and demonstrate state-of-the-art prompt alignment with the released FIBO model.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Schema Learning for Domain-Specific Structured Captions: Automatically derive and refine attribute schemas per domain to balance coverage, controllability, and token budget.<br>‚Ä¢ TaBR++: A Standardized Benchmark for Long-Prompt Controllability: Extend TaBR with public datasets, unified metrics, and human-in-the-loop assessments for robust evaluation of long structured prompts.<br>‚Ä¢ Token-Efficient Fusion for Multimodal Long Prompts: Generalize DimFusion to fuse text, layout, scene graphs, and other modalities under fixed token budgets with theoretical analysis of information retention.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">DIMO: Diverse 3D Motion Generation for Arbitrary Objects</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07409" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07409" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation. Our project page is available at https://linzhanm.github.io/dimo.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of scalable motion priors for arbitrary (non-template) objects; existing approaches rely on category-specific models like SMPL and motion capture.<br>‚Ä¢ Current 4D generation pipelines produce only one motion per expensive inference; diversity requires re-running diffusion and reconstruction, causing identity drift and high compute.<br>‚Ä¢ Absence of a unified, low-dimensional motion space that enables instant sampling, interpolation, and language-guided motion for general objects from a single image.<br>‚Ä¢ Existing methods overfit per-motion deformation networks and fail to jointly learn common motion patterns across multiple motions of the same object.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DIMO distills diverse motion priors from video diffusion models using LLM-generated motion prompts, represents each motion as structured neural key point trajectories, and learns a shared latent motion space via a latent-conditioned motion decoder that drives canonical 3D Gaussians with LBS. Optimized with photometric and ARAP losses, the model enables single-pass sampling, interpolation, and language-guided motion by mapping text embeddings to latent codes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Physics-Aware DIMO: Integrating differentiable physics and contact constraints into the latent motion space to improve realism and controllability for fluids, soft bodies, and articulated objects.<br>‚Ä¢ DIMO-InterAct: Generative 4D Motion for Multi-Object Interaction and Scene-Level Dynamics: Extend the latent space to jointly model interactions, collisions, and coordinated behaviors among multiple objects.<br>‚Ä¢ Self-Supervised DIMO: Online Adaptation from In-the-Wild Videos to Expand Motion Latent Spaces: Use self-supervised objectives to fit new motions from real videos by optimizing latent codes and updating the motion decoder without labeled templates.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07299" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07299" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Video anomaly understanding (VAU) aims to provide detailed interpretation and semantic comprehension of anomalous events within videos, addressing limitations of traditional methods that focus solely on detecting and localizing anomalies. However, existing approaches often neglect the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. In this paper, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe object Relation features with visual cues to enhance anomaly comprehension from video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture the causal context of each anomalous event. A Relation Feature Extractor and a COntrastive Relation Encoder (CORE) jointly model dynamic object interactions, producing compact relational representations for downstream reasoning. These visual and relational cues are integrated with LLMs to generate detailed, causally grounded descriptions and support robust anomaly-related question answering. Experiments on multiple real-world VAU benchmarks demonstrate that VADER achieves strong results across anomaly description, explanation, and causal reasoning tasks, advancing the frontier of explainable video anomaly analysis.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Conventional video anomaly detection (VAD) focuses on localization/classification and lacks semantic, causally grounded explanations, limiting decision-making utility.<br>‚Ä¢ Existing LLM/MLLM-based VAU methods largely overlook dynamic object interactions and deeper causal relationships, leading to shallow or inconsistent understanding of unusual behaviors.<br>‚Ä¢ Prior keyframe selection strategies often miss pre- and post-event context, producing fragmented narratives and weak causal reasoning for anomalies.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VADER scores frames with a CLIP-based anomaly scorer and applies Context-AwarE Sampling (CAES) to select keyframes that capture the causal lead-up, climax, and aftermath of anomalies. It extracts scene-graph-style relation features and uses a weakly supervised COntrastive Relation Encoder (CORE) to produce compact relational tokens, which are fused with visual cues in an LLM to generate causally consistent descriptions and answer anomaly-related questions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Interventional VADER: Counterfactual Simulation for Causal Video Anomaly Explanation: Introduce interventions (e.g., object removal/speed changes) to validate causal hypotheses and strengthen explanation fidelity.<br>‚Ä¢ Open-World Relation Token Learning for Unsupervised Video Anomaly Understanding: Learn relational tokens without labels to generalize to novel anomalies and unseen interaction patterns.<br>‚Ä¢ Edge-VADER: Real-Time, Low-Latency Causal Video Anomaly Understanding on Resource-Constrained Devices: Optimize relational encoding and sampling for on-device inference while preserving causal interpretability.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07253" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07253" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) have recently achieved impressive results in speech recognition across multiple modalities, including Auditory Speech Recognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech Recognition (AVSR). Despite this progress, current LLM-based approaches typically address each task independently, training separate models that raise computational and deployment resource use while missing potential cross-task synergies. They also rely on fixed-rate token compression, which restricts flexibility in balancing accuracy with efficiency. These limitations highlight the need for a unified framework that can support ASR, VSR, and AVSR while enabling elastic inference. To this end, we present Omni-AVSR, a unified audio-visual LLM that combines efficient multi-granularity training with parameter-efficient adaptation. Specifically, we adapt the matryoshka representation learning paradigm to efficiently train across multiple audio and visual granularities, reducing its inherent training resource use. Furthermore, we explore three LoRA-based strategies for adapting the backbone LLM, balancing shared and task-specific specialization. Experiments on LRS2 and LRS3 show that Omni-AVSR achieves comparable or superior accuracy to state-of-the-art baselines while training a single model at substantially lower training and deployment resource use. The model also remains robust under acoustic noise, and we analyze its scaling behavior as LLM size increases, providing insights into the trade-off between performance and efficiency.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Fragmented training and deployment: Most LLM-based ASR, VSR, and AVSR systems train separate models per task (and even per compression rate), inflating computation, memory, and maintenance costs while missing cross-task synergies.<br>‚Ä¢ Inflexible token compression: Fixed-rate audio/visual token compression prevents dynamic accuracy‚Äìefficiency trade-offs; finer granularity improves WER but sharply increases Transformer compute (quadratic with sequence length).<br>‚Ä¢ Inefficient multi-granularity training: Naively extending matryoshka representation learning across tasks requires CA + CV + CA¬∑CV LLM passes per batch, causing prohibitive training overhead and potential multi-objective interference.<br>‚Ä¢ Underperforming unification attempts: Prior unified frameworks either rely on cumbersome student‚Äìteacher pseudo-labeling or trail task-specific models in accuracy.<br>‚Ä¢ Practical robustness needs: ASR degrades in noise; unified audio-visual models must remain robust under acoustic corruptions and scale favorably with LLM size for real-world constraints.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Omni-AVSR trains a single LLM-based model for ASR, VSR, and AVSR by combining efficient matryoshka multi-granularity training‚Äîsampling one audio and one video compression rate per iteration to enable elastic inference‚Äîwith parameter-efficient adaptation via three LoRA schemes (shared, task-specific, and shared+task-specific) applied to the LLM (and video encoder) while keeping pretrained backbones mostly frozen.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Omni-AVSR-Streaming: Low-Latency Unified Audio-Visual Speech Recognition with Causal Matryoshka Inference: Develop chunked, causal tokenization and streaming decoding to achieve real-time elastic ASR/VSR/AVSR.<br>‚Ä¢ Cross-Lingual Omni-AVSR: Language-Agnostic Unified Multimodal Speech Recognition via Multilingual Pretraining: Extend to multilingual Whisper/visual encoders and prompts to generalize across languages and accents.<br>‚Ä¢ Learning to Budget: Adaptive Rate Selection for Elastic AVSR via Reinforcement Learning: Train a policy to dynamically choose audio/visual token rates per utterance based on resource and noise conditions.<br>‚Ä¢ Mixture-of-LoRA Experts for Unified AVSR: Router-Gated Specialization within a Single LLM: Replace static shared/task-specific LoRA with routed MoE-LoRA to balance sharing and specialization across tasks and conditions.<br>‚Ä¢ End-to-End Omni-AVSR with Jointly Trained Encoders: Unify training of audio/visual encoders and the LLM under matryoshka objectives to reduce modality mismatch and further improve efficiency.<br>‚Ä¢ Robust Omni-AVSR under Modality Corruption: Confidence-Guided Modality Gating and Reliability-Aware Training: Integrate uncertainty estimates to gate audio/visual contributions under noise, blur, or occlusions.<br>‚Ä¢ Beyond Recognition: Unified AVSR and Speech Translation with Elastic Multimodal Tokens: Expand tasks to include speech-to-text translation and summarization within the same elastic framework.<br>‚Ä¢ Knowledge Distillation for Tiny Omni-AVSR: Compress Unified Models to Sub-1B LLMs while Preserving Elasticity and Robustness: Distill from larger MLLMs to deployable small models without losing multi-granularity control.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.06174" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.06174" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapid progress of large language models (LLMs) has advanced numerous applications, yet efficient single-batch inference remains vital for on-device intelligence. While FPGAs offer fine-grained data control and high energy efficiency, recent GPU optimizations have narrowed their advantage, especially under arithmetic-based computation. To overcome this, we leverage FPGAs' abundant on-chip memory to shift LLM inference from arithmetic- to memory-based computation through table lookups. We present LUT-LLM, the first FPGA accelerator enabling 1B+ LLM inference via vector-quantized memory operations. Our analysis identifies activation-weight co-quantization as the most effective scheme, supported by (1) bandwidth-aware parallel centroid search, (2) efficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing data caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B model, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher energy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency gain over A100.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ GPUs with FlashAttention/FlashDecoding and advanced quantization (e.g., GPTQ) have eroded FPGA advantages for single-batch LLM inference; arithmetic-based FPGA designs now underperform in speed/efficiency.<br>‚Ä¢ FPGAs have abundant distributed on-chip memory but fewer compute units; arithmetic MACs underutilize memory bandwidth, suggesting a shift to memory-based computation to exploit FPGA strengths.<br>‚Ä¢ Existing memory-based accelerators do not scale to 1B+ LLMs: activation-only LUTs are huge (up to ~16√ó FP16 weights), decode becomes memory-bound, and single-pipeline centroid search creates latency bottlenecks.<br>‚Ä¢ LLM-specific execution challenges (centroid search pipelining in decoding, limited on-chip memory ports for LUT access, data movement with attention/KV-cache, mixed precisions) are not addressed by prior LUT accelerators.<br>‚Ä¢ Lack of a performance model to guide quantization scheme selection and architecture co-design under FPGA bandwidth/port constraints limits practical deployment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>LUT-LLM co-quantizes activations and weights and replaces linear-layer MACs with INT8 2D lookup-table accesses, guided by a performance model that identifies activation‚Äìweight co-quantization as optimal on FPGAs. It introduces a bandwidth-aware parallel centroid search to hide search latency, an efficient 2D LUT prefix-sum engine for high-throughput table lookups, and a spatial‚Äìtemporal hybrid pipeline to balance memory and compute, enabling 1B+ LLM inference with lower latency and higher energy efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ AutoLUT: Automated Design Space Exploration and Compiler Support for Memory-Based LLM Inference on FPGAs: Jointly optimize codebook sizes, group sizes, pipeline depth, and memory-port allocation using the performance model; generate schedules/mappings automatically.<br>‚Ä¢ Long-Context and Large-Model Co-Quantization with Efficient Training Kernels: Develop fused forward/backward kernels and codebook-learning strategies to scale co-quantized LUT-LLMs to 8k‚Äì32k contexts and >30B parameters with reduced training memory/time.<br>‚Ä¢ Adaptive LUT Compression and Caching for Streaming Decoding: Design entropy-aware 2D LUT compression, on-the-fly reconstruction, and predictive caching/reuse across layers/tokens to further reduce HBM bandwidth and improve decode throughput.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.05933" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.05933" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement "code 57.95 refers to urinary infection") maintain high cosine similarity between SFT and RL models, query representations (e.g., "what is code 57.95") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Clarify whether the perceived "alignment tax" (loss of factual recall after RL/SFT) holds for hierarchical, structured knowledge retrieval rather than only unstructured facts<br>‚Ä¢ Address LLM failures on large taxonomies where direct memorization is inadequate by enabling systematic traversal of hierarchical knowledge (e.g., medical codes)<br>‚Ä¢ Fill evaluation gaps by introducing metrics and settings that measure procedural path traversal and deep-retrieval complexity, not just final-answer accuracy<br>‚Ä¢ Disentangle whether RL adds new factual content or primarily improves navigation procedures by analyzing internal representations</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Combine structured prompting to explicitly guide hierarchical traversal, a complexity-stratified evaluation (MedConceptsQA and IPC) with a Path Matching Score to quantify procedural path recall, and layer-wise activation similarity analysis showing query representations diverge while factual representations remain similar‚Äîindicating RL primarily enhances navigation rather than knowledge content.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Reward Shaping for Hierarchical Navigation in LLMs: Design RL objectives that explicitly reward correct taxonomy traversal (using path-based metrics) and penalize off-path reasoning to optimize procedural navigation.<br>‚Ä¢ Auto-Structured Prompts for Taxonomy Traversal Without Retraining: Learn a prompt-generation policy that constructs hierarchical navigation prompts, closing RL‚ÄìSFT gaps across domains with minimal or no additional training.<br>‚Ä¢ Decoupling Knowledge and Navigation Across Domains: Build multi-domain benchmarks and diagnostic tools to systematically separate factual representation quality from navigation skill, using layer-wise analyses and deep-retrieval metrics.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 10</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-11 23:22:28 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 10;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>