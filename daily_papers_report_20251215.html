<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - December 15, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">December 15, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.11558" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.11558" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Current MLLMs lack fine-grained visual understanding of dental images, often missing diagnostically relevant cues and failing to reason to correct answers.<br>â€¢ Domain-specific multimodal dental data are scarce and underrepresented (e.g., ~0.3% of PubMedVision involves teeth), limiting effective knowledge injection and alignment.<br>â€¢ Generic complex reasoning modes yield only marginal gains on dental tasks; dentistry needs tailored reinforcement learning to incentivize better multimodal reasoning and verification.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DentalGPT trains a 7B MLLM in two stages: (1) multimodal understanding enhancement via a curated 120k dental imageâ€“text corpus (captioning, instruction tuning, and seeded chain-of-thought) to align fine-grained visual cues with domain knowledge; (2) GRPO-based reinforcement learning on dental MCQs with rule-based format and accuracy rewards to optimize diverse reasoning paths and final answer correctness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ DentalGPT-3D: Reasoning over CBCT and 3D intraoral scans: Extend DentalGPT to volumetric modalities for tooth-level spatial reasoning, lesion localization, and segmentation.<br>â€¢ RLHF for DentalGPT: Expert Preference Optimization for Clinically Safe Reasoning: Incorporate dentist-in-the-loop feedback, preference modeling, and uncertainty calibration to improve safety, reliability, and abstention behaviors.<br>â€¢ MMOral-Bench++: A Comprehensive Benchmark for Multimodal Dental Reasoning and Robustness: Build a richer, standardized benchmark spanning stepwise reasoning, adversarial/low-quality imaging, multilingual prompts, and safety/bias evaluation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">EgoX: Egocentric Video Generation from a Single Exocentric Video</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08269" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08269" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Extreme exocentricâ†’egocentric viewpoint shifts create minimal view overlap and large unseen regions that must be synthesized while preserving geometric consistency.<br>â€¢ The model must separate viewâ€‘relevant content from unrelated background in the exocentric input to avoid copying artifacts and maintain realism.<br>â€¢ Existing cameraâ€‘control video generators handle only modest pose changes and fail under large translations/rotations required by exoâ†’ego.<br>â€¢ Prior exoâ†’ego methods rely on extra inputs (first ego frame or multiple exo views) or separate spatial/temporal modules, limiting practicality and generalization.<br>â€¢ NaÃ¯ve channelâ€‘wise conditioning suffers from spatial misalignment, while crossâ€‘attention designs hinder reuse of powerful pretrained diffusion weights, reducing quality and scalability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>EgoX lifts the exocentric video into a 3D point cloud to render an egocentric prior, then conditions a pretrained video diffusion model via unified latent-space concatenationâ€”channel-wise with the ego prior and width-wise with a clean exocentric latentâ€”using lightweight LoRA fine-tuning. A geometry-guided self-attention injects 3D directional similarity as an attention bias to focus on geometrically aligned regions, enabling coherent, geometry-consistent egocentric video synthesis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Pose-Free EgoX: Jointly infer egocentric camera trajectories from a single exocentric video and generate ego views without pose inputs, using latent pose predictors and cycle consistency.<br>â€¢ Neural-Field EgoX: Replace point-cloud rendering with a learned neural radiance/gaussian field and differentiable geometry-guided attention for end-to-end, higher-fidelity exo-to-ego generation.<br>â€¢ Unpaired EgoX: Self-supervised exoâ†’ego training with unpaired data via geometric-cycle constraints and adversarial video objectives to scale to in-the-wild settings.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.11749" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.11749" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing text-to-image latent diffusion models depend on VAEs, whose low-dimensional bottlenecks can limit fidelity and require a separate encoder from those used for perception/understanding, hindering a unified vision stack.<br>â€¢ Training diffusion models entirely in Visual Foundation Model (VFM) feature space is underexplored, yet promises higher quality and efficiency as well as shared representations across tasks.<br>â€¢ Prior works mostly align to or jointly use VFM features while still relying on VAE latents; they have not demonstrated large-scale, high-resolution T2I training directly in VFM space.<br>â€¢ It remains unclear whether a single VFM-derived feature space can simultaneously support reconstruction, perception, high-fidelity generation, and semantic understanding without trade-offs.<br>â€¢ Real-world deployment demands scalable, efficient training/inference and strong semantic faithfulness (e.g., GenEval, DPG-Bench), which current pipelines struggle to deliver without complex multi-encoder setups.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SVG-T2I scales the SVG framework by training a standard text-to-image diffusion model directly in high-dimensional VFM feature space, removing the VAE and using a VFM-based autoencoding pathway to reconstruct images. This representation-driven latent diffusion achieves competitive performance while unifying the encoder infrastructure and improving training/inference efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Toward a Unified VFM Latent for Perception, Generation, and Understanding: Jointly train and evaluate a single VFM feature space to balance reconstruction accuracy, semantic understanding, and high-fidelity generation.<br>â€¢ Scaling High-Resolution Text-to-Image Diffusion in VFM Feature Space: Develop memory-efficient architectures and optimization strategies to push resolution and dataset scale without VAEs.<br>â€¢ Textâ€“VFM Alignment for Semantically Faithful Generation: Learn stronger alignment between text encoders and VFM latents to improve compositionality and benchmark performance (e.g., GenEval, DPG-Bench).</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.11799" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.11799" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of direct, editable control over intrinsic properties (albedo, normal, material, irradiance) in video generation/editing, which is crucial for physically grounded tasks like relighting and material editing.<br>â€¢ Existing video diffusion models entangle appearance, illumination, and geometry, lacking an explicit intrinsic representation; edits do not propagate consistently across time.<br>â€¢ Prior controlled video methods condition in pixel space or via global prompts/reference frames, limiting localized, multi-modality, keyframe-level edits and temporal coherence.<br>â€¢ Image-level intrinsic methods cannot propagate edits across frames; video Xâ†’RGB approaches often require a fully edited intrinsic sequence and fail to support sparse keyframe edits.<br>â€¢ NaÃ¯ve handling of missing/edited conditioning (e.g., empty tokens) incurs high memory cost and poor scalability when modeling multiple intrinsic channels.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>V-RGBX decomposes input videos into albedo, normal, material, and irradiance via a DiT-based inverse renderer, then synthesizes edited videos using a WAN-based diffusion transformer with interleaved intrinsic conditioning, keyframe reference latents, and a temporal-aware intrinsic embedding to coherently propagate sparse keyframe edits.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Physics-Guided V-RGBX with Differentiable Rendering: Integrate differentiable rendering and physical priors to enforce energy-consistent relighting and material constraints, improving irradiance and material fidelity.<br>â€¢ 3D-Consistent V-RGBX: Joint Geometry Reconstruction and Intrinsic Editing: Couple intrinsic-aware video editing with explicit 3D scene reconstruction to maintain long-range, view-consistent edits and geometry-aware relighting.<br>â€¢ Interactive Multimodal Keyframe Editing with Language and Intrinsics: Combine text prompts with intrinsic-channel controls to enable fine-grained, natural-language targeted edits and multi-touch workflows across time.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Sliding Window Attention Adaptation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.10411" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.10411" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving "sink" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Full self-attention scales quadratically with sequence length, making long-context inference prohibitively expensive.<br>â€¢ Naively switching FA-pretrained models to full SWA at inference causes severe long-context performance drops due to trainâ€“inference mismatch.<br>â€¢ Training models from scratch with SWA is costly and typically underperforms state-of-the-art FA models because pretraining data cannot be replicated.<br>â€¢ Training-free streaming/sink-token methods stabilize outputs but sacrifice access to distant information, degrading long-context task performance.<br>â€¢ There is a need for low-cost, architecture-preserving adaptation of FA-pretrained LLMs to SWA that maintains long-context quality while improving efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SWAA is a set of five composable recipes to adapt FA-pretrained LLMs to SWA without pretraining: apply SWA only during prefilling (FA decode), preserve the first k sink tokens, interleave FA and SWA layers, enforce Chain-of-Thought during decoding, and perform lightweight SWA-aware fine-tuning on long-context data. Carefully chosen combinations recover original long-context performance while retaining linear-time efficiency and requiring no architectural changes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Attention Gating for Sliding-Windowâ€“Adapted LLMs: Learn token- and layer-wise gates that dynamically choose between FA and SWA at inference to optimize the accuracyâ€“efficiency trade-off.<br>â€¢ Principled Sink Token Strategies for Long-Context Retention: Design algorithms to select, place, and update sink tokens beyond the first-k heuristic with theoretical guarantees on information preservation.<br>â€¢ Prefillâ€“Decode Co-Design for Efficient Long-Context Generation: Jointly optimize prefill/decoding attention patterns (e.g., variable windows, partial FA during decode) to balance speed and long-context fidelity.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PersonaLive! Expressive Portrait Image Animation for Live Streaming</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.11253" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.11253" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Diffusion-based portrait animation methods prioritize visual quality and expression realism but suffer from high generation latency, preventing real-time live streaming.<br>â€¢ Existing approaches struggle with long-sequence temporal stability and offer limited, less expressive motion control due to reliance on explicit signals.<br>â€¢ Inefficient denoising with appearance redundancy requires many steps and heavy computation, hindering practical deployment and responsiveness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>PersonaLive introduces a multi-stage diffusion framework that combines hybrid implicit signalsâ€”implicit facial representations with 3D implicit keypointsâ€”for expressive image-level motion control, a fewer-step appearance distillation to remove denoising redundancy and accelerate inference, and an autoregressive micro-chunk streaming paradigm for temporally stable, low-latency generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ MicroChunkNet: Learning Optimal Autoregressive Chunking for Low-Latency Portrait Animation: Automatically optimizes chunk length and context to balance latency and visual fidelity in streaming diffusion.<br>â€¢ ImplicitFusion: Multi-Modal Implicit Control for Expressive Live Avatars: Integrates audio, gaze, and gesture into implicit representations to achieve richer, more controllable real-time portrait animation.<br>â€¢ Distill-on-the-Fly: Online Few-Step Appearance Distillation for Personalized Real-Time Avatars: Adapts appearance distillation per user during live sessions to maintain identity fidelity with minimal steps and computation.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Exploring MLLM-Diffusion Information Transfer with MetaCanvas</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.11464" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.11464" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Multimodal LLMs are reduced to global text encoders in diffusion pipelines, underutilizing their reasoning and planning capabilities.<br>â€¢ Existing LLM+diffusion interfaces provide only global conditions, lacking explicit spatial and spatiotemporal control for precise layouts, object positions, relations, motion, and robust attribute binding.<br>â€¢ Geometry and temporal structure are compressed into 1D sequences, preventing patch-level guidance during generation and editing.<br>â€¢ There is a gap between strong multimodal understanding (complex layouts, attributes, knowledge reasoning) and weak structured controllable generation.<br>â€¢ Current methods struggle with knowledge-intensive, compositional scenes and cannot maintain fine-grained control across image and video tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MetaCanvas appends learnable 2D/3D latent canvas tokens to the MLLM input and uses multimodal RoPE plus a lightweight transformer-based connector to inject the MLLMâ€™s patch-wise canvas embeddings into diffusion latents with zero-initialized stability, enabling spatialâ€“temporal planning and fine-grained control across image/video generation and editing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Hierarchical MetaCanvas for Long-Horizon Visual Generation: Introduce multi-resolution, hierarchical canvas tokens and dynamic allocation to scale precise planning to large scenes and long videos.<br>â€¢ End-to-End Co-Training of MLLMâ€“Diffusion with Latent Canvas Feedback: Jointly train the MLLM and diffusion backbone with closed-loop gradient/RL feedback to learn optimal canvas semantics and connectors.<br>â€¢ Knowledge-Augmented MetaCanvas for Reasoning-Intensive Synthesis: Integrate external knowledge tools and schema constraints so MLLMs write knowledge-grounded plans into the canvas for accurate, compositional image/video generation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MeshSplatting: Differentiable Rendering with Opaque Meshes</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06818" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06818" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Point-based splatting (e.g., 3D Gaussian Splatting) is incompatible with mesh-based game/AR/VR pipelines due to transparency, sorting, and lack of depth-buffer/occlusion culling.<br>â€¢ Post-hoc conversion from Gaussians to meshes is non-differentiable, complex, and often degrades visual quality.<br>â€¢ Existing differentiable mesh methods require careful initialization and are limited to object-scale scenes, not large real-world environments.<br>â€¢ Prior triangle-based approaches produce semi-transparent, disconnected triangle soups, leading to quality drop in game engines and preventing physics/interaction.<br>â€¢ Need an end-to-end method that yields connected, opaque, colored meshes with faster training and lower memory for real-time engine compatibility.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MeshSplatting optimizes a vertex-sharing triangle mesh via differentiable rendering in two stages: first a semi-transparent triangle soup is trained, then restricted Delaunay triangulation enforces connectivity and a refinement stage with opacity and window-parameter scheduling yields fully opaque, high-quality meshes compatible with standard game engines.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Opaque Differentiable Mesh Rendering for Dynamic Scenes: Extend MeshSplatting to handle non-rigid and moving objects with temporally consistent connectivity, opacity scheduling, and motion-aware losses.<br>â€¢ Physically-Based MeshSplatting: Joint Optimization of Geometry, Materials, and Lighting: Integrate PBR BRDFs and view-dependent effects into the differentiable pipeline to learn materials and illumination alongside geometry.<br>â€¢ Topology-Aware MeshSplatting with Online Remeshing: Develop differentiable remeshing (split/merge) beyond restricted Delaunay to adapt topology during training for complex, thin structures and improved fidelity.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.11792" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.11792" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\% on VBench, 21-22\% lower FVD, and 71.4\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\%, surpassing REPA (92.91\%) by 2.60\%, and reduce FVD to 360.57, a 21.20\% and 22.46\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Video diffusion models struggle to generate structure-preserving motion for articulated/deformable entities (humans, animals), leading to limb shear, texture tearing, and loss of identity.<br>â€¢ Existing motion controls (e.g., optical flow, skeletons) are noisy proxies from imperfect models and require knowing motion a priori, making them unreliable for complex, long-range, occlusion-heavy dynamics.<br>â€¢ Scaling training data and mask-based supervision provide limited gains because masks are discrete/boundary-focused and do not capture fine-grained relational motion.<br>â€¢ Architectural asymmetry (bidirectional DiT vs. causal SAM2) hinders direct feature alignment, and aligning to generic video encoders or local signals fails to encode long-range, structure-centric motion.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Distill SAM2â€™s tracking-derived motion priors into a CogVideoX DiT by projecting intermediate diffusion features into the SAM2 feature space and aligning them using a Local Gram Flow KL loss, while fusing forward and backward SAM2 memory features to build a bidirectional teacher that encodes global video context.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Jointly-Optimized Tracker-Generator: End-to-End Structure Distillation for Video Diffusion: Integrate a learnable tracking module into the diffusion backbone to co-train tracking and generation, eliminating teacherâ€“student mismatch and enabling online motion supervision.<br>â€¢ Multi-Subject Structure-from-Tracking: Compositional Motion Priors for Humanâ€“Object and Animalâ€“Animal Interactions: Extend distillation to multiple tracked instances to encode relational constraints, occlusions, and contact dynamics for interaction-rich scenes.<br>â€¢ Beyond Local Gram Flow: Multi-Scale Relational Losses for Long-Range Articulated Motion: Develop multi-scale, longer-horizon relational alignment (varying neighborhoods and temporal windows) to better capture articulated motion over extended sequences.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.10605" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.10605" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing LLM-robot planning is mostly limited to single robot types and single-task scenarios, leading to poor generalization and limited autonomy.<br>â€¢ Many prior agent architectures are complex (multi-LLM, program-generation), hard to debug, unstable, and either lack closed-loop perception or robust natural language understanding.<br>â€¢ There is a gap in flexible, modular tool integration and bidirectional human-in-the-loop interaction, raising the threshold for deployment and hindering sim-to-real portability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>LEO-RobotAgent is a streamlined, ROS-integrated agent loop that constrains LLM outputs to JSON and couples a modular, registrable toolset with planâ€“actâ€“observeâ€“evaluate cycles, history accumulation, and optional human intervention. This design enables a single LLM to think, plan, act, and reflect coherently across diverse robots and tasks while maintaining robustness and ease of debugging.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Spatially Aware LLMs for 3D Robot Task Planning: Enhance LLM spatial cognition via 3D scene graphs, orientation-aware prompts, and tool-augmented perception to reduce control errors in embodied tasks.<br>â€¢ AutoTool: Automatic Tool Discovery, Selection, and Composition for General-purpose Robotic Agents: Develop meta-learning methods that infer tool capabilities, auto-generate tool descriptions, and compose tools (including auxiliary LLM/VLMs) to improve autonomy and robustness.<br>â€¢ LEOBench: Cross-Platform Benchmarking and Metrics for Streamlined LLM-Robot Agents: Establish standardized tasks across UAVs, arms, and mobile manipulators with metrics (success, token/time cost, hallucination, memory) to compare agent architectures and quantify sim-to-real transfer.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.11437" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.11437" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Integrating language models (LMs) in healthcare systems holds great promise for improving medical workflows and decision-making. However, a critical barrier to their real-world adoption is the lack of reliable evaluation of their trustworthiness, especially in multilingual healthcare settings. Existing LMs are predominantly trained in high-resource languages, making them ill-equipped to handle the complexity and diversity of healthcare queries in mid- and low-resource languages, posing significant challenges for deploying them in global healthcare contexts where linguistic diversity is key. In this work, we present CLINIC, a Comprehensive Multilingual Benchmark to evaluate the trustworthiness of language models in healthcare. CLINIC systematically benchmarks LMs across five key dimensions of trustworthiness: truthfulness, fairness, safety, robustness, and privacy, operationalized through 18 diverse tasks, spanning 15 languages (covering all the major continents), and encompassing a wide array of critical healthcare topics like disease conditions, preventive actions, diagnostic tests, treatments, surgeries, and medications. Our extensive evaluation reveals that LMs struggle with factual correctness, demonstrate bias across demographic and linguistic groups, and are susceptible to privacy breaches and adversarial attacks. By highlighting these shortcomings, CLINIC lays the foundation for enhancing the global reach and safety of LMs in healthcare across diverse languages.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of reliable, multilingual trustworthiness evaluation for healthcare LMs in high-stakes settings<br>â€¢ Existing LMs are trained primarily on high-resource languages and underperform on mid/low-resource languages, hindering global deployment<br>â€¢ Prior work focuses on narrow facets (e.g., diagnostic accuracy or single adversarial tests), is largely English-centric, and omits a holistic assessment across truthfulness, fairness, safety, robustness, and privacy<br>â€¢ Practical risks include misinformation, demographic and linguistic biases, privacy leaks, adversarial/jailbreak vulnerabilities, and overconfidence in medical outputs</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>CLINIC introduces a comprehensive multilingual benchmark that assesses five trustworthiness dimensions via 18 tasks over 28,800 expert-curated samples spanning six healthcare domains and 15 languages. It uses a two-step, linguistically grounded sample-generation process with clinical input and evaluates proprietary, open-weight, and medical-specialized models to reveal failures in factuality, bias, privacy, and robustness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Multimodal CLINIC: Extending Trustworthiness Evaluation to Speech and Imaging: Add speech and medical imaging tasks to assess cross-modal safety, factuality, and robustness across languages.<br>â€¢ Language-Aware Safety and Privacy Alignment for Medical LMs: Develop alignment methods and defenses (e.g., jailbreak- and leak-resistant prompting/training) tailored to linguistic and cultural contexts.<br>â€¢ Low-Resource Medical NLP for Trustworthy Care: Data Augmentation and Training Strategies: Create data generation, translation, and human-in-the-loop pipelines to boost fairness, robustness, and factuality in underrepresented languages.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.11150" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.11150" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>LLM-as-judge evaluation has become the de facto standard for scaling model assessment, but the practice is statistically unsound: uncalibrated scores can invert preferences, naive confidence intervals on uncalibrated scores achieve near-0% coverage, and importance-weighted estimators collapse under limited overlap despite high effective sample size (ESS). We introduce Causal Judge Evaluation (CJE), a framework that fixes all three failures. On n=4,961 Chatbot Arena prompts (after filtering from 5k), CJE achieves 99% pairwise ranking accuracy at full sample size (94% averaged across configurations), matching oracle quality, at 14x lower cost (for ranking 5 policies) by calibrating a 16x cheaper judge on just 5% oracle labels (~250 labels). CJE combines three components: (i) AutoCal-R, reward calibration via mean-preserving isotonic regression; (ii) SIMCal-W, weight stabilization via stacking of S-monotone candidates; and (iii) Oracle-Uncertainty Aware (OUA) inference that propagates calibration uncertainty into confidence intervals. We formalize the Coverage-Limited Efficiency (CLE) diagnostic, which explains why IPS-style estimators fail even when ESS exceeds 90%: the logger rarely visits regions where target policies concentrate. Key findings: SNIPS inverts rankings even with reward calibration (38% pairwise, negative Kendall's tau) due to weight instability; calibrated IPS remains near-random (47%) despite weight stabilization, consistent with CLE; OUA improves coverage from near-0% to ~86% (Direct) and ~96% (stacked-DR), where naive intervals severely under-cover.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Uncalibrated LLM-as-judge scores invert preferences, misaligning optimization targets with true oracle rewards and producing incorrect rankings and real-world failures.<br>â€¢ Naive uncertainty quantification yields near-0% coverage because it ignores calibration uncertainty; practitioners cannot trust judge-based estimates without valid CIs.<br>â€¢ Off-policy evaluation (IPS/SNIPS) collapses under limited overlap even with high ESS; existing weight stabilization is insufficient and lacks diagnostics, while prior calibration work focuses on single-model/binary outcomes without a unified framework for multi-policy, continuous rewards under distribution shift.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Causal Judge Evaluation (CJE) calibrates surrogate judge scores to oracle rewards via mean-preserving isotonic regression (AutoCal-R), stabilizes importance weights through variance-optimal stacking of S-monotone unit-mean candidates (SIMCal-W), and delivers valid uncertainty via Oracle-Uncertainty-Aware jackknife inference (OUA). It is unified by Design-by-Projection and complemented by CLE diagnostics and a mean transport test to ensure calibration portability across policies.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ CLE-Aware Off-Policy Evaluators for LLMs: Estimators that remain precise under limited overlap by modeling target-typical regions, adaptive downweighting of IPS components, or augmenting coverage with generated draws.<br>â€¢ Online AutoCal-R: Drift-Resilient Calibration and OUA Inference for Deployed LLM Judges: Real-time transport tests, automatic covariate-dependent recalibration, and calibration-aware CIs for production pipelines.<br>â€¢ Multi-Judge Stacking with Design-by-Projection: Surrogate Ensemble Calibration for Multi-Policy Ranking: Extend DbP to ensembles of heterogeneous judges and features, optimizing stacking to improve ranking accuracy and CI coverage under shift.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06951" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06951" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making. Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules. Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Long-horizon control accumulates small prediction errors, demanding robust recovery or highly accurate action prediction.<br>â€¢ Non-Markovian visual ambiguity (identical-looking states across different task phases) requires memory or explicit stage tracking to avoid wrong actions.<br>â€¢ Training data lacks recovery demonstrations, forcing policies to generalize to out-of-distribution states encountered during execution.<br>â€¢ Multi-modal and strongly correlated action distributions (temporal smoothness and cross-joint coordination) are poorly captured by standard i.i.d. Gaussian noise in flow matching, hindering early denoising steps.<br>â€¢ Existing VLA architectures either use autoregressive tokens (poor for high-frequency control) or rigid VLM-to-action attention patterns that limit expressivity and multi-task transfer.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Builds on Pi0.5 by introducing correlated-noise flow matching using empirical action covariance (enabling correlation-aware inpainting), explicit System 2 stage tracking with voting, and learnable mixed-layer attention; training employs multi-sample flow matching, and inference adds cubic-spline action compression and simple correction rules, with trainable task embeddings replacing text prompts.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Correlated-Noise Flow Matching for Structured Robot Actions: Ablations and Theory: Systematically evaluate covariance estimation, Î² schedules, and their effects on training efficiency, early denoising, and action smoothness; provide theoretical analysis of correlation-aware flows.<br>â€¢ Stage-Aware VLA: Integrating Explicit Stage Tracking with Memory for Non-Markovian Tasks: Combine stage prediction and voting with learned temporal memory (e.g., recurrent or state-space transformers) to resolve visual ambiguity and improve long-horizon reliability.<br>â€¢ Learning Corrective Policies from Off-Policy Failures: Data-Driven Alternatives to Heuristic Rules: Collect recovery data via perturbations/DAgger and train failure detectors and recovery controllers to replace hand-crafted correction rules and improve robustness.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in {pm 1, pm i}</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.02901" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.02901" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) have revolutionized artificial intelligence, yet their massive memory and computational demands necessitate aggressive quantization, increasingly pushing representations toward the theoretical limit of a single bit. While complex-valued LLMs, such as iFairy, offer a superior chance for low-bit representation compared to real-valued counterparts, they require training from scratch, preventing the utilization of the vast ecosystem of pre-trained real-valued foundation models. Here we present Fairy2i, a universal framework that transforms pre-trained real-valued layers into an equivalent widely-linear complex form, enabling extremely low-bit quantization while reusing existing checkpoints. By proving a lossless mathematical equivalence between real and widely-linear maps, we convert standard Transformers into the complex domain and employ a phase-aware quantization scheme with a highly efficient codebook of fourth roots of unity. Furthermore, we introduce a recursive residual quantization mechanism that iteratively minimizes quantization error, allowing inference to proceed via efficient multiplication-free accumulation. We demonstrate that Fairy2i restores the performance of LLaMA-2 7B at an effective 2-bit precision to levels nearly comparable with full-precision baselines, significantly outperforming state-of-the-art real-valued binary and ternary quantization methods. This work bridges the gap between the representational efficiency of complex-valued arithmetic and the practical utility of pre-trained models, paving a new way for efficient inference on commodity hardware.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Extremely low-bit (â‰¤2-bit) post-training quantization (PTQ) of LLMs suffers large accuracy drops due to limited codebooks, inadequate scaling, and outlier sensitivity.<br>â€¢ Quantization-aware training (QAT) at 1â€“2 bits typically requires training from scratch, is time-consuming and unstable, and cannot leverage strong pre-trained real-valued checkpoints.<br>â€¢ Real-valued binary/ternary schemes underutilize the 2-bit space and poorly fit heavy-tailed weight distributions, limiting capacity and performance.<br>â€¢ Complex-valued LLMs (e.g., iFairy) offer better 2-bit representational topology but are incompatible with existing real checkpoints, hindering practical adoption.<br>â€¢ There is a need for compact storage and multiplication-free, parallelizable inference on commodity hardware.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>FAIRY2I reparameterizes each real linear layer into an exactly equivalent widely-linear complex form and then applies phase-aware 2-bit quantization with the {Â±1, Â±i} codebook and shared axis scales, followed by recursive residual quantization to iteratively reduce error. This enables QAT that reuses real-valued checkpoints and supports efficient, multiplication-free, parallel inference.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Multiplication-Free CUDA Kernels for Complex Quantized LLMs: Design specialized GPU kernels that exploit FAIRY2Iâ€™s discrete {Â±1, Â±i} weights for add/sub/permute-only GEMMs to accelerate training and inference.<br>â€¢ Scaling FAIRY2I to 70B+ and Multimodal Foundation Models: Evaluate widely-linear conversion and phase-aware residual quantization at larger scales and across vision-language architectures.<br>â€¢ Groupwise Phase-Aware Quantization and Adaptive Codebooks: Extend axis-wise scaling to per-group strategies and explore learned complex codebooks to further reduce quantization error at fixed bit budgets.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Scaling Behavior of Discrete Diffusion Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.10858" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.10858" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Modern LLM pre-training consumes vast amounts of compute and training data, making the scaling behavior, or scaling laws, of different models a key distinguishing factor. Discrete diffusion language models (DLMs) have been proposed as an alternative to autoregressive language models (ALMs). However, their scaling behavior has not yet been fully explored, with prior work suggesting that they require more data and compute to match the performance of ALMs. We study the scaling behavior of DLMs on different noise types by smoothly interpolating between masked and uniform diffusion while paying close attention to crucial hyperparameters such as batch size and learning rate. Our experiments reveal that the scaling behavior of DLMs strongly depends on the noise type and is considerably different from ALMs. While all noise types converge to similar loss values in compute-bound scaling, we find that uniform diffusion requires more parameters and less data for compute-efficient training compared to masked diffusion, making them a promising candidate in data-bound settings. We scale our uniform diffusion model up to 10B parameters trained for 10^{22} FLOPs, confirming the predicted scaling behavior and making it the largest publicly known uniform diffusion model to date.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ The scaling behavior of discrete diffusion language models (DLMs), especially uniform and hybrid variants, is unclear at large scales.<br>â€¢ Prior DLM scaling studies fix crucial hyperparameters (batch size, learning rate) and assume zero irreducible loss, leading to potentially misleading compute/data optimality conclusions.<br>â€¢ Masked diffusion prevents token revision and may be less compute-efficient; a principled way to interpolate across noise types is needed.<br>â€¢ A unified, SNR-based formulation for discrete diffusion (with schedule invariance) has been lacking, hindering theoretical clarity and practical implementation.<br>â€¢ Practitioners need guidance on compute- vs token-bound regimes, optimal parameterâ€“data ratios, and predictable batch size/learning rate scaling for DLMs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper reframes the GIDD ELBO in log-SNR space and introduces an SNR-driven hybrid mixing distribution to smoothly interpolate between masked and uniform diffusion, yielding schedule-invariant, simpler training. It then uses CompleteP parameterization and systematic sweeps of batch size and learning rate to derive compute- and token-bound scaling laws across noise types, validating predictions up to 10B parameters.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Curriculum Annealing for Discrete Diffusion Language Models: Quantifying how annealing phases and data-mixture refinement affect DLM scaling laws and compute-optimality.<br>â€¢ Task-Adaptive SNR Schedules in Hybrid Discrete Diffusion: Learning noise-type and SNR mixing policies conditioned on domain/task to improve data efficiency and downstream performance.<br>â€¢ The Critical Batch Size of DLMs: A Theoretical and Empirical Study: Characterizing gradient noise, optimizer dynamics, and saturation thresholds to explain why DLMs tolerate larger batches than ALMs.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.10715" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.10715" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Uncertainty estimation is essential for the safe clinical deployment of medical image segmentation systems, enabling the identification of unreliable predictions and supporting human oversight. While prior work has largely focused on pixel-level uncertainty, landmark-based segmentation offers inherent topological guarantees yet remains underexplored from an uncertainty perspective. In this work, we study uncertainty estimation for anatomical landmark-based segmentation on chest X-rays. Inspired by hybrid neural network architectures that combine standard image convolutional encoders with graph-based generative decoders, and leveraging their variational latent space, we derive two complementary measures: (i) latent uncertainty, captured directly from the learned distribution parameters, and (ii) predictive uncertainty, obtained by generating multiple stochastic output predictions from latent samples. Through controlled corruption experiments we show that both uncertainty measures increase with perturbation severity, reflecting both global and local degradation. We demonstrate that these uncertainty signals can identify unreliable predictions by comparing with manual ground-truth, and support out-of-distribution detection on the CheXmask dataset. More importantly, we release CheXmask-U (huggingface.co/datasets/mcosarinsky/CheXmask-U), a large scale dataset of 657,566 chest X-ray landmark segmentations with per-node uncertainty estimates, enabling researchers to account for spatial variations in segmentation quality when using these anatomical masks. Our findings establish uncertainty estimation as a promising direction to enhance robustness and safe deployment of landmark-based anatomical segmentation methods in chest X-ray. A fully working interactive demo of the method is available at huggingface.co/spaces/matiasky/CheXmask-U and the source code at github.com/mcosarinsky/CheXmask-U.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Landmark-based anatomical segmentation lacks uncertainty quantification, even though it offers topological guarantees; existing landmark models are deterministic and provide no per-node confidence.<br>â€¢ Most prior uncertainty work targets pixel-level masks, which can be anatomically implausible; large datasets (e.g., CheXmask) provide only image-level quality scores, not spatially resolved reliability.<br>â€¢ Safe clinical deployment requires robust, interpretable uncertainty signals to flag unreliable predictions and detect out-of-distribution cases; standard VAE uncertainties can be misleading, necessitating task-specific validation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Leverages a variational CNNâ€“GCNN (HybridGNet) to derive two complementary uncertainties: global latent uncertainty from the VAE posterior (ÏƒÂ²) and node-wise predictive uncertainty via Monte Carlo decoding from latent samples. Validated under occlusions and Gaussian noise, used for OOD detection, and released CheXmask-U with precomputed per-node uncertainty for 657,566 X-rays.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Calibrating VAE-Based Uncertainty for Landmark Segmentation: Improve calibration and disentangle aleatoric vs. epistemic components to yield better decision thresholds and reliability.<br>â€¢ Uncertainty-Guided Interactive Refinement of Chest X-Ray Landmarks: Use per-node uncertainty to prioritize human-in-the-loop corrections and active learning for faster, safer deployment.<br>â€¢ Robust Landmark Graphs under Domain Shift via Uncertainty-Weighted Training: Combine semi-supervised learning and uncertainty weighting to enhance generalization across institutions and imaging protocols.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">The N-Body Problem: Parallel Execution from Single-Person Egocentric Video</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.11393" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.11393" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Predicting multi-agent parallel execution from a single-person egocentric video while respecting real-world constraints (spatial collisions, exclusive object use, and causal order).<br>â€¢ Existing scheduling/task-graph methods are infeasible for long, unscripted videos and VLMs struggle with spatial reasoning, leading to impractical plans with collisions and conflicts.<br>â€¢ Lack of ground-truth parallel executions necessitates a principled evaluation framework that measures performance (coverage, speed-up) and feasibility (collisions, object conflicts, causal violations) using available annotations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper formalizes the N-Body Problem and introduces a structured prompting strategy for a state-of-the-art VLM (Gemini 2.5 Pro) to reason about 3D space, object usage, and temporal dependencies, by discretizing environments and segmenting timelines into non-overlapping assignable units, then assigning segments to N agents to maximize speed-up while minimizing constraint violations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ From Prompts to Planners: Learning Constraint-Aware Multi-Agent Policies from Egocentric Video: Train a planning model (e.g., via reinforcement or imitation learning) that internalizes spatial, object, and causal constraints, moving beyond prompt engineering to robust policy generation.<br>â€¢ 3D-Aware VLMs for Collision-Free Parallel Execution: Integrate explicit 3D scene reconstruction and geometry into VLMs to improve spatial reasoning and reduce collisions and teleportation (jump) distances in predicted multi-agent plans.<br>â€¢ Automatic Task-Graph Induction for Long Unscripted Egocentric Streams: Develop models that infer hierarchical task graphs and causal dependencies at scale, enabling more accurate constraint-aware parallelization without manual annotations.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.11130" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.11130" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Stereo foundation models achieve strong zero-shot generalization but remain computationally prohibitive for real-time applications. Efficient stereo architectures, on the other hand, sacrifice robustness for speed and require costly per-domain fine-tuning. To bridge this gap, we present Fast-FoundationStereo, a family of architectures that achieve, for the first time, strong zero-shot generalization at real-time frame rate. We employ a divide-and-conquer acceleration strategy with three components: (1) knowledge distillation to compress the hybrid backbone into a single efficient student; (2) blockwise neural architecture search for automatically discovering optimal cost filtering designs under latency budgets, reducing search complexity exponentially; and (3) structured pruning for eliminating redundancy in the iterative refinement module. Furthermore, we introduce an automatic pseudo-labeling pipeline used to curate 1.4M in-the-wild stereo pairs to supplement synthetic training data and facilitate knowledge distillation. The resulting model can run over 10x faster than FoundationStereo while closely matching its zero-shot accuracy, thus establishing a new state-of-the-art among real-time methods. Project page: https://nvlabs.github.io/Fast-FoundationStereo/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Strong zero-shot stereo foundation models are computationally prohibitive, preventing real-time deployment in latency-bound systems.<br>â€¢ Existing efficient (real-time) stereo methods sacrifice robustness and require costly per-domain fine-tuning, lacking off-the-shelf generalization in the wild.<br>â€¢ There is a practical need to retain rich monocular and stereo priors while accelerating feature extraction, cost filtering, and refinement without large accuracy loss.<br>â€¢ Scarcity of large-scale, high-quality, in-the-wild stereo ground truth hampers training of generalizable efficient models; acceleration of stereo VFMs is underexplored.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Fast-FoundationStereo uses a divide-and-conquer acceleration: distill hybrid monocular+stereo priors from a FoundationStereo teacher into a single efficient backbone, redesign cost filtering via blockwise distillation and combinatorial search under latency budgets, and apply structured pruning with retraining to the ConvGRU refinement. An automatic pseudo-labeling pipeline curates 1.4M in-the-wild stereo pairs to enable effective distillation and robust zero-shot generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Fast-FoundationStereo: Online Budget-Aware Block Selection for Real-Time Stereo â€” Extend blockwise combinatorial search to runtime adaptation that switches block configurations based on latency, power, and scene difficulty constraints.<br>â€¢ Uncertainty-Aware Stereo Pseudo-Labeling at Internet Scale â€” Develop confidence-guided, ensemble and monocular-stereo fusion pipelines to generate cleaner, more diverse pseudo-labels for improved zero-shot generalization.<br>â€¢ Edge-Ready Fast-FoundationStereo via Mixed-Precision Quantization and Structured Pruning â€” Integrate quantization-aware training and deployment (e.g., 4â€“8 bit, TensorRT) with recurrent graph pruning to further reduce latency/memory while preserving accuracy.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">Sharp Monocular View Synthesis in Less Than a Second</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.10685" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.10685" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25-34% and DISTS by 21-43% versus the best prior model, while lowering the synthesis time by three orders of magnitude. Code and weights are provided at https://github.com/apple/ml-sharp</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Photorealistic, sharp novel view synthesis from a single photograph for nearby viewpoints, suitable for interactive AR/VR browsing<br>â€¢ Sub-second synthesis of a 3D scene representation to enable real-time user experiences on standard GPUs<br>â€¢ Metric 3D representation with absolute scale to accurately couple virtual cameras with physical devices<br>â€¢ Overcoming limitations of multi-view/per-scene optimized methods that are slow and require multiple inputs<br>â€¢ Addressing diffusion-based approaches that are slow (seconds to minutes) and often less sharp for small-baseline views<br>â€¢ Mitigating depth ambiguity and artifacts from frozen monocular depth estimators that degrade view synthesis quality</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SHARP is an end-to-end feed-forward network that regresses a high-resolution 3D Gaussian representation from a single image in under a second, enabling real-time photorealistic rendering of nearby views. It combines a pretrained Depth Pro encoder, a two-layer DPT depth decoder with a learned depth-adjustment module, a Gaussian decoder/composer for all attributes, and differentiable Gaussian rendering with targeted losses, trained via synthetic supervision and self-supervised finetuning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Extending SHARP to Walkable Baselines: Enabling Large Camera Motions from a Single Image: Integrate compact view-dependent appearance and occlusion reasoning to maintain sharpness under larger viewpoint changes.<br>â€¢ Metric Scale Without External Priors: Self-Supervised Absolute Scale Learning for Monocular Gaussian Regression: Learn absolute scale directly from monocular cues and render consistency, reducing reliance on dataset-specific calibration.<br>â€¢ SHARP-Diffuse: Hybrid Feed-Forward plus Lightweight Diffusion Refinement for Far-View Novel Synthesis: Preserve SHARP's speed for nearby views while adding a fast diffusion-based refinement for plausibility in far, low-overlap viewpoints.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Particulate: Feed-Forward 3D Object Articulation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.11798" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.11798" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Inferring complete articulated structure (3D parts, kinematic graph, motion constraints) from a single static 3D mesh<br>â€¢ Overcoming slow, per-object optimization pipelines by enabling fast feed-forward inference<br>â€¢ Scaling beyond rule-based/procedural systems that fail to cover the long tail of everyday articulated objects<br>â€¢ Generalizing to diverse assets, including AI-generated 3D meshes, and addressing the lack of human-aligned benchmarks/evaluation protocols</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>PARTICULATE introduces a Part Articulation Transformer that processes a point cloud of a static mesh and, in a single feed-forward pass, predicts 3D parts, kinematic structure, and motion constraints with native multi-joint support; predictions are then lifted to the input mesh to produce a fully articulated model in seconds. The network is trained end-to-end on diverse articulated 3D assets and generalizes to AI-generated meshes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Any-Scan PARTICULATE: Robust Feed-Forward Articulation from Partial and Noisy 3D Scans: Extend the model to handle partial observations and real sensor noise from RGB-D or LiDAR inputs.<br>â€¢ Image-to-Articulation: End-to-End Single-Image Learning of 3D Object Articulation: Jointly learn reconstruction and articulation directly from images, eliminating the separate image-to-3D stage.<br>â€¢ Beyond Revolute and Prismatic: Learning General Joint Types and Coupled Constraints for Everyday Objects: Expand the framework to model complex joints (e.g., helical, spherical) and coupled/non-rigid constraints.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.10092" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.10092" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding "trigger" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of interpreting models through their data.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Large-scale analysis of text corpora (e.g., model outputs and training data) is needed to detect behaviors and biases, but LLM-based labeling is expensive and prompt-sensitive.<br>â€¢ Dense embeddings enable fast similarity operations but lack interpretability and controllability over specific properties, while prior interpretable embeddings rely on predefined axes or heavy curation.<br>â€¢ There is a need for a cost-effective, data-centric interpretability toolkit that can reliably uncover dataset differences, unexpected concept correlations, and support controllable clustering and property-based retrieval.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Feed documents through a reader LLM, pass token-level hidden states into a pretrained sparse autoencoder to obtain monosemantic feature activations, and max-pool across tokens to form interpretable document embeddings. Use concept-filtered embeddings for dataset diffing, correlation discovery, controllable clustering, and property-based retrieval, achieving lower cost and higher reliability than LLM labeling and more control than dense embeddings.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Scaling SAE Embeddings for Cross-Model and Cross-Layer Interpretability: Study transferability and stability of SAE latents across different reader LLMs and layers to build standardized, reusable concept libraries.<br>â€¢ Domain-Adaptive Interpretable Embeddings with Sparse Autoencoders: Develop methods to adapt SAEs to specialized domains (e.g., legal, medical) to improve detection of nuanced properties and biases in domain-specific corpora.<br>â€¢ Causal Validation of SAE Concepts in LLMs: Use interventions and counterfactual experiments to test whether manipulating SAE latents causally changes model behavior, strengthening mechanistic interpretability and trust in the embeddings.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 7</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-12-15 23:05:29 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 7;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>