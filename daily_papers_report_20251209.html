<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - December 09, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">December 09, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07461" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07461" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing LLMs lack genuine native parallel reasoning: inference engines can‚Äôt enforce strict branch isolation/aggregation and RL objectives often suppress gradients on control tokens, breaking structure.<br>‚Ä¢ Inefficient hand-crafted or sampling-based parallelism: independent branches don‚Äôt reuse KV states, incur O(N) latency, and frequently regress to autoregressive decoding (pseudo-parallelism).<br>‚Ä¢ Dependence on teacher-distilled supervision: forces students to mimic teacher topologies, creating an "intelligence ceiling" and limiting discovery of model-intrinsic parallel strategies.<br>‚Ä¢ Engine-level instability for parallel RL: GPU KV-cache leaks/double-free, incorrect token-budget accounting, and illegal schema states undermine correctness and training stability.<br>‚Ä¢ Limited data and formats for parallel schemas: scarce high-quality, structured parallel traces without external teachers impede robust SFT/RL.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>NPR is a teacher-free, three-stage framework that induces and optimizes native parallel reasoning: (1) self-distilled format-follow RL to discover a tag-based Map‚ÄìProcess‚ÄìReduce schema, (2) rejection-sampled parallel SFT with parallel attention masks/positional encoding to ground genuine parallel execution, and (3) Native-Parallel RL with Parallel-Aware Policy Optimization (PAPO) that optimizes branching policies on-policy within a stabilized NPR Engine.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Toward Scalable Multimodal Native Parallel Reasoning via Self-Distilled RL: Extend NPR‚Äôs schema, masks, and PAPO to vision, tools, and code with modality-aware parallel blocks and cross-modal KV reuse.<br>‚Ä¢ Theoretical Foundations and Convergence of Parallel-Aware Policy Optimization: Formalize PAPO‚Äôs on-policy objective, gradient flow on special tokens, and stability/efficiency guarantees versus PPO/GRPO in parallel graphs.<br>‚Ä¢ Adaptive Branching Budgets and KV-Reuse Scheduling for Real-Time Parallel LLMs: Learn to allocate branch factors dynamically from uncertainty/latency signals and co-design engine schedulers for cluster-level KV sharing.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07525" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07525" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Standard RoPE discards the imaginary component of the complex-valued attention score, losing phase information that encodes valuable positional relations.<br>‚Ä¢ This information loss hinders modeling of long-context dependencies, where preserving fine-grained relative positional details becomes critical as context length increases.<br>‚Ä¢ Existing improvements (interpolation/rescaling, data-aware tuning, dimension partitioning) rarely revisit RoPE‚Äôs intrinsic computation and still suffer from poor length extrapolation and limited data sensitivity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Extend RoPE by re-incorporating the imaginary component of the complex dot product, forming a dual-component attention score that leverages both real and imaginary parts to preserve phase-based positional information for better long-context modeling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Complex Weighting for RoPE: Learn layer/head-specific gates to balance real and imaginary components dynamically across contexts.<br>‚Ä¢ Imaginary-Aware Positional Interpolation for Million-Token Contexts: Integrate complex-domain interpolation/rescaling to improve length extrapolation without losing phase fidelity.<br>‚Ä¢ Multimodal Complex Rotary Embeddings: Design unified complex rotary positional encodings that handle heterogeneous text‚Äìvision‚Äìaudio streams with modality-specific phase handling.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Unified Video Editing with Temporal Reasoner</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07469" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07469" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing unified, mask-free video editing lacks explicit spatial cues, leading to weak instruction-to-region mapping and inaccurate localization, especially in multi-instance scenarios.<br>‚Ä¢ Expert (mask-dependent) methods achieve precision but break unification and require extra inputs/adapters and per-task overhead.<br>‚Ä¢ Naive temporal in-context concatenation causes motion misalignment and prevents length extrapolation due to fixed RoPE index mapping and collisions.<br>‚Ä¢ Available datasets/benchmarks underrepresent instance-level, spatial reasoning; achieving precision typically needs large data, increasing training cost.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VideoCoF enforces a see‚Üíreason‚Üíedit Chain-of-Frames: the model first predicts grayscale reasoning tokens (edit-region latents) and then generates the edited video, removing user masks while improving alignment. A tailored RoPE alignment ([1..F], 0, [1..F]) isolates the reasoning frame, preserves motion, and enables length extrapolation beyond training, trained on 50k triplets with unified VideoDiT in-context concatenation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Physics-Aware Chain-of-Frames for Long-Horizon Video Editing: Incorporate physical constraints into reasoning tokens to maintain object interactions and motion consistency in very long videos.<br>‚Ä¢ Self-Supervised CoF: Unlabeled Video Pretraining for Maskless Temporal Reasoning: Learn reasoning-token prediction from unlabeled videos via pseudo-grounding and contrastive objectives to reduce data requirements.<br>‚Ä¢ Multimodal CoF: Audio- and Language-Grounded Temporal Reasoning for Instance-Level Editing: Fuse audio cues and richer language grounding into the reasoning stage to improve localization and disambiguation in crowded scenes.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">Voxify3D: Pixel Art Meets Volumetric Rendering</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07834" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07834" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Perspective projection causes pixel‚Äìvoxel misalignment, producing blurry gradients and preventing pixel-precise voxel art alignment.<br>‚Ä¢ Semantic collapse under extreme discretization (low voxel resolutions and small color palettes) makes critical features vanish; standard image-level perceptual losses fail to preserve local semantics.<br>‚Ä¢ Lack of end-to-end differentiable discrete color optimization and controllable palette extraction; existing methods either yield continuous colors, over-simplify geometry, or require heavy manual tuning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A two-stage differentiable pipeline: initialize a voxel radiance field (DVGO) then fine-tune with six-view orthographic pixel-art supervision using patch-based CLIP for semantic alignment, depth/alpha losses for structure, and palette-constrained Gumbel-Softmax over per-voxel color logits with controllable palette extraction and temperature scheduling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Interactive Palette-Aware Voxel Art Editing with Orthographic Supervision: Real-time user-in-the-loop editing that maintains discrete palette constraints and pixel-precise alignment via orthographic rendering.<br>‚Ä¢ Text-to-Voxel Art: CLIP-Guided Discrete Volumetric Stylization from Prompts: Replace mesh-only supervision with text prompts to generate voxel art directly, leveraging patch-based CLIP and palette quantization.<br>‚Ä¢ Temporal Voxify3D: Palette-Constrained Voxel Art for Animated Characters: Extend to sequences with temporal consistency losses in logit space to preserve semantics and palettes across motion.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Scaling Zero-Shot Reference-to-Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06905" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06905" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ R2V must simultaneously align with text prompts and preserve high-fidelity subject identity, a harder joint constraint than standard T2V/I2V.<br>‚Ä¢ Existing R2V methods rely on explicit reference‚Äìvideo‚Äìtext triplets, whose collection/annotation/filtering pipelines are costly and difficult to scale.<br>‚Ä¢ Limited diversity in current R2V datasets hampers generalization to unseen subjects and varying numbers of reference images.<br>‚Ä¢ Reference-to-video generation often suffers from copy‚Äìpaste artifacts that degrade realism and identity consistency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Saber is a zero-shot R2V framework trained solely on video‚Äìtext pairs using masked training that treats randomly sampled, partially masked frames as reference images to learn identity-consistent, reference-aware representations. It adds attention-mask‚Äìguided attention and mask augmentation to focus on reference features and mitigate copy‚Äìpaste artifacts, generalizing to a variable number of references.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ End-to-End Attention Mask Learning for Zero-Shot Reference-to-Video Generation: Learn reference-aware attention masks directly from weak supervision to replace hand-crafted guidance and further enhance identity fidelity.<br>‚Ä¢ Compositional Multi-Subject Zero-Shot R2V from Heterogeneous References: Extend zero-shot R2V to handle multiple subjects and attributes from disparate images with role-aware conditioning and compositional constraints.<br>‚Ä¢ Scaling Laws and Masking Strategies for Zero-Shot Reference-to-Video Generation: Quantify how data scale, model size, and masking/augmentation choices influence identity preservation, temporal consistency, and text alignment, providing practical training recipes.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06749" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06749" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Debugging LLM-based multi-agent systems is hard due to long, branching interaction traces and labor-intensive log inspection<br>‚Ä¢ Prevailing log-only failure attribution yields unvalidated, hypothesis-level blame without testing whether fixes actually resolve failures<br>‚Ä¢ Single-step/single-agent attribution is often ill-posed because multiple distinct interventions can independently repair a task and inter-agent misalignment makes ground-truth ambiguous<br>‚Ä¢ Outcome evaluation is too coarse (end-to-end pass/fail), lacking milestone/utility-based measures of partial progress<br>‚Ä¢ Need scalable, robust debugging that actively repairs failures, verifies hypotheses, and works across agent frameworks</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DoVer (Do-then-Verify) generates failure-attribution hypotheses from execution logs and actively tests them via targeted interventions (e.g., editing messages, plans, tool calls), re-executing from the intervention point and judging success by task resolution or milestone progress; it supports multi-step interventions and explicitly validates or refutes hypotheses.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Causal DoVer: Counterfactual Intervention Planning for Multi-Agent LLM Systems: Integrate causal inference to identify minimal sufficient intervention sets and quantify causal effects on task success<br>‚Ä¢ Learning to Intervene: Reinforcement-Learned Policies for Outcome-Oriented Auto-Debugging: Train policies (RL/bandits) that choose where and how to intervene to maximize success and minimize edit cost across diverse agent frameworks<br>‚Ä¢ Benchmarking Verified Repair: A Standardized Dataset and Metrics for Intervention-Driven Debugging of LLM Agents: Build trajectory-aware benchmarks with milestone utilities and validated failure annotations to compare verified repair methods</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06065" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06065" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Egocentric videos have rapid egomotion and frequent hand‚Äìobject interactions/occlusions, creating a severe domain gap where third‚Äëperson (exocentric) editors fail.<br>‚Ä¢ Existing diffusion-based video editing pipelines are offline and high-latency, making them unsuitable for interactive AR use.<br>‚Ä¢ Current paired editing datasets largely omit egocentric content and do not preserve hands and interactions, limiting training and evaluation.<br>‚Ä¢ Training-free streaming editors exist but exhibit a quality gap versus trained methods; sequence-wise conditioning further inflates compute for video.<br>‚Ä¢ There is no standardized benchmark focusing on instruction faithfulness, hand/interaction preservation, and temporal stability in egocentric settings.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>EgoEdit introduces a complete egocentric editing ecosystem: a curated paired dataset (EgoEditData), a real-time streaming editor that conditions via channel-wise concatenation and is trained with Rectified Flow then distilled with bidirectional DMD and Self-Forcing for autoregressive chunked inference, and a benchmark (EgoEditBench) targeting instruction fidelity, hand preservation, and temporal stability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ 3D-Aware EgoEdit: Geometry-Consistent Egocentric Video Editing with Persistent Scene Representations: Integrate online 3D scene/hand reconstruction (e.g., Gaussians/NeRF) to enforce occlusions and view-consistency for more stable edits under large egomotion.<br>‚Ä¢ On-Device EgoEdit: Sub-10W Real-Time Egocentric Editing for AR Glasses via Extreme Compression and Scheduling: Explore quantization, pruning, low-rank adapters, and KV-cache-aware schedulers to achieve mobile/edge deployment while preserving edit fidelity.<br>‚Ä¢ RLHF for Streaming Egocentric Editors: Learning Instruction Faithfulness and Hand Preservation from In-the-Loop AR Interactions: Use live user feedback and interaction signals to fine-tune the editor‚Äôs policy for better temporal stability and alignment under real usage.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Distribution Matching Variational AutoEncoder</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07778" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07778" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce Distribution-Matching VAE (DMVAE), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of explicit control over the aggregate latent distribution q(z); existing VAEs and alignment encoders impose indirect or fixed priors that do not shape global structure.<br>‚Ä¢ Per-sample regularization (KL to Gaussian, pointwise feature alignment) can yield "holey", multi-modal latent manifolds that are hard for priors to model.<br>‚Ä¢ Fixed Gaussian priors simplify modeling but cause information loss; frozen SSL encoders are easy to model but give poor reconstructions.<br>‚Ä¢ Alternatives are unstable or limited: AAEs rely on fragile GAN training and weak discriminators; direct diffusion priors are hard to backpropagate and can collapse.<br>‚Ä¢ No systematic framework to align q(z) to arbitrary, possibly complex priors (e.g., SSL features) to study which distributions best balance reconstruction and modeling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DMVAE matches the encoder‚Äôs aggregate posterior to an arbitrary reference distribution via diffusion-based score matching: train a frozen "real" diffusion on the reference, a "fake" diffusion on encoder latents, and update the encoder to minimize the score discrepancy while optimizing reconstruction. A projection head handles dimensionality mismatch, and stabilized joint training (initialization, alternating updates, low-dimensional latents) ensures robust convergence.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Adaptive Reference Priors for DMVAE: Jointly learn the reference distribution with the tokenizer and generator via bi-level optimization to tailor priors to tasks and data.<br>‚Ä¢ Hierarchical and Conditional DMVAE Priors: Integrate multi-scale and class/text-conditioned priors to improve modeling efficiency without sacrificing fine detail.<br>‚Ä¢ Efficient High-Dimensional Distribution Matching for Autoencoders: Develop scalable, variance-reduced score matching or preconditioned objectives enabling higher-dimensional latent alignment.<br>‚Ä¢ Theoretical Guarantees for Distribution-Level Latent Alignment: Analyze convergence, manifold connectivity, and generalization versus per-sample constraints, with metrics for "holey" manifolds.<br>‚Ä¢ Cross-Modal DMVAE for Video and 3D Generation: Extend distribution matching to temporal and 3D latents using cross-modal (audio/text/geometry) priors for improved synthesis.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Relational Visual Similarity</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07833" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07833" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing image similarity models (e.g., LPIPS, CLIP, DINO, dreamsim) conflate similarity with shared perceptual/attribute features and fail to capture relational similarity (correspondence among relationships, functions, or transformations among elements).<br>‚Ä¢ There is no dataset explicitly designed to teach or evaluate relational visual similarity; prominent resources (BAPPS, NIGHTS, ImageNet, LAION) emphasize attribute-based signals.<br>‚Ä¢ Many human-perceived similarities (e.g., ‚Äútransformation over time,‚Äù compositional analogies) are missed by current metrics, limiting retrieval and generation systems that require abstraction beyond surface content.<br>‚Ä¢ Representations that align images by underlying relational logic are needed to complement attribute-based similarity and better match human reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>They propose relsim, a vision‚Äìlanguage metric trained to align image embeddings with anonymous captions that abstract an image‚Äôs relational logic, enabling images sharing relations to be close in feature space. The pipeline filters LAION-2B for relationally interesting images using a fine-tuned VLM classifier, trains an anonymous captioner via grouped examples to produce object-agnostic relational captions, and then learns image‚Äìtext contrastive embeddings where relational similarity between images approximates similarity between their captions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ RelSim-Bench: A Human-Annotated Benchmark for Relational Visual Similarity: Build a standardized dataset and evaluation protocol with graded human judgments across diverse relation types and attribute confounders.<br>‚Ä¢ RelSim-Video: Modeling Spatiotemporal Relational Similarity in Videos: Extend relational similarity to video to capture dynamic relations and temporal analogies across scenes and actions.<br>‚Ä¢ Compositional Relational Embeddings: Factoring and Weighting Multiple Relations in Images: Learn embeddings that disentangle, compose, and weight multiple co-occurring relations to reflect their relative salience in similarity.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Multi-view Pyramid Transformer: Look Coarser to See Broader</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07806" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07806" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details," MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Quadratic cost of global self-attention makes multi-view transformers computationally and memory intensive when scaling to many high-resolution views.<br>‚Ä¢ Existing approaches trade off scalability and expressivity: Long-LRM's linear Mamba limits capacity; iLRM's global attention becomes a bottleneck as views grow; LVT's local-view attention hampers global 3D consistency and depends on predefined neighborhoods/known poses.<br>‚Ä¢ Global all-to-all attention suffers attention dilution in long-context regimes, degrading correspondence learning and multi-view feature alignment as the number of views increases.<br>‚Ä¢ There is a practical need for fast, feed-forward 3D reconstruction that remains accurate and globally consistent across tens to hundreds of input views.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MVP introduces a dual attention hierarchy: inter-view attention expands from frame-wise to group-wise to global, while intra-view tokens are progressively merged from fine to coarse to reduce sequence length and stabilize long-context reasoning. Coupled with pyramidal feature aggregation and a feed-forward 3D Gaussian Splatting decoder, it reconstructs large scenes efficiently from tens to hundreds of views.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Pose-Free MVP: Learning Camera-Agnostic Multi-View Grouping and Geometry: Remove explicit pose inputs by jointly estimating camera parameters and geometry, enabling robust reconstruction from unposed image sets.<br>‚Ä¢ Adaptive Grouped Attention for MVP: Content- and Pose-Aware Learnable Clustering: Replace fixed consecutive grouping with learnable, feature/pose-driven grouping to optimize inter-view interactions and improve scalability and accuracy.<br>‚Ä¢ Dynamic MVP: Dual Hierarchies for 4D Reconstruction of Non-Rigid Scenes: Extend MVP with temporal hierarchies and motion-aware tokens to reconstruct dynamic scenes with consistent geometry and appearance over time.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07831" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07831" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Video generation models trained primarily on RGB lack holistic world understanding and physical reasoning, missing complementary cues from depth, flow, segmentation, skeletons, and DensePose.<br>‚Ä¢ Existing approaches largely use one-way interactions (either condition generation on a single modality or estimate modalities from RGB), limiting cross-task synergy and emergent perception.<br>‚Ä¢ Isolated, stage-wise training leads to catastrophic forgetting and slower convergence; the benefits of unified multi-task, multi-modal training remain unclear.<br>‚Ä¢ A shortage of large-scale unified datasets and fair benchmarks impedes systematic evaluation of multi-modal, multi-task video models and their zero-shot generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>UnityVideo is a unified diffusion transformer that uses dynamic noise scheduling for probabilistic task routing across conditional generation (anything-to-video), modality estimation (video-to-anything), and joint generation (text-to-RGB+modality), coupled with a modality-adaptive switcher and in-context prompts to process heterogeneous modalities in a shared feature space. Trained on the 1.3M-sample OpenUni dataset and evaluated on UniBench, it accelerates convergence, improves physical consistency, and strengthens zero-shot generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Audio-Geometry-UnityVideo: Extending Unified Multi-Modal Video Learning to Audio and 3D Pose: Integrate audio, camera trajectories, and 3D meshes to further enhance world modeling, synchronization, and physical alignment.<br>‚Ä¢ AutoRoute: Learned Dynamic Task Routing and Noise Scheduling for Multi-Task Video Diffusion: Replace heuristic task probabilities with a learned scheduler that optimizes convergence and generalization across modalities and objectives.<br>‚Ä¢ PromptOps: In-Context Modality Prompting for Zero-Shot Video Segmentation and Control: Develop richer modality-aware prompt engineering and retrieval to activate detector-free segmentation and controllable generation in novel domains.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LongCat-Image Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07584" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07584" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Multilingual text rendering in generated images remains unreliable‚Äîespecially for complex and rare Chinese characters‚Äîleading to poor coverage and accuracy.<br>‚Ä¢ Open-source models often lag in photorealism and aesthetic quality compared to commercial SOTA systems.<br>‚Ä¢ Prevailing large (‚âà20B+ MoE) architectures impose high VRAM requirements, slow inference, and expensive deployment.<br>‚Ä¢ Open-source image editing models struggle with consistency and benchmark performance.<br>‚Ä¢ Developers lack a comprehensive open-source ecosystem with end-to-end training toolchains and intermediate checkpoints for reproducibility and customization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>LongCat-Image is a compact 6B bilingual diffusion model trained via rigorous data curation (filtering, meta-information extraction, multi-granularity captioning, stratification, synthesis) and post-trained with SFT and RLHF using curated reward models, paired with optimized text encoding, positional embeddings, and prompt engineering to maximize multilingual text rendering, photorealism, and efficient deployment for both generation and editing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Glyph-Aware RLHF for Multilingual Text Rendering: Integrate glyph-aware encoders and multi-objective reward models to further improve fidelity and coverage of rare characters across languages.<br>‚Ä¢ LongCat-Image-Lite: Distillation and Quantization for Sub-6B Real-Time Deployment: Develop distillation and quantization pipelines to reduce latency and VRAM while preserving aesthetics and text accuracy.<br>‚Ä¢ Unified Generation‚ÄìEditing Instruction Tuning with Interleaved Multimodal Corpora: Train a single model on interleaved prompts, images, and edit instructions to boost editing consistency, controllability, and generalization.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07783" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07783" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Unclear whether RL-based post-training truly extends reasoning capabilities beyond what is acquired during pre-training versus merely refining existing skills.<br>‚Ä¢ Lack of control and observability in modern training pipelines: opaque pre-training corpora, underexamined mid-training, and RL objectives interacting with unknown prior knowledge.<br>‚Ä¢ Need to isolate causal contributions of pre-training, mid-training, and RL to extrapolative (depth) and contextual (breadth) generalization.<br>‚Ä¢ Existing evaluations often focus on final answers, enabling reward/evaluation hacking and obscuring process fidelity.<br>‚Ä¢ Conflicting results in the literature stem from uncontrolled environments, making it hard to derive principled training strategies.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A fully controlled synthetic reasoning framework with DAG-defined atomic operations and parseable step-by-step traces, systematically manipulating pre-, mid-, and RL training distributions to measure causal effects. The evaluation spans extrapolative and contextual generalization and incorporates process-level rewards to reduce reward hacking and align reinforcement signals with valid reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Curriculum RL at the Edge of Competence for Reasoning LMs: Automated Calibration of Task Difficulty: Develop algorithms to dynamically estimate a model‚Äôs competence boundary and curate RL tasks that are difficult but solvable, maximizing genuine capability gains.<br>‚Ä¢ Minimal Exposure Pre-Training for Robust Contextual Generalization: Measuring and Optimizing Seed Coverage: Quantify the minimal pre-training exposure required for cross-context transfer and design data-efficient sampling strategies that provide sparse yet sufficient coverage.<br>‚Ä¢ Mid-Training as a Distributional Bridge: Compute-Optimal Pipelines for Reasoning Generalization: Establish scaling laws and adaptive data recipes that optimally allocate compute across pre-, mid-, and post-training to enhance out-of-distribution reasoning performance.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.03621" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.03621" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present a 3DGS-based cross-trajectory data curation strategy to eliminate the train-test gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Multi-trajectory driving videos are costly to collect (requiring synchronized vehicles or repeated passes), yet are crucial for 3D reconstruction and world-model training.<br>‚Ä¢ Reconstruction‚Äìthen‚Äìrepair pipelines overfit to training-time degradations and fail on out-of-distribution 3DGS/NeRF artifacts, causing geometry inconsistencies under novel viewpoints.<br>‚Ä¢ Pose-only camera-controlled generation yields imprecise viewpoint control (captures correlations rather than geometric causality), while LiDAR-based conditions are sparse/incomplete and expensive, leading to errors in occluded/distant regions.<br>‚Ä¢ Autonomous driving datasets lack ground-truth novel-trajectory supervision; prior pseudo-pair strategies learn mainly longitudinal motion, creating a train‚Äìtest mismatch and poor lateral trajectory generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ReCamDriving is a LiDAR-free, camera-controlled video generation framework that uses a two-stage latent diffusion training: Stage 1 learns pose-guided viewpoint transformation with relative poses via a flow-matched DiT; Stage 2 freezes the base and fuses novel-trajectory 3DGS renderings through rendering- and cross-attention to provide dense geometric guidance for precise, consistent control. A 3DGS-based cross-trajectory data curation builds ParaDrive and aligns training‚Äìinference patterns with lateral offsets from monocular videos.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Self-Supervised Camera-Controlled Video Generation Without 3D Reconstruction: Learn camera-conditioned viewpoint transformations directly from monocular videos using implicit geometry priors, removing reliance on 3DGS.<br>‚Ä¢ End-to-End Differentiable 3DGS-Conditioned Generators for Dynamic Scenes: Jointly train 3DGS and the video generator in a unified pipeline that models moving objects and occlusions for improved temporal and geometric consistency.<br>‚Ä¢ Web-Scale ParaDrive: Scaling Cross-Trajectory Supervision from Internet Videos: Automate pose recovery and lateral trajectory synthesis from web videos to build massive cross-trajectory datasets and further improve camera control robustness.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06533" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06533" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Decoding-based regression, which reformulates regression as a sequence generation task, has emerged as a promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as a Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as a robust and accurate paradigm for general-purpose numerical prediction.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Misalignment between token-level training objectives (e.g., cross-entropy) and continuous numerical targets, leading to poor capture of the global magnitude of values.<br>‚Ä¢ Existing decoding-based regression methods rely on token-level constraints, limiting precision, generalization, and sequence-level numerical coherence.<br>‚Ä¢ Traditional regression heads (pointwise, parametric, histogram) either impose rigid assumptions or are confined to structured data, hindering robust regression on unstructured modalities with LLMs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Formulate decoding-based regression as a Markov Decision Process and optimize generation with sequence-level rewards via reinforcement learning (e.g., ReMax, GRPO), aligning decoded sequences with global numerical accuracy and improving precision and sampling efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Reward Shaping for Decoding-based Regression: Design and calibrate sequence-level reward functions that balance precision, robustness, and numerical coherence across tasks.<br>‚Ä¢ Hybrid Supervision for Numerical Generation: Integrate token-level cross-entropy with RL-based sequence rewards to stabilize training and boost generalization in decoding-based regression.<br>‚Ä¢ Distributional Decoding-based Regression via RL: Extend RL training to produce calibrated uncertainty and full predictive distributions for numerical targets across modalities.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06373" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06373" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Tool-integrated visual reasoning models are frequently misled by unreliable or erroneous visual tool outputs (e.g., detection boxes), causing hallucinated referring and grounding reasoning.<br>‚Ä¢ Existing TiVR methods emphasize tool integration via RL/CoT but lack explicit mechanisms to analyze, verify, and correct tool feedback during reasoning.<br>‚Ä¢ There is no standardized way to measure a model‚Äôs refinement ability; robust metrics and fair evaluation protocols for tool-error correction are missing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VG-Refiner introduces a two-stage think‚Äìrethink mechanism that explicitly inspects and responds to tool feedback, trained with agentic reinforcement learning and a refinement reward to incentivize correcting poor tool results. It also provides new metrics and the PiTER evaluation protocol to quantify refinement under weak/strong tool conditions while using limited task-specific data.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Tool-Trust Estimation for TiVR: Learn instance-level reliability scores for tool outputs to guide when and how the think‚Äìrethink process engages correction.<br>‚Ä¢ Multi-Tool Arbitration with Agentic RL: Extend VG-Refiner to coordinate and fuse multiple heterogeneous tools (detectors/segmenters), resolving conflicts via learned arbitration policies.<br>‚Ä¢ Uncertainty-Aware Referring Grounded Reasoning: Integrate probabilistic uncertainty modeling for both tool outputs and model reasoning to produce confidence-calibrated corrections and robust grounding.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.03244" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.03244" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Dense, step-level feedback for RL is valuable but current Process Reward Models (PRMs) require expensive step annotations or ground-truth references, limiting scalability.<br>‚Ä¢ Existing RL verifiers (binary discriminators, exact-answer matching like RLVR) provide sparse outcome-level rewards, hindering stable training and credit assignment in multi-step reasoning.<br>‚Ä¢ Ground-truth dependence prevents RL in domains lacking verifiable answers or accessible references, blocking broader application of reasoning-enhanced LLMs.<br>‚Ä¢ PRMs and co-evolving verifier approaches (e.g., TANGO, PRIME) still rely on gold solutions, leaving a gap for fully reference-free, stepwise reward learning.<br>‚Ä¢ Reward hacking risk increases with learned reward models, necessitating mechanisms to enforce format and integrity during RL.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SPARK is a three-stage, reference-free framework: generate diverse solutions and verify them via parallel self-consistency and sequential meta-critique to produce synthetic step-level signals, then fine-tune a generative PRM on these signals and use it (with chain-of-thought verification and format constraints) as the reward model in RL.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Meta-Verification at Scale: Ensemble and Uncertainty Calibration for Reference-Free PRMs: Explore larger, diverse verifier ensembles, improved aggregation, and calibration to strengthen synthetic step-level labels and robustness.<br>‚Ä¢ SPARK-X: Reference-Free Process Rewards for Open-Ended and Non-Verifiable Tasks: Adapt PRM-CoT and format constraints to creative, planning, and agentic domains without ground truth, using human or heuristic preferences for evaluation.<br>‚Ä¢ Anti-Hacking PRMs: Formal Constraints and Adversarial Training for Safe Reward Models: Develop formal syntax constraints, parsing-based guards, and adversarial training to systematically reduce reward hacking in PRM-driven RL.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06589" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06589" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ MLLMs remain highly vulnerable to multimodal jailbreaks that bypass safety alignment, especially via images embedding concealed or typographic cues.<br>‚Ä¢ Existing benchmarks have limited risk coverage and ignore user intent types (consultative, imperative, declarative), reducing realism and diagnostic power.<br>‚Ä¢ Evaluations are fragmented and irreproducible across studies (different attack/defense sets, pipelines, and target models), hindering fair comparison.<br>‚Ä¢ Most works rely on a single metric (ASR), lacking nuanced safety‚Äìutility analysis (e.g., harmfulness severity, intent alignment, response detail).<br>‚Ä¢ There is no unified, open-source toolbox that standardizes datasets, attack‚Äìdefense implementations, and multi-dimensional evaluation for MLLMs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>OmniSafeBench-MM is a unified benchmark and toolbox that integrates a large multimodal dataset (9 risk domains, 50 subcategories, 3 inquiry types), 13 attack methods, and 15 defense strategies with modular APIs and standardized pipelines. It introduces a three-dimensional evaluation (harmfulness, intent alignment, response detail) and an automated data generation pipeline (risk text ‚Üí unsafe key phrases ‚Üí risk images), enabling reproducible attack‚Äìdefense studies across 18 MLLMs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Safety-Utility Calibration for MLLMs via Multi-Objective Optimization: Learn policies that jointly optimize harmfulness reduction, intent alignment, and response detail across risk domains and intent types.<br>‚Ä¢ Certified Robustness for Multimodal Jailbreaks under Joint Visual‚ÄìTextual Perturbations: Develop certification methods and provable guarantees against combined image/text jailbreak strategies.<br>‚Ä¢ AutoDefense-MM: Meta-Learned Defense Selection and Orchestration Using OmniSafeBench-MM Signals: Automatically select and compose off-/on-model defenses per sample and attack type to maximize safety while preserving utility.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">Group Representational Position Encoding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07805" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07805" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in SO(d) and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group GL. In Multiplicative GRAPE, a position n in Z (or t in R) acts as G(n)=exp(n,œâ,L) with a rank-2 skew generator L in R^{d times d}, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the d/2 planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at O(d) and O(r d) cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Transformers require positional mechanisms that are relative, translation-equivariant, and streamable; absolute codes break equivariance and table-based relatives add window-dependent overhead<br>‚Ä¢ RoPE, while norm-preserving and effective, fixes coordinate planes and a log-uniform spectrum, limiting cross-subspace feature coupling and contextual phase warping<br>‚Ä¢ Additive bias methods like ALiBi and FoX offer strong length extrapolation but are not unified with multiplicative approaches and lack a principled geometric framework<br>‚Ä¢ A unified, principled design space is needed that preserves exact relative laws, supports long-context efficiency, and extends expressivity with learned and contextual structures</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>GRAPE models positions as group actions via matrix exponentials G(n)=exp(n¬∑œâ¬∑L), with two families: multiplicative rotations in SO(d) using rank-2 skew generators (recovering and extending RoPE with learned commuting subspaces and compact non-commuting mixtures) and additive unipotent actions in GL producing linear logit biases (recovering ALiBi and FoX), all preserving exact relative laws and streaming cacheability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive GRAPE: State-Dependent Generators for Context-Aware Positional Geometry: Learn generators conditioned on sequence state/content to enable contextual rotations or biases while maintaining the relative law and streaming<br>‚Ä¢ Non-Commuting Mixture GRAPE: Efficient Cross-Subspace Coupling with Lie-Algebraic Regularization: Develop theory and kernels for compact non-commuting mixtures to enhance feature coupling and control complexity<br>‚Ä¢ Path-Integral Additive GRAPE: Content-Gated Biases and Extrapolation Guarantees: Formalize path-integral unipotent actions for additive biases, analyze monotonicity/stability, and provide length extrapolation bounds</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06835" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06835" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ RL post-training for LVLMs is bottlenecked by scarce, high-quality multimodal data in specialized domains (e.g., chemistry, earth science, multimodal math).<br>‚Ä¢ Synthetic data and self-rewarding strategies yield narrow, repetitive distributions and misaligned objectives, leading to reward hacking, policy-entropy collapse, and unstable training.<br>‚Ä¢ QA-only supervision overlooks rich contextual information in problem stems, incentivizing shortcut exploitation over genuine reasoning and harming generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DoGe decouples LVLM cognition into a learnable Thinker (context analysis on question-masked input) and a frozen Solver, training them in a two-stage RL loop where the Thinker is rewarded by the Solver‚Äôs pass rate, then annealed with GRPO on original tasks using correctness and format rewards. An evolving curriculum augments data diversity via a multimodal knowledge pool and an iteratively updated seed problem pool.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Context-First RL for Domain-Specific LVLMs: Learn dynamic masking and analysis prompting strategies per domain, integrating Solver uncertainty to refine Thinker rewards.<br>‚Ä¢ Multi-Solver Ensembles for Robust Self-Evolving Vision-Language Reasoning: Combine diverse frozen Solvers to provide richer, calibrated rewards that reduce reward hacking and improve stability.<br>‚Ä¢ Training-Free DoGe: In-Context Decoupled Reasoning with External Knowledge Pools: Shift evolution to inference-time by injecting curated context analyses and knowledge snippets, achieving DoGe-like gains without parameter updates.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07829" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07829" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Fundamental mismatch between high-dimensional, understanding-oriented self-supervised features (built for masked prediction) and the low-dimensional, noise-preserving latent spaces favored by generative models (diffusion/flows).<br>‚Ä¢ High-dimensional latents increase resource demands, slow convergence, and make diffusion dynamics sensitive/unstable; compact latents are better for stable denoising trajectories and efficiency.<br>‚Ä¢ Prior methods rely on complex objectives (alignment losses) and significant architectural changes (e.g., widening generators for high-dim features), raising system complexity and training cost.<br>‚Ä¢ Need a generic, simple, and plug-and-play adaptation that preserves semantic/spatial information for reconstruction and understanding while enabling efficient, high-quality generation across model families.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>FAE compresses pretrained visual embeddings into a compact generative code using a single attention layer plus a linear projection, and couples two decoders: one reconstructs the original feature space, while the generative model consumes the reconstructed features for image synthesis. This minimal, generic adapter integrates with diffusion and flow models and retains semantics without complex alignment losses or heavy architectural changes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond One Layer: Adaptive Capacity Allocation for Feature Compression in Generative Models: Study when more than one attention layer helps, with dataset/model-size adaptive depth/width strategies.<br>‚Ä¢ FAE-V: Temporal Feature Auto-Encoders for Video Generation: Extend FAE with temporal attention and decoders to adapt pretrained video encoders for diffusion/flow-based video synthesis.<br>‚Ä¢ Multimodal FAE for Text-to-Image and Beyond: Jointly compress and align visual and text embeddings (e.g., SigLIP/CLIP) into a shared low-dimensional generative space for controllable multimodal generation.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">VideoVLA: Video Generators Can Be Generalizable Robot Manipulators</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06963" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06963" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Deployable robots must generalize to unseen tasks, objects, and environments; current VLA systems still struggle with true out-of-distribution generalization.<br>‚Ä¢ Prior VLA methods primarily leverage pre-trained understanding models (vision/language/vision-language), which offer perception and instruction following but weakly capture physical dynamics and planning.<br>‚Ä¢ Existing pipelines often modularize video prediction and action inference, leading to weak alignment between imagined futures and executable actions.<br>‚Ä¢ There is no established way to transfer the strong generalization of large video generators (trained on massive real-world videos) into action control for manipulation.<br>‚Ä¢ Robots need data efficiency; leveraging powerful pre-trained video generators could reduce task-specific robot data while improving robustness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VideoVLA adapts a large pre-trained video diffusion transformer into a unified Video-Action Diffusion Transformer that, conditioned on T5-encoded instructions and a causal video-VAE latent of the current observation, jointly denoises future video latents and a 7D action sequence under a DDPM loss. This dual-prediction aligns action reliability with the fidelity of imagined futures, enabling strong generalization to novel objects and unseen skills.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Scaling Visual Imagination for Generalizable Manipulation: A Study of Data and Model Size in Video-Action Diffusion Transformers: Systematically analyze scaling laws to quantify how model/data size affects zero-shot generalization in manipulation.<br>‚Ä¢ Uncertainty-Aware VideoVLA: Calibrated Visual Futures for Safe and Reliable Robot Control: Incorporate uncertainty estimation and confidence-guided selection of actions based on the consistency/quality of imagined videos.<br>‚Ä¢ Cross-Embodiment Transfer with Adapter-Based VideoVLA: Parameter-efficient adapters and embodiment conditioning to rapidly port skills across different robot morphologies using minimal new data.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Rethinking Training Dynamics in Scale-wise Autoregressive Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06421" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06421" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Train‚Äìtest mismatch (teacher forcing vs. inference on self-generated latents) causes exposure bias and error accumulation across scales in next-scale AR, degrading image quality.<br>‚Ä¢ Scale-wise supervision/learning imbalance: coarse scales must synthesize global structure from weak/blurry inputs while fine scales act like super-resolution and cannot correct semantic errors, leading to uneven optimization and brittle generation.<br>‚Ä¢ Na√Øve student forcing in VAR is unstable, slow, and misaligned with ground-truth supervision, amplifying noise and breaking hierarchical consistency.<br>‚Ä¢ Existing fixes (image-level smoothing, equivariant VAEs, hybrid masked modeling at coarse scales) either worsen FID or collapse later scales into near super-resolution, failing to resolve the root train‚Äìtest and scale-imbalance issues.<br>‚Ä¢ Current VAR frameworks lack mechanisms to correct accumulated multi-scale errors during inference, limiting robustness and perceptual fidelity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Self-Autoregressive Refinement (SAR) combines Stagger-Scale Rollout (a lightweight two-pass, one-step student-forcing that conditions on model-generated latents) with a Contrastive Student-Forcing Loss that aligns student outputs to teacher-forced trajectories (with per-scale reweighting), closing the train‚Äìtest gap and stabilizing scale-wise optimization with minimal overhead.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Rollout Depth for Stable Hierarchical Autoregression: Learn confidence-aware rollout depth per scale to balance exposure to self-generated latents and training stability.<br>‚Ä¢ SAR for Conditional and Spatiotemporal Generation: Extend SSR+CSFL to text-to-image and video VAR to mitigate exposure bias across modality and time.<br>‚Ä¢ Joint Tokenizer‚ÄìGenerator Training under SAR: Co-train VAEs/quantizers and VAR with SAR to alleviate supervision imbalance and improve multiscale factorization and error correction.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06791" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06791" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Classical convergence guarantees for gradient-based learning in games require the pseudo-gradient to be (strongly) monotone in Euclidean geometry as shown by rosen(1965), a condition that often fails even in simple games with strong cross-player couplings. We introduce Small-Gain Nash (SGN), a block small-gain condition in a custom block-weighted geometry. SGN converts local curvature and cross-player Lipschitz coupling bounds into a tractable certificate of contraction. It constructs a weighted block metric in which the pseudo-gradient becomes strongly monotone on any region where these bounds hold, even when it is non-monotone in the Euclidean sense. The continuous flow is exponentially contracting in this designed geometry, and projected Euler and RK4 discretizations converge under explicit step-size bounds derived from the SGN margin and a local Lipschitz constant. Our analysis reveals a certified ``timescale band'', a non-asymptotic, metric-based certificate that plays a TTUR-like role: rather than forcing asymptotic timescale separation via vanishing, unequal step sizes, SGN identifies a finite band of relative metric weights for which a single-step-size dynamics is provably contractive. We validate the framework on quadratic games where Euclidean monotonicity analysis fails to predict convergence, but SGN successfully certifies it, and extend the construction to mirror/Fisher geometries for entropy-regularized policy gradient in Markov games. The result is an offline certification pipeline that estimates curvature, coupling, and Lipschitz parameters on compact regions, optimizes block weights to enlarge the SGN margin, and returns a structural, computable convergence certificate consisting of a metric, contraction rate, and safe step-sizes for non-monotone games.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Gradient-based learning in differentiable games often fails to converge in Euclidean geometry when strong cross-player couplings make the pseudo-gradient non-monotone, leading to oscillations or divergence.<br>‚Ä¢ Practical guarantees are needed that certify contraction and convergence (continuous flow and simple discretizations) on closed convex regions with explicit, computable step-size bounds.<br>‚Ä¢ Existing stabilizations like TTUR enforce asymptotic timescale separation via vanishing, unequal step sizes, which can be unnecessarily slow; a finite-timescale certification is desirable.<br>‚Ä¢ Current analyses underuse non-Euclidean/block geometries, lacking tools to convert local curvature and coupling bounds into strong monotonicity and contraction certificates.<br>‚Ä¢ There is no standard offline pipeline to estimate curvature/coupling/Lipschitz parameters, design metrics, and return structural convergence certificates for non-monotone games.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce Small-Gain Nash (SGN), a weighted block-diagonal geometry M(w) that certifies strong monotonicity of the pseudo-gradient via a positive-definite small-gain matrix C(w, Œ±), yielding exponential contraction and explicit step-size bounds for projected Euler and RK4 based on the SGN margin Œ± and a Lipschitz constant Œ≤. The framework provides a finite ‚Äútimescale band‚Äù over player-weight ratios and an offline certification pipeline that estimates curvature/coupling bounds, optimizes weights, and outputs a metric, contraction rate, and safe step sizes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Small-Gain Nash: Online Metric and Weight Learning in Stochastic Games: Develop algorithms that continually estimate curvature/coupling and adapt block weights and metrics to maintain SGN contraction under stochastic gradients and nonstationary dynamics.<br>‚Ä¢ SGN-Certified Training for GANs and Adversarial Networks: Apply SGN with mirror/Fisher geometries to modern adversarial training, producing practical certification pipelines that deliver safe step sizes and timescale bands without TTUR.<br>‚Ä¢ Robust SGN: Convergence Certificates Under Uncertain Curvature and Coupling Estimates: Incorporate interval/uncertainty bounds into SGN, derive robust small-gain conditions and step-size rules that remain valid under estimation error on certified regions.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">Vector Quantization using Gaussian Variational Autoencoder</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06609" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06609" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Training VQ-VAEs is hard due to non-differentiable quantization and frequent codebook collapse, requiring complex stabilizing tricks (commitment loss, EM, Gumbel-Softmax, entropy).<br>‚Ä¢ Existing VQ methods learn codebooks and involve heavy optimization without principled guidance on codebook size, leading to suboptimal rate‚Äìdistortion trade-offs.<br>‚Ä¢ There is a need for a simple, training-free way to discretize Gaussian VAEs into tokens for autoregressive models while preserving rate‚Äìdistortion performance.<br>‚Ä¢ Prior Gaussian-VAE discretization approaches (e.g., TokenBridge) lack alignment with bits-back coding rates and have limited theoretical guarantees and performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Gaussian Quant (GQ) samples a shared codebook from N(0,1) and quantizes each latent dimension by choosing the nearest codeword to the posterior mean, with theoretical guarantees that small quantization error occurs when log K matches/exceeds the bits-back coding rate. Target Divergence Constraint (TDC) trains the Gaussian VAE so each dimension‚Äôs KL (bits-back rate) matches a target, enabling effective, training-free conversion.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Per-Dimension Codebook Sizing for Gaussian Quantization: Select K_i per latent dimension based on its KL divergence to align bitrate with bits-back rates and improve rate‚Äìdistortion.<br>‚Ä¢ Multivariate Gaussian Quantization with Correlation-Aware Codebooks: Extend GQ from independent dimensions to joint quantization using structured/lattice or mixture-based codebooks that model latent correlations.<br>‚Ä¢ Beyond Gaussian Priors: Training-Free Quantization under Learned Latent Distributions: Generalize GQ to VAEs with non-Gaussian priors (flows, VampPrior) by sampling codebooks from the learned prior and deriving new error bounds.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.03704" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.03704" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a calibrated temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (55.4% on Phi-3.5) while maintaining robust zero-shot generalization. Our scaling analysis reveals a "Capacity-Stability Trade-off": while smaller models incur an "alignment tax" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves 50.8% win rate with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Long-context dialogue systems exhibit State Inertia: models over-attend to outdated history and fail to override past user intents when new, conflicting information appears.<br>‚Ä¢ Static alignment methods (RLHF/DPO) impose immutable historical priors, creating Temporal Attention Imbalance and penalizing necessary state updates, which leads to an "alignment tax" (perplexity surge and degraded general capabilities).<br>‚Ä¢ Existing long-context techniques optimize static retrieval and receptive field extension but do not address conflict-aware mutable state tracking, making real-world conversational adaptation unreliable.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DZ-TDPO integrates a conflict-aware dynamic KL-constrained DPO (TDPO-DKL) with a dual-zone temporal attention bias that selectively suppresses outdated context only when conflict is detected, enabling non-destructive state updates in long-context dialogue.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Conflict Detection for Dual-Zone Temporal Attention: Learn content-aware signals to trigger and calibrate attention suppression without manual heuristics.<br>‚Ä¢ Non-Destructive Alignment beyond Dialogue: Instruction and Code Editing: Extend DZ-TDPO to tasks with evolving objectives (instruction following, code edits) and evaluate generalization and alignment tax.<br>‚Ä¢ Scaling Laws for Temporal Attention Imbalance and Alignment Tax: Quantify capacity‚Äìstability trade-offs across model sizes and training regimes, and design calibration strategies that retain MMLU while improving mutable state tracking.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Embodied Referring Expression Comprehension in Human-Robot Interaction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06558" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06558" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, a multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as a valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Robots must comprehend embodied human instructions that combine language with gestures, gaze, and posture to enable natural and fluent HRI.<br>‚Ä¢ There is a lack of large-scale datasets capturing natural embodied interactions across diverse viewpoints and indoor/outdoor environments.<br>‚Ä¢ Existing datasets exhibit perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor settings.<br>‚Ä¢ Many current systems rely on single-modality inputs or scripted interactions, limiting robustness and adaptability in real-world scenarios.<br>‚Ä¢ Contemporary multimodal models fail to comprehensively capture embodied interactions, highlighting the need for better modality-specific signal extraction and integration.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces Refer360, a large-scale multi-view (exo, ego, depth) dataset of embodied verbal and nonverbal interactions collected in indoor and outdoor settings, and proposes MuRes‚Äîa multimodal guided residual module that acts as an information bottleneck to distill salient modality-specific cues and reinforce them into pre-trained representations, improving embodied referring expression comprehension.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ MuRes-XL: Guided Residual Fusion for Large Multimodal Models in HRI: Extend MuRes to large multimodal models and evaluate cross-dataset generalization and robustness on Refer360 and additional HRI benchmarks.<br>‚Ä¢ Refer360++: Cross-View Temporal Grounding of Embodied Referring Expressions: Enrich Refer360 with fine-grained temporal and gesture annotations and develop models that enforce temporal consistency and cross-view alignment for embodied grounding.<br>‚Ä¢ On-Robot Online Adaptation for Embodied Referring Comprehension in Dynamic Environments: Design continual learning and human-in-the-loop adaptation strategies leveraging MuRes to maintain performance under changing viewpoints, lighting, and outdoor conditions.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06032" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06032" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Explain why SAM2‚Äôs prompt-based (geometric/temporal) expertise fails to transfer to SAM3‚Äôs concept-driven, multimodal segmentation.<br>‚Ä¢ Systematically detail the non-transferable differences in prompting, architecture (vision-only vs vision‚Äìlanguage fusion), datasets (video masks vs multimodal concept labels), training objectives (geometric/temporal vs contrastive/grounding), and evaluation (IoU/temporal vs semantic/open-vocabulary).<br>‚Ä¢ Provide practitioner guidance by exposing new failure modes and outlining the skills, data, and metrics required for effective SAM3 deployment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A structured five-axis comparative analysis contrasting SAM2 and SAM3 across prompt modalities, architectures, datasets, training losses/hyperparameters, and evaluation metrics, supplemented by a technical breakdown of SAM3‚Äôs multimodal components (vision/text encoders, fusion, DETR-style decoder with object queries, presence head, MoE) and illustrative loss formulations to formalize non-transferability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Curriculum Multimodal Training to Bridge Prompt- to Concept-Driven Segmentation: Stage-wise unfreezing and loss scheduling to transition from geometric priors to robust vision‚Äìlanguage alignment.<br>‚Ä¢ Ontology-Guided Multimodal Dataset Construction for SAM3: Scalable data engines linking masks to concept labels, with ambiguity resolution, synonym handling, and multilingual coverage to improve open-vocabulary generalization.<br>‚Ä¢ Semantic-Preserving Augmentation and Benchmarks for Concept-Level Segmentation: Design augmentations that retain attribute semantics and build evaluation protocols for concept recall, grounding error, and language robustness.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 10</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-12-09 23:09:19 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 10;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>