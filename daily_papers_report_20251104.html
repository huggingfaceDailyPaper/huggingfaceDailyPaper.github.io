<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 04, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 04, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22115" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22115" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Ling 2.0, a series reasoning-oriented language foundation built upon the principle that every activation boosts reasoning capability. Designed to scale from tens of billions to one trillion parameters under a unified Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity, cross-scale consistency, and efficiency guided by empirical scaling laws. The series includes three non-thinking (instruct) models - Ling-mini-2.0, Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and achieving up to 7-fold active-compute efficiency compared with dense counterparts. Ling 2.0 integrates coordinated innovations across model architecture, pre-training, post-training, and infrastructure: a high-sparsity MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale FP8 training with fine-grained heterogeneous pipelines. At the trillion scale, Ling-1T establishes a new Pareto frontier of reasoning accuracy versus computational efficiency, demonstrating that sparse activation, when properly aligned with reasoning objectives, enables scalable and efficient intelligence. Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for advancing future reasoning and thinking models, including the Ring series built upon the same base.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Scaling reasoning to trillion-parameter LLMs without exploding compute cost‚Äîdense models are prohibitive, and stable, predictable scaling at extreme scale is hard (pp. 1‚Äì3)<br>‚Ä¢ Lack of reliable, low-cost extrapolation from small models to trillion scale for MoE hyperparameters/architectures‚Äîexisting ablations are expensive and often non-transferable (Sec. 2.3; Fig. 3)<br>‚Ä¢ Sparse MoE underutilization: routing/load-balance inefficiencies and unclear optimal activation granularity/sparsity hurt efficiency (Sec. 2.2‚Äì2.3; Fig. 2)<br>‚Ä¢ Sustained reasoning improvement is brittle across pre-, mid-, and post-training; building reasoning-centric corpora and transferring CoT behaviors remain difficult (Sec. 3.1‚Äì3.2)<br>‚Ä¢ Conventional LR decay constrains flexibility and adds tuning burden during long runs (Sec. 3.2.3; Fig. 7)<br>‚Ä¢ RLHF for complex reasoning suffers from noisy/ambiguous rewards and unstable token/sequence-level policy updates (Sec. 4.2‚Äì4.3; Fig. 10‚Äì11)<br>‚Ä¢ Infrastructure bottlenecks at trillion scale: stable FP8 training, pipeline bubbles from heterogeneous modules (MTP/First-K-Dense), MoE communication overhead, long-context stability, and slow evaluation (Sec. 5; Fig. 15‚Äì18)</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A high‚Äësparsity, fine‚Äëgrained MoE architecture (256 experts; 8+1 shared active ‚âà3.5% tokens) guided by Ling Scaling Laws and a standardized ‚Äúwind‚Äëtunnel‚Äù pipeline delivers ~7√ó active‚Äëcompute efficiency and stable trillion‚Äëscale extrapolation. The end‚Äëto‚Äëend stack couples reasoning‚Äëoriented pretraining (reasoning‚Äëheavy data, mid‚Äëtraining CoT pre‚Äëactivation, WSM checkpoint‚Äëmerge scheduler) with post‚Äëtraining (DFT, Evo‚ÄëCoT, sentence‚Äëlevel LPO, Group Arena Reward) and full‚Äëscale FP8 + heterogeneous fine‚Äëgrained pipelines for efficient, stable 1T training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond GQA: Linear/Sparse Attention for 128K‚Äì1M Context Ling Models: Replace/augment GQA with linear/sparse attention to lift long‚Äëcontext efficiency while preserving reasoning accuracy.<br>‚Ä¢ Adaptive Router + A2A Overlap: Communication‚ÄëComputation Co‚ÄëDesign for Trillion‚ÄëScale MoE: Jointly optimize routing balance and expert communication overlap to reduce bottlenecks under extreme sparsity.<br>‚Ä¢ Auto Wind‚ÄëTunnel: Zero‚ÄëShot Extrapolation for Foundation Model Design: Automate scaling‚Äëlaw‚Äëdriven ‚Äúwind‚Äëtunnel‚Äù search across modalities (text/code/math/multimodal) and training recipes with <1% compute.<br>‚Ä¢ LPO‚ÄëX: Structured‚ÄëUnit Policy Optimization Beyond Sentences: Generalize sentence‚Äëlevel RL to logic‚Äësteps/paragraphs with theoretical stability and variance analyses on complex reasoning.<br>‚Ä¢ Cost‚ÄëAware Evo‚ÄëCoT: Dynamic Token Budgeting for Efficient Deep Reasoning: Learn controllers that trade accuracy vs. average tokens, pushing the Pareto frontier on AIME/Omni‚ÄëMATH.<br>‚Ä¢ FP8++ Safeguards: Online Precision Governance for Ultra‚ÄëLarge LMs: Extend underflow/distortion metrics to mixed‚Äëformat quantization and adaptive per‚Äëlayer precision for further MFU gains.<br>‚Ä¢ GAR‚ÄëX: Groupwise Preference Learning with Human‚Äëin‚Äëthe‚ÄëLoop Tournaments: Combine arena‚Äëstyle relative rewards with sparse human adjudication and provide variance/error bounds.<br>‚Ä¢ Unified Reasoning Data Engine: Concept‚ÄëGraph‚ÄëDriven Synthesis and Governance at Scale: Automate math/code data synthesis, decontamination, and curriculum with traceable lakehouse pipelines.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.00086" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.00086" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Test-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking that optimal architectures and model combinations can vary across tasks. Therefore, we study the novel problem of searching for compute-optimal model combinations and architectures in TTS under a fixed budget. We formalize it as a multi-LLM collaboration graph, where nodes encode roles and LLM model assignments, and edges capture information flow. This problem is challenging because (i) the combinatorial search space is prohibitively large, and (ii) task-specific requirements demand tailored designs. To address these, we reformulate the problem as probabilistic graph optimization and, through pilot experiments, derive three empirical insights into TTS collaboration graphs. Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented framework that mirrors the REINFORCE pipeline by mapping sampling-gradient-update to sampling-feedback-update, where feedback serves as a textual gradient to update the probabilistic graph and efficiently search for optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE outperforms both traditional and LLM-based baselines in sample efficiency and search performance, and effectively identifies optimal graphs under joint objectives of accuracy and inference latency.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Static, predefined TTS architectures (sequential/parallel/fixed hybrids) and single-model pipelines fail to adapt topology and model mix to task needs; different tasks prefer different topologies and model sizes (e.g., MATH vs. MMLU).<br>‚Ä¢ Compute-optimal TTS under a fixed budget is a combinatorial search over graphs and model assignments (up to 10^18‚Äì10^26 candidates for 12 nodes), making brute-force or naive search infeasible.<br>‚Ä¢ Existing optimizers are mismatched: BO is sample-inefficient in large discrete spaces; pure REINFORCE is slow and prone to local optima; prior TTS overlooks non-monotonic width/depth optima and their interaction, wasting compute.<br>‚Ä¢ Lacks a unified, budget-aware formulation with principled cost models (FLOPs/$) and support for multi-objective trade-offs (accuracy‚Äìlatency), hindering practical deployment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Frame TTS as optimizing a probabilistic multi-LLM collaboration graph (nodes = roles {assistant,fuser} with model assignments; edges = information flow) under a budget, and solve it with Agent-REINFORCE‚Äîan LLM-agent-augmented REINFORCE that initializes model family/size, samples graphs, and uses execution feedback as textual gradients to update edge/role/model distributions, guided by three empirical insights on model choice and width‚Äìdepth optima/coupling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Instance-Adaptive Test-Time Graphs via Contextual Bandits: Learn per-input graph topology and model mix selection with lightweight signals to allocate compute optimally at the instance level.<br>‚Ä¢ Theory of Budget-Optimal TTS Graphs: Regret Bounds and Convergence of Agent-REINFORCE: Provide theoretical guarantees and sample-complexity analyses for probabilistic graph search with textual feedback.<br>‚Ä¢ Hardware- and Energy-Aware Multi-Objective TTS Graph Search: Jointly optimize accuracy, latency, memory, and energy/carbon under hardware constraints to yield deployable, green test-time graphs.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">The Underappreciated Power of Vision Models for Graph Structural Understanding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24788" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24788" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Graph Neural Networks operate through bottom-up message-passing, fundamentally differing from human visual perception, which intuitively captures global structures first. We investigate the underappreciated potential of vision models for graph understanding, finding they achieve performance comparable to GNNs on established benchmarks while exhibiting distinctly different learning patterns. These divergent behaviors, combined with limitations of existing benchmarks that conflate domain features with topological understanding, motivate our introduction of GraphAbstract. This benchmark evaluates models' ability to perceive global graph properties as humans do: recognizing organizational archetypes, detecting symmetry, sensing connectivity strength, and identifying critical elements. Our results reveal that vision models significantly outperform GNNs on tasks requiring holistic structural understanding and maintain generalizability across varying graph scales, while GNNs struggle with global pattern abstraction and degrade with increasing graph size. This work demonstrates that vision models possess remarkable yet underutilized capabilities for graph structural understanding, particularly for problems requiring global topological awareness and scale-invariant reasoning. These findings open new avenues to leverage this underappreciated potential for developing more effective graph foundation models for tasks dominated by holistic pattern recognition.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Message-passing GNNs struggle with global/topological understanding (expressiveness limits, long-range dependency capture, over-squashing), whereas humans perceive graphs globally first; this cognitive gap motivates alternative approaches (pp. 1‚Äì3).<br>‚Ä¢ Standard graph benchmarks conflate node/edge features with topology, making it hard to measure pure structural reasoning; some models match performance with fixed or mismatched topologies, masking weaknesses in topological understanding (pp. 2‚Äì3).<br>‚Ä¢ There is no rigorous benchmark to test scale-invariant, human-like structural perception (e.g., recognizing archetypes, symmetry, connectivity strength, critical bridges) under OOD graph sizes (Section 4.3, p. 8).<br>‚Ä¢ The underexplored potential of vision models for holistic pattern recognition in graphs suggests a promising but untested alternative to message passing (pp. 2‚Äì3); initial evidence shows vision backbones rival or surpass GNNs on structural tasks and generalize better across scales (Table 2, p. 9; Table 3, p. 10).<br>‚Ä¢ Importance: many real-world tasks (molecules, social/biological networks, infrastructure) depend on global topology and scale invariance; improving these capabilities yields more robust, interpretable, and generalizable models (pp. 1‚Äì2, 9‚Äì10).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Render graphs as images using standard layout algorithms (e.g., Kamada‚ÄìKawai, Spectral, ForceAtlas2) and apply off-the-shelf vision encoders (ResNet, ViT, Swin, ConvNeXt) to graph-level tasks; introduce GraphAbstract‚Äîa scale-aware benchmark (topology classification, symmetry via automorphisms, spectral-gap regression, bridge counting)‚Äîto rigorously evaluate global structural understanding and OOD generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Vision-Centric Graph Foundation Models via Large-Scale Pretraining on Graph Visualizations: Pretrain vision backbones on massive, multi-domain graph-image corpora with topology-preserving augmentations and attribute encodings to yield universal, scale-invariant graph learners.<br>‚Ä¢ Layout-Learning: Differentiable Graph Rendering Optimized for Structural Reasoning: Jointly learn task-aware, differentiable layout modules with visual encoders to expose symmetry, bottlenecks, and community structure, outperforming fixed layouts across tasks and scales.<br>‚Ä¢ Hybrid Visual‚ÄìGraph Networks with Structural Priors: Inject Laplacian/spectral positional priors and multi-layout ensembles into vision encoders (or fuse with GNN layers) to combine global visual perception with explicit topology, improving OOD and interpretability.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01678" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01678" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Relighting is a crucial task with both practical demand and artistic value, and recent diffusion models have shown strong potential by enabling rich and controllable lighting effects. However, as they are typically optimized in semantic latent space, where proximity does not guarantee physical correctness in visual space, they often produce unrealistic results, such as overexposed highlights, misaligned shadows, and incorrect occlusions. We address this with UniLumos, a unified relighting framework for both images and videos that brings RGB-space geometry feedback into a flow matching backbone. By supervising the model with depth and normal maps extracted from its outputs, we explicitly align lighting effects with the scene structure, enhancing physical plausibility. Nevertheless, this feedback requires high-quality outputs for supervision in visual space, making standard multi-step denoising computationally expensive. To mitigate this, we employ path consistency learning, allowing supervision to remain effective even under few-step training regimes. To enable fine-grained relighting control and supervision, we design a structured six-dimensional annotation protocol capturing core illumination attributes. Building upon this, we propose LumosBench, a disentangled attribute-level benchmark that evaluates lighting controllability via large vision-language models, enabling automatic and interpretable assessment of relighting precision across individual dimensions. Extensive experiments demonstrate that UniLumos achieves state-of-the-art relighting quality with significantly improved physical consistency, while delivering a 20x speedup for both image and video relighting. Code is available at https://github.com/alibaba-damo-academy/Lumos-Custom.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Diffusion-based relighting often breaks physical plausibility because it operates in semantic latent space, leading to misaligned shadows, incorrect occlusions, wrong light directions, and over/under-exposed highlights.<br>‚Ä¢ Video relighting remains hard: frame-wise or training-free pipelines are computationally heavy and temporally unstable, while trained methods still lack explicit physics-grounded supervision.<br>‚Ä¢ RGB-space, geometry-aware supervision is desirable but expensive, since it typically requires high-quality multi-step denoising to yield reliable signals for feedback.<br>‚Ä¢ Existing metrics (e.g., FID, LPIPS) and unstructured prompts fail to evaluate or control lighting-specific attributes (direction, intensity, color temperature, etc.), limiting interpretability and controllability.<br>‚Ä¢ Real-world relighting needs unified handling of both images and videos with spatial realism and temporal consistency, which prior methods struggle to balance.<br>‚Ä¢ Data scarcity of structured illumination annotations hinders fine-grained conditioning, training, and benchmarking.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>UniLumos is a unified image/video relighting framework built on a flow-matching backbone that adds physics-plausible feedback: it decodes RGB, estimates depth and normals with a frozen dense predictor, and supervises generation to align lighting with scene geometry; path-consistency learning enables few-step, fast inference. It further introduces a six-dimensional lighting annotation/data pipeline (LumosData) and a VLM-based attribute benchmark (LumosBench) for fine-grained conditioning and evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ PhysLight: Calibrated Radiance and Illuminance Prediction for Generative Relighting: Extend UniLumos to output physically measurable HDR radiance/illuminance maps with photometric calibration and energy consistency constraints.<br>‚Ä¢ KeyLight Editor: Interactive Light-Rig Control for Diffusion Relighting: Enable explicit, editable key/fill/rim lights, intensity ramps, and environmental reflections with differentiable light rigs and intuitive user controls.<br>‚Ä¢ Lumos3D: Multi-View and 3D-Consistent Relighting with Joint Scene Reconstruction: Fuse UniLumos with multi-view geometry (e.g., NeRF/GS) to achieve view-consistent, temporally stable relighting across cameras and time.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01163" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01163" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Unified multimodal models (UMMs) have emerged as a powerful paradigm for seamlessly unifying text and image understanding and generation. However, prevailing evaluations treat these abilities in isolation, such that tasks with multimodal inputs and outputs are scored primarily through unimodal reasoning, i.e., textual benchmarks emphasize language-based reasoning, while visual benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce ROVER to address this pressing need to test reciprocal cross-modal reasoning, the use of one modality to guide, verify, or refine outputs in the other, an ability central to the vision of unified multimodal intelligence. ROVER is a human-annotated benchmark that explicitly targets reciprocal cross-modal reasoning, which contains 1312 tasks grounded in 1876 images, spanning two complementary settings. Verbally-augmented reasoning for visual generation evaluates whether models can use verbal prompts and reasoning chains to guide faithful image synthesis. Visually-augmented reasoning for verbal generation evaluates whether models can generate intermediate visualizations that strengthen their own reasoning processes for question answering. Experiments on 17 unified models reveal two key findings: (i) Cross-modal reasoning determines visual generation quality, with interleaved models significantly outperforming non-interleaved ones; notably, combining strong unimodal models fails to achieve comparable reasoning. (ii) Models show dissociation between physical and symbolic reasoning: they succeed at interpreting perceptual concepts literally but fail to construct visual abstractions for symbolic tasks, where faulty reasoning harms performance. These results highlight reciprocal cross-modal reasoning as a critical frontier for enabling true omnimodal generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing evaluations isolate modalities, scoring text tasks via language-only reasoning and image tasks via pixel similarity, thus failing to test reciprocal cross-modal reasoning.<br>‚Ä¢ Unified multimodal models need to use one modality to guide, verify, or refine outputs in the other, but no benchmark explicitly targets this core ability.<br>‚Ä¢ Text-only metrics overlook whether generated images reflect valid reasoning; image-only metrics cannot verify the logical soundness of the reasoning behind the pixels; human evaluation is accurate but prohibitively costly at scale.<br>‚Ä¢ Prior benchmarks largely lack interleaved image‚Äìtext reasoning, process evaluation, vision necessity checks, multi-dimensional scoring, and hybrid (auto + human) evaluation; many rely on similarity scores and ignore intermediate rationales.<br>‚Ä¢ Without a rigorous, auditable process+output evaluation, progress on truly omnimodal generation risks optimizing unimodal surrogates instead of cross-modal reasoning fidelity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ROVER introduces a human-verified benchmark (1,312 tasks, 1,876 images) that explicitly measures reciprocal cross-modal reasoning via two complementary settings: verbally-augmented reasoning for visual generation (ROVER-IG) and visually-augmented reasoning for verbal generation (ROVER-TG). It uses a multi-dimensional VLM-as-judge protocol calibrated with expert validation to score reasoning process, reasoning‚Äìvisual alignment/consistency, and output quality (including image consistency/quality or final-answer accuracy).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Interleaved Cross-Modal Chain-of-Thought for UMMs: Train with interleaved text‚Äìimage reasoning trajectories and losses on process‚Äìoutput alignment to boost reciprocal reasoning.<br>‚Ä¢ Visual Abstraction Bootstrapping for Symbolic Geometry and Puzzles: Curriculum and synthetic visual-aid generation to teach models to create and use auxiliary constructions for symbolic tasks.<br>‚Ä¢ Process-Supervised Omnimodal Generation via Consistency Rewards: Optimize models with rewards that couple reasoning coherence and visual/answer fidelity using multi-judge or human preference signals.<br>‚Ä¢ Unified World Models with Differentiable Physics for Visually-Augmented Planning: Integrate light-weight physics engines and state visualization supervision to improve world-model tasks.<br>‚Ä¢ Robust LMM-as-a-Judge for Cross-Modal Evaluation: Build multi-judge, counterfactual, and adversarially-calibrated evaluators to improve reliability of process and alignment scoring.<br>‚Ä¢ Video-ROVER: Extending Reciprocal Cross-Modal Reasoning to Video and Audio: Benchmark interleaved reasoning with temporal multimodality and assess process‚Äìoutput consistency over time.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PHUMA: Physically-Grounded Humanoid Locomotion Dataset</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26236" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26236" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Motion imitation is a promising approach for humanoid locomotion, enabling agents to acquire humanlike behaviors. Existing methods typically rely on high-quality motion capture datasets such as AMASS, but these are scarce and expensive, limiting scalability and diversity. Recent studies attempt to scale data collection by converting large-scale internet videos, exemplified by Humanoid-X. However, they often introduce physical artifacts such as floating, penetration, and foot skating, which hinder stable imitation. In response, we introduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that leverages human video at scale, while addressing physical artifacts through careful data curation and physics-constrained retargeting. PHUMA enforces joint limits, ensures ground contact, and eliminates foot skating, producing motions that are both large-scale and physically reliable. We evaluated PHUMA in two sets of conditions: (i) imitation of unseen motion from self-recorded test videos and (ii) path following with pelvis-only guidance. In both cases, PHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant gains in imitating diverse motions. The code is available at https://davian-robotics.github.io/PHUMA.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ High-quality mocap datasets (e.g., AMASS) are scarce, expensive, and dominated by simple motions, limiting scale and diversity for motion imitation.<br>‚Ä¢ Internet video‚Äìderived datasets (e.g., Humanoid-X) introduce physical artifacts‚Äîfloating, ground penetration, joint-limit violations, and foot skating‚Äîdue to video-to-motion errors (notably pelvis/global translation) and under-constrained retargeting (see Fig. 1, Fig. 4).<br>‚Ä¢ Existing retargeting methods trade off style vs. physics: IK loses motion fidelity; SINK preserves style but ignores physics, yielding infeasible references that hinder stable imitation (Table 2) and degrade dynamic motion tracking and path following (Tables 3‚Äì5).<br>‚Ä¢ Establishing a consistent ground/contact frame from in-the-wild videos is non-trivial; unreliable ground-plane/contact estimation causes floating/penetration artifacts (Sec. 3.1, App. A.1.2).<br>‚Ä¢ Real-world deployment of humanoids requires training corpora that are both large-scale and physically reliable to learn stable, humanlike, and generalizable locomotion (Fig. 2 shows scale+quality‚Üíhigher success).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>PHUMA builds a large-scale, physically reliable humanoid dataset via a two-stage pipeline: physics-aware motion curation (low-pass filtering, robust ground-plane estimation by majority voting over SMPL-X foot vertices, and clip-level filtering by jerk, balance to base-of-support, pelvis height, and contact) followed by PhySINK‚Äîshape-adaptive IK augmented with joint-limit, grounding, and anti-skating losses‚Äîto produce physically plausible retargeted motions. Policies trained with MaskedMimic on PHUMA achieve higher imitation and pelvis path-following success across robots and motion types.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Sim-to-Real Humanoid Locomotion with PHUMA: Deploy physics-grounded imitation policies on real humanoids using system identification, contact-consistent domain randomization, and sensory latency compensation.<br>‚Ä¢ Vision-Conditioned Humanoid Control on PHUMA: Learn video/depth-conditioned controllers that replace privileged motion states while preserving physical reliability via perception-aware grounding and contact objectives.<br>‚Ä¢ PHUMA-Contact: Extending Physics-Grounded Retargeting to Object Interactions and Uneven Terrain: Incorporate object/terrain contact modeling and force-aware constraints for whole-body locomotion and early manipulation.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">UniREditBench: A Unified Reasoning-based Image Editing Benchmark</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01295" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01295" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in multi-modal generative models have driven substantial improvements in image editing. However, current generative models still struggle with handling diverse and complex image editing tasks that require implicit reasoning, underscoring the need for a comprehensive benchmark to systematically assess their performance across various reasoning scenarios. Existing benchmarks primarily focus on single-object attribute transformation in realistic scenarios, which, while effective, encounter two key challenges: (1) they largely overlook multi-object interactions as well as game-world scenarios that involve human-defined rules, which are common in real-life applications; (2) they only rely on textual references to evaluate the generated images, potentially leading to systematic misjudgments, especially in complex reasoning scenarios. To this end, this work proposes UniREditBench, a unified benchmark for reasoning-based image editing evaluation. It comprises 2,700 meticulously curated samples, covering both real- and game-world scenarios across 8 primary dimensions and 18 sub-dimensions. To improve evaluation reliability, we introduce multimodal dual-reference evaluation, providing both textual and ground-truth image references for each sample assessment. Furthermore, we design an automated multi-scenario data synthesis pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel on this dataset and develop UniREdit-Bagel, demonstrating substantial improvements in both in-domain and out-of-distribution settings. Through thorough benchmarking of both open-source and closed-source image editing models, we reveal their strengths and weaknesses across various aspects.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing benchmarks mostly target single-object attribute edits in realistic scenes, overlooking multi-object interactions and rule-based game scenarios, thus failing to assess complex, real-world reasoning needs.<br>‚Ä¢ Current evaluations rely on text-only references, which often misjudge complex reasoning-intensive edits; the lack of ground-truth visual references reduces reliability.<br>‚Ä¢ Image editing models struggle with implicit reasoning, physical plausibility, logical constraints, and long-horizon planning, revealing a gap between instruction following and true reasoning.<br>‚Ä¢ There is no large-scale, high-quality dataset with chain-of-thought (CoT) annotations for reasoning-based image editing, limiting model training and progress.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces UniREditBench, a 2,700-sample benchmark spanning 8 primary and 18 sub-dimensions across real- and game-world scenarios, and proposes a dual-reference evaluation that compares outputs to both textual descriptions and ground-truth images. It further presents a scalable multi-scenario synthesis pipeline to build UniREdit-Data-100K with CoT reasoning, and fine-tunes Bagel into UniREdit-Bagel, achieving strong in-domain and out-of-distribution performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond Dual-Reference: Learning a Multimodal Judge for Reasoning-Based Image Editing: Train a dedicated VLM evaluator that unifies text and image evidence with calibration to reduce scoring bias and improve robustness over diverse tasks.<br>‚Ä¢ Agentic Editors: Planning-and-Acting Models for Multi-Step, Rule-Grounded Image Editing: Develop editors that plan, verify, and iteratively refine edits under explicit game/physics rules, using tool-use and self-consistency checks.<br>‚Ä¢ VideoREditBench: Benchmarking Temporal and Causal Reasoning for Instruction-Guided Video Editing: Extend the benchmark to video with long-horizon, temporal consistency, and causal effect evaluation using dual-reference frames and clips.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">World Simulation with Video Foundation Models for Physical AI</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.00062" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.00062" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5times smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Training embodied agents directly in the real world is slow, costly, and risky; scalable world simulators are needed as safe proxies (Introduction, p.3; Abstract, p.1)<br>‚Ä¢ Existing video world models (e.g., diffusion-based Cosmos-Predict1) suffer from lower video fidelity, weaker instruction alignment, fragmented Text/Image/Video pipelines, and limited long-horizon coherence and control (Abstract, p.1; Introduction, p.3)<br>‚Ä¢ Web-scale video data is noisy and redundant; prior curation pipelines lack robust, Physical-AI-specific filtering, caption grounding, and deduplication at petabyte scale (Data pipeline and Fig. 1, p.5)<br>‚Ä¢ There is a domain gap between simulators and reality; improving Sim2Real/Real2Real visual translation and photorealism is necessary for policy learning and evaluation (Abstract, p.1; Table 1, p.4)<br>‚Ä¢ Robust multi-view, camera-controllable, and action-conditioned world generation is limited, hindering autonomous driving and robotics closed-loop simulation (Abstract, p.1; Table 1, p.4)</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Cosmos-Predict2.5 is a unified flow-matching video world model that combines Text2World, Image2World, and Video2World in one architecture, grounded by the Cosmos-Reason1 VLM, trained on 200M curated clips and refined with reinforcement learning-based post-training to boost quality and instruction alignment (Abstract, p.1; Introduction, p.3). Complementing it, Cosmos-Transfer2.5 is a compact control-net style world-translation framework for Sim2Real/Real2Real that delivers higher fidelity, robust long-horizon, multi-view, and camera-controlled video generation (Abstract, p.1; Table 1, p.4).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Agent-Aware RL Post-Training for World Simulators: Close the loop by optimizing the simulator with agent-driven rewards and safety constraints to maximize downstream policy success and reliability.<br>‚Ä¢ Physics-Constrained Flow World Models for Minutes-Scale Coherence: Integrate differentiable physics and causality constraints into flow matching to improve stability, object permanence, and long-horizon consistency.<br>‚Ä¢ Multisensor World Generation Beyond Video: Extend Cosmos-Transfer2.5 to jointly model and translate synchronized video, LiDAR, radar, IMU, and audio for autonomous systems and robotics.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.27363" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.27363" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recently, large language models (LLMs) have demonstrated remarkable problem-solving capabilities by autonomously integrating with external tools for collaborative reasoning. However, due to the inherently complex and diverse nature of multimodal information, enabling multimodal large language models (MLLMs) to flexibly and efficiently utilize external tools during reasoning remains an underexplored challenge. In this work, we introduce ToolScope, an agentic framework designed to unify global planning with local multimodal perception, adopting a specialized Perceive tool to mitigates visual context degradation in long-horizon VQA task. ToolScope comprises three primary components: the Global Navigator, the Agentic Executor, and the Response Synthesizer. The Global Navigator functions as a "telescope", offering high-level strategic guidance. The Agentic Executor operates iteratively to augment MLLM with local perception through the integration of external tools-Search, Code, and Perceive. Finally, the Response Synthesizer consolidates and organizes the reasoning process into a coherent, user-friendly output. We evaluate ToolScope on four VQA benchmarks across diverse domains, including VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong generalization capabilities, achieving an average performance improvement of up to +6.69% across all datasets.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Limited Global Planning: Existing multimodal agents often rely on step-by-step local decisions without a global strategy, causing suboptimal tool selection and disjointed reasoning trajectories.<br>‚Ä¢ Visual Context Degradation: Visual information is not retained or re-attended across multi-step inference, leading to loss of grounding and errors in long-horizon VQA.<br>‚Ä¢ Practicality Gaps in Existing Methods: Training-based agents (SFT/RL) are resource-intensive, while many training-free methods are text-centric or rely on standalone visual tools that increase system complexity and hinder integration/usability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ToolScope is a training-free multimodal agent that unifies global planning (Global Navigator) with iterative, tool-augmented execution (Agentic Executor using Search, Code, and a native Perceive tool) and final response synthesis. Its Perceive tool treats the image as a queryable perceptual memory to re-attend to visual details and mitigate visual context degradation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning to Plan: Reinforcement-Learned Global Navigators for Multimodal Tool Agents: Train the planner to optimize long-horizon strategies and tool selection via RL across diverse tasks and backbones.<br>‚Ä¢ Persistent Perceptual Memory for Open-World Vision-Language Agents: Extend Perceive into a long-term, multi-image memory that supports sequential tasks and cross-session grounding in real-world settings.<br>‚Ä¢ Safety-First ToolScope: Audited Search and Sandboxed Code for Reliable Deployment: Build governance, citation verification, and sandboxing pipelines to reduce hallucinated references, unsafe code, and leakage during tool use.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">OpenSIR: Open-Ended Self-Improving Reasoner</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.00602" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.00602" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in large language model (LLM) reasoning through reinforcement learning rely on annotated datasets for verifiable rewards, which may limit models' ability to surpass human-level performance. While self-play offers a promising alternative, existing approaches depend on external verifiers or cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner (OpenSIR), a self-play framework where an LLM learns to generate and solve novel problems by alternating teacher and student roles without external supervision. To generate novel problems, OpenSIR optimises for both difficulty and diversity, rewarding problems that challenge appropriately while exploring distinct concepts, enabling open-ended mathematical discovery. Starting from a single trivial seed problem, OpenSIR substantially improves instruction models: Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to 34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through co-evolving teacher-student roles that adaptively calibrate difficulty and drive diverse exploration, progressing autonomously from basic to advanced mathematics.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ RL with verifiable rewards (RLVF) for LLM reasoning depends on large human-annotated datasets, creating scalability bottlenecks and potentially capping performance at human level.<br>‚Ä¢ In mathematical self-play there is no ground-truth verifier, making it hard to generate reliable reward signals for both problem generation and solution checking.<br>‚Ä¢ Existing self-play approaches often rely on external verifiers (e.g., compilers, game rules) or simplistic repetition penalties, limiting open-ended learning and domain generality.<br>‚Ä¢ Lack of adaptive difficulty calibration: generated problems are often too easy or too hard/invalid, yielding weak or noisy training signals.<br>‚Ä¢ Insufficient incentives for conceptual diversity cause models to repeatedly practice familiar problem types rather than explore novel mathematical concepts.<br>‚Ä¢ Absence of a co-evolving teacher‚Äìstudent setup within a single policy to autonomously bootstrap from a trivial seed and create an ever-improving curriculum.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>OpenSIR trains a single policy that alternates teacher and student roles: the teacher generates new math problems and the student solves them with multiple attempts, using majority voting to form a reference answer and a solve-rate signal. A unified novelty reward blends solvability (target solve-rate), solution length (for multi-step complexity), embedding-based diversity, and format checks, and the policy is optimized on-policy via a GRPO-like objective with role-specific rewards while maintaining a growing problem pool seeded from a trivial question.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ AutoCal-SIR: Adaptive Difficulty Calibration for Open-Ended Self-Play ‚Äî Learn solve-rate targets and thresholds online to balance challenge and validity, reducing noisy signals from overly hard or malformed problems.<br>‚Ä¢ MetaNovel-SIR: Learning Diversity Metrics for Open-Ended Exploration ‚Äî Meta-learn task-aware embeddings or concept taxonomies to better measure and reward substantive conceptual novelty beyond surface differences.<br>‚Ä¢ OpenSIR-MM: Extending Open-Ended Self-Improvement to Multimodal and Code Reasoning ‚Äî Generalize the framework to text‚Äìvision‚Äìmath and program synthesis, exploring weak/implicit verification signals while preserving open-ended discovery.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24794" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24794" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large reasoning models (LRMs) show strong capabilities in complex reasoning, yet their marginal gains on evidence-dependent factual questions are limited. We find this limitation is partially attributable to a reasoning-answer hit gap, where the model identifies the correct facts during reasoning but fails to incorporate them into the final response, thereby reducing factual fidelity. To address this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment framework that enhances factuality without relying on external verifiers. MR-ALIGN quantifies state transition probabilities along the model's thinking process and constructs a transition-aware implicit reward that reinforces beneficial reasoning patterns while suppressing defective ones at the atomic thinking segments. This re-weighting reshapes token-level signals into probability-aware segment scores, encouraging coherent reasoning trajectories that are more conducive to factual correctness. Empirical evaluations across four factual QA datasets and one long-form factuality benchmark show that MR-ALIGN consistently improves accuracy and truthfulness while reducing misleading reasoning. These results highlight that aligning the reasoning process itself, rather than merely the outputs, is pivotal for advancing factuality in LRMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Evidence-dependent factual QA shows limited gains from LRMs because models often miss surfacing the right fact in the final answer even after finding it during reasoning (the ‚Äúreasoning‚Äìanswer hit gap‚Äù).<br>‚Ä¢ Test-time scaling and RL-tuned long chains can negate earlier correct candidates, increasing inconsistency and misleading reasoning.<br>‚Ä¢ Existing alignment methods largely optimize outputs (or rely on external verifiers/retrievers) rather than aligning the internal reasoning process itself.<br>‚Ä¢ Cold-start pretraining induces rigid, over-structured think traces that prioritize early evaluation over evidence acquisition and synthesis.<br>‚Ä¢ There is a need to improve factual fidelity without external verification by reshaping how models transition between reasoning states.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MR-ALIGN aligns factuality by modeling meta-reasoning as transitions among 15 annotated thinking states and integrating transition-aware implicit rewards into KTO to reinforce beneficial reasoning patterns. It estimates positive/negative/global transition matrices via EM from segment-labeled CoT traces and reweights token contributions by local-to-global transition ratios, improving accuracy and reducing misleading without external verifiers.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Scalable Meta-Reasoning Alignment for Frontier Models: Extend MR-ALIGN to larger LRMs and broader domains with compute-efficient training and distributed transition estimation.<br>‚Ä¢ Annotator-Free Meta-Reasoning Discovery for Factual Alignment: Replace LLM-driven labels with weakly/unsupervised latent-state discovery (e.g., HMM/variational methods) and minimal human calibration.<br>‚Ä¢ Retrieval-Aware MR-ALIGN: Joint Optimization of Evidence and Transitions: Learn the retriever and transition-aware policy jointly so evidence selection and reasoning dynamics co-adapt for factuality.<br>‚Ä¢ Transition-Constrained Decoding for Test-Time Control: Use learned transition priors to guide decoding (e.g., penalties for regressions/self-loops) for on-the-fly reduction of misleading reasoning.<br>‚Ä¢ Causal Interventions on Reasoning Paths to Reduce Hallucinations: Identify causally harmful transitions and intervene with targeted rewards/prompts or structural constraints.<br>‚Ä¢ Low-Cost Long-Form Factuality Rewards at Segment Level: Build scalable automatic evaluators that deliver segment-level factual rewards to train long-form factual alignment.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.27571" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.27571" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The prevailing video retrieval paradigm is structurally misaligned, as narrow benchmarks incentivize correspondingly limited data and single-task training. Therefore, universal capability is suppressed due to the absence of a diagnostic evaluation that defines and demands multi-dimensional generalization. To break this cycle, we introduce a framework built on the co-design of evaluation, data, and modeling. First, we establish the Universal Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to measure performance but also to diagnose critical capability gaps across tasks and domains. Second, guided by UVRB's diagnostics, we introduce a scalable synthesis workflow that generates 1.55 million high-quality pairs to populate the semantic space required for universality. Finally, we devise the Modality Pyramid, a curriculum that trains our General Video Embedder (GVE) by explicitly leveraging the latent interconnections within our diverse data. Extensive experiments show GVE achieves state-of-the-art zero-shot generalization on UVRB. In particular, our analysis reveals that popular benchmarks are poor predictors of general ability and that partially relevant retrieval is a dominant but overlooked scenario. Overall, our co-designed framework provides a practical path to escape the limited scope and advance toward truly universal video retrieval.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Fragmented evaluation and training: narrow, single-task benchmarks (e.g., coarse-grained MSRVTT) encourage overfitting and fail to diagnose multi-dimensional generalization.<br>‚Ä¢ Poor real-world coverage: existing retrievers struggle with fine-grained spatial/temporal reasoning, long-context retrieval, composed (text+image/video) queries, and visual-only search; partially relevant retrieval is common but underexplored.<br>‚Ä¢ Data limitations: available video-text corpora are noisy, small, or semantically narrow; prior synthetic data lacks quality control and balanced coverage across tasks/domains.<br>‚Ä¢ Learning limitations: na√Øve multi-task training ignores hierarchical dependencies among abilities, allowing easy tasks to dominate and hindering transfer; CLIP-based models are language/temporal-weak while MLLM-based embedders lack systematic evaluation across complex scenarios.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Co-design benchmark, data, and model: UVRB (16 datasets) diagnoses abilities across tasks and domains; V-SynFlow synthesizes UVRD (1.55M high-quality pairs) via multi-granular filtering, caption enrichment, and multimodal task extension. Train a General Video Embedder (GVE) with a Modality Pyramid curriculum that dynamically schedules from simple to composite tasks and optimizes a unified symmetric InfoNCE with hard-negative mining.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Audio-Enhanced Universal Video Retrieval: Integrating Speech and Sound into UVRB and GVE: Extend the benchmark/data synthesis to audio and ASR, and jointly train embeddings for robust long-context and partially relevant retrieval.<br>‚Ä¢ Adaptive Token‚ÄìFrame Selection for Efficient Universal Retrieval: Learn policies that dynamically select frames and visual tokens per query to maximize accuracy under compute budgets, improving long-context and compositional tasks.<br>‚Ä¢ UVRB++: Universal Retrieval in Specialized Domains (Medical/Industrial/Surveillance): Expand evaluation and synthesis to domain-specific videos, testing cross-domain generalization and domain-adaptive curricula.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01833" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01833" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The frontier of visual reasoning is shifting toward models like OpenAI o3, which can intelligently create and operate tools to transform images for problem-solving, also known as thinking-with-images in chain-of-thought. Yet existing benchmarks fail to fully capture this advanced capability. Even Visual Search, the most common benchmark for current thinking-with-images methods, tests only basic operations such as localization and cropping, offering little insight into more complex, dynamic, and tool-dependent reasoning. We introduce TIR-Bench, a comprehensive benchmark for evaluating agentic thinking-with-images across 13 diverse tasks, each requiring novel tool use for image processing and manipulation in chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from leading open-sourced and proprietary models to those with explicit tool-use augmentation. Results show that TIR-Bench is universally challenging, and strong performance requires genuine thinking-with-images capabilities. Finally, we present a pilot study comparing direct versus agentic fine-tuning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing benchmarks mostly test static visual understanding or simple visual search/cropping and fail to assess agentic ‚Äúthinking-with-images,‚Äù where models must manipulate images with tools during reasoning.<br>‚Ä¢ Real-world tasks require dynamic, tool-dependent visual reasoning (e.g., rotation, enhancement, segmentation, drawing, maze solving, jigsaw assembly), which prior evaluations do not cover.<br>‚Ä¢ There is a need for deterministic, contamination-resilient evaluation with objectively verifiable answers and newly created/annotated samples to ensure fairness and reproducibility.<br>‚Ä¢ Current MLLMs‚Äô function-calling and code-writing abilities for image operations (parameter selection, iterative trials) are under-measured, obscuring critical capabilities for tool-integrated reasoning.<br>‚Ä¢ Training strategies for image-operation tasks are unclear; direct SFT often underperforms, and the field needs evidence and protocols showing when agentic (tool-use) SFT is necessary and beneficial.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TIR-Bench: a 13-task, 1,215-sample benchmark purpose-built to require tool-based image manipulation within chain-of-thought (zoom, rotate, enhance, draw, segment, measure, pathfind), used to evaluate 22 MLLMs (open, proprietary, and tool-using). The paper also introduces a function-calling protocol on a rotation task and a pilot fine-tuning study showing agentic SFT on tool-use trajectories outperforms direct SFT for rotated OCR.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Plug-and-Play Vision Tool APIs for Agentic MLLMs: Standardize callable segmentation/detection/OCR tools and tool-selection policies so models can seamlessly invoke external vision models during reasoning.<br>‚Ä¢ Agentic SFT at Scale: Trajectory-Level Training for Thinking-with-Images: Pretrain/finetune on large multi-task datasets of full tool-use trajectories (code, intermediate images, decisions) to induce robust, transferable image-manipulation skills.<br>‚Ä¢ Structured Function Calling and Parameter Search for Visual Tool Use: Develop planning and verification methods (e.g., adaptive sweeps, uncertainty-guided retries, budgeted search) to improve accuracy and efficiency of iterative tool parameter selection.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LongCat-Flash-Omni Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.00279" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.00279" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction. By adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability. Building upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction. For training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation. We provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Cross‚Äëmodal heterogeneity: unify text, audio, image, and video into one model without degrading any single‚Äëmodality performance (Section 1; Fig. 2 on p.6 shows the unified architecture)<br>‚Ä¢ Unified offline + streaming: integrate strong offline multimodal understanding with real‚Äëtime audio‚Äëvisual interaction, including temporal alignment, barge‚Äëin handling, and multi‚Äëturn memory (Section 1; Section 2.4)<br>‚Ä¢ Low‚Äëlatency real‚Äëtime I/O at scale: achieve millisecond‚Äëlevel response with streaming audio/video input and speech output despite a 560B‚Äëparameter MoE backbone (27B activated) (Abstract; Section 2.4.2; Section 6.2)<br>‚Ä¢ Training efficiency under heterogeneity: overcome data/model heterogeneity that breaks standard FSDP/na√Øve PP, sustaining >90% of text‚Äëonly throughput (Section 5; Table 2 p.18; Figure 7 p.18)<br>‚Ä¢ Robust video processing: handle extreme durations/resolutions with token‚Äëbudget control; fixed‚Äëresolution ViTs and na√Øve frame sampling lose detail or waste compute (Section 2.1, 2.4.1)<br>‚Ä¢ Fast speech reconstruction: avoid slow diffusion/flow + vocoder pipelines to enable streaming TTS (Section 2.2; Fig. 3 p.7)<br>‚Ä¢ Open‚Äësource gap: few open models match closed systems on omni‚Äëbenchmarks and real‚Äëtime AV interaction; need a unified, open, SOTA omni‚Äëmodal model (Abstract; Figure 1 p.1; Section 7)</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Build an end‚Äëto‚Äëend omni‚Äëmodal LLM on a Shortcut‚Äëconnected MoE (ScMoE) backbone with lightweight native‚Äëresolution vision/audio encoders and a multi‚Äëcodebook streaming audio decoder, trained via a curriculum‚Äëstyle early‚Äëfusion pipeline that progressively adds speech, images, and video. Real‚Äëtime quality/latency are achieved by synchronized chunk‚Äëwise audio‚Äëvideo feature interleaving, an asynchronous serving pipeline, and a modality‚Äëdecoupled parallelism (MDP) training scheme that decouples encoders and LLM for efficient large‚Äëscale training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Thinking for Real‚ÄëTime Omni‚ÄëModal LLMs: Dynamic thought‚Äëbudget allocation and speculative decoding for streaming AV dialogue that balances latency with reasoning depth.<br>‚Ä¢ Elastic Modality‚ÄëDecoupled Parallelism 2.0: An elastic, heterogeneity‚Äëaware scheduler that extends MDP to mixed accelerators and volatile workloads across training and inference.<br>‚Ä¢ Token‚ÄëFrugal Native‚ÄëResolution Video Understanding at 128K: Learned hierarchical token compression for long videos that preserves fine spatial‚Äëtemporal details while minimizing compute in ultra‚Äëlong contexts.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">NaviTrace: Evaluating Embodied Navigation of Vision-Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26909" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26909" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-language models demonstrate unprecedented performance and generalization across a wide range of tasks and scenarios. Integrating these foundation models into robotic navigation systems opens pathways toward building general-purpose robots. Yet, evaluating these models' navigation capabilities remains constrained by costly real-world trials, overly simplified simulations, and limited benchmarks. We introduce NaviTrace, a high-quality Visual Question Answering benchmark where a model receives an instruction and embodiment type (human, legged robot, wheeled robot, bicycle) and must output a 2D navigation trace in image space. Across 1000 scenarios and more than 3000 expert traces, we systematically evaluate eight state-of-the-art VLMs using a newly introduced semantic-aware trace score. This metric combines Dynamic Time Warping distance, goal endpoint error, and embodiment-conditioned penalties derived from per-pixel semantics and correlates with human preferences. Our evaluation reveals consistent gap to human performance caused by poor spatial grounding and goal localization. NaviTrace establishes a scalable and reproducible benchmark for real-world robotic navigation. The benchmark and leaderboard can be found at https://leggedrobotics.github.io/navitrace_webpage/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Real-world evaluation of VLM navigation is costly, slow, non-scalable, and irreproducible, limiting broad assessment across environments.<br>‚Ä¢ Simulation-based evaluation lacks realism (simplified physics, mostly static scenes, limited semantics), making it hard to encode terrain properties and social norms and leading to sim-to-real gaps.<br>‚Ä¢ Existing VLN/VQA benchmarks are mismatched to VLMs: they use text-only answers or discrete action spaces, focus mainly on indoor human navigation, and rarely test multiple embodiments or trace-level plans.<br>‚Ä¢ There is no standardized, human-aligned metric for comparing navigation traces that jointly measures path similarity, goal reaching, and semantic/embodiment safety.<br>‚Ä¢ Practitioners lack diagnostics for core failure modes (e.g., spatial grounding and goal localization) in diverse real-world navigation scenarios.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>NaviTrace is a real-image VQA benchmark where, given an instruction and embodiment (human, legged robot, wheeled robot, bicycle), a model outputs a 2D navigation trace in image space across 1,000 diverse scenarios. A semantic-aware score combines Dynamic Time Warping, endpoint error, and embodiment-conditioned per-pixel penalties from semantic segmentation, scaled to a 0‚Äì100 leaderboard.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ VideoNaviTrace: From Single Images to Temporal, Dynamic Embodied Navigation: Extend the benchmark to videos to evaluate temporal reasoning, dynamic obstacle avoidance, and multi-step planning.<br>‚Ä¢ GlobalNaviTrace: Geo-Diverse Benchmarking of Navigation VLMs: Expand data collection across regions to study cross-geography generalization, infrastructure variance, and cultural/social norm differences.<br>‚Ä¢ Trace-to-Control: Closed-Loop Robot Navigation from Image-Space Traces: Convert predicted traces into receding-horizon control policies and validate end-to-end on real robots for each embodiment.<br>‚Ä¢ 3D-Grounded Goal Localization for VLM Navigation: Fuse monocular depth/SLAM with language grounding to improve target localization and spatial grounding for path shaping.<br>‚Ä¢ Learning Human-Preference Trace Metrics: Replace hand-tuned penalties with learned human-preference models that support region-based goals and uncertainty-aware evaluation.<br>‚Ä¢ Embodiment-Generalist Trace Planning Beyond Ground Vehicles: Incorporate additional embodiments (e.g., drones, wheelchairs, strollers) and train a single model to adapt traces to diverse constraints.<br>‚Ä¢ Socially Compliant, Uncertainty-Aware Multi-Trace Prediction: Predict calibrated sets of plausible traces and evaluate social norm compliance and safety in crowded or ambiguous scenes.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">left|,circlearrowright,text{BUS},right|: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01340" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01340" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters to represent words or phrases creatively) requires a variety of skills such as image recognition, cognitive skills, commonsense reasoning, multi-step reasoning, image-based wordplay, etc., making this a challenging task for even current Vision-Language Models. In this paper, we present left|,circlearrowright,text{BUS},right|, a large and diverse benchmark of 1,333 English Rebus Puzzles containing different artistic styles and levels of difficulty, spread across 18 categories such as food, idioms, sports, finance, entertainment, etc. We also propose RebusDescProgICE, a model-agnostic framework which uses a combination of an unstructured description and code-based, structured reasoning, along with better, reasoning-based in-context example selection, improving the performance of Vision-Language Models on left|,circlearrowright,text{BUS},right| by 2.1-4.1% and 20-30% using closed-source and open-source models respectively compared to Chain-of-Thought Reasoning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of a large, diverse, and well-annotated benchmark to evaluate VLMs on visual wordplay (rebus) that requires image recognition, commonsense, and multi-step reasoning; prior rebus datasets are smaller, less diverse, and lack rich metadata [Section 1; Figure 3 on page 3 shows diverse metadata combinations]<br>‚Ä¢ Existing evaluations often use clean, low-distraction images; models are not stress-tested for robustness under realistic visual clutter; the paper addresses this via ControlNet-based distractor backgrounds [Section 3.1.3; examples illustrated by diversity in Figure 5 on page 4]<br>‚Ä¢ VLMs struggle with layered, procedural reasoning needed to combine images, letters, and symbols into phrases; common prompting (e.g., zero-shot, CoT) underperforms on rebus puzzles [Sections 1‚Äì2, 4]<br>‚Ä¢ No model-agnostic, low-training solution that improves both closed- and open-source VLMs; prior approaches typically rely on a single reasoning style (e.g., CoT) and random example selection [Sections 2, 3.3, 4.2]<br>‚Ä¢ Need for principled in-context example selection tailored to reasoning patterns, rather than random sampling, to better guide VLMs on complex visual-linguistic composition [Section 3.3]</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce |M v|, a 1,333-sample English rebus benchmark with rich metadata across 18 categories and ControlNet-augmented distractor backgrounds, and propose RebusDescProgICE: a model-agnostic in-context framework that fuses unstructured image descriptions with structured, code-based visual programs plus learned example selection. This combination yields 2.1‚Äì4.1% gains for closed-source and 20‚Äì30% for open-source models over Chain-of-Thought on word-level F1 [Tables 1‚Äì2 on page 6].</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Multilingual RebusBench: Cross-Lingual Evaluation of Visual Wordplay Reasoning: Extend |M v| to multiple languages and cultures to study cross-lingual transfer, idiomatic variability, and cultural priors in visual wordplay understanding.<br>‚Ä¢ Program-Executable Agents for Visual Wordplay: Integrating VisProg with Tool Use and Self-Correction: Combine executable visual programs, OCR, retrieval, and verifier loops to enable iterative, tool-augmented solving with execution feedback.<br>‚Ä¢ Robust Rebus Reasoning under Adversarial Visual Distractors: Beyond ControlNet Backgrounds: Systematically generate and benchmark adversarial distractors and clutter, define robustness metrics, and develop defense strategies (e.g., attention masking, distraction-invariant embeddings).</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26865" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26865" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reading measurement instruments is effortless for humans and requires relatively little domain expertise, yet it remains surprisingly challenging for current vision-language models (VLMs) as we find in preliminary evaluation. In this work, we introduce MeasureBench, a benchmark on visual measurement reading covering both real-world and synthesized images of various types of measurements, along with an extensible pipeline for data synthesis. Our pipeline procedurally generates a specified type of gauge with controllable visual appearance, enabling scalable variation in key details such as pointers, scales, fonts, lighting, and clutter. Evaluation on popular proprietary and open-weight VLMs shows that even the strongest frontier VLMs struggle measurement reading in general. A consistent failure mode is indicator localization: models can read digits or labels but misidentify the key positions of pointers or alignments, leading to big numeric errors despite plausible textual reasoning. We have also conducted preliminary experiments with reinforcement learning over synthetic data, and find encouraging results on in-domain synthetic subset but less promising for real-world images. Our analysis highlights a fundamental limitation of current VLMs in fine-grained spatial grounding. We hope this resource can help future advances on visually grounded numeracy and precise spatial perception of VLMs, bridging the gap between recognizing numbers and measuring the world.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ VLMs struggle with fine-grained instrument reading: they often mislocalize pointers/menisci and misalign ticks-to-values, causing large numeric errors despite plausible reasoning (Figure 7, p.8).<br>‚Ä¢ Practical importance: accurate reading of gauges, thermometers, meters, and clocks is critical for safety, monitoring, and embodied/robotic systems (pp.1‚Äì2).<br>‚Ä¢ Existing benchmarks emphasize OCR, charts, or artificial low-level cues and rarely require mapping physical scales to numeric values; prior works cover narrow device types (clocks/rulers/gauges) and lack diversity (pp.1‚Äì3, Related Work).<br>‚Ä¢ Empirical gap: even the best models reach only ~30% overall on real images and ~26% on synthetic, while unit recognition exceeds 90%‚Äîshowing value reading, not OCR, is the bottleneck (Table 2, p.5; Table 3, p.6).<br>‚Ä¢ Failure persists across designs: digital is relatively easy; dial/linear are hard; composite is hardest, requiring multi-component fusion and arithmetic (Table 3, p.6).<br>‚Ä¢ Scaling LLMs or adding test-time ‚Äúthinking‚Äù brings little benefit without better visual grounding; token-heavy reasoning barely helps (Figure 6, pp.7‚Äì8).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces MeasureBench: a 2,442-sample benchmark for instrument reading with a hybrid 2D programmatic and 3D physically based synthesis pipeline, covering four readout designs, standardized labels, and an interval-based evaluation that parses numeric values and units. It further explores reinforcement finetuning (GRPO) on synthetic data, showing large in-domain gains and modest real-world transfer, and analyzes failure modes centered on indicator localization and fine-grained spatial grounding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Perception-First Instrument Readers: Keypoint- and Tick-Grounded Networks for Pointer-to-Number Mapping: Build models that first localize pointers/menisci and tick marks, then convert geometry to values via differentiable scale mappings, reducing reasoning-by-guessing.<br>‚Ä¢ High-Resolution Multiscale Visual Tokenization for VLMs on Fine-Grained Gauges: Design visual encoders with adaptive high-frequency tokenization and super-resolution features to preserve small ticks and needle tips.<br>‚Ä¢ Verifiable RL on Synthetic Instruments: Curriculum and Domain Randomization for Real-World Transfer: Use MeasureBench‚Äôs precise labels for interval-based rewards, progressively harder curricula, and photorealistic randomization to close the sim-to-real gap.<br>‚Ä¢ Program-Induced Parsing of Composite Meters: Decompose-Measure-Compose with Executable Plans: Parse complex devices into sub-readings and operations (e.g., right-to-left dial protocols), verifying each step against geometric constraints.<br>‚Ä¢ Uncertainty-Calibrated Measurement: Predicting Intervals and Confidence for Robust Instrument Reading: Train models to output calibrated value intervals and confidence, aligning training and evaluation with tolerance bands.<br>‚Ä¢ Geometry-Aware Test-Time Adaptation: Self-Consistency over Visual Landmarks for Robust Indicator Localization: At inference, refine readings by enforcing consistency across multiple crops/rotations and landmark detections.<br>‚Ä¢ Physics-Guided Rendering for Sim-to-Real Bridging: Modeling Glare, Reflections, and Occlusions in 3D for Gauge Reading: Extend the 3D pipeline with physically faithful artifacts and contextual objects to improve robustness to real-world artifacts.<br>‚Ä¢ Bench+Teach: Active Data Generation from Failure Modes on MeasureBench: Mine hardest cases (e.g., near-tick ambiguities, multiple needles) to drive targeted synthetic generation and fine-tuning loops.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Trove: A Flexible Toolkit for Dense Retrieval</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01857" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01857" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Trove, an easy-to-use open-source retrieval toolkit that simplifies research experiments without sacrificing flexibility or speed. For the first time, we introduce efficient data management features that load and process (filter, select, transform, and combine) retrieval datasets on the fly, with just a few lines of code. This gives users the flexibility to easily experiment with different dataset configurations without the need to compute and store multiple copies of large datasets. Trove is highly customizable: in addition to many built-in options, it allows users to freely modify existing components or replace them entirely with user-defined objects. It also provides a low-code and unified pipeline for evaluation and hard negative mining, which supports multi-node execution without any code changes. Trove's data management features reduce memory consumption by a factor of 2.6. Moreover, Trove's easy-to-use inference pipeline incurs no overhead, and inference times decrease linearly with the number of available nodes. Most importantly, we demonstrate how Trove simplifies retrieval experiments and allows for arbitrary customizations, thus facilitating exploratory research.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Retrieval data management is cumbersome and memory‚Äëintensive: existing toolkits rely on large, preprocessed files per experiment variant, duplicating data and hindering exploration and reproducibility (Figure 1 on page 2).<br>‚Ä¢ General-purpose data tools (e.g., HF Datasets) operate instance-wise and don‚Äôt suit retrieval where each query depends on the entire corpus, making on‚Äëthe‚Äëfly filtering/selection/combination impractical (pages 1‚Äì2).<br>‚Ä¢ Distributed evaluation is hard to scale: many frameworks are single-node only or require multi-step orchestration; shared corpus encoding prevents trivial parallelization (page 3), limiting rapid experimentation.<br>‚Ä¢ Modeling flexibility is constrained: encoders are wrapped in fixed classes, restricting direct access to backbones and preventing custom losses, pooling, or graduated relevance labels (pages 2‚Äì3).<br>‚Ä¢ Evaluation/mining pipelines suffer from CPU-bound top‚Äëk tracking (Python heapq), stalling GPUs and exploding runtimes at scale (Table 3 on page 6).<br>‚Ä¢ Lack of a unified, low‚Äëcode pipeline for evaluation and hard‚Äënegative mining that supports multi‚Äënode/GPU without code changes slows exploratory research (pages 1‚Äì2, 5).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Trove is an open-source toolkit that performs efficient, on-the-fly retrieval data processing via MaterializedQRel (Polars grouping + memory‚Äëmapped Apache Arrow) and exposes a modular retriever/encoder/loss stack fully compatible with HF Transformers. It provides a unified multi-node/GPU evaluator with fair sharding and a GPU‚Äëaccelerated top‚Äëk tracker (FastResultHeapq), plus caching and collators, enabling scalable evaluation and hard‚Äënegative mining with minimal code.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Trove‚ÄëX: Extending On‚Äëthe‚ÄëFly, Memory‚ÄëMapped Data Management to Multimodal Dense Retrieval: Generalize MaterializedQRel and caching to image‚Äìtext/audio‚Äìtext corpora with unified cross‚Äëmodal encoders and evaluators.<br>‚Ä¢ Elastic, Heterogeneity‚ÄëAware Distributed Retrieval Evaluation: Build on fair sharding to support elastic scaling, preemption tolerance, and energy‚Äëaware scheduling across mixed GPU clusters.<br>‚Ä¢ Declarative and Versioned IR Pipelines for Reproducible Dense Retrieval: Design a DSL and provenance system over Trove configs to track data transforms, label maps, and caches end‚Äëto‚Äëend for auditable, reproducible experiments.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01618" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01618" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved 2D visual understanding, prompting interest in their application to complex 3D reasoning tasks. However, it remains unclear whether these models can effectively capture the detailed spatial information required for robust real-world performance, especially cross-view consistency, a key requirement for accurate 3D reasoning. Considering this issue, we introduce Viewpoint Learning, a task designed to evaluate and improve the spatial reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset, consisting of 100K object-centric image pairs with diverse viewpoints and corresponding question-answer pairs. Our approach employs a two-stage fine-tuning strategy: first, foundational knowledge is injected to the baseline MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in significant improvements across multiple tasks; second, generalization is enhanced through Reinforcement Learning using the Group Relative Policy Optimization (GRPO) algorithm on a broader set of questions. Additionally, we introduce a hybrid cold-start initialization method designed to simultaneously learn viewpoint representations and maintain coherent reasoning thinking. Experimental results show that our approach significantly activates the spatial reasoning ability of MLLM, improving performance on both in-domain and out-of-domain reasoning tasks. Our findings highlight the value of developing foundational spatial skills in MLLMs, supporting future progress in robotics, autonomous systems, and 3D scene understanding.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ MLLMs excel at 2D visual understanding but struggle with fine-grained 3D spatial reasoning, especially cross-view consistency across viewpoints.<br>‚Ä¢ Current models often rely on superficial 2D cues (2D continuity) rather than true 3D consistency, leading to errors in multi-view reasoning and pose-related tasks.<br>‚Ä¢ Prior methods typically inject extra spatial cues/prompts or 3D features, but overlook foundational training on viewpoint understanding and frames of reference.<br>‚Ä¢ Pure RL fine-tuning is insufficient to overcome entrenched 2D biases (e.g., high KL divergence), indicating the need for targeted data and training curricula.<br>‚Ä¢ Robust 3D reasoning is critical for real-world applications (robotics, autonomous systems, 3D scene understanding), yet benchmarks reveal near-random performance on multi-view tasks without specialized training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Actial introduces Viewpoint Learning via the Viewpoint-100K dataset (100K real object-centric image pairs with accurate camera geometry) and a two-stage training pipeline: supervised fine-tuning to inject viewpoint knowledge using a hybrid cold-start CoT initialization, followed by GRPO-based reinforcement learning on the SAT dataset to enhance generalization beyond the viewpoint task.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond Viewpoint Learning: End-to-End 6-DoF Pose Reasoning in MLLMs: Move from multiple-choice viewpoint classification to continuous regression of full camera pose with differentiable geometric constraints and uncertainty estimation.<br>‚Ä¢ Geo-Consistent RL for Multimodal Models: Reward Shaping that Preserves Foundational Spatial Skills: Design multi-objective RL that balances answer correctness, KL control, and chain-of-thought fidelity to avoid forgetting and stabilize cross-task spatial reasoning.<br>‚Ä¢ From Objects to Scenes: Scaling Viewpoint-100K to Scene-Centric, Multi-Object, and Video Settings: Extend data to complex scenes with vertical motion, occlusions, and long-horizon multi-view/video sequences, adding loop-closure and cross-view consistency supervision.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Towards Robust Mathematical Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01846" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01846" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Finding the right north-star metrics is highly critical for advancing the mathematical reasoning capabilities of foundation models, especially given that existing evaluations are either too easy or only focus on getting correct short answers. To address these issues, we present IMO-Bench, a suite of advanced reasoning benchmarks, vetted by a panel of top specialists and that specifically targets the level of the International Mathematical Olympiad (IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench first tests models on 400 diverse Olympiad problems with verifiable short answers. IMO-Proof Bench is the next-level evaluation for proof-writing capabilities, which includes both basic and advanced IMO level problems as well as detailed grading guidelines to facilitate automatic grading. These benchmarks played a crucial role in our historic achievement of the gold-level performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4% respectively. We also showed that autograders built with Gemini reasoning correlate well with human evaluations and construct IMO-GradingBench, with 1000 human gradings on proofs, to enable further progress in automatic evaluation of long-form answers. We hope that IMO-Bench will help the community towards advancing robust mathematical reasoning and release it at https://imobench.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing math reasoning benchmarks are saturated (e.g., GSM8K, MATH, AIME) and over-index on short answers, failing to measure rigorous multi-step reasoning at IMO level.<br>‚Ä¢ Final-answer matching is unreliable‚Äîmodels can guess or format answers differently; many Olympiad problems require proofs, not short answers.<br>‚Ä¢ Public evaluations are vulnerable to data contamination and memorization; small perturbations of known problems cause large performance drops.<br>‚Ä¢ Current automatic evaluators are brittle (often numeric/SymPy-only), sensitive to formatting, and lack human-calibrated judgment for long-form proofs.<br>‚Ä¢ The field lacks a standardized, vetted suite for both proof writing and proof grading, plus scalable autograding that correlates with expert IMO grading.<br>‚Ä¢ Human expert grading is accurate but costly and slow, limiting rapid iteration and fair comparison across models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce IMO-Bench‚Äîa vetted Olympiad-level suite with three components: (1) IMO-AnswerBench (400 robustified short-answer problems), (2) IMO-ProofBench (60 proof problems with detailed rubrics), and (3) IMO-GradingBench (1,000 human-graded solutions). Build LLM-based autograders (AnswerAutoGrader and ProofAutoGrader) using Gemini 2.5 Pro to flexibly parse/verify answers and grade proofs against reference solutions and guidelines, showing near-human agreement and strong correlation with expert scores.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond Answer Matching: Human-Calibrated LLM Proof Graders for Olympiad Mathematics: Develop graders that detect high-level logical flaws, handle unconventional but correct solutions with principled partial credit, and quantify uncertainty calibrated to IMO rubric variance.<br>‚Ä¢ Contamination-Resistant Math Benchmarks via Regenerating, Parametric IMO-Style Tasks: Create automatically refreshed, parameterized, and functionally equivalent problem pools with contamination detection to preserve benchmark integrity over time.<br>‚Ä¢ Hybrid Formal‚ÄìInformal Verification Pipelines for LLM Proofs: Combine natural-language proof grading with formal proof checkers/DSLs (e.g., Lean, geometry DSLs) to verify critical steps, reduce false positives, and improve reliability on advanced reasoning.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01718" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01718" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-language-action (VLA) models aim to understand natural language instructions and visual observations and to execute corresponding actions as an embodied agent. Recent work integrates future images into the understanding-acting loop, yielding unified VLAs that jointly understand, generate, and act -- reading text and images and producing future images and actions. However, these models either rely on external experts for modality unification or treat image generation and action prediction as separate processes, limiting the benefits of direct synergy between these tasks. Our core philosophy is to optimize generation and action jointly through a synchronous denoising process, where the iterative refinement enables actions to evolve from initialization, under constant and sufficient visual guidance. We ground this philosophy in our proposed Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process that integrates multiple modalities into a single denoising trajectory to serve as the key mechanism enabling understanding, generation, and acting to be intrinsically synergistic. Our model and theory are built on a unified tokenized space of all modalities and a hybrid attention mechanism. We further propose a two-stage training pipeline and several inference-time techniques that optimize performance and efficiency. Our approach achieves state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and SimplerEnv with 4times faster inference than autoregressive methods, and we demonstrate its effectiveness through in-depth analysis and real-world evaluations. Our project page is available at https://irpn-eai.github.io/UD-VLA.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing unified VLAs either rely on extrinsic experts (separate encoders/decoders), causing misalignment, higher complexity, and weak coupling between visual generation and action prediction.<br>‚Ä¢ Many methods keep image generation and action prediction as separate processes; future images are often only auxiliary during training and not used to guide actions at inference.<br>‚Ä¢ Autoregressive decoding gives each action token only a single context pass, limiting guidance from predicted images and compounding errors; decoding latency is high when vision and action are decoded separately (AR + diffusion).<br>‚Ä¢ There is a need for an intrinsically synergistic mechanism where actions evolve under continuous visual guidance via iterative refinement, and for unified decoding to reduce latency.<br>‚Ä¢ Lack of a unified discrete token space and an attention scheme that enables rich intra-modal interactions while enforcing causal cross-modal conditioning without leakage.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>UD-VLA introduces a Joint Discrete Denoising Diffusion Process (JD3P) that jointly denoises future image and action tokens in one synchronous trajectory over a unified discrete token space (VQ image tokens + FAST action tokens) using hybrid attention (bidirectional within modalities, causal across modalities) so actions are iteratively refined under visual foresight. A two-stage training pipeline (video post-training for future-frame prediction, then joint JD3P on robot data) plus efficient inference (KV cache, pre-filled special tokens, confidence-guided decoding, modality-specific vocab remapping) delivers state-of-the-art performance with ~4√ó faster decoding than autoregressive baselines.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Scaling UD-VLA with Generative Video Pretraining for High-Fidelity Foresight: Pretrain the visual branch on large-scale video diffusion corpora and integrate into JD3P to improve temporal realism and fine-grained cues that enhance control.<br>‚Ä¢ Hierarchical JD3P: Multi-Scale Subgoal and Action Denoising for Long-Horizon Manipulation: Introduce coarse-to-fine joint denoising over subgoal images and action chunks to strengthen planning and reduce error accumulation on extended tasks.<br>‚Ä¢ Uncertainty-Aware and Safe UD-VLA via Calibrated Joint Denoising: Incorporate epistemic/aleatoric uncertainty modeling and risk-sensitive, constraint-aware decoding within JD3P to improve robustness and safety in real-world deployment.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">MotionStream: Real-Time Video Generation with Interactive Motion Controls</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01266" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01266" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons: (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing motion‚Äëcontrolled video diffusion is too slow and offline (minutes per clip, ~50 denoising steps), preventing real‚Äëtime interaction.<br>‚Ä¢ Non‚Äëcausal, bidirectional attention requires full future trajectories upfront, so users can‚Äôt see partial results or stream generation.<br>‚Ä¢ Short training horizons lead to drift/error accumulation on long videos; throughput/latency grow with context length.<br>‚Ä¢ Prior control architectures (e.g., ControlNet-style) add heavy compute and latency; track encoding is inefficient and ambiguous (occlusion vs. unspecified).<br>‚Ä¢ Lack of a single‚ÄëGPU solution that offers precise motion following, natural dynamics, and constant‚Äëlatency, infinite‚Äëlength streaming.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MotionStream distills a high‚Äëquality, motion‚Äëguided bidirectional diffusion teacher (with a lightweight sinusoidal track head and joint text‚Äìmotion guidance) into a causal autoregressive student via Self Forcing with Distribution Matching Distillation. Using sliding‚Äëwindow causal attention with attention sinks and rolling KV caches trained under self‚Äërollout, it achieves constant‚Äëlatency, infinite‚Äëlength streaming; stochastic mid‚Äëframe masking and a Tiny VAE further improve robustness and throughput.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Attention Sinks for Scene Transitions in Streaming Video Diffusion: Dynamically refresh and schedule anchor tokens to handle complete scene changes without sacrificing long‚Äëhorizon stability.<br>‚Ä¢ Learning from Noisy Trajectories: Physics‚ÄëAware Robust Motion Control for Real‚ÄëTime Video Generation: Train with trajectory perturbations and lightweight physical priors to handle fast, imperfect, or implausible user inputs.<br>‚Ä¢ Visibility‚ÄëAware Track Conditioning for Occlusion‚ÄëConsistent Interactive Video Synthesis: Add explicit visibility/occlusion channels and reasoning to disambiguate missing vs. hidden controls and reduce popping artifacts.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.27545" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.27545" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Implicit policies parameterized by generative models, such as Diffusion Policy, have become the standard for policy learning and Vision-Language-Action (VLA) models in robotics. However, these approaches often suffer from high computational cost, exposure bias, and unstable inference dynamics, which lead to divergence under distribution shifts. Energy-Based Models (EBMs) address these issues by learning energy landscapes end-to-end and modeling equilibrium dynamics, offering improved robustness and reduced exposure bias. Yet, policies parameterized by EBMs have historically struggled to scale effectively. Recent work on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs to high-dimensional spaces, but their potential for solving core challenges in physically embodied models remains underexplored. We introduce a new energy-based architecture, EBT-Policy, that solves core issues in robotic and real-world settings. Across simulated and real-world tasks, EBT-Policy consistently outperforms diffusion-based policies, while requiring less training and inference computation. Remarkably, on some tasks it converges within just two inference steps, a 50x reduction compared to Diffusion Policy's 100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior models, such as zero-shot recovery from failed action sequences using only behavior cloning and without explicit retry training. By leveraging its scalar energy for uncertainty-aware inference and dynamic compute allocation, EBT-Policy offers a promising path toward robust, generalizable robot behavior under distribution shifts.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Diffusion/flow-based visuomotor policies are compute-heavy and slow, relying on externally defined noise schedules and many denoising steps; in benchmarks EBT-Policy often matches or exceeds DP with as few as 2 steps vs 100 for DP (Table 4; Figure 5 on page 7).<br>‚Ä¢ Exposure bias and cascading error in chain-structured denoising make diffusion policies brittle under distribution shifts and unstable during inference (pages 2‚Äì4).<br>‚Ä¢ Existing BC/VLA systems typically use fixed compute budgets and lack native uncertainty awareness; they struggle to decide when to continue, adjust, or retry after failure. EBT‚Äôs scalar energy provides an interpretable uncertainty signal (Figure 2 on page 3) that enables dynamic compute allocation and retry behavior (Figure 6 on page 8).<br>‚Ä¢ Classical energy-based policies were historically hard to scale and train stably in high-dimensional action spaces, limiting their adoption despite theoretical advantages (pages 2‚Äì4).<br>‚Ä¢ Robotics needs robust, generalizable policies that maintain performance under real-world distribution shifts and can recover from failed contacts without explicit retry supervision‚Äîcapabilities not reliably present in prior BC methods (pages 1‚Äì3, 8).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>EBT-Policy is an Energy-Based Transformer that learns an explicit scalar energy over action trajectories conditioned on vision, proprioception, and language, and generates actions by iterative energy minimization (Langevin dynamics) with adaptive, uncertainty-aware compute. Stability and efficiency come from energy-scaled step sizes, randomized MCMC steps, scaled Langevin noise, pre-sample normalization, Nesterov acceleration, gradient clipping, and adaptive termination based on gradient norms.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Scaling Energy-Based World Models for Manipulation: Scale EBT-Policy architectures and datasets to test generalization across diverse objects, contacts, and environments.<br>‚Ä¢ Uncertainty-Calibrated EBM Policies for Safe Robotics: Calibrate energy as a risk measure to enable safety-aware stopping, dynamic compute, and intervention policies under uncertainty.<br>‚Ä¢ Hierarchical EBT-Policy for Long-Horizon Planning: Combine low-level energy minimization with high-level hierarchical goals to tackle extended, multi-stage tasks.<br>‚Ä¢ Vision‚ÄìLanguage‚ÄìEnergy Pretraining for Open-World Control: Pretrain multimodal encoders (vision/language) jointly with energy objectives to improve zero-shot generalization and grounding.<br>‚Ä¢ Equilibrium RL: Reinforcement Learning Fine-Tuning of EBM Policies: Use policy gradients or equilibrium matching to fine-tune the energy landscape from sparse task rewards.<br>‚Ä¢ Cross-Embodiment Energy Landscapes for Transfer: Learn shared energy functions that transfer across robots and embodiments while preserving stability and retry behaviors.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Data-Efficient RLVR via Off-Policy Influence Guidance</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.26491" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.26491" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Data selection is a critical aspect of Reinforcement Learning with Verifiable Rewards (RLVR) for enhancing the reasoning capabilities of large language models (LLMs). Current data selection methods are largely heuristic-based, lacking theoretical guarantees and generalizability. This work proposes a theoretically-grounded approach using influence functions to estimate the contribution of each data point to the learning objective. To overcome the prohibitive computational cost of policy rollouts required for online influence estimation, we introduce an off-policy influence estimation method that efficiently approximates data influence using pre-collected offline trajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we employ sparse random projection to reduce dimensionality and improve storage and computation efficiency. Leveraging these techniques, we develop Curriculum RL with Off-Policy Influence guidance (CROPI), a multi-stage RL framework that iteratively selects the most influential data for the current policy. Experiments on models up to 7B parameters demonstrate that CROPI significantly accelerates training. On a 1.5B model, it achieves a 2.66x step-level acceleration while using only 10\% of the data per stage compared to full-dataset training. Our results highlight the substantial potential of influence-based data selection for efficient RLVR.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ RLVR needs strong data selection, but existing methods are largely heuristic (difficulty/uncertainty), lacking theoretical guarantees and poor generalizability.<br>‚Ä¢ Estimating data influence in RLVR typically requires costly online rollouts; this is prohibitive for LLM-scale policies and prevents timely, adaptive selection.<br>‚Ä¢ High-dimensional full-parameter gradients in LLMs make gradient-based influence computation/storage expensive and numerically noisy (Gradient-Scale Issue).<br>‚Ä¢ Static/global selection cannot track evolving policy dynamics in RL training, leading to suboptimal sample- and step-efficiency.<br>‚Ä¢ There is a need to accelerate RLVR (fewer steps, less data) without sacrificing accuracy, ideally with a principled, rollout-free influence estimator.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>POPI (Practical Off-Policy Influence): approximate on-policy per-prompt gradients using offline trajectories from a KL-constrained behavior policy, then compute influence via cosine similarity between sparse-random-projection-compressed gradient features of training prompts and validation sets (with Reciprocal Rank Fusion). CROPI: a curriculum RL framework that iteratively selects the top-Œ± influential prompts per phase and trains with GRPO, yielding faster, data-efficient RLVR.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Higher-Order, Optimizer-Aware Influence for RLVR: Extend POPI with second-order/implicit-Hessian influence and provide bias‚Äìvariance/error analyses under Adam/KL constraints.<br>‚Ä¢ Replay- and Proxy-Enhanced Influence Estimation: Incorporate replay buffers and small-model rollouts, add positive/negative examples to avoid zero gradients, and learn a lightweight proxy scorer to approximate POPI online.<br>‚Ä¢ CROPI Across Tasks and Scales: Apply to multi-turn dialog, agentic and multimodal RLVR, and conduct scaling studies on larger models and longer training with adaptive curricula and selection ratios.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01775" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01775" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Foundation models in video generation are demonstrating remarkable capabilities as potential world models for simulating the physical world. However, their application in high-stakes domains like surgery, which demand deep, specialized causal knowledge rather than general physical rules, remains a critical unexplored gap. To systematically address this challenge, we present SurgVeo, the first expert-curated benchmark for video generation model evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel, four-tiered framework tailored to assess model outputs from basic appearance to complex surgical strategy. On the basis of the SurgVeo benchmark, we task the advanced Veo-3 model with a zero-shot prediction task on surgical clips from laparoscopic and neurosurgical procedures. A panel of four board-certified surgeons evaluates the generated videos according to the SPP. Our results reveal a distinct "plausibility gap": while Veo-3 achieves exceptional Visual Perceptual Plausibility, it fails critically at higher levels of the SPP, including Instrument Operation Plausibility, Environment Feedback Plausibility, and Surgical Intent Plausibility. This work provides the first quantitative evidence of the chasm between visually convincing mimicry and causal understanding in surgical AI. Our findings from SurgVeo and the SPP establish a crucial foundation and roadmap for developing future models capable of navigating the complexities of specialized, real-world healthcare domains.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Bridge the plausibility gap: current video generators (e.g., Veo‚Äë3) look photorealistic in natural scenes but lack causal, domain-specific understanding required in surgery (instrument use, tissue response, procedural strategy).<br>‚Ä¢ Lack of surgical benchmarks and metrics: existing evaluations target natural scenes or generic video quality; there is no expert-curated benchmark or rubric to assess surgical plausibility.<br>‚Ä¢ Inadequate evaluation dimensions: prior metrics ignore action‚Äìconsequence‚Äìintent; they do not measure instrument operation, environment feedback, or surgical intent.<br>‚Ä¢ High-stakes clinical need: safe, reliable surgical simulators are needed for training, planning, intraoperative guidance, and robotics‚Äîdemanding models that reason about anatomy, physiology, and workflows.<br>‚Ä¢ Unknown zero-shot capability: it is unclear whether foundation video models can predict plausible surgical futures from minimal context or benefit from stage-aware prompts.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Propose SurgVeo, an expert-curated benchmark of 50 clips (laparoscopic hysterectomy and endoscopic pituitary surgery) and the Surgical Plausibility Pyramid (SPP), a four-tier rubric (visual appearance, instrument action, environment feedback, surgical intent). Evaluate Veo‚Äë3 zero-shot 8-second continuations from a single frame under baseline vs. stage-aware prompts, with four board-certified surgeons scoring plausibility at 1s/3s/8s on a 1‚Äì5 scale.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Physics- and Physiology-Informed Surgical World Models: Integrate explicit biomechanical and physiological priors to model tissue deformation, bleeding, and energy‚Äìtissue effects for causal video prediction.<br>‚Ä¢ Stage- and Instrument-Conditioned Controllable Surgical Video Generation: Learn controllable conditioning on surgical phase, tools, and targets to produce stage-appropriate actions and consequences.<br>‚Ä¢ Long-Horizon Error-Correcting Temporal Architectures for Surgery: Design architectures that mitigate error accumulation and maintain causal coherence over extended horizons in surgical scenes.<br>‚Ä¢ AutoSPP: Automated Metrics for the Surgical Plausibility Pyramid: Develop learning-based and physics-based proxies that approximate expert SPP scoring for scalable evaluation.<br>‚Ä¢ Domain-Augmented Pretraining for Surgical Video Generators: Pretrain on large curated surgical corpora with structured annotations (phases, instruments, anatomy) to imbue domain knowledge beyond natural videos.<br>‚Ä¢ Closed-Loop Surgical Simulators with Expert Constraints: Embed hard safety and logic constraints and human-in-the-loop feedback to ensure valid instrument operations and intent-aligned predictions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.00405" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.00405" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The remarkable success of multimodal large language models (MLLMs) has driven advances in multimodal embeddings, yet existing models remain inherently discriminative, limiting their ability to benefit from reasoning-driven generation paradigm. In this work, we pioneer the exploration of generative embeddings, unifying embedding tasks within a generative paradigm. We propose UME-R1, a universal multimodal embedding framework consisting of a two-stage training strategy: a cold-start supervised fine-tuning equips the model with reasoning capabilities and enables it to generate both discriminative and generative embeddings; a subsequent reinforcement learning enhances reasoning and further optimizes generative embedding quality. This pioneering work reveals four key insights: 1) generative embeddings unlock substantial performance gains over conventional discriminative embeddings by leveraging the powerful generative reasoning capabilities of MLLMs; 2) discriminative and generative embeddings are complementary, whose combined oracle performance far exceeding that of either alone; 3) RL can effectively enhance generative embeddings, establishing a scalable optimization paradigm.; 4) repeated sampling at inference boosts downstream task coverage (pass@k), highlighting the inference-time scalability potential of generative embeddings. Evaluated on the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual documents, UME-R1 significantly outperforms conventional discriminative embedding models and offers a foundation for more interpretable, reasoning-driven generative multimodal embeddings. Our code, models, and datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing MLLM-based embedding models are inherently discriminative, extracting the last input token without generating tokens, so they cannot leverage chain-of-thought reasoning for stronger, more interpretable embeddings.<br>‚Ä¢ Prior works that add next-token prediction during training still perform discriminative inference, leaving the generative reasoning signal unused at test time.<br>‚Ä¢ Reinforcement learning has been underexplored for embeddings due to the lack of verifiable rewards; naive threshold-based rewards cause zero-gradient issues across pairs of varying difficulty.<br>‚Ä¢ The complementary strengths of discriminative and generative embeddings are not exploited; a large oracle gap shows substantial untapped performance.<br>‚Ä¢ It is unknown whether embeddings can benefit from inference-time scaling (e.g., repeated sampling) similar to reasoning LLMs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>UME-R1 is a two-stage framework that equips an MLLM to output both discriminative (<disc emb>) and generative (<gen emb>) embeddings: Stage 1 SFT on CoT-augmented query‚Äìtarget pairs with contrastive losses on both emb tokens and an autoregressive loss over reasoning/summary; Stage 2 RL with verifiable rewards (GRPO) combining a format reward and an embedding reward that jointly considers ranking and similarity gaps to optimize reasoning-driven generative embeddings.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ AutoSwitch: Adaptive Selection between Discriminative and Generative Embeddings for Multimodal Retrieval: Learn a selector/policy to choose the embedding mode per instance, closing the oracle gap.<br>‚Ä¢ CoT-RL for Embeddings: Curriculum and Dataset-Aware Verifiable Rewards: Design adaptive, difficulty-aware reward shaping (e.g., calibrated thresholds, batch-normalized gaps, curricula) to stabilize and amplify RL gains across modalities.<br>‚Ä¢ Test-Time Compute for Embeddings: Diversified Sampling and Reranking to Maximize Pass@k: Develop diversity-promoting decoding, multi-sample aggregation, and efficient reranking to harness inference-time scaling under fixed compute.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01617" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01617" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In the retrieval domain, candidates' fusion from heterogeneous retrievers is a long-standing challenge, particularly for complex, multi-modal data such as videos. While typical fusion techniques are training-free, they rely solely on rank or score signals, disregarding candidates' representations. This work introduces Vote-in-Context (ViC), a generalized, training-free framework that re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a Vision-Language Model (VLM). The core insight is to serialize both content evidence and retriever metadata directly within the VLM's prompt, allowing the model to adaptively weigh retriever consensus against visual-linguistic content. We demonstrate the generality of this framework by applying it to the challenging domain of cross-modal video retrieval. To this end, we introduce the S-Grid, a compact serialization map that represents each video as an image grid, optionally paired with subtitles to enable list-wise reasoning over video candidates. ViC is evaluated both as a single-list reranker, where it dramatically improves the precision of individual retrievers, and as an ensemble fuser, where it consistently outperforms strong baselines like CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the framework establishes new state-of-the-art zero-shot retrieval performance, demonstrating its effectiveness in handling complex visual and temporal signals alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1% (t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive gains of up to +40 Recall@1 over previous state-of-the-art baselines. We present ViC as a simple, reproducible, and highly effective recipe for turning modern VLMs into powerful zero-shot rerankers and fusers. Code and resources are publicly available at: https://github.com/mohammad2012191/ViC</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Multi-retriever fusion is content-blind: traditional methods (RRF, CombSUM/MNZ) rely only on ranks/scores, ignore candidates‚Äô visual-linguistic evidence, and use fixed, non-adaptive formulas.<br>‚Ä¢ First-stage dual-encoder retrievers use coarse/global embeddings, often mis-rank top items by missing fine-grained, query-specific details‚Äîespecially hard in videos with complex temporal signals.<br>‚Ä¢ Conventional rerankers are costly, require in-domain fine-tuning, or depend on retriever-specific features, limiting generality and reproducibility.<br>‚Ä¢ VLMs can perform zero-shot listwise reasoning but need a compact, uniform way to ingest video content and retriever metadata within a limited context window.<br>‚Ä¢ Practical constraints (latency, context length) demand a reranking approach whose per-candidate cost is bounded and independent of raw video length.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Vote-in-Context (ViC) turns a frozen VLM into a training-free, hyperparameter-free listwise reranker/fuser by serializing both candidate content and retriever metadata (rank, cross-list multiplicity via duplicate-aware round-robin interleaving) into the prompt so the VLM outputs a final permutation. For video, ViC uses S-Grid: a single image grid of uniformly sampled frames plus optional subtitles, enabling zero-shot, content-aware reranking and fusion.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Query-Aware S-Grid: Adaptive Keyframe and Subtitle Selection for Robust VLM Reranking: Replace uniform frame sampling with query-conditioned and saliency-driven selection to better capture short, critical events under fixed token budgets.<br>‚Ä¢ Small ViC: Distilling Zero-Shot VLM Rerankers into Lightweight, Low-Latency Listwise Fusers: Use prompt optimization and (LoRA) distillation to compress ViC into sub-2B models with near-SOTA accuracy and much lower latency.<br>‚Ä¢ Learning to Assemble: Optimizing Candidate Interleaving and Metadata Encoding for Vote-in-Context: Learn policies to order, duplicate, and weight candidates (and metadata) to maximize VLM effectiveness, incorporating uncertainty and calibration.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.00810" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.00810" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have some native grounding capability, nested within their attentions, we propose GUI-AIMA, an attention-based and coordinate-free supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation on simplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate a plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2% on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ GUI grounding must map diverse natural-language instructions to precise screen regions across heterogeneous apps and platforms, making consistency hard<br>‚Ä¢ Directly generating pixel coordinates from images with MLLMs is unintuitive, brittle, and compute/data intensive compared with human-like coarse-to-fine localization<br>‚Ä¢ Reliance on structured inputs (HTML/accessibility trees) has limited availability, is verbose to process, and can miss critical visual/layout/icon cues<br>‚Ä¢ Coordinate-based SFT methods often need massive datasets and OCR or RL to connect text to coordinates, hurting scalability and generalization<br>‚Ä¢ Prior coordinate-free approaches: vanilla attention grounding aggregates all query tokens/heads heuristically and inaccurately; embedding-based heads (e.g., GUI-Actor) add modules and warm-up, reducing training efficiency<br>‚Ä¢ High-resolution screens induce offset errors due to image downsampling and coarse patch granularity; models need flexible, zoomable localization (see Fig. 1, page 2)</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>GUI-AIMA aligns an MLLM‚Äôs intrinsic multi-head self-attention with overlap- and center-aware patch labels by introducing a learnable <ANCHOR> token to aggregate query‚Üívisual attentions and weighting heads via ‚Äúvisual-sink‚Äù query tokens selected from hidden-state similarity; this yields a coordinate-free, data-efficient grounding map (illustrated in Figure 2, page 4). An optional crop-and-zoom second pass further corrects offsets on high-resolution screenshots.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Anchor-Token Multi-Region Grounding for Complex GUIs: Extend the single <ANCHOR> to multiple disentangled anchors to jointly ground multiple targets and disambiguate multi-thing instructions (motivated by Appendix C, page 16)<br>‚Ä¢ Learning to Select Semantic Heads for Grounding: Train a probe/policy to dynamically select and weight MHSA heads using hidden-state signals, improving robustness across backbones and domains<br>‚Ä¢ Progressive Zoom Policy for High-Resolution GUI Grounding: Replace the fixed two-step crop-and-zoom with a learned multi-step zoom controller that optimizes accuracy‚Äìlatency trade-offs (cf. Figure 1, page 2; Table 5, page 11)</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01706" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01706" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Natural Language Explanations (NLEs) describe how Large Language Models (LLMs) make decisions, drawing on both external Context Knowledge (CK) and Parametric Knowledge (PK) stored in model weights. Understanding their interaction is key to assessing the grounding of NLEs, yet it remains underexplored. Prior work has largely examined only single-step generation, typically the final answer, and has modelled PK and CK interaction only as a binary choice in a rank-1 subspace. This overlooks richer forms of interaction, such as complementary or supportive knowledge. We propose a novel rank-2 projection subspace that disentangles PK and CK contributions more accurately and use it for the first multi-step analysis of knowledge interactions across longer NLE sequences. Experiments on four QA datasets and three open-weight instruction-tuned LLMs show that diverse knowledge interactions are poorly represented in a rank-1 subspace but are effectively captured in our rank-2 formulation. Our multi-step analysis reveals that hallucinated NLEs align strongly with the PK direction, context-faithful ones balance PK and CK, and Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing PK reliance. This work provides the first framework for systematic studies of multi-step knowledge interactions in LLMs through a richer rank-2 subspace disentanglement. Code and data: https://github.com/copenlu/pk-ck-knowledge-disentanglement.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing analyses of PK‚ÄìCK interaction focus on single-step (final answer) generation, leaving multi-step knowledge dynamics in Natural Language Explanations (NLEs) unexplored.<br>‚Ä¢ Prior methods model PK‚ÄìCK as a binary, rank-1 conflict direction, which cannot represent supportive or complementary interactions and collapses richer behaviors.<br>‚Ä¢ Rank-1 probes are non-identifiable for disentangling individual PK and CK contributions when both are present, causing most interaction signal to reside in the orthogonal complement (missed by rank-1).<br>‚Ä¢ Understanding PK‚ÄìCK interplay is crucial for assessing explanation grounding/faithfulness, diagnosing hallucinations, and evaluating how CoT prompting shifts reliance toward context.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Learn a rank-2 projection subspace with two orthonormal directions that encode Parametric Knowledge (PK) and Context Knowledge (CK), identified via activation patching, and project token-level hidden states to compute normalized PK/CK contributions across NLE sequence steps. This enables identifiable, multi-step disentanglement of knowledge sources and supports analyses of hallucinations and CoT alignment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Subspace Steering for Controllable Knowledge Reliance in LLMs: Develop inference-time controllers that modulate PK‚ÄìCK balance along learned rank-2 directions to improve contextual grounding without finetuning.<br>‚Ä¢ Rank-2 Knowledge Interaction Analysis Beyond Explanations: Extend the framework to summarization, dialogue, and retrieval-augmented generation to map task-specific PK‚ÄìCK dynamics and optimize retrieval integration.<br>‚Ä¢ Causal Hallucination Mitigation via PK-Axis Regularization: Design training or decoding schemes that penalize excessive PK alignment and promote CK balance to reduce hallucinations while preserving accuracy.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 10</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-04 23:14:15 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 10;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>