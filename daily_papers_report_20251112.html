<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 12, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 12, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Grounding Computer Use Agents on Human Demonstrations</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07332" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07332" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Building reliable computer-use agents requires grounding: accurately connecting natural language instructions to the correct on-screen elements. While large datasets exist for web and mobile interactions, high-quality resources for desktop environments are limited. To address this gap, we introduce GroundCUA, a large-scale desktop grounding dataset built from expert human demonstrations. It covers 87 applications across 12 categories and includes 56K screenshots, with every on-screen element carefully annotated for a total of over 3.56M human-verified annotations. From these demonstrations, we generate diverse instructions that capture a wide range of real-world tasks, providing high-quality data for model training. Using GroundCUA, we develop the GroundNext family of models that map instructions to their target UI elements. At both 3B and 7B scales, GroundNext achieves state-of-the-art results across five benchmarks using supervised fine-tuning, while requiring less than one-tenth the training data of prior work. Reinforcement learning post-training further improves performance, and when evaluated in an agentic setting on the OSWorld benchmark using o3 as planner, GroundNext attains comparable or superior results to models trained with substantially more data,. These results demonstrate the critical role of high-quality, expert-driven datasets in advancing general-purpose computer-use agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Accurate grounding in complex desktop UIs is a bottleneck for computer-use agents; dense, high-resolution screens with tiny, visually similar elements make mapping instructions to precise targets difficult, and grounding failures cascade to multi-step task failure.<br>â€¢ There is a desktop grounding data gap: existing resources are web/mobile or synthetic/automated; they miss small icon-only controls, have incomplete/inconsistent accessibility labels, or lack real desktop complexity; high-quality expert-annotated desktop datasets are scarce.<br>â€¢ Prior models rely on millions of noisy samples and intricate RL rewards, yielding poor data efficiency and limited robustness across applications; an approach that attains state-of-the-art grounding with far less, higher-quality data is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce GROUNDCUA, a large-scale, expert-annotated desktop grounding dataset, and train GROUNDNEXT (3B/7B) models with a two-stage pipeline: supervised fine-tuning on 700K curated instructionâ€“element pairs followed by lightweight RL on 10K samples to map natural-language instructions to exact UI elements, achieving state-of-the-art accuracy with an order of magnitude less data than prior work.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ End-to-End Plannerâ€“Grounder Co-Training for Reliable Computer-Use Agents: Jointly optimize planning and grounding with closed-loop rollouts to reduce error propagation and improve multi-step task success.<br>â€¢ Continual and Personalized Desktop Grounding via Safe Online Learning from User Interactions: Adapt grounding models to new apps and user configurations using clicks and corrections while preserving privacy and ensuring safe updates.<br>â€¢ Structure-Aware UI Grounding with Accessibility Trees and UI Graphs: Fuse accessibility signals and widget hierarchies with visual features to better disambiguate small icon-only controls and enhance cross-application generalization.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.06221" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.06221" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Challenging the prevailing consensus that small models inherently lack robust reasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense model developed via our Spectrum-to-Signal Principle (SSP). This challenges the prevailing approach of scaling model parameters to enhance capabilities, as seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework first employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a broad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL) to amplify the correct signal. With a total training cost of only $7,800, VibeThinker-1.5B demonstrates superior reasoning capabilities compared to closed-source models like Magistral Medium and Claude Opus 4, and performs on par with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses the 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8), AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial improvement over its base model (6.7, 4.3, and 0.6, respectively). On LiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its base model's 0.0. These findings demonstrate that small models can achieve reasoning capabilities comparable to large models, drastically reducing training and inference costs and thereby democratizing advanced AI research.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Heavy reliance on parameter scaling (hundreds of billions to trillions of params) to gain reasoning ability makes training and inference prohibitively expensive, energy-intensive, and inaccessible.<br>â€¢ Conventional SFT maximizes Pass@1, suppressing output diversity and capping the ceiling of subsequent RL improvements; test-time scaling further increases compute cost.<br>â€¢ RL pipelines typically use static data and uniform curricula, failing to prioritize high-uncertainty, high-yield problems and leading to inefficient learning.<br>â€¢ The field lacks a clear, empirical demonstration that tiny dense models (â‰ˆ1.5B) can achieve large-model reasoning, reinforcing an unnecessary size-capability dogma.<br>â€¢ Data contamination concerns complicate fair evaluation; rigorous decontamination is needed to validate true generalization.<br>â€¢ Base small models underperform on hard math and coding, highlighting the need for a principled post-training pipeline that unlocks reasoning without massive scale.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Spectrum-to-Signal Principle (SSP): first maximize solution diversity in SFT (Pass@K) via Two-Stage Diversity-Exploring Distillationâ€”Domain-Aware Diversity Probing to pick subdomain specialist checkpoints and Expert Model Fusion by parameter averagingâ€”then use MaxEnt-Guided Policy Optimization (MGPO), an entropy-weighted GRPO variant, to focus RL on high-uncertainty problems and amplify low-probability correct traces. Applied to a 1.5B model, this post-training pipeline delivers large-model-level reasoning at low cost.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning-to-Diversify: End-to-End Pass@K-Optimized SFT with Differentiable Diversity Objectives: Develop surrogate losses that directly optimize diversity (Pass@K) during SFT without post hoc checkpoint selection.<br>â€¢ Adaptive Expert Fusion for Tiny Reasoners: Learned Weighting and Routing Across Subdomains: Replace uniform parameter averaging with learned fusion or lightweight MoE routing to better preserve subdomain strengths.<br>â€¢ MGPO++: Verifier-Weighted and Step-Level Credit Assignment for Long-CoT: Extend MGPO with verifier scores and token/step-level rewards to sharpen signal amplification of correct reasoning paths.<br>â€¢ SSP for Multimodal Reasoning: Diversity-Driven Optimization in Visionâ€“Language and Tool-Use Tasks: Generalize SSP and MGPO to VLMs and tool-augmented settings, exploring diversity across modalities and tools.<br>â€¢ Online Spectrum-to-Signal: Interleaved SFTâ€“RL Co-Training with Automatic Subdomain Discovery: Jointly learn subdomains and cycle SFT/RL online to maintain a fresh, diverse spectrum aligned with evolving policy.<br>â€¢ Test-Time SSP: Diversity-Aware Sampling and Verifier-Guided Selection Under Compute Budgets: Optimize inference-time diversity allocation and selection policies to maximize Pass@K with limited decoding budget.<br>â€¢ Contamination-Robust Data Curation for Tiny Reasoners: Active Filtering and Synthetic Generation: Build decontamination-aware data pipelines and targeted synthetic data to expand spectrum while safeguarding evaluation integrity.<br>â€¢ Theory of Diversityâ€“Accuracy Coupling: Formal Links Between Pass@K, Uncertainty, and RL Convergence: Analyze when and why maximizing Pass@K in SFT raises RLâ€™s attainable Pass@1 and sample efficiency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Adaptive Multi-Agent Response Refinement in Conversational Systems</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.08319" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.08319" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLMs often fail to ensure factuality, personalization, and cross-turn coherence in multi-turn conversations, leading to error propagation and degraded user experience.<br>â€¢ It is impractical to rely on users to detect and request corrections; systems need proactive, automated response refinement before delivering outputs.<br>â€¢ Existing single-agent self-refinement approaches suffer from overconfidence and bias amplification, and fixed refinement pipelines cannot adapt to the diverse, query-specific needs of conversations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MARA is a multi-agent framework where specialized LLM agents (fact, persona, coherence) sequentially refine an initial response under the guidance of a planner agent that adaptively selects and orders the relevant agents per query. Each agent operates via role-specific prompts and uses planner-provided justifications to collaboratively improve the output.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning an Adaptive Planner for Multi-Agent Response Refinement: Train the planner with reinforcement learning or human feedback to optimize agent selection and ordering for different conversational contexts.<br>â€¢ Retrieval-Augmented Multi-Agent Refinement with Tool-Oriented Agents: Integrate external tools (retrievers, knowledge graphs, persona memory) to strengthen factual grounding and personalized alignment within the multi-agent pipeline.<br>â€¢ Cost- and Uncertainty-Aware Stopping Policies for Conversational Refinement: Develop confidence estimation and budget-aware policies to decide when to refine, when to stop, and when to ask the user for clarification, balancing quality, latency, and token cost.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">Wasm: A Pipeline for Constructing Structured Arabic Interleaved Multimodal Corpora</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07080" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07080" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The performance of large language models (LLMs) and large multimodal models (LMMs) depends heavily on the quality and scale of their pre-training datasets. Recent research shows that large multimodal models trained on natural documents where images and text are interleaved outperform those trained only on image-text pairs across a wide range of benchmarks, leveraging advanced pre- trained models to enforce semantic alignment, image-sequence consistency, and textual coherence. For Arabic, however, the lack of high-quality multimodal datasets that preserve document structure has limited progress. In this paper, we present our pipeline Wasm for processing the Common Crawl dataset to create a new Arabic multimodal dataset that uniquely provides markdown output. Unlike existing Arabic corpora that focus solely on text extraction, our approach preserves the structural integrity of web content while maintaining flexibility for both text-only and multimodal pre-training scenarios. We provide a comprehensive comparative analysis of our data processing pipeline against those used for major existing datasets, highlighting the convergences in filtering strategies and justifying our specific design choices. To support future research, we publicly release a representative dataset dump along with the multimodal processing pipeline for Arabic.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Absence of high-quality Arabic interleaved multimodal corpora that preserve document structure (layout, sectioning, imageâ€“text associations), limiting Arabic LMM pretraining.<br>â€¢ Existing Arabic datasets are mostly text-only, flattening or discarding structural and visual context crucial for multimodal reasoning.<br>â€¢ English-centric filtering heuristics (stopword, punctuation, common-word ratios) misclassify valid Arabic text, risking excessive data loss in a low-resource web setting (~0.6% Arabic in Common Crawl).<br>â€¢ Prior interleaved pipelines (e.g., OBELICS, MMC4) do not target Arabic and often lack full hierarchical preservation or Arabic-specific quality control.<br>â€¢ Inefficiencies in late-stage language filtering and coarse document-level deduplication waste resources and remove useful content mixed with boilerplate.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Wasm is an Arabic-adapted pipeline that performs early Arabic language filtering on Common Crawl, converts HTML to structured Markdown preserving DOM hierarchy and interleaved images, applies Arabic-specific quality filters using a custom KenLM perplexity model and relaxed heuristics, conducts node-level deduplication via Needlemanâ€“Wunsch, and performs URL-level image filtering to output flexible text-only or multimodal datasets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Structural Markdown for Arabic LMM Pretraining: Quantifying the Gains of Hierarchical Preservation over Flattened Interleaving<br>â€¢ Dialect-Aware Perplexity Filtering for Arabic Web Corpora: From KenLM to Neural Quality Models at Scale<br>â€¢ Adaptive, Deferred Image Acquisition for Arabic Multimodal Datasets: URL-Level Safety to Content-Aware Downloading and Alignment</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">KLASS: KL-Guided Fast Inference in Masked Diffusion Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.05664" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.05664" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Masked diffusion models have demonstrated competitive results on various tasks including language generation. However, due to its iterative refinement process, the inference is often bottlenecked by slow and static sampling speed. To overcome this problem, we introduce `KL-Adaptive Stability Sampling' (KLASS), a fast yet effective sampling method that exploits token-level KL divergence to identify stable, high-confidence predictions. By unmasking multiple tokens in each iteration without any additional model training, our approach speeds up generation significantly while maintaining sample quality. On reasoning benchmarks, KLASS achieves up to 2.78times wall-clock speedups while improving performance over standard greedy decoding, attaining state-of-the-art results among diffusion-based samplers. We further validate KLASS across diverse domains, including text, image, and molecular generation, showing its effectiveness as a broadly applicable sampler across different models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Inference in masked diffusion models is slow due to iterative, static sampling (e.g., Top-k or one-token schedules), limiting practical deployment.<br>â€¢ Fixed or planner-based samplers either underutilize parallelism or add training/latency overhead and can misalign with the base modelâ€™s distribution.<br>â€¢ Confidence-only heuristics can prematurely unmask incorrect tokens; there is a need for a robust, training-free signal to decide when tokens are stable enough to unmask.<br>â€¢ Desire to speed up generation without sacrificing (and ideally improving) accuracy across diverse domains (reasoning, text, images, molecules).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>KLASS is a training-free sampler that adaptively unmasks multiple tokens per step by selecting tokens with both high confidence and low token-level KL divergence between consecutive timesteps, indicating stability. This dynamic schedule accelerates inference (up to 2.78Ã—) while maintaining or improving quality across tasks and modalities.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning KL-Guided Threshold Schedules for Masked Diffusion Sampling: Learn timestep- and position-dependent policies for KL and confidence thresholds to further optimize speedâ€“quality trade-offs.<br>â€¢ Theoretical Guarantees for KL-Stability in Discrete Diffusion Decoding: Establish formal links between token-level KL stability and correctness, providing error bounds and convergence guarantees for adaptive unmasking.<br>â€¢ Multimodal KL-Adaptive Samplers for Cross-Modal Generation: Generalize KLASS with modality-aware stability metrics for joint textâ€“image and molecular generation, improving alignment and efficiency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VideoSSR: Video Self-Supervised Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.06281" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.06281" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement Learning with Verifiable Rewards (RLVR) has substantially advanced the video understanding capabilities of Multimodal Large Language Models (MLLMs). However, the rapid progress of MLLMs is outpacing the complexity of existing video datasets, while the manual annotation of new, high-quality data remains prohibitively expensive. This work investigates a pivotal question: Can the rich, intrinsic information within videos be harnessed to self-generate high-quality, verifiable training data? To investigate this, we introduce three self-supervised pretext tasks: Anomaly Grounding, Object Counting, and Temporal Jigsaw. We construct the Video Intrinsic Understanding Benchmark (VIUBench) to validate their difficulty, revealing that current state-of-the-art MLLMs struggle significantly on these tasks. Building upon these pretext tasks, we develop the VideoSSR-30K dataset and propose VideoSSR, a novel video self-supervised reinforcement learning framework for RLVR. Extensive experiments across 17 benchmarks, spanning four major video domains (General Video QA, Long Video QA, Temporal Grounding, and Complex Reasoning), demonstrate that VideoSSR consistently enhances model performance, yielding an average improvement of over 5\%. These results establish VideoSSR as a potent foundational framework for developing more advanced video understanding in MLLMs. The code is available at https://github.com/lcqysl/VideoSSR.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing RLVR video datasets are too easy or too biased for strong MLLMs, producing bimodal, zero-variance rewards (all-correct or all-wrong) that yield ineffective GRPO training.<br>â€¢ Multi-agent and manual annotations are expensive and introduce systemic biases/artifacts, especially when annotators are weaker than target models, corrupting verifiable rewards.<br>â€¢ Rapid MLLM progress outpaces dataset complexity, creating a difficulty mismatch that limits further gains and can even degrade performance.<br>â€¢ There is a need for scalable, verifiable supervision sourced directly from intrinsic video signals, avoiding human/MLLM annotations while enabling controllable task difficulty.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>VideoSSR generates verifiable training data via three self-supervised pretext tasksâ€”Anomaly Grounding, Object Counting, and Temporal Jigsawâ€”with parametrically scalable difficulty, forming the VideoSSR-30K dataset and training MLLMs using GRPO with tailored smooth reward functions to counter reward sparsity. The VIUBench benchmark validates task difficulty, and the approach yields consistent >5% average gains across 17 video benchmarks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Curriculum VideoSSR: Online Difficulty Scheduling for RLVR: Dynamically adjust pretext-task difficulty and sample selection based on model competence to maintain informative, non-saturated reward signals.<br>â€¢ Audio-Visual VideoSSR: Self-Supervised RLVR with Multi-Sensory Pretext Tasks: Extend pretext tasks to audio-visual cues (e.g., A/V sync, sound-source localization) to improve temporal and causal understanding.<br>â€¢ VideoSSR at Web Scale: Training MLLMs on Hundreds of Millions of Unlabeled Videos: Scale self-supervised RLVR with curriculum, deduplication, and compute-efficient training to study data scaling laws and generalization.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07003" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07003" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models have significantly advanced Multilingual Machine Translation (MMT), yet the broad language coverage, consistent translation quality, and English-centric bias remain open challenges. To address these challenges, we introduce LMT, a suite of Large-scale Multilingual Translation models centered on both Chinese and English, covering 60 languages and 234 translation directions. During development, we identify a previously overlooked phenomenon of directional degeneration, where symmetric multi-way fine-tuning data overemphasize reverse directions (X to En/Zh), leading to excessive many-to-one mappings and degraded translation quality. We propose Strategic Downsampling, a simple yet effective method to mitigate this degeneration. In addition, we design Parallel Multilingual Prompting (PMP), which leverages typologically related auxiliary languages to enhance cross-lingual transfer. Through rigorous data curation and refined adaptation strategies, LMT achieves SOTA performance among models of comparable language coverage, with our 4B model (LMT-60-4B) surpassing the much larger Aya-101-13B and NLLB-54B models by a substantial margin. We release LMT in four sizes (0.6B/1.7B/4B/8B) to catalyze future research and provide strong baselines for inclusive, scalable, and high-quality MMT \href{https://github.com/NiuTrans/LMT{https://github.com/NiuTrans/LMT}}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Broad, inclusive multilingual MT remains hard: existing LLM-based systems struggle to scale language coverage while delivering consistent quality across languages.<br>â€¢ Persistent English-centric bias from pre-training corpora leads to weaker modeling and translation for non-English, especially Chinese-centric and low-resource directions.<br>â€¢ Severe data scarcity and imbalance: long-tail multilingual distributions and limited parallel data (notably for Zh-centric pairs) constrain effective supervision and transfer.<br>â€¢ Suboptimal adaptation strategies: symmetric multi-way SFT induces directional degeneration (overemphasis on Xâ†’En/Zh many-to-one mappings), degrading translation quality.<br>â€¢ Reliance on outdated or less capable backbones and insufficiently curated CPT data further limits cross-lingual generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces LMT, a Zh/En-centric multilingual MT suite over 60 languages built via a CPTâ†’SFT pipeline with rigorously curated multilingual and pseudo-parallel data. It proposes Strategic Downsampling to mitigate directional degeneration in symmetric multi-way SFT and Parallel Multilingual Prompting to enhance cross-lingual transfer by injecting auxiliary typologically related parallel sentences.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning-to-Sample: Adaptive Strategies to Prevent Directional Degeneration in Multilingual MT: Develop online, gradient- or uncertainty-driven per-direction sampling schedules that dynamically counteract many-to-one collapse during SFT.<br>â€¢ Typology-Aware Parallel Multilingual Prompting at Scale: Systematically model and select auxiliary languages based on typological distance and script similarities to maximize transfer gains in PMP.<br>â€¢ Continual LMT: Expanding to Low-Resource Languages with Data Synthesis and Uncertainty-Aware Filtering: Combine iterative pseudo-parallel generation, quality estimation, and filtering to safely add new languages without catastrophic forgetting.<br>â€¢ From Text to Multimodal: PMP-Enhanced Speech and Image-Grounded Multilingual Translation: Extend PMP to ASR/MT and image-caption translation settings, leveraging aligned multimodal cues for improved robustness and fidelity.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">The Path Not Taken: RLVR Provably Learns Off the Principals</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.08567" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.08567" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement Learning with Verifiable Rewards (RLVR) reliably improves the reasoning performance of large language models, yet it appears to modify only a small fraction of parameters. We revisit this paradox and show that sparsity is a surface artifact of a model-conditioned optimization bias: for a fixed pretrained model, updates consistently localize to preferred parameter regions, highly consistent across runs and largely invariant to datasets and RL recipes. We mechanistically explain these dynamics with a Three-Gate Theory: Gate I (KL Anchor) imposes a KL-constrained update; Gate II (Model Geometry) steers the step off principal directions into low-curvature, spectrum-preserving subspaces; and Gate III (Precision) hides micro-updates in non-preferred regions, making the off-principal bias appear as sparsity. We then validate this theory and, for the first time, provide a parameter-level characterization of RLVR's learning dynamics: RLVR learns off principal directions in weight space, achieving gains via minimal spectral drift, reduced principal-subspace rotation, and off-principal update alignment. In contrast, SFT targets principal weights, distorts the spectrum, and even lags RLVR. Together, these results provide the first parameter-space account of RLVR's training dynamics, revealing clear regularities in how parameters evolve. Crucially, we show that RL operates in a distinct optimization regime from SFT, so directly adapting SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, as evidenced by our case studies on advanced sparse fine-tuning and LoRA variants. We hope this work charts a path toward a white-box understanding of RLVR and the design of geometry-aware, RLVR-native learning algorithms, rather than repurposed SFT-era heuristics.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Resolve the paradox that RLVR yields large reasoning gains with apparently sparse parameter updates, clarifying where and how learning occurs in weight space.<br>â€¢ Provide a mechanistic, parameter-space account of RLVRâ€™s optimization dynamics, which are poorly understood compared to SFT.<br>â€¢ Address the misalignment of SFT-era PEFT methods (e.g., principal-weight targeting, PiSSA) when naively applied to RLVR, leading to degraded performance.<br>â€¢ Explain the model-conditioned invariance of update locality across datasets/recipes and the role of numerical precision (bfloat16) in hiding micro-updates.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes a Three-Gate Theoryâ€”KL Anchor, Model Geometry, and Precisionâ€”that mechanistically explains a model-conditioned optimization bias steering RLVR updates off principal directions into low-curvature, spectrum-preserving subspaces, with precision masking micro-updates. It validates this via parameter-space analyses (principal-mask overlap via SVD, principal-angle curves, spectral drift), function-preserving orthogonal rotations, and case studies contrasting principal-targeted vs. off-principal PEFT and LoRA variants.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Geometry-Aware RLVR: Designing Off-Principal PEFT for Reinforcement-Tuned LLMs: Develop RL-native adapters and masks that target low-curvature, non-principal subspaces to preserve spectra and align with RLVR dynamics.<br>â€¢ Precision-Gated Reinforcement Learning: Leveraging Numerical Precision to Reveal and Control Micro-Updates: Study and schedule precision (e.g., FP32/FP16/bfloat16/FP8) to surface hidden updates and modulate sparsity for better stability and performance.<br>â€¢ KL-Anchor Scheduling and Compass Control: Modulating On-Policy KL to Shape Optimization Trajectories: Explore dynamic KL constraint policies to balance exploration, spectrum preservation, and off-principal alignment during RLVR.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07587" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07587" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, we propose the Generative Semantic Workspace (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. Our framework comprises an Operator, which maps incoming observations to intermediate semantic structures, and a Reconciler, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) huet_episodic_2025 comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to 20\%. Furthermore, GSW is highly efficient, reducing query-time context tokens by 51\% compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLMs struggle with long-context reasoning: documents often exceed finite context windows and performance degrades with sequence length (e.g., context rot, lost-in-the-middle).<br>â€¢ Standard RAG embeds chunks independently, leading to incomplete retrieval when queries depend on information spread across multiple chunks.<br>â€¢ Graph-based RAG methods are optimized for fact retrieval and lack space-time-anchored narrative representations needed to track entities, roles, and state transitions through events.<br>â€¢ Real-world corpora (e.g., reports, filings, news) are episodic narratives; current methods fail to model evolving situations across time and space.<br>â€¢ There is a need for more token-efficient, accurate retrieval and reasoning by building an internal world model akin to human episodic memory.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes the Generative Semantic Workspace (GSW), a neuro-inspired generative memory that constructs a persistent, structured representation of evolving entities, roles, actions, and spatiotemporal context. An Operator maps incoming observations to intermediate semantic structures, and a Reconciler integrates them into a coherent workspace that enforces temporal, spatial, and logical consistency, enabling episodic reasoning with reduced query-time tokens.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ End-to-End Learning of Generative Semantic Workspaces for Episodic RAG: Jointly train Operator and Reconciler modules to optimize parsing, integration, and downstream QA performance across diverse episodic corpora.<br>â€¢ Multimodal GSW: Episodic Memory Across Text, Vision, and Audio: Extend the workspace to integrate video, images, and audio streams for richer event grounding and cross-modal tracking of entities and roles.<br>â€¢ Continual and Real-Time Episodic Memory with GSW: Online Updating, Compression, and Forgetting: Develop algorithms for streaming updates, conflict resolution, memory consolidation, and token-efficient retention over very long horizons.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.08029" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.08029" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ High-quality hard-negative mining in biomedical IR is difficult: positives and near-duplicates are hard to distinguish, while cross-encoder or static-embedding mining is compute-heavy and often surfaces easy/noisy negatives, limiting retriever discrimination.<br>â€¢ Biomedical domain adaptation is data- and compute-constrained: labeled relevance and click logs are scarce and queries are often low-quality; existing strong methods rely on large models or expensive supervision, hindering practical deployment.<br>â€¢ Current dense retrievers struggle with zero-shot generalization and long-tailed topics while meeting latency constraints; there is a need for small, efficient models that improve in-domain (PubMed) and out-of-domain retrieval without extensive fine-tuning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>BiCA mines citation-aware hard negatives by building a two-hop PubMed citation neighborhood around each positive document, constructing a semantic similarity graph over cited papers, and greedily traversing it from the query-nearest node to select challenging non-duplicate negatives. Using T5-generated queries and multiple negative ranking loss, the approach fine-tunes compact GTE-based retrievers (BiCA-Base/Small) to achieve strong zero-shot performance efficiently.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Temporal BiCA: Citation-Aware Hard Negative Mining on Dynamic Biomedical Graphs: Incorporate citation time and recency to adapt negatives over evolving literature and reduce leakage/concept drift.<br>â€¢ BiCA-RL: User-Feedback-Guided Hard Negative Curriculum for Biomedical Dense Retrieval: Use implicit interaction signals (clicks, dwell time) to adaptively weight and schedule hard negatives for iterative retriever improvement.<br>â€¢ Cross-Domain BiCA: Leveraging Heterogeneous Link Structures for Zero-Shot Retrieval Across Scientific and Clinical Corpora: Generalize citation-aware mining to arXiv, clinical trials, and guidelines by unifying diverse link graphs (citations, references, UMLS links) for robust transfer.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Intelligence per Watt: Measuring Intelligence Efficiency of Local AI</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07885" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07885" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals 3 findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Cloud-centric LLM inference faces scaling limits in compute, power, and cost as query volumes grow to billionsâ€“trillions per day, motivating offload to local devices.<br>â€¢ Lack of a unified metric coupling model capability with energy efficiency prevents fair comparison of modelâ€“hardware stacks under device power constraints.<br>â€¢ Existing evaluations emphasize accuracy on academic benchmarks, with limited real-world telemetry and cross-hardware, energy-aware measurement at scale.<br>â€¢ Uncertainty about what fraction of real-world queries small local LMs can handle, how this changes over time, and how much hybrid routing can save.<br>â€¢ Measurement tooling gaps: inconsistent energy sampling, limited device coverage, and absence of a reproducible, hardware-agnostic profiling harness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce Intelligence per Watt (IPW)â€”task accuracy per unit powerâ€”along with energy-based variants (accuracy/perplexity per joule), and build an open, cross-platform profiling harness that collects high-resolution (50 ms) telemetry to evaluate 20+ local LMs on 8 accelerators across 1M real chat and reasoning queries. Use these measurements to quantify coverage, longitudinal IPW trends (2023â€“2025), local vs. cloud efficiency gaps, and simulate hybrid routing to estimate achievable energy/compute/cost savings.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Co-Designing Small LMs and Edge Accelerators for Maximizing Intelligence per Watt: Jointly optimize architectures, quantization, KV-cache policies, and on-device tensor cores to close the 1.4â€“7.4Ã— IPW/IPJ gap with cloud hardware.<br>â€¢ IPW-Aware Training and Post-Training Objectives for Local LMs: Incorporate energy-aware regularization into pretraining, distillation, and RLHF to directly optimize accuracy-per-watt/joule under device power budgets.<br>â€¢ Learning-to-Route for Hybrid Localâ€“Cloud Serving Under Energy and Quality SLAs: Develop uncertainty- and cost-aware routers that maximize end-to-end IPJ while meeting domain-specific quality targets across chat, reasoning, and technical workloads.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DynaAct: Large Language Model Reasoning with Dynamic Action Spaces</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.08043" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.08043" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In modern sequential decision-making systems, the construction of an optimal candidate action space is critical to efficient inference. However, existing approaches either rely on manually defined action spaces that lack scalability or utilize unstructured spaces that render exhaustive search computationally prohibitive. In this paper, we propose a novel framework named DynaAct for automatically constructing a compact action space to enhance sequential reasoning in complex problem-solving scenarios. Our method first estimates a proxy for the complete action space by extracting general sketches observed in a corpus covering diverse complex reasoning problems using large language models. We then formulate a submodular function that jointly evaluates candidate actions based on their utility to the current state and their diversity, and employ a greedy algorithm to select an optimal candidate set. Extensive experiments on six diverse standard benchmarks demonstrate that our approach significantly improves overall performance, while maintaining efficient inference without introducing substantial latency. The implementation is available at https://github.com/zhaoxlpku/DynaAct.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing LLM reasoning either relies on manually engineered action spaces that are non-scalable and poorly generalize, or on unstructured natural language generation that makes search computationally prohibitive.<br>â€¢ There is a need for action spaces that are both scalable (automatically learned from demonstrations) and compact (small, information-dense, state-dependent candidate sets) to guide multi-step reasoning efficiently.<br>â€¢ Heuristic action spaces are often too broad or too specific, failing to balance utility and diversity, leading to redundant candidates and inefficient exploration.<br>â€¢ Without explicit, compact action spaces, methods depend on very powerful base models and extensive test-time scaling, raising cost and limiting applicability.<br>â€¢ Improving reasoning accuracy across diverse tasks while avoiding significant inference latency requires principled, dynamic action space construction.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DynaAct learns a proxy action space by extracting general reasoning sketches from a diverse corpus and dynamically selects a compact per-step candidate set by greedily maximizing a learned submodular objective that balances utility and diversity. Actions are then chosen via an MCTS-estimated Q-function to generate the next reasoning step, training only a lightweight embedding model while keeping the base LLM frozen.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Submodular Action Spaces End-to-End for LLM Reasoning: Jointly learn utility/diversity scoring and the Q-function with reinforcement learning to adapt action selection to tasks and states.<br>â€¢ Continual and Domain-Adaptive Dynamic Action Spaces for Cross-Task Reasoning: Online expand and refine the proxy action space from new domains while mitigating forgetting and preserving compactness.<br>â€¢ Tool- and Modality-Aware Dynamic Action Spaces for Multi-Modal Reasoning: Incorporate external tools and multimodal actions, with state-conditioned submodular selection for efficient, cross-modal decision-making.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.06428" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.06428" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Background: Large Language Models emerged with the potential of provoking a revolution in software development (e.g., automating processes, workforce transformation). Although studies have started to investigate the perceived impact of LLMs for software development, there is a need for empirical studies to comprehend how to balance forward and backward effects of using LLMs. Objective: We investigated how LLMs impact software development and how to manage the impact from a software developer's perspective. Method: We conducted 22 interviews with software practitioners across 3 rounds of data collection and analysis, between October (2024) and September (2025). We employed socio-technical grounded theory (STGT) for data analysis to rigorously analyse interview participants' responses. Results: We identified the benefits (e.g., maintain software development flow, improve developers' mental model, and foster entrepreneurship) and disadvantages (e.g., negative impact on developers' personality and damage to developers' reputation) of using LLMs at individual, team, organisation, and society levels; as well as best practices on how to adopt LLMs. Conclusion: Critically, we present the trade-offs that software practitioners, teams, and organisations face in working with LLMs. Our findings are particularly useful for software team leaders and IT managers to assess the viability of LLMs within their specific context.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Practitioners lack empirically grounded guidance to balance the benefits and drawbacks of LLMs in software development across individual, team, organizational, and societal levels.<br>â€¢ Existing research is skewed toward productivity metrics and tool-centric evaluations, with limited attention to communication, humanâ€“human collaboration, mentorship, and managing socio-technical trade-offs.<br>â€¢ Organizations and team leaders need evidence-based best practices to assess LLM viability and implement responsible adoption strategies.<br>â€¢ The shift from traditional knowledge communities (e.g., Stack Overflow) to LLM tools raises concerns about knowledge ecosystem health and developer support patterns.<br>â€¢ Prior studies provide fragmented insights (e.g., isolated benefits or risks) and lack a holistic, practitioner-centered view of trade-offs and management approaches.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper conducts 22 semi-structured interviews with software practitioners across three rounds (Oct 2024â€“Sept 2025) and applies socio-technical grounded theory (STGT) to analyze responses, deriving multi-level benefits, drawbacks, trade-offs, and best-practice guidance for LLM adoption.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Designing Organizational Guardrails for Balanced LLM Adoption in Software Teams: Develop and evaluate policies, workflows, and metrics that optimize benefits while mitigating harms across individual, team, and organizational levels.<br>â€¢ Longitudinal Effects of LLM Use on Developersâ€™ Critical Thinking, Personality, and Professional Reputation: Track long-term changes in cognition, well-being, and reputation among developers using LLMs, with mixed-methods measurement.<br>â€¢ AI Pair Programming and Team Communication: Controlled Studies of Collaboration Dynamics with LLM Assistants: Experimentally assess how LLMs impact mentorship, humanâ€“human collaboration, and developer flow, and test interventions to improve team outcomes.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Optimizing Diversity and Quality through Base-Aligned Model Collaboration</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.05650" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.05650" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Alignment improves instruction-following and task performance but markedly reduces output diversity, causing aligned models to produce highly similar generations across samples.<br>â€¢ This diversityâ€“quality trade-off undermines open-ended applications (e.g., creative writing, dialogue) by encouraging formulaic language, diminishing creativity, and suppressing ideation in humanâ€“AI interaction.<br>â€¢ Training-time diversity methods (e.g., modifying preference optimization) can be costly and may compromise desirable alignment properties such as safety and helpfulness.<br>â€¢ Inference-time baselines (e.g., prompt engineering, multi-sampling) often boost diversity at the expense of quality, require expensive decoding, and offer limited controllability, motivating a post-hoc, single-pass approach that jointly improves diversity and quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>BACO performs inference-time, token-level collaboration between a base LLM and its aligned counterpart, routing which model emits each token using next-token prediction uncertainty and the predicted semantic role of upcoming content. This dynamic routing balances diversity and quality in a single pass while providing controllability without retraining.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning-to-Route: Reinforcement Learning for Token-Level Diversityâ€“Quality Pareto Optimization: Train router policies that directly optimize a Pareto objective over diversity, quality, and safety constraints.<br>â€¢ Safety-Aware Collaborative Decoding for Aligned LLMs: Incorporate risk-sensitive uncertainty and content filters into routing to preserve safety/helpfulness while leveraging base-model diversity.<br>â€¢ Cross-Lingual Baseâ€“Aligned Collaboration for Multilingual Creativity: Extend BACO to multilingual models to enhance diversity without sacrificing alignment across languages and cultural contexts.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 5</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-12 23:07:18 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 5;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>