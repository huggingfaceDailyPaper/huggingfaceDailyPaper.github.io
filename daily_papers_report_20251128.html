<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 28, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 28, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Video Generation Models Are Good Latent Reward Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.21541" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.21541" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reward feedback learning (ReFL) has proven effective for aligning image generation with human preferences. However, its extension to video generation faces significant challenges. Existing video reward models rely on vision-language models designed for pixel-space inputs, confining ReFL optimization to near-complete denoising steps after computationally expensive VAE decoding. This pixel-space approach incurs substantial memory overhead and increased training time, and its late-stage optimization lacks early-stage supervision, refining only visual quality rather than fundamental motion dynamics and structural coherence. In this work, we show that pre-trained video generation models are naturally suited for reward modeling in the noisy latent space, as they are explicitly designed to process noisy latent representations at arbitrary timesteps and inherently preserve temporal information through their sequential modeling capabilities. Accordingly, we propose Process Reward Feedback Learning~(PRFL), a framework that conducts preference optimization entirely in latent space, enabling efficient gradient backpropagation throughout the full denoising chain without VAE decoding. Extensive experiments demonstrate that PRFL significantly improves alignment with human preferences, while achieving substantial reductions in memory consumption and training time compared to RGB ReFL.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Extending reward feedback learning (ReFL) from images to videos faces critical computational and optimization barriers.<br>‚Ä¢ Existing video reward models operate in pixel (RGB) space using vision‚Äìlanguage models, requiring VAE decoding and only allowing optimization at near-complete denoising steps.<br>‚Ä¢ Pixel-space supervision imposes substantial memory and training-time overhead and restricts gradients to late stages, refining appearance but failing to shape early-stage denoising, motion dynamics, and structural coherence.<br>‚Ä¢ There is a need for reward models that operate directly on noisy latent representations across arbitrary timesteps while preserving temporal information.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes Process Reward Feedback Learning (PRFL), which treats pre-trained video generation models as latent reward models and conducts preference optimization entirely in the noisy latent space across timesteps. This enables efficient, end-to-end gradient backpropagation through the full denoising chain without VAE decoding, improving human-preference alignment while reducing memory and time costs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Multi-Scale Temporal Reward Models in Latent Space: Integrate rewards at frame-, clip-, and trajectory-level timesteps to better shape motion and long-horizon coherence.<br>‚Ä¢ Online Human-in-the-Loop Process ReFL for Video Diffusion: Collect and incorporate real-time pairwise preferences on latent trajectories to adapt rewards during training.<br>‚Ä¢ Physics- and Consistency-Aware Latent Rewards for Video Generation: Inject differentiable physical priors and cross-frame consistency constraints into PRFL to improve plausibility and stability.<br>‚Ä¢ Theoretical Foundations of Process Reward Feedback in Diffusion Models: Analyze convergence, credit assignment across timesteps, and stability of latent-space reward shaping.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Canvas-to-Image: Compositional Image Generation with Multimodal Controls</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.21691" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.21691" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Diffusion models struggle to follow multiple controls simultaneously (text, subject references, spatial layouts, poses, bounding boxes), leading to poor compositional fidelity.<br>‚Ä¢ Existing approaches rely on task-specific controllers/heuristics and fragmented pipelines, which conflict across modalities and generalize poorly to multi-control scenarios.<br>‚Ä¢ Identity preservation and adherence to complex layout/pose constraints degrade under heterogeneous, jointly specified controls, limiting practical use.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Encode diverse control signals into a single composite canvas image and train the diffusion model with a Multi-Task Canvas Training strategy on curated multi-task datasets, enabling unified visual-spatial reasoning and robust integration of heterogeneous controls during inference.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Canvas-to-Video: Spatiotemporal Multimodal Controls for Generative Video: Extend the unified canvas to temporal trajectories (poses, layouts, motion paths) to control video generation with coherent identity and dynamics.<br>‚Ä¢ Interactive Canvas Agents: Real-Time Iterative Editing for Compositional Diffusion Models: Integrate an interactive loop that incrementally updates the canvas and model outputs, enabling user-guided refinement and conflict resolution across controls.<br>‚Ä¢ Semi-Supervised Canvas Learning: Weakly-Annotated Multimodal Controls for Data-Efficient Generation: Leverage weak labels and synthetic canvases to reduce annotation costs while maintaining control adherence and identity preservation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MIRA: Multimodal Iterative Reasoning Agent for Image Editing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.21087" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.21087" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructions, especially those involving compositional relationships, contextual cues, or referring expressions, leading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perception-reasoning-action loop, effectively simulating multi-turn human-model interaction processes. Instead of issuing a single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset, MIRA-Editing, combined with a two-stage SFT + GRPO training pipeline, enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA significantly improves both semantic consistency and perceptual quality, achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Diffusion-based editors struggle with complex instructions (compositional relations, context, referring expressions), causing semantic drift and incomplete edits.<br>‚Ä¢ Even proprietary multimodal systems show limitations in fine-grained consistency and controllability for complex, multi-step edits.<br>‚Ä¢ Prompt-refinement approaches are static and do not adapt iteratively to visual feedback, limiting alignment with user intent.<br>‚Ä¢ Agentic multi-tool orchestration frameworks are powerful but computationally heavy and hard to scale within open-source ecosystems.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MIRA is a lightweight, plug-and-play multimodal agent that performs image editing via an iterative perception‚Äìreasoning‚Äìaction loop, decomposing instructions into atomic edits and selecting next actions or stop decisions with visual feedback, while orchestrating open-source diffusion editors. It is trained on a 150K multimodal tool-use dataset (MIRA-EDITING) using a two-stage SFT + GRPO pipeline to learn step-wise reasoning and tool calling, improving semantic consistency and perceptual quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Hierarchical MIRA: Learning Multi-Granular Plans for Long-Horizon Image Editing: Introduce hierarchical planning with subgoal decomposition and atomic action execution to boost efficiency and interpretability in complex edit sequences.<br>‚Ä¢ MIRA-Video: Iterative Multimodal Reasoning for Temporally Consistent Instruction-Guided Video Editing: Extend the agentic loop to video with frame-wise feedback, temporal constraints, and consistency-aware editing tools.<br>‚Ä¢ Adaptive Tool Discovery in Agentic Image Editors via Reinforcement Learning and Preference Modeling: Enable dynamic selection and discovery of editing tools using RL with human/synthetic preferences to optimize edit quality and controllability.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.21662" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.21662" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing LMM judge benchmarks typically assign a single overall preference label, obscuring criterion-level trade-offs and conflicts (e.g., Logic vs. No Hallucination, Reflection vs. Efficiency)<br>‚Ä¢ The ability of LMM judges to follow diverse, fine-grained evaluation criteria and to switch criteria on demand remains underexplored and inconsistent, especially in open-ended evaluation<br>‚Ä¢ Current training (e.g., critic fine-tuning with holistic judgment signals) improves specific aspects like visual grounding but fails to generalize to pluralistic, criterion-level judgment<br>‚Ä¢ Lack of systematic metrics and datasets to assess pluralistic adherence, criterion-switching flexibility, and conflict recognition limits the development of reliable, steerable multimodal evaluation</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Multi-Crit constructs a benchmark of challenging response pairs with multi-criterion human annotations across open-ended generation and verifiable reasoning tasks, and introduces three metrics‚Äîpluralistic adherence, criterion-switching flexibility, and conflict recognition‚Äîto evaluate LMM judges‚Äô ability to follow pluralistic criteria. The authors comprehensively assess 25 LMMs to reveal adherence gaps, switching limitations, and the shortcomings of holistic critic fine-tuning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Criterion-Aware Fine-Tuning for Multimodal Judges: Train LMM judges with per-criterion supervision and reward models to improve adherence and flexible switching across diverse evaluation criteria<br>‚Ä¢ Conflict-Sensitive Aggregation in Multimodal Evaluation: Develop multi-objective or Pareto-based methods that detect and reconcile criterion-level conflicts to produce balanced, transparent judgments<br>‚Ä¢ Steerable Multimodal Judges with Test-Time Criterion Control: Design architectures and prompting strategies that reliably follow specified criteria, adapt to trade-offs, and maintain consistency in open-ended evaluation</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">What does it mean to understand language?</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19757" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19757" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Language understanding entails not just extracting the surface-level meaning of the linguistic input, but constructing rich mental models of the situation it describes. Here we propose that because processing within the brain's core language system is fundamentally limited, deeply understanding language requires exporting information from the language system to other brain regions that compute perceptual and motor representations, construct mental models, and store our world knowledge and autobiographical memories. We review the existing evidence for this hypothesis, and argue that recent progress in cognitive neuroscience provides both the conceptual foundation and the methods to directly test it, thus opening up a new strategy to reveal what it means, cognitively and neurally, to understand language.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of a clear, neurally grounded definition of what it means to ‚Äúunderstand‚Äù language, and of the distinction between shallow (form/statistics-based) and deep (situation-model-based) understanding.<br>‚Ä¢ Need to reconcile language-selective processing in the core language network with widespread content-selective activations across the brain during comprehension (a unifying account vs. ‚Äúwhole brain does language‚Äù).<br>‚Ä¢ Methodological limitations in prior work: group-averaging that blurs functionally specific regions, heavy reliance on tasks that force imagery or responses (not passive comprehension), and weak reverse inferences without individual functional localizers.<br>‚Ä¢ Empirical clues (e.g., language areas‚Äô insensitivity to real-world plausibility; preserved conceptual knowledge after language-area damage) suggest meaning construction must involve extra-linguistic systems‚Äîcurrent models do not explain how.<br>‚Ä¢ AI models trained on text alone largely capture shallow competence; the field lacks a framework linking brain mechanisms of deep understanding to computational architectures that go beyond text-only statistics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes the exportation hypothesis: the core language network constructs shallow, abstract representations that are exported to functionally specific extra-linguistic systems (e.g., ToM, physics, navigation, perceptual/motor, semantic/episodic) to achieve deep understanding. It outlines a testing protocol using individually localized, content-selective regions and passive comprehension in fMRI/related methods, complemented by in-silico augmented LLMs, to detect and validate exportation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Measuring the Prevalence of Exportation in Naturalistic Language Comprehension: Quantify how much narrative comprehension is explained by the core language network versus extra-linguistic systems using individual localizers and passive listening/reading.<br>‚Ä¢ From Vectors to Programs: Translating Language-System Outputs for Extra-Linguistic Processors: Identify representational transformations that interface language representations with ToM, physics, and navigation networks via encoding/RSA/CCA and computational models.<br>‚Ä¢ Routing or Broadcasting? Mechanisms of Information Flow from the Language Network: Test selective routing vs global broadcasting using MEG/iEEG/laminar fMRI and causal perturbations (TMS), linked to model-based predictions.<br>‚Ä¢ Hippocampal Coordination of Situation Models Across Domain-Specific Networks: Probe hippocampal-dependent integration of ToM, spatial, physical, and memory representations during long-context narratives.<br>‚Ä¢ Content-Selective Engagement of Perceptual and Motor Cortex During Passive Reading: With individual FFA/PPA/OPA/RSC/motor localizers, test whether vivid verbal descriptions elicit category- and effector-specific activations without explicit imagery tasks.<br>‚Ä¢ Person and Goal Effects on Exportation During Comprehension: Examine how expertise, motivation, alertness, and imagery ability modulate exportation likelihood and depth, using mixed-effects models and preregistered manipulations.<br>‚Ä¢ Behavioral Benchmarks for Deep Understanding Beyond Shallow Linguistic Competence: Develop and validate task batteries that dissociate shallow vs deep understanding and correlate them with extra-linguistic neural engagement.<br>‚Ä¢ Neuro-Symbolic Language Models with Explicit Exportation Modules as Model Organisms: Build LLMs augmented with ToM/physics/navigation/memory tools and test whether their internal dynamics and brain-prediction profiles exhibit exportation signatures.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Agentic Learner with Grow-and-Refine Multimodal Semantic Memory</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.21678" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.21678" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ MLLMs solve each query de novo, repeatedly making the same mistakes and lacking persistent, transferable knowledge across tasks.<br>‚Ä¢ Existing memory-augmented agents are logic-centric and trajectory-based, suffering from brevity bias and detail erosion, which leads to catastrophic forgetting of essential domain knowledge.<br>‚Ä¢ Current memories capture single-modality traces and discard visual grounding, failing to integrate visual attention with logical reasoning‚Äîdespite visual perception errors being a primary bottleneck that cascade into logical hallucinations.<br>‚Ä¢ Naive retrieval (e.g., image similarity or semantic similarity alone) is insufficient for multimodal problem solving; question-aware visual attention and precise logical retrieval are needed to avoid recurring distractions and hallucinations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ViLoMem is a dual-stream, grow-and-refine memory framework that separately models visual distraction patterns and logical hallucination errors as compact schemas, with tailored retrieval (question-aware attention for visuals and precise positioning‚Äìselection for logic) and similarity-based merge/create updates in a closed-loop memory cycle (retrieve‚Äìutilize‚Äìverify‚Äìgenerate).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Toward a Unified Multimodal Semantic Hub for Cross-Task Transfer: Integrate visual and logical schemas via a central hub with adaptive routing to maximize transfer while minimizing interference across domains.<br>‚Ä¢ Uncertainty-Aware Grow-and-Refine Memory for Continual Multimodal Agents: Calibrate memory updates and merge thresholds using confidence/uncertainty estimates to reduce erroneous consolidation and catastrophic forgetting.<br>‚Ä¢ Learning Distraction Priors: End-to-End Question-Aware Visual Attention for MLLMs: Train a module that predicts task-relevant ‚Äòtrap regions‚Äô and generates attention masks from supervision on visual error patterns to further cut visual-to-logical cascades.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20937" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20937" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format. Framed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition-affordance recognition, action-effect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibit anthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of a unified, objective evaluation of embodied cognition in VLMs that tightly couples egocentric perception with embodied interaction; prior work is fragmented (static scenes, language-only planning) or subjective taxonomies.<br>‚Ä¢ Need to assess interactive, long-horizon reasoning under partial observability without confounding by photorealistic video synthesis; existing video prediction metrics conflate generative fidelity with reasoning ability.<br>‚Ä¢ Scarcity of scalable, automatically generated benchmarks grounded in physical state for everyday activities; real-world datasets lack ground-truth scene dynamics and do not scale.<br>‚Ä¢ Unmeasured anthropocentric biases in VLMs (e.g., right-handed action priors, human-like camera intrinsics/viewpoints) that affect generalization and evaluation.<br>‚Ä¢ Unknown human‚Äìmodel performance gap as interaction horizons grow, limiting guidance for model/data design.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ENACT frames embodied cognition evaluation as sequence-reordering VQA under a POMDP, using egocentric RGB observations and actions encoded as scene-graph changes, with forward (order observations given actions) and inverse (order actions given observations) tasks. A scalable pipeline on BEHAVIOR simulation extracts key-frame state changes to auto-generate thousands of QA pairs and evaluates with task and pairwise accuracy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Action-Conditioned World Models for Egocentric VLMs: Train and finetune VLMs on ENACT-style forward modeling to improve action‚Äìeffect reasoning and long-horizon memory under partial observability.<br>‚Ä¢ Debiasing Embodied Perception in Vision‚ÄìLanguage Models: Quantify and mitigate right-handedness and human-intrinsics biases via domain randomization, viewpoint/intrinsics augmentation, and balanced action datasets.<br>‚Ä¢ ENACT-Real: From Simulation to Household Robots for Embodied Decision Making: Extend ENACT to real-world egocentric robot data and couple evaluation with policy learning to study sim-to-real generalization and interactive performance.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 3</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-28 23:04:16 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 3;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>