<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - October 28, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "SF Pro Display", -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Helvetica Neue", "Arial", "Noto Sans", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", sans-serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">October 28, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23607" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23607" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Humans learn abstract concepts through multisensory synergy, and once formed, such representations can often be recalled from a single modality. Inspired by this principle, we introduce Concerto, a minimalist simulation of human concept learning for spatial cognition, combining 3D intra-modal self-distillation with 2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more coherent and informative spatial features, as demonstrated by zero-shot visualizations. It outperforms both standalone SOTA 2D and 3D self-supervised models by 14.2% and 4.8%, respectively, as well as their feature concatenation, in linear probing for 3D scene perception. With full fine-tuning, Concerto sets new SOTA results across multiple scene understanding benchmarks (e.g., 80.7% mIoU on ScanNet). We further present a variant of Concerto tailored for video-lifted point cloud spatial understanding, and a translator that linearly projects Concerto representations into CLIP's language space, enabling open-world perception. These results highlight that Concerto emerges spatial representations with superior fine-grained geometric and semantic consistency.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：单模态自监督（2D图像与3D点云）学到的空间表示互补而不重叠，简单拼接虽有提升却无法产生统一、可预测的空间概念表示<br>• 重要性：空间认知任务（自动驾驶、混合现实、机器人）需要无标注、可泛化、几何-语义一致的表示；更强的底座表示能显著提升下游3D场景理解与数据效率<br>• 现有局限：2D SSL（如DINOv2）缺乏几何意识，3D SSL（如Sonata）难捕获细粒度纹理/语义；后期特征级融合缺少训练期的跨模态协同，难以形成可由单模态召回的统一表征<br>• 现有跨模态做法多“模仿”图像特征或在推理时依赖图像，忽视跨模态互促与表示自洽；3D SSL易陷入“几何捷径”，泛化与开放词汇能力不足<br>• 证据与差距：作者实证显示2D/3D拼接优于任一单模态，但仍不及联合学习；而Concerto在ScanNet线性探测提升至77.3%mIoU（较Sonata+4.8%，较DINOv2+14.2%），表明存在更优的跨模态协同表征空间</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>Concerto以简洁的联合自监督框架，将3D点云的自蒸馏（Sonata式教师-学生、在线聚类）与2D→3D的JEPA风格联合嵌入预测相耦合：利用相机参数建立点-像素对应，用点特征预测图像补丁特征并以余弦相似度对齐，同时保持3D分支的自蒸馏稳定优化，从而在单点云推理时涌现出几何-语义一致的统一空间表示。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Native 2D–3D Joint Pretraining: 解冻图像与点云编码器，构建对称目标与协同优化，实现端到端原生多模态自监督表征空间<br>• Deep Language Grounding for 3D: 超越线性投影的文本对齐，设计细粒度/组合语义目标与多粒度对齐头，赋能开放词汇与复杂语言指令<br>• Unified Omni-Domain 3D SSL: 融合室内/户外/激光雷达/视频抬升点云的统一预训练范式与尺度定律，兼顾LoRA等轻量适配以提升跨域泛化</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ReCode: Unify Plan and Action for Universal Granularity Control</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23564" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23564" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 现实任务需要在不同粒度间灵活切换决策，但现有LLM代理缺乏“粒度自适应”能力，难以在高层规划与低层动作间流畅过渡（见第2页图1）<br>• 现有范式刚性地将规划与执行分离：ReAct只能细粒度逐步行动，Planner–Executor二段式难以动态调整粒度，导致复杂/长程任务上脆弱、泛化差（第2页与第3页相关讨论）<br>• 代理通常在固定、预定义的动作/计划空间中决策，难以为未知情境生成合适的新决策；静态计划易失效，层级/重规划方法仍未在表示上统一“计划=高层动作”（第3页）<br>• 训练数据多为扁平轨迹，缺乏可学习的层级结构与多粒度标注，限制了数据效率与层级推理能力的习得（第2页）<br>• 推理链条冗长、API调用与token开销大，成本高且易累积错误（第8页表3显示ReCode显著降低成本）</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>ReCode将“计划与行动”统一为单一代码表示：把高层计划建模为占位函数，递归地展开为子函数直至原子可执行动作，在深度优先的执行-扩展循环中动态控制决策粒度，形成层级决策树与可执行轨迹（见第2页图1(c)，第5页算法1）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 自适应粒度策略优化：用强化学习优化“何时分解、何时执行”的展开策略与最大递归深度，以效率与成功率为联合目标<br>• 轨迹驱动的层级预训练：利用ReCode生成的多粒度决策树进行自监督/指令化预训练，学习统一的层级表示与显式状态写入策略<br>• 跨模态与具身泛化的ReCode：将递归代码生成范式扩展到视觉/机器人环境，在真实感知与物理约束下评估粒度控制与鲁棒性</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">A Survey of Data Agents: Emerging Paradigm or Overstated Hype?</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23587" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23587" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapid advancement of large language models (LLMs) has spurred the emergence of data agents--autonomous systems designed to orchestrate Data + AI ecosystems for tackling complex data-related tasks. However, the term "data agent" currently suffers from terminological ambiguity and inconsistent adoption, conflating simple query responders with sophisticated autonomous architectures. This terminological ambiguity fosters mismatched user expectations, accountability challenges, and barriers to industry growth. Inspired by the SAE J3016 standard for driving automation, this survey introduces the first systematic hierarchical taxonomy for data agents, comprising six levels that delineate and trace progressive shifts in autonomy, from manual operations (L0) to a vision of generative, fully autonomous data agents (L5), thereby clarifying capability boundaries and responsibility allocation. Through this lens, we offer a structured review of existing research arranged by increasing autonomy, encompassing specialized data agents for data management, preparation, and analysis, alongside emerging efforts toward versatile, comprehensive systems with enhanced autonomy. We further analyze critical evolutionary leaps and technical gaps for advancing data agents, especially the ongoing L2-to-L3 transition, where data agents evolve from procedural execution to autonomous orchestration. Finally, we conclude with a forward-looking roadmap, envisioning the advent of proactive, generative data agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 术语混乱与能力边界不清：当前“数据智能体”一词被广泛且不一致使用，从简单问答到复杂自治系统都被称为Agent，导致能力、责任与适用范围模糊<br>• 用户期望与治理风险：概念不清引发预期错配与问责难题，出现数据泄露、合规失败或错误分析时责任难以界定，阻碍产业采纳<br>• 缺乏统一分级框架：缺少类似SAE J3016的自治分级标准，难以客观比较系统能力、明确人机分工并指导研发路线<br>• 研究与应用碎片化：现有工作多聚焦单点任务（管理/准备/分析某一环节），缺少覆盖数据全生命周期的统一视角与系统化综述<br>• 关键技术鸿沟（L2→L3）：从“程序化执行”到“自主编排”的跃迁仍未解决，存在对预定义算子/工具的依赖、有限的战略性推理、生命周期覆盖不全等局限</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出首个面向数据智能体的L0–L5自治分级体系，按人机主导权转移刻画从人工到完全自治的六级能力与责任边界；据此对数据管理、数据准备与数据分析进行分级式系统综述，解析关键演进跃迁与技术缺口（尤其L2→L3），并给出迈向L4/L5的前瞻路线图。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Toward True L3: 面向多任务的自主管线编排与优化的数据智能体<br>• Skill/Tool Autogenesis: 数据技能自动发现与可验证的工具演化机制<br>• LifecycleOps: 覆盖管理—准备—分析的一体化数据智能体体系结构<br>• Proactive L4: 面向数据湖的自主问题发现与长期自治理框架<br>• Agentic RL for Data Agents: 支持长时序与战略推理的自监督/强化学习训练范式<br>• SafeDAgent: 数据智能体的安全、可信与问责评测基准与治理协议</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">FARMER: Flow AutoRegressive Transformer over Pixels</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23588" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23588" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Directly modeling the explicit likelihood of the raw data distribution is key topic in the machine learning area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose a self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design a one-step distillation scheme to significantly accelerate inference speed and introduce a resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 连续像素自回归存在“序列极长+高维连续密度难建模”的瓶颈，训练与采样都慢且易受长程依赖影响（见图1(a)，Sec.1）<br>• 传统正态化流将复杂图像分布强行映射到各向同性高斯，分布落差大导致采样退化与质量下降，尽管具备精确似然但表达与可控性受限（见图1(b)，Sec.3.1）<br>• 像素冗余使高维token的AR建模异常困难；AF反向严格自回归导致推理成为主要瓶颈（Sec.1，Sec.3.3，Sec.3.5）<br>• 现有像JetFormer的维度因子化假设冗余通道独立且服从标准高斯，易造成信息丢失并限制与条件c的交互（Sec.3.3）</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出FARMER：以可逆自回归流(AF)将图像patch映射为潜在序列，并由因果AR Transformer用GMM建模其条件分布，从而在像素域实现端到端且具显式似然的生成；同时通过自监督通道维度划分（信息/冗余，前者逐token建模、后者共享GMM）、重采样式CFG与单步蒸馏加速AF反向，实现更易建模与更快采样。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 学习式去量化FARMER：以可学习/可逆去量化替代固定高斯噪声，兼顾显式似然与细节保真<br>• 几何一致的Log-Det正则化：针对体积变化引入几何约束与稳定化项，缓解高logdet造成的模糊与重建误差<br>• 自适应通道分组与层次共享GMM：从单一共享分布扩展到动态/层次分组，提升信息-冗余解耦与采样效率</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23581" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23581" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Audio-driven human animation models often suffer from identity drift during temporal autoregressive generation, where characters gradually lose their identity over time. One solution is to generate keyframes as intermediate temporal anchors that prevent degradation, but this requires an additional keyframe generation stage and can restrict natural motion dynamics. To address this, we propose Lookahead Anchoring, which leverages keyframes from future timesteps ahead of the current generation window, rather than within it. This transforms keyframes from fixed boundaries into directional beacons: the model continuously pursues these future anchors while responding to immediate audio cues, maintaining consistent identity through persistent guidance. This also enables self-keyframing, where the reference image serves as the lookahead target, eliminating the need for keyframe generation entirely. We find that the temporal lookahead distance naturally controls the balance between expressivity and consistency: larger distances allow for greater motion freedom, while smaller ones strengthen identity adherence. When applied to three recent human animation models, Lookahead Anchoring achieves superior lip synchronization, identity preservation, and visual quality, demonstrating improved temporal conditioning across several different architectures. Video results are available at the following link: https://lookahead-anchoring.github.io.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：分段自回归的音频驱动人像视频在长时生成中出现身份漂移与质量退化，小误差随段落累积，唇形同步与细节逐步变差；而DiT的二次复杂度使单次只能生成短窗（≈5s），不得不分段，漂移更显著。<br>• 冲突根源：现有“起始帧（硬约束）+参考图像（软引导）”的双条件相互冲突，模型往往优先满足起始帧的一致性而逐步忽视参考身份，导致外观偏离。<br>• 既有方案局限：关键帧插值法将关键帧当作段落边界的硬约束，需额外关键帧生成器、表达受限且最终质量受关键帧上限制约；纯推理时窗/噪声技巧或训练法多缺乏身份保持机制，长时一致性改善有限。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出Lookahead Anchoring：将关键帧放在当前生成窗口之外的未来D帧处，作为持续“追随”的方向性锚点，用音频驱动当下运动、用远期锚点维持身份。具体在视频DiT中追加一帧干净的条件潜变量并赋予远期时间位置编码，配合跨内外窗的距离随机采样微调与“自关键帧”（直接以参考图像为远期目标），无需单独关键帧模型即可在长视频中兼顾身份一致性与表达性。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 自适应前视距离调度的长视频人像生成：在线估计唇形同步与身份一致性信号，动态调整D与锚点强度，实现表达性—一致性的自动权衡。<br>• 多锚点前视与叙事可控的人像动画：串联/并行使用经文本编辑生成的多张锚点图，学习多锚融合与时序切换策略，实现情绪/姿态/场景的叙事驱动。<br>• 距离感知时间位置编码与影响衰减建模：设计连续化、可学习的距离—影响函数（位置编码/注意力调制/损失约束），系统刻画锚点随时间距离的软引导强度并提升泛化。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21817" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21817" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current Vision-Language-Action (VLA) models are often constrained by a rigid, static interaction paradigm, which lacks the ability to see, hear, speak, and act concurrently as well as handle real-time user interruptions dynamically. This hinders seamless embodied collaboration, resulting in an inflexible and unresponsive user experience. To address these limitations, we introduce VITA-E, a novel embodied interaction framework designed for both behavioral concurrency and nearly real-time interruption. The core of our approach is a dual-model architecture where two parallel VLA instances operate as an ``Active Model'' and a ``Standby Model'', allowing the embodied agent to observe its environment, listen to user speech, provide verbal responses, and execute actions, all concurrently and interruptibly, mimicking human-like multitasking capabilities. We further propose a ``model-as-controller'' paradigm, where we fine-tune the VLM to generate special tokens that serve as direct system-level commands, coupling the model's reasoning with the system's behavior. Experiments conducted on a physical humanoid platform demonstrate that VITA-E can reliably handle complex interactive scenarios. Our framework is compatible with various dual-system VLA models, achieving an extremely high success rate on emergency stops and speech interruptions while also successfully performing concurrent speech and action. This represents a significant step towards more natural and capable embodied assistants.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 现有VLA多采用静态、顺序式交互范式，缺乏“同时看-听-说-做”的并发能力，导致协作低效且不自然（第2页三大限制）<br>• 不可中断：一旦开始说话或执行动作即被锁定，无法被用户打断或紧急制动，难以适应实时变化（第2页）<br>• 交互僵化：通常假设一次性、静态指令，必须等当前原子动作/推理完成后才能处理新指令，响应延迟高（第4页相关工作）<br>• 架构与训练权衡：端到端方法易削弱VLM理解与推理；双系统方法多忽略交互动态；逐步耦合语言命令的方法限制可用VLM规模与性能（第3–4页）<br>• 缺少统一的系统级控制接口，将高层推理与系统行为紧密耦合以实现可抢占、安全的控制（表1第6页揭示所需控制token语义）</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出VITA-E：通过双模型并行架构（Active/Standby）与“模型即控制器”范式，令VLM生成特殊控制token（如[RES]、[ACT]、[INST]、[HALT]、[END]）直接驱动系统在“听-行动”状态间切换，并与扩散式动作专家协同，从而实现并发、抢占与急停（见图3第5页、表1第6页；图4第7页示四种交互模式）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Hierarchical VITA-E for Long-Horizon Embodied Tasks：以层级规划将长任务分解为可中断的原子步骤，并结合在线人类反馈稳健执行<br>• Resource-Efficient Dual-Model via Weight Sharing and Distillation：通过权重共享、异步调度与知识蒸馏，降低双模型算力与时延开销而不牺牲抢占能力<br>• Safe and Smooth Task Switching without Neutral Retraction：以轨迹接续与约束优化替代回中立位撤退，实现更顺滑的任务切换并给出形式化安全/时延保证</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">ACG: Action Coherence Guidance for Flow-based VLA models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22201" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22201" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion and flow matching models have emerged as powerful robot policies, enabling Vision-Language-Action (VLA) models to generalize across diverse scenes and instructions. Yet, when trained via imitation learning, their high generative capacity makes them sensitive to noise in human demonstrations: jerks, pauses, and jitter which reduce action coherence. Reduced action coherence causes instability and trajectory drift during deployment, failures that are catastrophic in fine-grained manipulation where precision is crucial. In this paper, we present Action Coherence Guidance (ACG) for VLA models, a training-free test-time guidance algorithm that improves action coherence and thereby yields performance gains. Evaluated on RoboCasa, DexMimicGen, and real-world SO-101 tasks, ACG consistently improves action coherence and boosts success rates across diverse manipulation tasks. Code and project page are available at https://github.com/DAVIAN-Robotics/ACG and https://DAVIAN-Robotics.github.io/ACG , respectively.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：模仿学习下的扩散/流匹配VLA模型会“记忆”人类演示中的抖动、停顿、抖颤等噪声，导致动作序列时序不一致（动作连贯性下降）。<br>• 重要性：动作不连贯会在执行中造成两类失败——关键时刻不稳定（如抓取时抖动推开物体）与小噪声累积引发轨迹漂移；在按钮按压、插入等精细操作任务中尤为致命。<br>• 现有方法局限：<br> - 时域平滑/多次采样集成仅带来温和提升，易模糊细粒度动作细节，牺牲准确性；<br> - 动作分块/递进视界可减小累积误差，但无法消除分块内（intra-chunk）不连贯；对流策略做时序集成推理开销大；<br> - 语言条件的无分类器引导（CFG）在VLA中易不稳定/过度强化文本条件，与提升连贯性并不一致，甚至损伤样本多样性；<br> - 基于噪声的负向扰动（如白噪声）存在“连贯—准确”权衡：噪声过小无效、过大破坏已学先验。<br>• 需求缺口：缺少一种训练外、与语言无关、低开销且直接面向时序连贯性的推理期引导方法，能在不再训练的前提下提升动作连贯性与成功率。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出动作连贯性引导（ACG）：在推理期构造“故意不连贯”的向量场（将自注意力的注意力图替换为单位矩阵，切断时间步通信），再用引导向量v_ACG=(1+λ)v−λv_IC沿相反方向采样，促使生成的动作分块更连贯且无需再训练。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 联合分块内外一致性的自引导流策略：将ACG与跨分块一致性引导（如Self-GAD）联合，自适应权重平衡内外连贯性<br>• 自适应层选择的注意力扰动引导：在线学习选择最有效的自注意力层与扰动强度λ，实现任务与场景自适应<br>• 面向安全与接触的连贯性引导：引入力/触觉与安全约束，将接触稳定性纳入引导目标，降低精细操作失误<br>• ACG-RL：基于强化信号的引导强度元学习：用稀疏成功奖励微调λ与扰动策略，无需重训主体模型<br>• 矩形流引导的理论分析：刻画ACG对向量场偏置、稳定性与收敛性的影响，给出连贯性与性能保证<br>• 超越注意力的“坏模型”构造：探索丢弃单元、时间洗牌、跨注意力屏蔽、视觉token扰动等多种负向变体<br>• 实时高效ACG：利用中后层复用与部分前向，降至<1.2×推理开销以满足实时控制</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Open Multimodal Retrieval-Augmented Factual Image Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22521" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22521" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Multimodal Models (LMMs) have achieved remarkable progress in generating photorealistic and prompt-aligned images, but they often produce outputs that contradict verifiable knowledge, especially when prompts involve fine-grained attributes or time-sensitive events. Conventional retrieval-augmented approaches attempt to address this issue by introducing external information, yet they are fundamentally incapable of grounding generation in accurate and evolving knowledge due to their reliance on static sources and shallow evidence integration. To bridge this gap, we introduce ORIG, an agentic open multimodal retrieval-augmented framework for Factual Image Generation (FIG), a new task that requires both visual realism and factual grounding. ORIG iteratively retrieves and filters multimodal evidence from the web and incrementally integrates the refined knowledge into enriched prompts to guide generation. To support systematic evaluation, we build FIG-Eval, a benchmark spanning ten categories across perceptual, compositional, and temporal dimensions. Experiments demonstrate that ORIG substantially improves factual consistency and overall image quality over strong baselines, highlighting the potential of open multimodal retrieval for factual image generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 现有LMM/扩散模型在图像生成中易出现事实不一致与幻觉，尤其在细粒度属性（尺寸、材质、相对比例）与时间敏感事件上偏差显著（见第1页图1）。<br>• 仅依赖模型参数记忆或封闭/静态检索库的RAG方法无法捕捉最新事实，且多为单模态或浅层证据集成，难以将外部知识转化为可控的生成信号。<br>• 事实图像生成需要文本+图像的互补证据：文本提供属性/关系，图像提供外观/比例/空间配置，但现有方法往往分离处理两种模态，导致难以联合对齐与控制。<br>• 缺乏系统化评测事实一致性的基准，现有评测多聚焦质量/对齐/组合，忽视对感知、组合与时间三维事实一致性的量化；本文据此提出FIG-Eval基准（第5页表1、第7页表3）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出ORIG：一种面向开放网络的多轮“文本+图像”联动检索-筛选-累积-扩展式提示构造框架，通过自适应子问题规划、跨模态粗到细过滤与多模态要素抽取，将精炼证据注入增强提示并与参考图联合引导生成，从而提升感知、组合与时间三维的事实一致性（第4页图2）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• End-to-End ORIG with Reinforcement Learning：以事实一致性得分为回报，端到端联合优化检索规划、证据选择与生成器控制，减少冗余检索并增强可控性。<br>• Streaming-FIG：面向流式新闻/社媒的时间感知事实图像生成，构建可增量更新的多模态知识库以处理实体状态与事件演化。<br>• Region-Level Evidence Grounding for FIG：引入区域级跨模态对齐与注意力控制，将文本属性与参考图像区域精确映射到生成过程，实现细粒度外观与关系的可控合成。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">E^2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22733" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22733" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Text embedding models serve as a fundamental component in real-world search applications. By mapping queries and documents into a shared embedding space, they deliver competitive retrieval performance with high efficiency. However, their ranking fidelity remains limited compared to dedicated rerankers, especially recent LLM-based listwise rerankers, which capture fine-grained query-document and document-document interactions. In this paper, we propose a simple yet effective unified framework E^2Rank, means Efficient Embedding-based Ranking (also means Embedding-to-Rank), which extends a single text embedding model to perform both high-quality retrieval and listwise reranking through continued training under a listwise ranking objective, thereby achieving strong effectiveness with remarkable efficiency. By applying cosine similarity between the query and document embeddings as a unified ranking function, the listwise ranking prompt, which is constructed from the original query and its candidate documents, serves as an enhanced query enriched with signals from the top-K documents, akin to pseudo-relevance feedback (PRF) in traditional retrieval models. This design preserves the efficiency and representational quality of the base embedding model while significantly improving its reranking performance. Empirically, E^2Rank achieves state-of-the-art results on the BEIR reranking benchmark and demonstrates competitive performance on the reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also show that the ranking training process improves embedding performance on the MTEB benchmark. Our findings indicate that a single embedding model can effectively unify retrieval and reranking, offering both computational efficiency and competitive ranking accuracy.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 嵌入式检索器虽高效，但排名保真度不及LLM列表式重排器，难以捕捉细粒度的查询-文档与文档-文档交互，存在显著性能差距（如BEIR基准）。<br>• 现有LLM列表式重排推理昂贵、时延高：需一次性长上下文编码并自回归生成排序，prefill与解码均开销大、难以并行，难落地实时场景。<br>• 两阶段“检索+重排”体系评分函数不统一、文档向量难复用，系统复杂度与成本偏高。<br>• 生成式解码并非排序所必需，关键在于列表上下文的交互信号；但如何将其注入嵌入空间并用统一余弦相似度打分仍未解决。<br>• 伪相关反馈（PRF）在密集检索中有效，但鲁棒性与适用性受限，尚未在LLM列表式重排框架中被系统化、统一地利用。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>E2RANK将“查询+Top-K候选文档”的列表式提示视作PRF增强查询，取其[EOS]位置向量与各文档向量做余弦相似度，统一完成检索与重排。训练采用“两阶段+多任务”：先用InfoNCE对比学习获得强嵌入，再引入基于LLM标注对的RankNet损失对列表式提示进行优化，将文档-文档/查询-文档交互注入嵌入空间。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• AutoPRF-Rank：自适应选择PRF文档与动态K的列表式提示优化；结合不确定性估计/强化学习，在固定时延预算下提升重排效果。<br>• TokenBudget-PRF：面向低时延的令牌预算PRF压缩与提示编排；通过文档摘要/表示池化/可学习压缩器在有限上下文内最大化交互信息密度。<br>• Click2Rank：基于真实点击/停留等交互日志的在线E2RANK增量学习；利用成对/列表偏好信号与延迟约束实现持续优化与领域自适应。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22706" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22706" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Humans naturally perceive the geometric structure and semantic content of a 3D world as intertwined dimensions, enabling coherent and accurate understanding of complex scenes. However, most prior approaches prioritize training large geometry models for low-level 3D reconstruction and treat high-level spatial understanding in isolation, overlooking the crucial interplay between these two fundamental aspects of 3D-scene analysis, thereby limiting generalization and leading to poor performance in downstream 3D understanding tasks. Recent attempts have mitigated this issue by simply aligning 3D models with specific language models, thus restricting perception to the aligned model's capacity and limiting adaptability to downstream tasks. In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an end-to-end large unified transformer to unify the knowledge for both spatial reconstruction and instance-level contextual understanding. Specifically, we design a 3D-Consistent Contrastive Learning strategy that guides IGGT to encode a unified representation with geometric structures and instance-grounded clustering through only 2D visual inputs. This representation supports consistent lifting of 2D visual inputs into a coherent 3D scene with explicitly distinct object instances. To facilitate this task, we further construct InsScene-15K, a large-scale dataset with high-quality RGB images, poses, depth maps, and 3D-consistent instance-level mask annotations with a novel data curation pipeline.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 统一表征缺失：现有“几何重建→语义理解”的分段流水线彼此割裂，误差在阶段间传播，无法让几何与语义相互增强，限制下游任务（机器人、AR/VR、规划、空间QA）。<br>• VLM强耦合弊端：将几何特征强行对齐到特定VLM会过度平滑，损伤高频几何细节与多视角一致性。<br>• 难以升级与泛化：与单一VLM架构强耦合，难以即插即用集成更强的新模型（如CLIP、SigLIP），限制适配性与可扩展性。<br>• 实例级区分不足：类别级对齐难以分辨同类不同实例，影响跨视角实例跟踪与空间问答等高层任务。<br>• 数据瓶颈：缺乏大规模、高质量、跨视角3D一致的实例级掩码标注，难以有效监督联合学习。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出IGGT：基于大型统一Transformer联合训练几何头与实例头，从多视图RGB端到端预测相机、深度/点图与3D一致的实例特征，并通过窗口移位的交叉注意力将细粒度几何结构注入实例表征；辅以3D一致的多视图对比学习约束实例特征、无监督聚类生成实例掩码，并以“实例掩码为桥”即插即用连接VLM/LMM，实现开放词汇分割与QA场景定位，同时构建InsScene-15K提供高质量监督。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Detr-IGGT: 可学习实例头替代聚类的统一几何-语义Transformer：以DETR式实例查询替换HDBSCAN，实现端到端、边界更精确的实例分割/跟踪。<br>• IGGT-SSL: 基于3D一致性的自监督实例特征学习：利用跨视角一致性与伪标签进行无/弱标注训练，降低对大规模实例标注的依赖。<br>• Open-World IGGT: 面向室外与大尺度场景的实例一致三维重建：扩展至户外/城市级数据，提升尺度鲁棒性与长序列多目标跟踪。<br>• GAUSS-IGGT: 将实例感知几何蒸馏进3DGS/NeRF辐射场：融合可渲染与可查询的实例级语义-几何场，支持高质量新视角与开放词汇检索。<br>• Plug-and-Play IGGT-VLM Benchmark: 跨VLM实例桥接的系统评测：构建统一协议评测CLIP/SigLIP/OpenSeg等在实例桥接下的开放词汇性能与一致性。<br>• Streaming-IGGT: 在线多视图增量重建与实例跟踪：面向机器人/AR实时场景，支持在线相机估计、几何更新与实例持续跟踪/重识别。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23451" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23451" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reward models (RMs) play a critical role in aligning AI behaviors with human preferences, yet they face two fundamental challenges: (1) Modality Imbalance, where most RMs are mainly focused on text and image modalities, offering limited support for video, audio, and other modalities; and (2) Preference Rigidity, where training on fixed binary preference pairs fails to capture the complexity and diversity of personalized preferences. To address the above challenges, we propose Omni-Reward, a step toward generalist omni-modal reward modeling with support for free-form preferences, consisting of: (1) Evaluation: We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form preferences, covering nine tasks across five modalities including text, image, video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal preference dataset comprising 248K general preference pairs and 69K instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We propose Omni-RewardModel, which includes both discriminative and generative RMs, and achieves strong performance on Omni-RewardBench as well as other widely used reward modeling benchmarks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 模态不均衡：现有奖励模型主要面向文本与图像，难以覆盖视频、音频、3D等弱资源模态，无法为任意输入—任意输出的全模态模型提供一致对齐与评测（见摘要与第1节）。<br>• 偏好刻板：主流训练依赖固定的二元偏好对，难以表达与适配用户“自由表述”的多维评价准则与个性化偏好，导致对齐灵活性与可解释性不足（第1节）。<br>• 评测缺口：多数基准集中于单一/少数模态与固定标准，较少支持包含“平局（tie）”与自由准则的系统性评测，难以客观衡量RM在复杂多模态任务中的判别与泛化能力（第2节）。<br>• 可解释性与通用性不足：现有判别式RM打分过程不透明、诊断困难；专用RM跨任务/跨模态迁移能力有限（摘要与第3节）。<br>• 重要性：RLHF对RM高度依赖；全模态交互场景（如语音指令、图像编辑、视频生成）对“有用—无害—可信”的对齐需求强烈（第1节）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出Omni-Reward统一框架：构建覆盖5模态9任务、带自由表述准则与含Tie设置的Omni-RewardBench；汇集317K多模态偏好对（含69K指令调优）的Omni-RewardData；训练两类通用RM——基于Bradley–Terry的判别式RM与基于GRPO强化学习、可生成“评语+偏好”的可解释生成式RM。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Omni-RewardBench++：面向多轮对话的百万级自由偏好多模态评测与Tie校准——扩展规模与任务细粒度，纳入对话上下文与更稳健的平局判定。<br>• R1-Critic：具备因果证据与过程监督的可解释生成式奖励建模——在生成式RM中引入因果链路与证据追踪，结合过程监督提升可解释性与鲁棒性。<br>• Persona-RM：基于系统提示泛化的个性化与可控偏好对齐——将用户画像/系统消息泛化与偏好控制结合，实现按需动态调节与跨模态个性化对齐。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Knocking-Heads Attention</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23052" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23052" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multi-head attention (MHA) has become the cornerstone of modern large language models, enhancing representational capacity through parallel attention heads. However, increasing the number of heads inherently weakens individual head capacity, and existing attention mechanisms - whether standard MHA or its variants like grouped-query attention (GQA) and grouped-tied attention (GTA) - simply concatenate outputs from isolated heads without strong interaction. To address this limitation, we propose knocking-heads attention (KHA), which enables attention heads to "knock" on each other - facilitating cross-head feature-level interactions before the scaled dot-product attention. This is achieved by applying a shared, diagonally-initialized projection matrix across all heads. The diagonal initialization preserves head-specific specialization at the start of training while allowing the model to progressively learn integrated cross-head representations. KHA adds only minimal parameters and FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention variants. We validate KHA by training a 6.1B parameter MoE model (1.01B activated) on 1T high-quality tokens. Compared to baseline attention mechanisms, KHA brings superior and more stable training dynamics, achieving better performance across downstream tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 多头注意力各头彼此独立，只做拼接缺少强交互；增加头数会削弱单头维度与表达，形成低秩瓶颈与冗余（第1-2页）<br>• 现有talking-heads在注意力矩阵上做变换，计算/内存开销大且与FlashAttention不兼容；协作/混合头等方法或牺牲头专化、或交互有限且训练复杂（第2页，第13页表6）<br>• GQA/MQA/GTA虽优化KV缓存，但本质仍缺跨头特征级通信，导致效率与效果难兼得（第2、6-7页表3）<br>• 预训练中存在loss spikes，需要一种具有隐式正则、能稳定训练且增量极小的机制（第3、7-9页图1、图2）<br>• 期望方案可即插即用、通用于多种注意力变体，参数/FLOPs增量<1%，并能在推理时无额外开销（第3-5页，复杂度分析第5页）</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出Knocking-Heads Attention：在WQ/WK/WV之后、注意力计算之前对所有头施加共享投影（线性或对V的门控MLP），并以对角初始化使其初期近似恒等、训练中逐步学习跨头特征交互，其中对V的共享变换最关键。线性版可在推理时吸收到原投影矩阵中，总体参数与FLOPs增量<1%，兼容MHA/GQA/GTA/MQA/MLA与FlashAttention。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 自适应跨头交互调度与结构搜索：基于层/头粒度自动化搜索与软/硬约束，按训练阶段动态调节对角与非对角强度<br>• 面向高效推理的KHA融合与量化：编译器级融合吸收、与KV缓存/FlashAttention协同的低比特量化与蒸馏<br>• 将KHA拓展至MoE与多模态Transformer：结合专家路由的动态跨头交互，并在视觉/多模态任务上验证与理论化分析</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23603" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23603" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multimodal large language models (MLLMs) have demonstrated strong general-purpose capabilities in open-world visual comprehension. However, most existing MLLMs primarily focus on holistic, scene-level understanding, often overlooking the need for fine-grained, object-centric reasoning. In this paper, we present PixelRefer, a unified region-level MLLM framework that enables advanced fine-grained understanding over user-specified regions across both images and videos. Motivated by the observation that LLM attention predominantly focuses on object-level tokens, we propose a Scale-Adaptive Object Tokenizer (SAOT) to generate compact and semantically rich object representations from free-form regions. Our analysis reveals that global visual tokens contribute mainly in early LLM layers, inspiring the design of PixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion module to pre-fuse global context into object tokens. This yields a lightweight Object-Only Framework that substantially reduces computational cost while maintaining high semantic fidelity. To facilitate fine-grained instruction tuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction dataset. Extensive experiments across a range of benchmarks validate that PixelRefer achieves leading performance with fewer training samples, while PixelRefer-Lite offers competitive accuracy with notable gains in efficiency.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 现有MLLM多做整体场景理解，缺乏对任意粒度（对象/部件）与时空一致的精细化对象级推理能力，难以支持HCI、具身智能、医疗与遥感等高要求应用（见第1–2页，图1）。<br>• 代表性区域级方法存在局限：SoM标记易歧义且低效；DAM需重复编码且偏单对象描述；PAM多限于描述、缺乏复杂推理并依赖语义缺失的SAM2中间特征、需巨量数据，且任务特定架构削弱通用性（第2页）。<br>• 小目标与尺度变化导致掩码区域经patch化后特征不稳；LLM对全局视觉token的注意主要集中在浅层，深层几乎由对象token主导，然而全局token极耗算力与显存（第4–6页，图3、图4）。<br>• 缺少覆盖对象级识别、描述与多轮/多对象时空QA的高质量指令数据与统一评测，限制模型对复杂视频推理的提升（第10–11页，图8）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出PixelRefer统一框架（第6页图5），以尺度自适应对象分词器SAOT（第7页图6）将任意像素级区域动态缩放、位置编码与聚类压冗后生成紧凑且语义丰富的对象token，并与全局视觉token联合推理；同时给出高效变体PixelRefer‑Lite，通过对象中心注入OCI模块在LLM前分两步注入局部与全局上下文，仅以对象token解码（第9页算法1）。此外构建PixelRefer‑2.2M并采用“两阶段”训练以完成细粒度对齐与指令跟随（第10页图8）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 自适应对象Token预算与早层视觉压缩用于长视频推理：结合层间视觉裁剪与动态token分配，进一步降低时空推理开销、保持语义完整。<br>• 面向多对象交互的时空因果推理框架：引入关系图建模与反事实数据增强，提升交互、因果与未来预测等复杂视频问答能力。<br>• 无监督对象分词器的对齐学习：摆脱外部分割依赖，利用时空一致性与自蒸馏学习高质量对象token，增强跨域与小目标鲁棒性。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23393" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23393" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The application of Reinforcement Learning with Verifiable Rewards (RLVR) to mathematical and coding domains has demonstrated significant improvements in the reasoning and problem-solving abilities of Large Language Models. Despite its success in single generation problem solving, the reinforcement learning fine-tuning process may harm the model's exploration ability, as reflected in decreased diversity of generations and a resulting degradation of performance during Best-of-N sampling for large N values. In this work, we focus on optimizing the max@k metric, a continuous generalization of pass@k. We derive an unbiased on-policy gradient estimate for direct optimization of this metric. Furthermore, we extend our derivations to the off-policy updates, a common element in modern RLVR algorithms, that allows better sample efficiency. Empirically, we show that our objective effectively optimizes max@k metric in off-policy scenarios, aligning the model with the Best-of-N inference strategy.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题1：RLVR（可验证奖励的强化学习）虽能显著提升单次生成（pass@1），却常导致多样性下降，进而在大N的Best-of-N（BoN）采样下出现性能劣化（高k的pass@k下降）。实证依据见图1（第3页，低k提升而高k下降）与图2（第3页，熵分布向0偏移显示置信度上升但多样性下降）。<br>• 关键问题2：二元奖励（全测例全过=1，否则=0）稀疏且难优化，易损伤能力；相对地，连续奖励更稳定高效但仍可能牺牲高k性能。表1（第2页）显示：二元奖励训练显著退化，连续奖励能提升pass@1但略降pass@128，提示需直接对齐推理时的BoN目标。<br>• 关键问题3：现有“推理感知”方法多局限于on-policy且多为二元奖励设定，难以兼容现代RLVR中的off-policy更新（如PPO/GRPO），并存在样本效率与稳定性不足的问题。需要一种既支持连续奖励、又能在off-policy下优化、且与BoN一致的训练目标。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出直接优化max@k（pass@k的连续推广）的策略梯度目标：在on-policy下给出无偏梯度估计并化为可计算的“奖励重加权”，在off-policy下基于重要性比率ρ≈1进行一阶近似，得到高效的近似梯度（权重校正）。该目标无缝集成于GRPO/PPO式更新，将训练与BoN推理对齐并显著提升大k的max@k。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 从BoN到多聚合：联合优化max@k与多数投票的一致性训练：将多数投票等聚合函数纳入统一目标，拓展max@k到更一般的推理策略对齐。<br>• 数学推理的连续可验证奖励：面向max@k的一致性RLVR：为数学领域设计连续可验证指标，并用max@k优化对齐BoN与步骤验证。<br>• 动态k与目标调度的推理感知RL：从max@k到max@1的课程式训练：随训练进程自适应调整k与目标，平衡探索（高k）与利用（k=1）。<br>• 单样本-多样本分布对齐：缩小一次生成与Best-of-N最优之间的差距：通过分布匹配/蒸馏，使单次生成更接近BoN的最优候选。<br>• 条件式BoN生成与迭代改写：利用先前样本进行条件采样提升多样性：在训练与推理中引入条件化与重采样机制，加强多样性与可修正性。<br>• 稀疏-稠密奖励混合优势估计：二元通过与测例通过率的多粒度融合：设计混合优势与基线，兼顾稳定性与对最终通过率的敏感性。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22946" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22946" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Unified multimodal models have recently shown remarkable gains in both capability and versatility, yet most leading systems are still trained from scratch and require substantial computational resources. In this paper, we show that competitive performance can be obtained far more efficiently by strategically fusing publicly available models specialized for either generation or understanding. Our key design is to retain the original blocks while additionally interleaving multimodal self-attention blocks throughout the networks. This double fusion mechanism (1) effectively enables rich multi-modal fusion while largely preserving the original strengths of the base models, and (2) catalyzes synergistic fusion of high-level semantic representations from the understanding encoder with low-level spatial signals from the generation encoder. By training with only ~ 35B tokens, this approach achieves strong results across multiple benchmarks: 0.91 on GenEval for compositional text-to-image generation, 82.16 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By fully releasing the entire suite of code, model weights, and datasets, we hope to support future research on unified multimodal modeling.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 统一多模态模型（UMM）虽表现强，但主流做法需从零训练、耗费巨大算力与海量数据，缺乏高效复用现有开源模型的路径（见图1，第2页，展示了在较少token下追平/超越的目标）。<br>• 单栈统一训练将自回归与扩散目标混训，存在优化冲突；后续“理解-生成”双通路虽缓解冲突，但仍依赖大规模训练与算力（第1–3页相关工作）。<br>• 浅融合/轻连接器仅使用理解分支末层表征条件生成，信息被压缩、跨模态交互不足，导致任务泛化与编辑精度受限（对比图5，第9页，深融合优于浅融合）。<br>• 需要在不破坏预训练能力的前提下，实现理解端高层语义（ViT）与生成端低层细节（VAE）的早期、深度、连续交互，同时保留原模型结构与能力（架构图2，第4页）。<br>• 公共多源数据质量参差、任务配比失衡，影响生成遵循与编辑一致性，亟需高质量、任务均衡且多样的UMM调优数据（第4页数据集与第5页表1的多任务对比）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出“双重融合”框架：保留并行的VLM（Qwen2.5‑VL‑7B，理解）与DiT（Wan2.2‑TI2V‑5B，生成）原始结构，在每层后插入零初始化的多模态自注意力块，采用广义因果注意力让文本/ViT/VAE三类token自早至深持续交互，同时冻结理解分支以保留推理能力。结合NaViT尺度策略与阶段化数据配比，仅∼35B seen tokens即在GenEval/DPG/编辑基准上取得强结果（见图1与表2–表5）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 自适应深度双重融合：基于路由/门控的层级选择与条件注入学习，实现按任务与token自适应决定何层、以何强度融合，提升泛化与效率。<br>• 统一多粒度条件编辑：在ViT+VAE双通路上引入显式空间控制（掩码/深度/分割/姿态），实现结构可控、细节可保真的通用编辑器。<br>• 时空统一UMM扩展：将双重融合推广至视频/音频-文本任意到任意生成，结合时序注意力与时空因果VAE，实现高效的跨模态时空理解与生成。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">LongCat-Video Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22200" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22200" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Video generation is a critical pathway toward world models, with efficient long video inference as a key capability. Toward this end, we introduce LongCat-Video, a foundational video generation model with 13.6B parameters, delivering strong performance across multiple video generation tasks. It particularly excels in efficient and high-quality long video generation, representing our first step toward world models. Key features include: Unified architecture for multiple tasks: Built on the Diffusion Transformer (DiT) framework, LongCat-Video supports Text-to-Video, Image-to-Video, and Video-Continuation tasks with a single model; Long video generation: Pretraining on Video-Continuation tasks enables LongCat-Video to maintain high quality and temporal coherence in the generation of minutes-long videos; Efficient inference: LongCat-Video generates 720p, 30fps videos within minutes by employing a coarse-to-fine generation strategy along both the temporal and spatial axes. Block Sparse Attention further enhances efficiency, particularly at high resolutions; Strong performance with multi-reward RLHF: Multi-reward RLHF training enables LongCat-Video to achieve performance on par with the latest closed-source and leading open-source models. Code and model weights are publicly available to accelerate progress in the field.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：如何在分钟级时长上保持高质量与时间一致性的长视频生成，避免颜色漂移与质量衰减等误差累积问题（见摘要与第1节）<br>• 关键问题：不同应用场景需要T2V、I2V、视频续写等多任务能力，但现有方法往往分散、难以统一建模与高效训练（第1节“统一架构”）<br>• 关键问题：高分辨率与高帧率导致注意力计算随时空token数二次增长，推理成本与时延难以接受（第1节“高效推理”）<br>• 重要性：长视频生成是通往世界模型的关键路径，支撑数字人、具身智能、自动驾驶等复杂场景（第1节）<br>• 现有局限：多数方法需额外微调才能改善长时一致性；对齐手段单一，难同时优化画质、运动、指令遵循等多维指标；推理阶段缺乏系统性的降本增效方案（第1节）</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出LongCat-Video：基于Diffusion Transformer的13.6B统一视频生成模型，以“视频续写”预训练获得长时一致性，在同一模型内原生支持T2V/I2V/续写。推理采用时空双轴的粗到细生成与块稀疏注意力（配合高分辨率LoRA专家），并用多奖励GRPO-RLHF进行人类偏好对齐，实现720p@30fps的高效分钟级视频生成。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• LongCat-Video-Memory: 基于层级记忆与规划的超长视频生成，显式建模跨段记忆以进一步抑制误差累积<br>• GRPO-PhysAware: 面向物理一致性与人类偏好的多模态多奖励学习，联合运动物理、审美与指令遵循的可学习奖励模型<br>• Sparse4K-Video: 面向4K/60fps长视频的可扩展块稀疏注意力与上下文并行推理框架，提升超高分辨率与超长时长的效率与稳定性</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LimRank: Less is More for Reasoning-Intensive Information Reranking</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23544" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23544" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Existing approaches typically rely on large-scale fine-tuning to adapt LLMs for information reranking tasks, which is computationally expensive. In this work, we demonstrate that modern LLMs can be effectively adapted using only minimal, high-quality supervision. To enable this, we design LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating diverse, challenging, and realistic reranking examples. Using this synthetic data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and FollowIR for instruction-following retrieval. Our experiments demonstrate that LIMRANK achieves competitive performance, while being trained on less than 5% of the data typically used in prior work. Further ablation studies demonstrate the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization capabilities of LIMRANK across downstream tasks, including scientific literature search and retrieval-augmented generation for knowledge-intensive problem solving.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：现有LLM重排序器在推理密集型检索中，当相关性依赖多步推理、隐式关系与上下文敏感性时，基于表层语义相似的匹配易失效（见p.1图1上部）。<br>• 重要性：真实检索需求涵盖复杂推理与指令跟随（如BRIGHT与FOLLOWIR），并直接影响科学文献检索与RAG等应用的效果；文中在GPQA-RAG与LitSearch上的结果显示提升具有实际价值（见p.5表3）。<br>• 现有方法局限：多依赖大规模监督微调或复杂训练（如大量推理轨迹、RL等），成本高、数据依赖重；常用训练集（如MS MARCO）查询简单、缺乏多域与硬负例，难以激活LLM潜在推理能力。论文主张以小而精的高质量数据替代海量数据（摘要与p.2–3的合成数据指南、p.4表2对比）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出LIMRANK-SYNTHESIZER：按“领域多样性—真实对齐—难度多样性”指南，用Persona驱动生成日常/专家查询，结合CoT先产出正例材料再合成硬负例，并以DeepSeek-R1进行相关性判定过滤，得到小而精的训练集。基于该数据，对Qwen2.5-7B进行轻量微调获得点式重排序器LIMRANK，在使用不到既有方法5%数据的条件下于BRIGHT与FOLLOWIR上取得7B级领先表现（p.4表2、p.5表3）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• AutoLIM：自动化的推理型重排序数据生成与验证框架——以一致性校验/判别器替代人工审核，降低成本并提升可扩展性。<br>• Listwise-LIMRANK：将“少即是多”的高质数据迁移到列表式/集合式重排序——系统比较不同结构在推理密集场景中的收益。<br>• Intent-Aware LIMRANK：面向查询意图的自适应相关性建模——区分直接/间接相关与稀疏相关，动态分配推理与评分策略以提升稳健性。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Code Aesthetics with Agentic Reward Feedback</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23272" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23272" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) have become valuable assistants for developers in code-related tasks. While LLMs excel at traditional programming tasks such as code generation and bug fixing, they struggle with visually-oriented coding tasks, often producing suboptimal aesthetics. In this paper, we introduce a new pipeline to enhance the aesthetic quality of LLM-generated code. We first construct AesCode-358K, a large-scale instruction-tuning dataset focused on code aesthetics. Next, we propose agentic reward feedback, a multi-agent system that evaluates executability, static aesthetics, and interactive aesthetics. Building on this, we develop GRPO-AR, which integrates these signals into the GRPO algorithm for joint optimization of functionality and code aesthetics. Finally, we develop OpenDesign, a benchmark for assessing code aesthetics. Experimental results show that combining supervised fine-tuning on AesCode-358K with reinforcement learning using agentic reward feedback significantly improves performance on OpenDesign and also enhances results on existing benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o and GPT-4.1, and achieves performance comparable to large open-source models with 480B-685B parameters, underscoring the effectiveness of our approach.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• LLM 在可视化编程（网页/图表）上缺乏“审美意识”，常出现遮挡、配色不佳、结构混乱等，影响可读性与用户体验<br>• 现有训练多依赖文本模态奖励（可执行性/单元测试/过程奖励/文本偏好），无法评估渲染后的视觉美感，更无法覆盖交互性<br>• 缺少面向“代码美学”的大规模指令数据与权威评测基准，训练与评估不系统<br>• 仅靠 SFT 易记忆难泛化，难以在开放指令和复杂交互中兼顾功能正确性与审美质量</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出“Agentic Reward Feedback”多智能体奖励系统（执行代理、静态美学代理、交互美学代理），将多源信号加权聚合并结合 GRPO 形成 GRPO-AR 强化学习训练；配套构建 AesCode-358K 数据与 OpenDesign 基准，采用 SFT+RL 两阶段联合优化功能与美学。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Toward Open Aesthetic Reward Models for Code：以开放多模态与人类偏好训练替代专有评审，降低偏差与成本，提升可复现性<br>• Interactive-Web Agent for Aesthetic Usability Evaluation：提升 GUI 代理的鲁棒交互与判别能力，提供更细粒度、可验证的交互美学奖励<br>• Controllable Code Aesthetics via Multi-Objective RL：基于多目标与偏好条件化，实现功能/性能/多风格审美的可控权衡与个性化生成</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models with Conditional Score Distillation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21003" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21003" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Image Auto-regressive (AR) models have emerged as a powerful paradigm of visual generative models. Despite their promising performance, they suffer from slow generation speed due to the large number of sampling steps required. Although Distilled Decoding 1 (DD1) was recently proposed to enable few-step sampling for image AR models, it still incurs significant performance degradation in the one-step setting, and relies on a pre-defined mapping that limits its flexibility. In this work, we propose a new method, Distilled Decoding 2 (DD2), to further advances the feasibility of one-step sampling for image AR models. Unlike DD1, DD2 does not without rely on a pre-defined mapping. We view the original AR model as a teacher model which provides the ground truth conditional score in the latent embedding space at each token position. Based on this, we propose a novel conditional score distillation loss to train a one-step generator. Specifically, we train a separate network to predict the conditional score of the generated distribution and apply score distillation at every token position conditioned on previous tokens. Experimental results show that DD2 enables one-step sampling for image AR models with an minimal FID increase from 3.40 to 5.43 on ImageNet-256. Compared to the strongest baseline DD1, DD2 reduces the gap between the one-step sampling and original AR model by 67%, with up to 12.3times training speed-up simultaneously. DD2 takes a significant step toward the goal of one-step AR generation, opening up new possibilities for fast and high-quality AR modeling. Code is available at https://github.com/imagination-research/Distilled-Decoding-2.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 视觉自回归（AR）模型生成质量强但推理极慢，因需逐token顺序采样，无法满足低时延应用需求<br>• 现有加速方法局限：集合预测在一步极限下破坏token依赖、推测式解码在图像AR中提速有限（<3×）<br>• DD1虽首创一步采样但存在明显精度下降、依赖预定义ODE映射、训练慢且灵活性不足<br>• 亟需一种无需预定义映射、能在一步内匹配教师AR分布的生成范式，同时保持高质与高效<br>• 训练稳定性与收敛效率受离散-连续结构不匹配与初始化不当影响，需系统性初始化与优化策略</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出Distilled Decoding 2（DD2）：将教师AR视作条件score模型（借助Rectified Flow闭式条件score），以条件Score Distillation在每个token位置对齐生成器分布与教师分布；同时交替训练“一步生成器”和“条件引导网络”，并通过AR-diffusion初始化与GTS损失稳健启训，最终实现一步高质量采样。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 条件Score蒸馏用于连续空间AR：无VQ的扩散式AR一体化一步蒸馏框架<br>• 扩展DD2至文本到图像与多模态AR：大规模条件生成的一步自回归学习<br>• 弥合一步生成与教师差距：条件score对齐的目标设计与架构增强研究</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RobotArena infty: Scalable Robot Benchmarking via Real-to-Sim Translation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23571" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23571" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The pursuit of robot generalists - instructable agents capable of performing diverse tasks across diverse environments - demands rigorous and scalable evaluation. Yet real-world testing of robot policies remains fundamentally constrained: it is labor-intensive, slow, unsafe at scale, and difficult to reproduce. Existing simulation benchmarks are similarly limited, as they train and test policies within the same synthetic domains and cannot assess models trained from real-world demonstrations or alternative simulation environments. As policies expand in scope and complexity, these barriers only intensify, since defining "success" in robotics often hinges on nuanced human judgments of execution quality. In this paper, we introduce a new benchmarking framework that overcomes these challenges by shifting VLA evaluation into large-scale simulated environments augmented with online human feedback. Leveraging advances in vision-language models, 2D-to-3D generative modeling, and differentiable rendering, our approach automatically converts video demonstrations from widely used robot datasets into simulated counterparts. Within these digital twins, we assess VLA policies using both automated VLM-guided scoring and scalable human preference judgments collected from crowdworkers, transforming human involvement from tedious scene setup, resetting, and safety supervision into lightweight preference comparisons. To measure robustness, we systematically perturb simulated environments along multiple axes, such as textures and object placements, stress-testing policy generalization under controlled variation. The result is a continuously evolving, reproducible, and scalable benchmark for real-world trained robot manipulation policies, addressing a critical missing capability in today's robotics landscape.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 现实评测不可扩展：需要人工搭建/复位与安全监管，昂贵、缓慢、难复现，跨机构比较公平性差<br>• 现有模拟基准闭环自洽：多在同一仿真域内训练与测试，难评估由真实演示或异源仿真训练的通用策略<br>• 成功判定难以标准化：任务完成常需细粒度人类判断，缺少可扩展、可复现的自动评分方案<br>• 泛化与鲁棒性缺口：缺乏系统化、可控的背景/颜色/物体位姿扰动来检验跨分布能力<br>• 集中式线下挑战成本高、频次低：无法形成如CV/NLP那样持续、可比较的排行榜生态</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出RobotArena ∞：将真实演示视频自动转译为可交互仿真，部署VLA并用VLM进度评分与众包成对偏好（Bradley–Terry）得到全局排名。管线含可微渲染的机-相机标定、VLM分割与2D→3D资产重建/姿态估计、背景修复、系统辨识，并施加可控扰动（背景/颜色/物体位姿）以可扩展、可复现地评测跨域与鲁棒性。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Real2Sim+多视角与手腕相机的全景评测基准：扩展到多相机/手腕视角与时空一致重建，覆盖更细粒度操作<br>• 高保真接触物理与微插配任务的仿真基准：引入精细接触/摩擦/公差建模，评测插拔等高难物理任务<br>• 人类偏好驱动的评测-学习闭环：将众包成对偏好用于在线排名与偏好学习，联合提升评测与策略<br>• 跨数据集与跨体现的鲁棒性光谱：构建标准化OOD与扰动谱系，量化泛化边界与失效模式<br>• 生成式资产与场景的自监督扩展：用2D→3D/视频→3D与材质推断大规模合成多样可交互数字孪生<br>• VLM任务进度评分的置信度与偏差校准：校准VLM评分与人类一致性并给出不确定度估计</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23479" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23479" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-language alignment in multi-modal large language models (MLLMs) typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). SFT is stable and efficient but requires large-scale human annotations and cannot capture subtle preferences, while RL brings in a reward signal for training, but suffers from overhead and instability. These limitations highlight a trade-off between scalability, robustness, and alignment quality. To address this, we propose MergeMix, a training-time augmentation paradigm that bridges SFT and RL. It first applies an attention-aware image mixing via token merge with more cluster representation and spatial context, and then presents a preference-driven training paradigm for MLLMs by building preference pairs with mixed images and raw images, and optimizing via SimPO loss. As a mixup augmentation, MergeMix enhances attention consistency and efficiency, surpassing other heuristic-based methods in classification. Extensive experiments demonstrate that MergeMix achieves competitive accuracy with improved efficiency, providing a scalable approach to preference alignment in classification and MLLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 多模态偏好对齐的“效率—质量”矛盾：SFT稳定高效但依赖大量人工标注且难显式建模相对偏好；RL/RLHF可引入偏好信号但训练代价高且不稳定，难以兼顾可扩展性、鲁棒性与对齐质量（页1-2）<br>• 现有“增广造负例”不可控：如SeVa等以随机裁剪等启发式生成输家并用DPO筛选，掩码高度随机、与数据弱耦合，难控制“输家”质量，影响VQA等任务并减少有效训练样本（页1）<br>• mixup在ViT/MLLM中的两难与信息对齐缺失：显著性/教师引导的掩码代价高、TopK贪心丢失空间关系，混合比与样本信息密度不匹配，导致效率—性能权衡不佳、校准与鲁棒性不足（图1与第4.1节）</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出MergeMix：用Token Merging（ToMe）与双向软匹配恢复注意力，生成连续、注意力一致的混合掩码，并以高斯重标定将合并比与混合比对齐；分类任务用一热+混合CE训练，MLLM将“混合图像–原图”构成偏好对并以SimPO排名损优化，无需奖励模型，桥接SFT与RL（图2-3，式8-12）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• TextMix-PO：在MLLM中引入“文本侧mixup+图像侧MergeMix”的协同偏好优化，细粒度控制多模态输赢难度<br>• Learnable-ToMeMix：将静态Token合并升级为可学习/可微分聚类的注意力驱动合并，端到端联合学习掩码与比率<br>• Lambda-Curriculum SimPO：用混合比λ驱动自适应排名边际/难度课程，统一提升对齐质量、校准与遮挡鲁棒性</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23594" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23594" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce PRISM-Bench, a benchmark of puzzle-based visual challenges designed to evaluate not only whether models can solve problems, but how their reasoning unfolds. Unlike prior evaluations that measure only final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error, models must identify the first incorrect step. This setting enables fine-grained assessment of logical consistency, error detection, and visual reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric, and analogical reasoning, resisting shortcuts based on superficial pattern matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap between fluent generation and faithful reasoning: models that produce plausible CoTs often fail to locate simple logical faults. By disentangling answer generation from reasoning verification, PRISM-Bench offers a sharper lens on multimodal reasoning competence and underscores the need for diagnostic evaluation protocols in the development of trustworthy MLLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 现有多模态评测多聚焦最终答案，混淆视觉感知、浅层匹配与推理，无法定位推理链中最早出错的步骤，难以衡量推理忠实度与一致性（图1见第2页展示了所需的双轨评测思路）。<br>• 缺乏要求多步符号、几何、类比推理的高质量视觉谜题，许多样本可被文本捷径或表面相似性破解，难以客观考察真正的跨模态推理能力（图2第4页示例了六大题型）。<br>• 模型会生成流畅但不可靠的链式推理，难以自我校验：表2（第7页）显示首错检测最佳模型仅约62%，表3（第8页）VQA整体也偏低，且两者相关性仅中等（图5第10页，ρ≈0.62；τ≈0.47），表明答案正确不等同于过程正确，迫切需要诊断式评测与可训练的错误信号。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出PRISM-Bench：构建含1044题、六大类别的视觉谜题基准，为每题提供真实推理链与仅含单一错误的“腐化”推理链；通过在随机步骤注入24类错误并人工核验，确保首错唯一可判，并以“答案准确率+首错检测”双轨协议解耦生成与验证。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 面向多模态首错定位的可训练验证器：利用步级标注与错误类型标签训练判别式/奖励模型，提升模型自检与纠错能力<br>• 难度可控的视觉谜题自动生成与校准：基于程序化生成与人机校验扩展题库，按错误类型与步骤深度进行难度分层<br>• 生成-验证协同优化的推理鲁棒化：将首错信号融入联合训练或强化学习，使模型在求解同时学习审计与修正推理步骤</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VoMP: Predicting Volumetric Mechanical Property Fields</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22975" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22975" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Physical simulation relies on spatially-varying mechanical properties, often laboriously hand-crafted. VoMP is a feed-forward method trained to predict Young's modulus (E), Poisson's ratio (nu), and density (rho) throughout the volume of 3D objects, in any representation that can be rendered and voxelized. VoMP aggregates per-voxel multi-view features and passes them to our trained Geometry Transformer to predict per-voxel material latent codes. These latents reside on a manifold of physically plausible materials, which we learn from a real-world dataset, guaranteeing the validity of decoded per-voxel materials. To obtain object-level training data, we propose an annotation pipeline combining knowledge from segmented 3D datasets, material databases, and a vision-language model, along with a new benchmark. Experiments show that VoMP estimates accurate volumetric properties, far outperforming prior art in accuracy and speed.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 高保真物理仿真需要体素级的杨氏模量E、泊松比ν、密度ρ，但现有3D数据与重建结果普遍缺乏此类标注，人工设定费时且主观<br>• 现有方法多需逐对象优化或运行时调用大模型（VLM/视频模型），速度慢、流程脆弱、难以规模化应用<br>• 许多工作输出的是模拟器特定参数或粗材质类别，难以在不同仿真框架间移植，物理真实性与可重现性不足<br>• 大多仅覆盖表面属性，无法为物体内部赋材，导致体积力学行为（弹性、碰撞、波传播等）模拟失真<br>• 方法常对特定3D表示（如Splats/NeRF）定制，缺乏跨表示的通用性与一致性<br>• 预测结果未被物理可行域约束，易产生不符合真实材料的参数组合</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>VoMP是一个跨表示的前馈模型：将任意可渲染/体素化的3D资产的多视角DINOv2图像特征提升到体素后，由几何Transformer预测每体素的材料潜变量，并通过基于真实材料三元组训练的MatVAE解码为物理有效的E、ν、ρ。配套的自动标注管线结合分件3D资产、材质数据库与VLM，引导生成大规模体素级监督数据。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Aniso-VoMP: 从各向同性到各向异性体积材料场的前馈预测：将材料表示扩展为完整弹性张量，学习方向相关的力学属性<br>• SimAdapt: 跨模拟器的材料场等效映射与标定：把物理有效(E,ν,ρ)自适应映射到FEM/MPM/XPBD等不同引擎的等效参数域<br>• Adaptive-Voxel VoMP: 基于八叉树/隐式表示的自适应多尺度材料场重建：缓解固定网格带来的过平滑，细化复杂内部结构<br>• PhysAware Data Engine: 面向材料场的主动学习与不确定性驱动标注体系：减少对VLM的依赖，提升标签可信度与覆盖度<br>• Diff-VoMP: 融合可微仿真的端到端材料场校准：以仿真重投影误差反馈微调材料潜空间与预测器，提高仿真一致性</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Language Server CLI Empowers Language Agents with Process Rewards</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22907" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22907" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models routinely hallucinate APIs and mislocalize edits, while language servers compute verified, IDE-grade facts about real code. We present Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language Server Protocol (LSP) server for coding agents and CI, exposing deterministic, replayable workflows. Our position is that language servers provide not only structural information (definitions, references, types, diagnostics) but also an actionable process reward: machine-checked, step-wise signals that align an agent's planning loop with program reality. In this work, Lanser-CLI contributes: (i) a robust addressing scheme beyond brittle "file:line:col" via a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a principled relocation algorithm; (ii) deterministic Analysis Bundles that normalize Language Server responses and capture environment/capability metadata with stable content hashes; (iii) a safety envelope for mutating operations (rename, code actions) with preview, workspace jails, and Git-aware, transactional apply; and (iv) a process-reward functional derived from Language Server facts (diagnostic deltas, disambiguation confidence, and safe-apply checks) that is computable online and replayable offline. We formalize determinism under frozen snapshots and establish a monotonicity property for the process reward, making it suitable for process supervision and counterfactual analysis. Project Page: https://github.com/yifanzhang-pro/lanser-cli</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 语言模型在代码结构、API与编辑定位上易产生幻觉和漂移，代理的计划-执行环路与真实程序状态脱节；而原生LSP虽提供事实，却难以直接支撑稳健的代理集成。<br>• 原生LSP集成缺乏确定性与可复现性（响应排序不稳、环境/版本/positionEncoding不一致、UTF-16/UTF-8索引错配），且file:line:col坐标脆弱，编辑易失配。<br>• 自动化变更缺少安全护栏（预览、事务化应用、工作区监狱、Git整洁性、编码/索引一致性），难在大规模与CI场景中可靠落地。<br>• 现有方法侧重终局指标，缺少可机检、可回放的步骤级过程监督信号，无法对代理的中间决策进行对齐与信用分配。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出Lanser-CLI：一个CLI优先的LSP编排层，固定服务器与环境，输出字节稳定、可回放的Analysis Bundles与过程奖励。其核心包括可存活编辑的Selector DSL与确定性重定位算法、环境与编码规范化和稳定哈希、带预览/事务化/工作区监狱的安全变更管线，以及基于诊断增量、安全检查与歧义置信度的过程奖励函数。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 跨语言Lanser-CLI：统一Selector与过程奖励在多语言生态中的泛化：将DSL与Bundle规范扩展到TypeScript/Java/C++等多服务器，打通跨语言的稳定标识与奖励对齐。<br>• 学习增强的重定位与歧义消解：从启发式到可学习打分模型：用历史Bundle与轨迹训练可学习的候选打分/消歧模型，在可审计前提下提升定位鲁棒性与置信度估计。<br>• 静态-动态融合的过程奖励：将LSP事实与测试/覆盖/性能信号联合：把诊断变化与单测通过率、覆盖率、回归检测等融合为可回放的复合过程奖励，用于更强的过程监督与CI策略。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">Once Upon an Input: Reasoning via Per-Instance Program Synthesis</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22849" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22849" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) excel at zero-shot inference but continue to struggle with complex, multi-step reasoning. Recent methods that augment LLMs with intermediate reasoning steps such as Chain of Thought (CoT) and Program of Thought (PoT) improve performance but often produce undesirable solutions, especially in algorithmic domains. We introduce Per-Instance Program Synthesis (PIPS), a method that generates and refines programs at the instance-level using structural feedback without relying on task-specific guidance or explicit test cases. To further improve performance, PIPS incorporates a confidence metric that dynamically chooses between direct inference and program synthesis on a per-instance basis. Experiments across three frontier LLMs and 30 benchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question answering tasks, relational reasoning tasks, and mathematical reasoning tasks show that PIPS improves the absolute harmonic mean accuracy by up to 8.6% and 9.4% compared to PoT and CoT respectively, and reduces undesirable program generations by 65.1% on the algorithmic tasks compared to PoT with Gemini-2.0-Flash.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：LLM在复杂多步推理上易不稳定且不忠实，CoT/PoT虽有提升但常出现“为正确答案编写空壳代码”、类型/语法错误与不可执行逻辑，难以在算法类任务中可靠落地（见第3页图3a与第4页图4）。<br>• 何时用代码？开放域实例层面缺少“是否应进行程序综合而非直接CoT”的可靠判定，导致在非算法化实例上徒增一次代码执行但无收益（第3节3.1；第3页图3b）。<br>• 无任务规格的综合难题：缺乏I/O样例或形式化规格，无法指导候选程序搜索与验证，易产生硬编码答案、占位符代码、类型/语法错误（第2.2节，第9页表2）。<br>• 非结构化输入鸿沟：程序偏好结构化输入，但多数问题给出文本/图像等原始形态，现有方法常让程序去“解析图像”而非借助模型感知，脆弱且易失败（第4-5页，图4b）。<br>• 重要性与影响：程序执行可提供可验证、可复现的精准计算；论文在30个基准上相对PoT的调和平均准确率提升最高达8.6%，并在算法类任务上将不良程序生成降低65.1%（第7页图5；第8-9页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出PIPS：实例级可选择的程序综合框架。先用自评式置信度开关判定“直接CoT”或“代码综合”；若综合，则先将原始输入抽取为实例专属的符号JSON，再在结构化检查（输入依赖、返回格式、语法/类型、占位符、符号抽取问题与sanity等）反馈下迭代生成与修复程序，直至通过检查并执行得到答案，无需任务专用规格或测试用例。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 可组合CoT-代码混合推理：自动分段与执行调度：学习把复杂问题分解为子段并为每段自适应选择CoT或代码，结合执行反馈实现闭环纠错。<br>• 无测试样例的语义级反馈与验证驱动综合：在结构检查之外引入属性测试、差分执行、形式验证/静态分析与模糊测试，降低“答案对但理由错”。<br>• 更可信的实例级符号抽取与校验：通过对齐约束、信息保持正则与多模态一致性检测，提升c(x)的忠实度与可审计性，并联动程序迭代修复漏抽/错抽。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DiffusionLane: Diffusion Model for Lane Detection</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22236" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22236" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In this paper, we present a novel diffusion-based model for lane detection, called DiffusionLane, which treats the lane detection task as a denoising diffusion process in the parameter space of the lane. Firstly, we add the Gaussian noise to the parameters (the starting point and the angle) of ground truth lanes to obtain noisy lane anchors, and the model learns to refine the noisy lane anchors in a progressive way to obtain the target lanes. Secondly, we propose a hybrid decoding strategy to address the poor feature representation of the encoder, resulting from the noisy lane anchors. Specifically, we design a hybrid diffusion decoder to combine global-level and local-level decoders for high-quality lane anchors. Then, to improve the feature representation of the encoder, we employ an auxiliary head in the training stage to adopt the learnable lane anchors for enriching the supervision on the encoder. Experimental results on four benchmarks, Carlane, Tusimple, CULane, and LLAMAS, show that DiffusionLane possesses a strong generalization ability and promising detection performance compared to the previous state-of-the-art methods. For example, DiffusionLane with ResNet18 surpasses the existing methods by at least 1\% accuracy on the domain adaptation dataset Carlane. Besides, DiffusionLane with MobileNetV4 gets 81.32\% F1 score on CULane, 96.89\% accuracy on Tusimple with ResNet34, and 97.59\% F1 score on LLAMAS with ResNet101. Code will be available at https://github.com/zkyntu/UnLanedet.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 现有基于可学习锚点的车道线检测在分布迁移（域偏移）场景下泛化差，往往需要针对新域重新训练，降低部署便捷性与鲁棒性（见论文对CLRNet等方法的讨论）。<br>• 若将扩散直接施加在整条车道的所有采样点上，计算与优化负担过重、训练困难；需要一种在参数空间更轻量的扩散建模方式。<br>• 从随机/低质锚点出发会削弱编码器的特征表征，导致性能下滑；需要新的解码与监督策略弥补随机锚点带来的表征不足与正样本稀疏问题。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>将车道检测建模为参数空间（起点x/y与角度）的扩散-去噪：训练时对GT参数加高斯噪声并学习逐步复原，推理时从高斯采样的随机锚点经少量DDIM步迭代精化得到车道。为缓解随机锚点带来的表征不足，提出混合扩散解码器（全局RoIGather+局部自注意力/动态卷积）与仅训练期的辅助头，并配合锚点重采样对齐训推分布。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• FastDiffLane: 轻量混合解码与步长自适应的高速车道扩散检测——通过解码器蒸馏、稀疏锚点与自适应采样步显著提升推理速度<br>• TemporalDiffLane: 引入时序一致性的扩散式视频车道检测——跨帧一致性损失与时序扩散更新提升稳定性与鲁棒性<br>• CrossDomainDiffLane: 面向无监督域自适应的扩散车道检测——结合域不变噪声调度、风格/特征对齐与伪标签自训练强化跨域泛化</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20512" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20512" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in accelerating text-to-image (T2I) diffusion models have enabled the synthesis of high-fidelity images even in a single step. However, personalizing these models to incorporate novel concepts remains a challenge due to the limited capacity of one-step models to capture new concept distributions effectively. We propose a bidirectional concept distillation framework, EchoDistill, to enable one-step diffusion personalization (1-SDP). Our approach involves an end-to-end training process where a multi-step diffusion model (teacher) and a one-step diffusion model (student) are trained simultaneously. The concept is first distilled from the teacher model to the student, and then echoed back from the student to the teacher. During the EchoDistill, we share the text encoder between the two models to ensure consistent semantic understanding. Following this, the student model is optimized with adversarial losses to align with the real image distribution and with alignment losses to maintain consistency with the teacher's output. Furthermore, we introduce the bidirectional echoing refinement strategy, wherein the student model leverages its faster generation capability to feedback to the teacher model. This bidirectional concept distillation mechanism not only enhances the student ability to personalize novel concepts but also improves the generative quality of the teacher model. Our experiments demonstrate that this collaborative framework significantly outperforms existing personalization methods over the 1-SDP setup, establishing a novel paradigm for rapid and effective personalization in T2I diffusion models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 现有个性化方法在一步扩散模型上普遍失效：学生模型难以通过仅调文本编码器学习新概念，直接微调骨干会破坏先验（如Textual Inversion、Custom Diffusion、IP-Adapter在SD-Turbo/TCD上效果不佳）。<br>• 传统“teacher-first”两阶段蒸馏低效且不可靠：教师需多步采样监督、训练代价高，且教师本身对少样本概念学习可能失败，生成的监督样本会误导学生。<br>• 迫切需求是在保持一步/少步高速生成的同时，实现对新概念的高保真个性化（1-SDP），缩小“速度—个性化”鸿沟。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出EchoDistill：端到端联合训练多步教师与一步学生，共享文本编码器，学生用对齐损失（IPA身份特征、MSE、MS‑SWD，含时间权重）对齐教师输出，并通过多判别器（DINOv1/DINOv2/CLIP）对抗损失贴近真实概念分布；随后以学生快速生成的样本作为“回声”反向优化教师与学生，形成双向概念蒸馏。仅微调两端UNet的K/V投影，参数高效并可兼容少步推理。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• EchoDistill-Lite：基于自蒸馏与自适应单判别器的一步个性化加速框架，降低多判别器带来的训练开销与不稳定性。<br>• One-Shot EchoDistill：结合对比学习与元学习的单样本个性化蒸馏，提升极低样本下的稳定性与概念保真度。<br>• Compositional EchoDistill：面向多主体/多概念的可组合与持续个性化学习，在一步/少步扩散中实现跨概念融合与遗忘抑制。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">Scaling Laws for Deepfake Detection</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.16320" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.16320" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This paper presents a systematic study of scaling laws for the deepfake detection task. Specifically, we analyze the model performance against the number of real image domains, deepfake generation methods, and training images. Since no existing dataset meets the scale requirements for this research, we construct ScaleDF, the largest dataset to date in this field, which contains over 5.8 million real images from 51 different datasets (domains) and more than 8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we observe power-law scaling similar to that shown in large language models (LLMs). Specifically, the average detection error follows a predictable power-law decay as either the number of real domains or the number of deepfake methods increases. This key observation not only allows us to forecast the number of additional real domains or deepfake methods required to reach a target performance, but also inspires us to counter the evolving deepfake technology in a data-centric manner. Beyond this, we examine the role of pre-training and data augmentations in deepfake detection under scaling, as well as the limitations of scaling itself.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 缺乏可预测的“数据规模→性能”关系，导致深伪检测难以规划数据采集与训练资源；同时生成技术不断演进，模型对新伪造方法与新真实域的泛化不足（见第1页“Introduction”与图1(d)）。<br>• 现有数据集规模与多样性不足：多数仅1–2个真实域、伪造方法覆盖有限，无法支撑跨域/跨方法的系统标度研究与稳健评估（见第2页图1(c)、第5页“3.2 Comparing ScaleDF…”）。<br>• 既有方法多依赖小规模数据上的架构/特征技巧，缺少在大规模下对预训练、数据增强与饱和点/局限性的量化分析（见第3页“Contributions”与第1–3页摘要/图1(d)）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>构建ScaleDF（5.8M真实图像/51个真实域，8.8M伪造/102种方法，覆盖FS/FR/FF/FE/TF五类，跨域+跨方法划分），并以ViT将任务建模为二分类，系统改变“真实域数”“伪造方法数”“训练图像数”。通过拟合与验证检测误差的幂律1−AUC=A·N^{-α}与双饱和幂律1−AUC=c+K·(N+N0)^{-γ}（图1(d)，第2–3页），同时分析预训练与数据增强在标度下的作用与局限（如在46域/88方法下，图像数>10^7后出现饱和趋势）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 面向视频的时序深度伪造检测标度律：将标度研究从图像扩展到时序线索与时空模型，量化帧率、序列长度与时序建模对幂律参数与外推性的影响。<br>• 开放世界与分布漂移下的标度律：系统考察压缩/重编码、对齐与裁剪、混合边界、随机扰动等部署因素对标度曲线与跨基准泛化的影响，并建立鲁棒性标度模型。<br>• 数据-模型-算力三维联合标度与主动采样：联合优化数据多样性、模型规模/预训练策略与训练预算，利用标度律做预算分配与新域/新方法的主动获取与外推。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view Human Reconstruction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.07723" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.07723" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Photorealistic 3D full-body human reconstruction from a single image is a critical yet challenging task for applications in films and video games due to inherent ambiguities and severe self-occlusions. While recent approaches leverage SMPL estimation and SMPL-conditioned image generative models to hallucinate novel views, they suffer from inaccurate 3D priors estimated from SMPL meshes and have difficulty in handling difficult human poses and reconstructing fine details. In this paper, we propose SyncHuman, a novel framework that combines 2D multiview generative model and 3D native generative model for the first time, enabling high-quality clothed human mesh reconstruction from single-view images even under challenging human poses. Multiview generative model excels at capturing fine 2D details but struggles with structural consistency, whereas 3D native generative model generates coarse yet structurally consistent 3D shapes. By integrating the complementary strengths of these two approaches, we develop a more effective generation framework. Specifically, we first jointly fine-tune the multiview generative model and the 3D native generative model with proposed pixel-aligned 2D-3D synchronization attention to produce geometrically aligned 3D shapes and 2D multiview images. To further improve details, we introduce a feature injection mechanism that lifts fine details from 2D multiview images onto the aligned 3D shapes, enabling accurate and high-fidelity reconstruction. Extensive experiments demonstrate that SyncHuman achieves robust and photo-realistic 3D human reconstruction, even for images with challenging poses. Our method outperforms baseline methods in geometric accuracy and visual fidelity, demonstrating a promising direction for future 3D generation models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 任务与重要性：从单张RGB图重建高保真穿衣全身人体，服务于AR/VR、虚拟试衣、游戏与影视等应用（摘要、1节）<br>• 关键挑战：单视角固有歧义与强自遮挡，导致几何恢复与外观重建困难（摘要、1节）<br>• 现有2D多视图生成局限：依赖SMPL先验，但SMPL在遮挡/大姿态下易不准，且只表示裸体，难覆盖宽松衣物与复杂拓扑，造成跨视不一致与结构伪影（图2a-b，1节）<br>• 原生3D生成局限：结构一致但细节与输入忠实度不足，生成形状偏粗、外观缺乏真实性（图2c，1节）<br>• 需求缺口：期望一种联合2D与3D优势、提升跨视一致性与细节保真、并尽量摆脱不可靠SMPL依赖的单视图人体重建框架（图3，1节）</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出SyncHuman：首次联合2D多视图扩散与原生3D生成，通过2D-3D同步注意力在训练中双向对齐（3D指导2D提升跨视一致性，2D反哺3D提升细节与忠实度）；并以多视图引导解码器将DINOv2特征像素对齐注入结构潜变量，直接解码为高保真纹理网格/3DGS（图3、图4、式(4)）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Illumination-Aware SyncHuman：可分解光照与材质的2D-3D同步生成，缓解非均匀光照伪影并实现重光照（针对图10的限制）<br>• SDF-SyncHuman：以SDF原生3D生成与拓扑/水密约束替代体素分支，消除网格破洞并提升几何可编辑性（针对附录C.1的限制）<br>• ScaleSyncHuman：结合视频扩散与大规模多视角人体数据的联合预训练，进一步提升跨视一致性、细节与泛化并加速推理</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23605" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23605" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity of the subject. However, personalized 3D/4D generation is still largely underexplored. In this work, we introduce TIRE (Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation. It takes an initial 3D asset produced by an existing 3D generative model as input and uses video tracking to identify the regions that need to be modified. Then, we adopt a subject-driven 2D inpainting model for progressively infilling the identified regions. Finally, we resplat the modified 2D multi-view observations back to 3D while still maintaining consistency. Extensive experiments demonstrate that our approach significantly improves identity preservation in 3D/4D generation compared to state-of-the-art methods. Our project website is available at https://zsh2000.github.io/track-inpaint-resplat.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 当前3D/4D生成在跨视角的“身份保持”上表现欠佳，未观测区域（侧/后视）常被错误“幻觉”，出现系统性色偏与纹理漂移，难以满足个性化生成需求。<br>• 单图/单视频驱动的方法对未见视角缺乏线索，SDS类优化既耗时又易导致外观/运动被平均化，难以实用部署。<br>• 多视图扩散虽可快速外推，但受训练数据偏差与一致性缺陷影响，难以可靠还原遮挡区域的真实外观。<br>• 原生3D/4D快速生成（feed-forward）方法聚焦效率与美观，却普遍忽视个性化与身份一致性，需一种可与之互补且模型无关的增强方案。<br>• 在补全纹理的同时保持跨视角一致性与几何质量仍具挑战，常出现鬼影与结构错乱等问题。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出TIRE三阶段流水线：先用视频点跟踪的“反向跟踪”从目标视角回到源视角，精准定位需补纹理的遮挡区域；再以个性化LoRA微调的2D扩散式修复进行“渐进式”视角扩展补纹理；最后用掩码感知的多视图扩散只在未见区域去噪校正一致性，并将多视图结果重投回3D（resplat），整体模型无关、可叠加到任意3D/4D生成器上。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Tuning-free TIRE: 无需每目标微调的免训练个性化3D/4D渐进补纹理框架：以适配器/提示注入替代LoRA微调，显著提升效率<br>• Time-aware TIRE-4D: 面向时间变外观的时序一致个性化4D生成：显式建模随时间变化的材质/花纹并与运动绑定<br>• Geo-Perceptual Identity Metric: 面向3D/4D个性化的几何感知身份一致性指标：融合VLM评分与可见性/法线/深度一致性<br>• End-to-End TIRE: 可微分的端到端“跟踪-修复-重投”联合学习：用单一损失联合优化遮挡估计、补纹理与3D一致性<br>• Learned Visibility over Tracking: 用可学习可见性场替代视频跟踪的遮挡区域预测：结合体渲染或可见性网络稳健跨视角<br>• Representation-agnostic Resplat: 面向高保真网格/NeRF/高斯的统一重投与一致性整合模块：提升不同3D表示的可用性</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-11">

    <div class="paper">
        <h2 class="paper-title">Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22603" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22603" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) have recently advanced auditory speech recognition (ASR), visual speech recognition (VSR), and audio-visual speech recognition (AVSR). However, understanding of their internal dynamics under fine-tuning remains limited. In natural language processing, recent work has revealed attention sinks, tokens that attract disproportionately high attention, and associated massive activations in which some features of sink tokens exhibit huge activation in LLMs. In this work, we are the first to study these phenomena in multimodal speech recognition. Through a detailed analysis of audio-visual LLMs, we identify attention sinks and massive activations not only at the BOS token but also at intermediate low-semantic tokens across ASR, VSR, and AVSR. We show that massive activations originate in the MLP layers and correspond to fixed feature indices across all sink tokens. We further show that intermediate sink tokens exhibit high cosine similarity to the BOS token, thereby amplifying attention and activation. Building on these insights, we introduce a simple decorrelation loss that reduces cosine similarity between BOS and other tokens, effectively mitigating intermediate sinks and massive activations. Furthermore, our method improves word error rate (WER) under high audio-visual feature downsampling while remaining stable at lower downsampling rates.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：LLM驱动的ASR/VSR/AVSR在微调后出现注意力汇聚（attention sinks）与超大激活（massive activations），其中除BOS外的“中间低语义token”也成为sink，转移注意力、破坏音视对齐，尤其在高压缩率下恶化WER（见页2图1(a–d)、页3图2(a)）。<br>• 重要性：这些现象在多模态语音识别领域尚未被系统研究，影响模型稳健性与效率；作者实证显示大激活起源于MLP第2层的GLU门控，且sink共享固定激活特征，根因是与BOS隐藏态的高余弦相似度（见页3图2(b)、图3(a)(b)）。<br>• 现有方法局限：现有BOS-sink缓解策略（如Softmax变体、占位token）多需全量预训练且面向长上下文；ACT仅推理期重分配注意力、增加开销且无法消除大激活，且与LoRA微调范式不兼容（页4表2），缺乏训练期、轻量、同时缓解sink与大激活的方案。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出去相关损失：在中间层惩罚BOS与其他token隐藏态的余弦相似度（平方），与交叉熵联合优化，既不改模型结构也无推理开销、兼容LoRA，从源头削弱中间sink与大激活并在高压缩下显著提升WER（见页4表1）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Sink-感知的自适应去相关正则用于流式AVSR：动态调节λ与层选择，适配长上下文与实时场景，权衡稳健性与延迟。<br>• 从MLP门控到注意力的统一正则：面向massive activations的门控统计/谱约束与去相关联合训练，从源头抑制放大链路。<br>• 无Sink化的跨模态投影与提示设计：通过正交子空间/可学习旋转与prompt工程，避免低语义token成为sink并强化音视对齐。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22317" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22317" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present memory-based language modeling as an efficient, eco-friendly alternative to deep neural network-based language modeling. It offers log-linearly scalable next-token prediction performance and strong memorization capabilities. Implementing fast approximations of k-nearest neighbor classification, memory-based language modeling leaves a relatively small ecological footprint both in training and in inference mode, as it relies fully on CPUs and attains low token latencies. Its internal workings are simple and fully transparent. We compare our implementation of memory-based language modeling, OLIFANT, with GPT-2 and GPT-Neo on next-token prediction accuracy, estimated emissions and speeds, and offer some deeper analyses of the model.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 降低大模型训练/推理成本与碳足迹：主流Transformer LM训练与推理耗能高、依赖GPU，难以在资源受限与可持续场景部署；论文提出基于CPU的记忆式LM，实测推理碳排显著更低（如图7第9页、表1第8页）。<br>• 提升可解释性与可溯源性：神经LM内部机理不透明，难以解释预测来源；本文方法可返回最近邻/路径及其计数分布，支持源级溯源（第10–11页）。<br>• 解决记忆式LM推理效率瓶颈：朴素kNN推理复杂度高（O(nd)），难以实用；论文以前缀Trie与信息增益排序构建高效索引，并通过TRIBL2/IGTree实现低延迟推理（图6第8页）。<br>• 澄清与利用可扩展性：Transformer训练涉及数据/模型/算力三维耦合，缩放规律复杂；记忆式LM“模型即数据”，训练单维线性扩展，呈近似对数线性学习曲线，并在≈10^9 tokens处出现更陡增益（图5第7页）。<br>• 覆盖长尾与可控记忆：神经LM对长尾词与精确背诵受限；本文模型天然强化长尾（第12页、图11）并能在上下文4的条件下较高比例复述训练数据（第9–10页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出OLIFANT：以信息增益（gain ratio）排序的前缀Trie为索引，在固定小上下文（默认4 token）上进行下一词预测，并提供三种推理模式（IB1-IG纯kNN、TRIBL2树+局部kNN、IGTree纯决策树）实现一次遍历的增量训练与CPU低延迟、稀疏分布输出，同时系统评估准确率、延迟与碳排（图5–7、表1）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Exemplar-Weighted OLIFANT：面向在线个性化与遗忘的注意力化投票机制——以示例权重与时间衰减优先近期/特定域数据，实现快速微调与可控遗忘。<br>• 融合MVDM与非位置全局特征的记忆语言模型——将多值差异度量与BoW/主题/实体等全局信号融入，突破仅靠位置特征与4-token上下文的限制。<br>• 投机解码与Transformer插值的低碳混合NN-LM——将OLIFANT与Transformer做kNN插值/束搜索/投机解码，兼顾长上下文优势与长尾覆盖，在CPU上实现高能效推理。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">FlowOpt: Fast Optimization Through Whole Flow Processes for Training-Free Editing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22010" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22010" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The remarkable success of diffusion and flow-matching models has ignited a surge of works on adapting them at test time for controlled generation tasks. Examples range from image editing to restoration, compression and personalization. However, due to the iterative nature of the sampling process in those models, it is computationally impractical to use gradient-based optimization to directly control the image generated at the end of the process. As a result, existing methods typically resort to manipulating each timestep separately. Here we introduce FlowOpt - a zero-order (gradient-free) optimization framework that treats the entire flow process as a black box, enabling optimization through the whole sampling path without backpropagation through the model. Our method is both highly efficient and allows users to monitor the intermediate optimization results and perform early stopping if desired. We prove a sufficient condition on FlowOpt's step-size, under which convergence to the global optimum is guaranteed. We further show how to empirically estimate this upper bound so as to choose an appropriate step-size. We demonstrate how FlowOpt can be used for image editing, showcasing two options: (i) inversion (determining the initial noise that generates a given image), and (ii) directly steering the edited image to be similar to the source image while conforming to a target text prompt. In both cases, FlowOpt achieves state-of-the-art results while using roughly the same number of neural function evaluations (NFEs) as existing methods. Code and examples are available on the project's webpage.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 迭代采样导致端到端控制困难：扩散/流匹配模型需多次NFE，直接对最终生成图像做梯度优化在算力与内存上不可行（见第1页摘要、第2页引言）。<br>• 分时步方法误差累积：现有编辑/恢复/压缩多在每个时间步独立干预，缺乏对最终结果的直接监督，误差沿时间步累积，重建与编辑精度受限（第1–2页）。<br>• 全流程梯度优化不具扩展性：如D-Flow需对整条去噪器链反传，难以扩展到大模型与高分辨率、交互式编辑场景（第3页相关工作）。<br>• 现有“无反传”优化仍是分时步：SDS/DDS/PDS/iRFDS按随机时刻更新，未实现贯穿整条流的联合优化（第3页）。<br>• 需要训练免、黑盒可用且高效的方案：既能对最终图像直接优化，又能在有限NFE内运行、可逐步监控与早停（第1–2页，图3–4）。<br>• 需要可证收敛与可调步长：以往零阶法η=1在现代模型上不收敛，需给出充分条件与经验上界估计（第5–6页，定理1与表1、图5）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出FlowOpt：将整条流采样过程封装为黑盒映射f，对最终图像的L2等损失执行零阶迭代z_{t}^{(i+1)}=z_{t}^{(i)}−η(f(z_{t}^{(i)},c)−y)，无须反传即可跨全流程联合优化；并给出基于压缩映射的步长充分条件（式7）及经验估计（表1），在FLUX与SD3上以少量步数实现SOTA的重建与直接编辑，且可中途监控与早停（见第4–6页，图3–6）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Adaptive-FlowOpt：自适应/线搜索步长与时刻调度的零阶全流程优化，加速收敛并提升稳健性。<br>• FlowOpt-RestoreCompress：零阶全流程优化在图像恢复与压缩中的统一框架与理论误差界。<br>• Jacobian-Light FlowOpt：引入低秩/局部雅可比近似的半零阶法，兼顾效率与更紧的收敛保证。<br>• Video/Audio-FlowOpt：面向视频/音频的训练免零阶全流程编辑，增强时序一致与跨模态适配。<br>• Masked-FlowOpt for Large-Region Edits：结合掩膜与结构先验，攻克论文指出的大范围区域编辑难例。<br>• Tight-Bounds for FlowOpt：针对具体求解器与模型的步长上界紧化与误差传播理论，改进稳定性与速度。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-12">

    <div class="paper">
        <h2 class="paper-title">Sprint: Sparse-Dense Residual Fusion for Efficient Diffusion Transformers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21986" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21986" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion Transformers (DiTs) deliver state-of-the-art generative performance but their quadratic training cost with sequence length makes large-scale pretraining prohibitively expensive. Token dropping can reduce training cost, yet na\"ive strategies degrade representations, and existing methods are either parameter-heavy or fail at high drop ratios. We present SPRINT, Sparse--Dense Residual Fusion for Efficient Diffusion Transformers, a simple method that enables aggressive token dropping (up to 75%) while preserving quality. SPRINT leverages the complementary roles of shallow and deep layers: early layers process all tokens to capture local detail, deeper layers operate on a sparse subset to cut computation, and their outputs are fused through residual connections. Training follows a two-stage schedule: long masked pre-training for efficiency followed by short full-token fine-tuning to close the train--inference gap. On ImageNet-1K 256x256, SPRINT achieves 9.8x training savings with comparable FID/FDD, and at inference, its Path-Drop Guidance (PDG) nearly halves FLOPs while improving quality. These results establish SPRINT as a simple, effective, and general solution for efficient DiT training.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 训练开销高：Diffusion Transformer训练成本随序列长度呈二次增长，导致大规模预训练计算与显存不可承受（见第1页摘要/引言）。<br>• 简单丢token失效：朴素或随机的token dropping会削弱表示、在推理用全token时出现显著的train–inference gap，质量下滑（第1页）。<br>• 现有方法局限：许多方法要么参数/模块开销大，要么只能在中等掩码（≤50%）下稳定；在激进比例（如75%）时性能崩溃，且与对齐类损失（如REPA）往往不兼容或不稳定（第2页相关工作）。<br>• 架构冗余：同质DiT让深层对所有稠密token重复计算局部细节，深层应聚焦全局语义却浪费大量FLOPs，导致收敛慢（第4页“瓶颈”）。<br>• 推理成本高：标准CFG需两次前向使FLOPs近乎翻倍，限制部署效率（第5页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>SPRINT将DiT划分为编码器—中间块—解码器：编码器在全部token上提取稠密浅层局部特征；在进入中间块前以高比例（如75%）丢弃token，仅在稀疏子集上建模全局语义，并用[MASK]回填至原长度后与稠密浅层特征通道拼接、线性投影融合，再交由解码器预测全token。训练采用“两阶段”：长时高掩码预训练+短暂全token微调以弥合训练-推理差距；推理端提出Path-Drop Guidance，用浅路径近似无条件分支，基本将采样FLOPs减半并提升质量。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 自适应稀疏路由的SPRINT：引入可学习的token重要性/时刻自适应drop ratio，替代随机丢弃以进一步提升高掩码稳定性与质量。<br>• 时空SPRINT用于视频扩散：将稠密-稀疏残差融合扩展到时空轴与时序注意力，降低视频DiT的计算复杂度并保持语义一致性。<br>• PDG++低成本引导框架：结合浅路径自蒸馏与可学习融合权重，系统比较/统一PDG、CFG与Auto-Guidance在不同噪声日程下的效率—质量权衡。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MARS-M: When Variance Reduction Meets Matrices</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21800" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21800" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Matrix-based preconditioned optimizers, such as Muon, have recently been shown to be more efficient than scalar-based optimizers for training large-scale neural networks, including large language models (LLMs). On the other hand, recent benchmarks on optimizers for LLM pre-training have demonstrated that variance-reduction techniques such as MARS can achieve substantial speedups over standard optimizers that do not employ variance reduction. In this paper, to achieve the best of both worlds, we introduce MARS-M, a new optimizer that integrates the variance reduction technique in MARS with Muon. Under standard regularity conditions, we prove that Muon-M converges to a first-order stationary point at a rate of mathcal{O}(T^{-1/3}), which improves upon mathcal{O}(T^{-1/4}) rate attained by Muon. Our empirical results on language modeling and computer vision tasks demonstrate that MARS-M consistently yields lower losses and improved performance across various downstream benchmarks. The implementation of MARS-M is available at https://github.com/AGI-Arena/MARS/MARS_M.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：如何把对LLM有效的“矩阵型”预条件优化（如Muon/Moonlight）与能显著降噪提速的方差缩减方法（如MARS）有机结合，兼得二者优势。<br>• 重要性：矩阵型优化器在大模型训练中已优于标量型方法，而方差缩减在近期预训练基准中带来额外加速；若能融合，将直接提升LLM与视觉大模型的收敛速度与稳定性。<br>• 现有局限1：传统方差缩减（SAG/SVRG/SARAH/SPIDER/STORM等）与深度学习实践、预条件优化存在不兼容，难以在大规模神经网络中落地稳定有效。<br>• 现有局限2：已有融合尝试（如MARS-Shampoo）在实践中仍不如向量型变体，表明“如何与矩阵预条件器正确耦合”仍未解决。<br>• 现有局限3：Muon在理论上仅有O(T^{-1/4})收敛率；且精确方差校正需双次梯度计算，计算/工程代价偏高，需要高效近似方案。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出MARS-M：在Moonlight（Muon变体）的矩阵动量上引入MARS式“缩放的梯度校正+裁剪”，并用Newton–Schulz近似UtVt^T进行矩阵预条件更新，同时保留Moonlight的0.2·√max(m,n)尺度与权重衰减；给出精确与近似两种实现，理论上达到O~(T^{-1/3})收敛率，且近似版可等价为“动量参数重设”的Moonlight以降低计算开销。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• MARS-M++：自适应γ与分层动量的矩阵方差缩减优化器——按层/按形状自适应调节γ与动量，提升稳健性并给出统一收敛分析与大模型实证<br>• 大批量与异步并行下的MARS-M：理论与系统协同优化——面向分布式/流水线/异步训练设计通信高效的校正策略与误差补偿机制<br>• 张量/稀疏结构上的MARS-M：与Shampoo/SOAP/PolarGrad的统一框架——将方差缩减推广到Kronecker/张量预条件与稀疏/低秩结构，兼顾理论保证与工程可扩展性</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Multi-Agent Evolve: LLM Self-Improve through Co-evolution</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23595" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23595" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement Learning (RL) has demonstrated significant potential in enhancing the reasoning capabilities of large language models (LLMs). However, the success of RL for LLMs heavily relies on human-curated datasets and verifiable rewards, which limit their scalability and generality. Recent Self-Play RL methods, inspired by the success of the paradigm in games and Go, aim to enhance LLM reasoning capabilities without human-annotated data. However, their methods primarily depend on a grounded environment for feedback (e.g., a Python interpreter or a game engine); extending them to general domains remains challenging. To address these challenges, we propose Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in solving diverse tasks, including mathematics, reasoning, and general knowledge Q&A. The core design of MAE is based on a triplet of interacting agents (Proposer, Solver, Judge) that are instantiated from a single LLM, and applies reinforcement learning to optimize their behaviors. The Proposer generates questions, the Solver attempts solutions, and the Judge evaluates both while co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves an average improvement of 4.54% on multiple benchmarks. These results highlight MAE as a scalable, data-efficient method for enhancing the general reasoning abilities of LLMs with minimal reliance on human-curated supervision.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 现有用于提升LLM推理能力的RL方法高度依赖人工标注数据与可验证奖励（如标准答案/可执行环境），难以规模化且通用性有限（第1页导言）。<br>• 现有LLM自博弈(Self-Play)多依赖具象化环境（Python解释器、游戏引擎）提供可验证反馈，难迁移到开放领域；通用推理任务的奖励设计本质含糊（第1页）。<br>• 传统两人零和对弈范式与通用任务不完全契合，模型间交互受限且训练易不稳、数据集易劣化，需要一种稳定的自进化闭环与数据质量控制（第3页零和讨论；第9-11页稳定性与质量过滤）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>将同一LLM实例化为Proposer–Solver–Judge三角色，形成“提出-求解-评判”的闭环；以Judge的通用评分为自奖励，结合难度奖励（Solver失误越多越高）、质量/格式奖励与题目质量过滤，并用Task-Relative REINFORCE++对三角色同步更新，实现无需人工标注与外部验证器的跨领域自进化（见图1第2页、算法1第6页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Scaling Multi-Agent Evolve to Larger Backbones and Multi-Role Systems：系统性研究规模律与稳定性，在更大基座与新增角色（如Planner/Critic/Verifier）下的收益与失稳边界。<br>• Hybrid Verifiable-and-Judge Rewards for Open-Domain Self-Evolution：将可验证环境（代码执行/单元测试/检索校验）与LLM-as-a-Judge融合，降低奖励偏差与“投机取巧”，提升可泛化性。<br>• Difficulty-Aware Curriculum and Convergence Theory for Co-evolutionary LLMs：建立协同进化的难度控制与收敛理论，设计自适应难度调度器与反奖励攻击防护，保证长期稳定增益。</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 12</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>← 上一页</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button><button class="page-btn" onclick="goToPage(11)">11</button><button class="page-btn" onclick="goToPage(12)">12</button></span>
            <button id="next-btn" onclick="changePage(1)">下一页 →</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-10-28 23:18:08 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 12;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>