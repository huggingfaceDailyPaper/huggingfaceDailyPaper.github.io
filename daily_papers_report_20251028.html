<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - October 28, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "SF Pro Display", -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Helvetica Neue", "Arial", "Noto Sans", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", sans-serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">October 28, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23607" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23607" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Humans learn abstract concepts through multisensory synergy, and once formed, such representations can often be recalled from a single modality. Inspired by this principle, we introduce Concerto, a minimalist simulation of human concept learning for spatial cognition, combining 3D intra-modal self-distillation with 2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more coherent and informative spatial features, as demonstrated by zero-shot visualizations. It outperforms both standalone SOTA 2D and 3D self-supervised models by 14.2% and 4.8%, respectively, as well as their feature concatenation, in linear probing for 3D scene perception. With full fine-tuning, Concerto sets new SOTA results across multiple scene understanding benchmarks (e.g., 80.7% mIoU on ScanNet). We further present a variant of Concerto tailored for video-lifted point cloud spatial understanding, and a translator that linearly projects Concerto representations into CLIP's language space, enabling open-world perception. These results highlight that Concerto emerges spatial representations with superior fine-grained geometric and semantic consistency.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šå•æ¨¡æ€è‡ªç›‘ç£ï¼ˆ2Då›¾åƒä¸3Dç‚¹äº‘ï¼‰å­¦åˆ°çš„ç©ºé—´è¡¨ç¤ºäº’è¡¥è€Œä¸é‡å ï¼Œç®€å•æ‹¼æ¥è™½æœ‰æå‡å´æ— æ³•äº§ç”Ÿç»Ÿä¸€ã€å¯é¢„æµ‹çš„ç©ºé—´æ¦‚å¿µè¡¨ç¤º<br>â€¢ é‡è¦æ€§ï¼šç©ºé—´è®¤çŸ¥ä»»åŠ¡ï¼ˆè‡ªåŠ¨é©¾é©¶ã€æ··åˆç°å®ã€æœºå™¨äººï¼‰éœ€è¦æ— æ ‡æ³¨ã€å¯æ³›åŒ–ã€å‡ ä½•-è¯­ä¹‰ä¸€è‡´çš„è¡¨ç¤ºï¼›æ›´å¼ºçš„åº•åº§è¡¨ç¤ºèƒ½æ˜¾è‘—æå‡ä¸‹æ¸¸3Dåœºæ™¯ç†è§£ä¸æ•°æ®æ•ˆç‡<br>â€¢ ç°æœ‰å±€é™ï¼š2D SSLï¼ˆå¦‚DINOv2ï¼‰ç¼ºä¹å‡ ä½•æ„è¯†ï¼Œ3D SSLï¼ˆå¦‚Sonataï¼‰éš¾æ•è·ç»†ç²’åº¦çº¹ç†/è¯­ä¹‰ï¼›åæœŸç‰¹å¾çº§èåˆç¼ºå°‘è®­ç»ƒæœŸçš„è·¨æ¨¡æ€ååŒï¼Œéš¾ä»¥å½¢æˆå¯ç”±å•æ¨¡æ€å¬å›çš„ç»Ÿä¸€è¡¨å¾<br>â€¢ ç°æœ‰è·¨æ¨¡æ€åšæ³•å¤šâ€œæ¨¡ä»¿â€å›¾åƒç‰¹å¾æˆ–åœ¨æ¨ç†æ—¶ä¾èµ–å›¾åƒï¼Œå¿½è§†è·¨æ¨¡æ€äº’ä¿ƒä¸è¡¨ç¤ºè‡ªæ´½ï¼›3D SSLæ˜“é™·å…¥â€œå‡ ä½•æ·å¾„â€ï¼Œæ³›åŒ–ä¸å¼€æ”¾è¯æ±‡èƒ½åŠ›ä¸è¶³<br>â€¢ è¯æ®ä¸å·®è·ï¼šä½œè€…å®è¯æ˜¾ç¤º2D/3Dæ‹¼æ¥ä¼˜äºä»»ä¸€å•æ¨¡æ€ï¼Œä½†ä»ä¸åŠè”åˆå­¦ä¹ ï¼›è€ŒConcertoåœ¨ScanNetçº¿æ€§æ¢æµ‹æå‡è‡³77.3%mIoUï¼ˆè¾ƒSonata+4.8%ï¼Œè¾ƒDINOv2+14.2%ï¼‰ï¼Œè¡¨æ˜å­˜åœ¨æ›´ä¼˜çš„è·¨æ¨¡æ€ååŒè¡¨å¾ç©ºé—´</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>Concertoä»¥ç®€æ´çš„è”åˆè‡ªç›‘ç£æ¡†æ¶ï¼Œå°†3Dç‚¹äº‘çš„è‡ªè’¸é¦ï¼ˆSonataå¼æ•™å¸ˆ-å­¦ç”Ÿã€åœ¨çº¿èšç±»ï¼‰ä¸2Dâ†’3Dçš„JEPAé£æ ¼è”åˆåµŒå…¥é¢„æµ‹ç›¸è€¦åˆï¼šåˆ©ç”¨ç›¸æœºå‚æ•°å»ºç«‹ç‚¹-åƒç´ å¯¹åº”ï¼Œç”¨ç‚¹ç‰¹å¾é¢„æµ‹å›¾åƒè¡¥ä¸ç‰¹å¾å¹¶ä»¥ä½™å¼¦ç›¸ä¼¼åº¦å¯¹é½ï¼ŒåŒæ—¶ä¿æŒ3Dåˆ†æ”¯çš„è‡ªè’¸é¦ç¨³å®šä¼˜åŒ–ï¼Œä»è€Œåœ¨å•ç‚¹äº‘æ¨ç†æ—¶æ¶Œç°å‡ºå‡ ä½•-è¯­ä¹‰ä¸€è‡´çš„ç»Ÿä¸€ç©ºé—´è¡¨ç¤ºã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Native 2Dâ€“3D Joint Pretraining: è§£å†»å›¾åƒä¸ç‚¹äº‘ç¼–ç å™¨ï¼Œæ„å»ºå¯¹ç§°ç›®æ ‡ä¸ååŒä¼˜åŒ–ï¼Œå®ç°ç«¯åˆ°ç«¯åŸç”Ÿå¤šæ¨¡æ€è‡ªç›‘ç£è¡¨å¾ç©ºé—´<br>â€¢ Deep Language Grounding for 3D: è¶…è¶Šçº¿æ€§æŠ•å½±çš„æ–‡æœ¬å¯¹é½ï¼Œè®¾è®¡ç»†ç²’åº¦/ç»„åˆè¯­ä¹‰ç›®æ ‡ä¸å¤šç²’åº¦å¯¹é½å¤´ï¼Œèµ‹èƒ½å¼€æ”¾è¯æ±‡ä¸å¤æ‚è¯­è¨€æŒ‡ä»¤<br>â€¢ Unified Omni-Domain 3D SSL: èåˆå®¤å†…/æˆ·å¤–/æ¿€å…‰é›·è¾¾/è§†é¢‘æŠ¬å‡ç‚¹äº‘çš„ç»Ÿä¸€é¢„è®­ç»ƒèŒƒå¼ä¸å°ºåº¦å®šå¾‹ï¼Œå…¼é¡¾LoRAç­‰è½»é‡é€‚é…ä»¥æå‡è·¨åŸŸæ³›åŒ–</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ReCode: Unify Plan and Action for Universal Granularity Control</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23564" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23564" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°å®ä»»åŠ¡éœ€è¦åœ¨ä¸åŒç²’åº¦é—´çµæ´»åˆ‡æ¢å†³ç­–ï¼Œä½†ç°æœ‰LLMä»£ç†ç¼ºä¹â€œç²’åº¦è‡ªé€‚åº”â€èƒ½åŠ›ï¼Œéš¾ä»¥åœ¨é«˜å±‚è§„åˆ’ä¸ä½å±‚åŠ¨ä½œé—´æµç•…è¿‡æ¸¡ï¼ˆè§ç¬¬2é¡µå›¾1ï¼‰<br>â€¢ ç°æœ‰èŒƒå¼åˆšæ€§åœ°å°†è§„åˆ’ä¸æ‰§è¡Œåˆ†ç¦»ï¼šReActåªèƒ½ç»†ç²’åº¦é€æ­¥è¡ŒåŠ¨ï¼ŒPlannerâ€“ExecutoräºŒæ®µå¼éš¾ä»¥åŠ¨æ€è°ƒæ•´ç²’åº¦ï¼Œå¯¼è‡´å¤æ‚/é•¿ç¨‹ä»»åŠ¡ä¸Šè„†å¼±ã€æ³›åŒ–å·®ï¼ˆç¬¬2é¡µä¸ç¬¬3é¡µç›¸å…³è®¨è®ºï¼‰<br>â€¢ ä»£ç†é€šå¸¸åœ¨å›ºå®šã€é¢„å®šä¹‰çš„åŠ¨ä½œ/è®¡åˆ’ç©ºé—´ä¸­å†³ç­–ï¼Œéš¾ä»¥ä¸ºæœªçŸ¥æƒ…å¢ƒç”Ÿæˆåˆé€‚çš„æ–°å†³ç­–ï¼›é™æ€è®¡åˆ’æ˜“å¤±æ•ˆï¼Œå±‚çº§/é‡è§„åˆ’æ–¹æ³•ä»æœªåœ¨è¡¨ç¤ºä¸Šç»Ÿä¸€â€œè®¡åˆ’=é«˜å±‚åŠ¨ä½œâ€ï¼ˆç¬¬3é¡µï¼‰<br>â€¢ è®­ç»ƒæ•°æ®å¤šä¸ºæ‰å¹³è½¨è¿¹ï¼Œç¼ºä¹å¯å­¦ä¹ çš„å±‚çº§ç»“æ„ä¸å¤šç²’åº¦æ ‡æ³¨ï¼Œé™åˆ¶äº†æ•°æ®æ•ˆç‡ä¸å±‚çº§æ¨ç†èƒ½åŠ›çš„ä¹ å¾—ï¼ˆç¬¬2é¡µï¼‰<br>â€¢ æ¨ç†é“¾æ¡å†—é•¿ã€APIè°ƒç”¨ä¸tokenå¼€é”€å¤§ï¼Œæˆæœ¬é«˜ä¸”æ˜“ç´¯ç§¯é”™è¯¯ï¼ˆç¬¬8é¡µè¡¨3æ˜¾ç¤ºReCodeæ˜¾è‘—é™ä½æˆæœ¬ï¼‰</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>ReCodeå°†â€œè®¡åˆ’ä¸è¡ŒåŠ¨â€ç»Ÿä¸€ä¸ºå•ä¸€ä»£ç è¡¨ç¤ºï¼šæŠŠé«˜å±‚è®¡åˆ’å»ºæ¨¡ä¸ºå ä½å‡½æ•°ï¼Œé€’å½’åœ°å±•å¼€ä¸ºå­å‡½æ•°ç›´è‡³åŸå­å¯æ‰§è¡ŒåŠ¨ä½œï¼Œåœ¨æ·±åº¦ä¼˜å…ˆçš„æ‰§è¡Œ-æ‰©å±•å¾ªç¯ä¸­åŠ¨æ€æ§åˆ¶å†³ç­–ç²’åº¦ï¼Œå½¢æˆå±‚çº§å†³ç­–æ ‘ä¸å¯æ‰§è¡Œè½¨è¿¹ï¼ˆè§ç¬¬2é¡µå›¾1(c)ï¼Œç¬¬5é¡µç®—æ³•1ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è‡ªé€‚åº”ç²’åº¦ç­–ç•¥ä¼˜åŒ–ï¼šç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–â€œä½•æ—¶åˆ†è§£ã€ä½•æ—¶æ‰§è¡Œâ€çš„å±•å¼€ç­–ç•¥ä¸æœ€å¤§é€’å½’æ·±åº¦ï¼Œä»¥æ•ˆç‡ä¸æˆåŠŸç‡ä¸ºè”åˆç›®æ ‡<br>â€¢ è½¨è¿¹é©±åŠ¨çš„å±‚çº§é¢„è®­ç»ƒï¼šåˆ©ç”¨ReCodeç”Ÿæˆçš„å¤šç²’åº¦å†³ç­–æ ‘è¿›è¡Œè‡ªç›‘ç£/æŒ‡ä»¤åŒ–é¢„è®­ç»ƒï¼Œå­¦ä¹ ç»Ÿä¸€çš„å±‚çº§è¡¨ç¤ºä¸æ˜¾å¼çŠ¶æ€å†™å…¥ç­–ç•¥<br>â€¢ è·¨æ¨¡æ€ä¸å…·èº«æ³›åŒ–çš„ReCodeï¼šå°†é€’å½’ä»£ç ç”ŸæˆèŒƒå¼æ‰©å±•åˆ°è§†è§‰/æœºå™¨äººç¯å¢ƒï¼Œåœ¨çœŸå®æ„ŸçŸ¥ä¸ç‰©ç†çº¦æŸä¸‹è¯„ä¼°ç²’åº¦æ§åˆ¶ä¸é²æ£’æ€§</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">A Survey of Data Agents: Emerging Paradigm or Overstated Hype?</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23587" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23587" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapid advancement of large language models (LLMs) has spurred the emergence of data agents--autonomous systems designed to orchestrate Data + AI ecosystems for tackling complex data-related tasks. However, the term "data agent" currently suffers from terminological ambiguity and inconsistent adoption, conflating simple query responders with sophisticated autonomous architectures. This terminological ambiguity fosters mismatched user expectations, accountability challenges, and barriers to industry growth. Inspired by the SAE J3016 standard for driving automation, this survey introduces the first systematic hierarchical taxonomy for data agents, comprising six levels that delineate and trace progressive shifts in autonomy, from manual operations (L0) to a vision of generative, fully autonomous data agents (L5), thereby clarifying capability boundaries and responsibility allocation. Through this lens, we offer a structured review of existing research arranged by increasing autonomy, encompassing specialized data agents for data management, preparation, and analysis, alongside emerging efforts toward versatile, comprehensive systems with enhanced autonomy. We further analyze critical evolutionary leaps and technical gaps for advancing data agents, especially the ongoing L2-to-L3 transition, where data agents evolve from procedural execution to autonomous orchestration. Finally, we conclude with a forward-looking roadmap, envisioning the advent of proactive, generative data agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ æœ¯è¯­æ··ä¹±ä¸èƒ½åŠ›è¾¹ç•Œä¸æ¸…ï¼šå½“å‰â€œæ•°æ®æ™ºèƒ½ä½“â€ä¸€è¯è¢«å¹¿æ³›ä¸”ä¸ä¸€è‡´ä½¿ç”¨ï¼Œä»ç®€å•é—®ç­”åˆ°å¤æ‚è‡ªæ²»ç³»ç»Ÿéƒ½è¢«ç§°ä¸ºAgentï¼Œå¯¼è‡´èƒ½åŠ›ã€è´£ä»»ä¸é€‚ç”¨èŒƒå›´æ¨¡ç³Š<br>â€¢ ç”¨æˆ·æœŸæœ›ä¸æ²»ç†é£é™©ï¼šæ¦‚å¿µä¸æ¸…å¼•å‘é¢„æœŸé”™é…ä¸é—®è´£éš¾é¢˜ï¼Œå‡ºç°æ•°æ®æ³„éœ²ã€åˆè§„å¤±è´¥æˆ–é”™è¯¯åˆ†ææ—¶è´£ä»»éš¾ä»¥ç•Œå®šï¼Œé˜»ç¢äº§ä¸šé‡‡çº³<br>â€¢ ç¼ºä¹ç»Ÿä¸€åˆ†çº§æ¡†æ¶ï¼šç¼ºå°‘ç±»ä¼¼SAE J3016çš„è‡ªæ²»åˆ†çº§æ ‡å‡†ï¼Œéš¾ä»¥å®¢è§‚æ¯”è¾ƒç³»ç»Ÿèƒ½åŠ›ã€æ˜ç¡®äººæœºåˆ†å·¥å¹¶æŒ‡å¯¼ç ”å‘è·¯çº¿<br>â€¢ ç ”ç©¶ä¸åº”ç”¨ç¢ç‰‡åŒ–ï¼šç°æœ‰å·¥ä½œå¤šèšç„¦å•ç‚¹ä»»åŠ¡ï¼ˆç®¡ç†/å‡†å¤‡/åˆ†ææŸä¸€ç¯èŠ‚ï¼‰ï¼Œç¼ºå°‘è¦†ç›–æ•°æ®å…¨ç”Ÿå‘½å‘¨æœŸçš„ç»Ÿä¸€è§†è§’ä¸ç³»ç»ŸåŒ–ç»¼è¿°<br>â€¢ å…³é”®æŠ€æœ¯é¸¿æ²Ÿï¼ˆL2â†’L3ï¼‰ï¼šä»â€œç¨‹åºåŒ–æ‰§è¡Œâ€åˆ°â€œè‡ªä¸»ç¼–æ’â€çš„è·ƒè¿ä»æœªè§£å†³ï¼Œå­˜åœ¨å¯¹é¢„å®šä¹‰ç®—å­/å·¥å…·çš„ä¾èµ–ã€æœ‰é™çš„æˆ˜ç•¥æ€§æ¨ç†ã€ç”Ÿå‘½å‘¨æœŸè¦†ç›–ä¸å…¨ç­‰å±€é™</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºé¦–ä¸ªé¢å‘æ•°æ®æ™ºèƒ½ä½“çš„L0â€“L5è‡ªæ²»åˆ†çº§ä½“ç³»ï¼ŒæŒ‰äººæœºä¸»å¯¼æƒè½¬ç§»åˆ»ç”»ä»äººå·¥åˆ°å®Œå…¨è‡ªæ²»çš„å…­çº§èƒ½åŠ›ä¸è´£ä»»è¾¹ç•Œï¼›æ®æ­¤å¯¹æ•°æ®ç®¡ç†ã€æ•°æ®å‡†å¤‡ä¸æ•°æ®åˆ†æè¿›è¡Œåˆ†çº§å¼ç³»ç»Ÿç»¼è¿°ï¼Œè§£æå…³é”®æ¼”è¿›è·ƒè¿ä¸æŠ€æœ¯ç¼ºå£ï¼ˆå°¤å…¶L2â†’L3ï¼‰ï¼Œå¹¶ç»™å‡ºè¿ˆå‘L4/L5çš„å‰ç»è·¯çº¿å›¾ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Toward True L3: é¢å‘å¤šä»»åŠ¡çš„è‡ªä¸»ç®¡çº¿ç¼–æ’ä¸ä¼˜åŒ–çš„æ•°æ®æ™ºèƒ½ä½“<br>â€¢ Skill/Tool Autogenesis: æ•°æ®æŠ€èƒ½è‡ªåŠ¨å‘ç°ä¸å¯éªŒè¯çš„å·¥å…·æ¼”åŒ–æœºåˆ¶<br>â€¢ LifecycleOps: è¦†ç›–ç®¡ç†â€”å‡†å¤‡â€”åˆ†æçš„ä¸€ä½“åŒ–æ•°æ®æ™ºèƒ½ä½“ä½“ç³»ç»“æ„<br>â€¢ Proactive L4: é¢å‘æ•°æ®æ¹–çš„è‡ªä¸»é—®é¢˜å‘ç°ä¸é•¿æœŸè‡ªæ²»ç†æ¡†æ¶<br>â€¢ Agentic RL for Data Agents: æ”¯æŒé•¿æ—¶åºä¸æˆ˜ç•¥æ¨ç†çš„è‡ªç›‘ç£/å¼ºåŒ–å­¦ä¹ è®­ç»ƒèŒƒå¼<br>â€¢ SafeDAgent: æ•°æ®æ™ºèƒ½ä½“çš„å®‰å…¨ã€å¯ä¿¡ä¸é—®è´£è¯„æµ‹åŸºå‡†ä¸æ²»ç†åè®®</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">FARMER: Flow AutoRegressive Transformer over Pixels</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23588" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23588" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Directly modeling the explicit likelihood of the raw data distribution is key topic in the machine learning area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose a self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design a one-step distillation scheme to significantly accelerate inference speed and introduce a resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ è¿ç»­åƒç´ è‡ªå›å½’å­˜åœ¨â€œåºåˆ—æé•¿+é«˜ç»´è¿ç»­å¯†åº¦éš¾å»ºæ¨¡â€çš„ç“¶é¢ˆï¼Œè®­ç»ƒä¸é‡‡æ ·éƒ½æ…¢ä¸”æ˜“å—é•¿ç¨‹ä¾èµ–å½±å“ï¼ˆè§å›¾1(a)ï¼ŒSec.1ï¼‰<br>â€¢ ä¼ ç»Ÿæ­£æ€åŒ–æµå°†å¤æ‚å›¾åƒåˆ†å¸ƒå¼ºè¡Œæ˜ å°„åˆ°å„å‘åŒæ€§é«˜æ–¯ï¼Œåˆ†å¸ƒè½å·®å¤§å¯¼è‡´é‡‡æ ·é€€åŒ–ä¸è´¨é‡ä¸‹é™ï¼Œå°½ç®¡å…·å¤‡ç²¾ç¡®ä¼¼ç„¶ä½†è¡¨è¾¾ä¸å¯æ§æ€§å—é™ï¼ˆè§å›¾1(b)ï¼ŒSec.3.1ï¼‰<br>â€¢ åƒç´ å†—ä½™ä½¿é«˜ç»´tokençš„ARå»ºæ¨¡å¼‚å¸¸å›°éš¾ï¼›AFåå‘ä¸¥æ ¼è‡ªå›å½’å¯¼è‡´æ¨ç†æˆä¸ºä¸»è¦ç“¶é¢ˆï¼ˆSec.1ï¼ŒSec.3.3ï¼ŒSec.3.5ï¼‰<br>â€¢ ç°æœ‰åƒJetFormerçš„ç»´åº¦å› å­åŒ–å‡è®¾å†—ä½™é€šé“ç‹¬ç«‹ä¸”æœä»æ ‡å‡†é«˜æ–¯ï¼Œæ˜“é€ æˆä¿¡æ¯ä¸¢å¤±å¹¶é™åˆ¶ä¸æ¡ä»¶cçš„äº¤äº’ï¼ˆSec.3.3ï¼‰</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºFARMERï¼šä»¥å¯é€†è‡ªå›å½’æµ(AF)å°†å›¾åƒpatchæ˜ å°„ä¸ºæ½œåœ¨åºåˆ—ï¼Œå¹¶ç”±å› æœAR Transformerç”¨GMMå»ºæ¨¡å…¶æ¡ä»¶åˆ†å¸ƒï¼Œä»è€Œåœ¨åƒç´ åŸŸå®ç°ç«¯åˆ°ç«¯ä¸”å…·æ˜¾å¼ä¼¼ç„¶çš„ç”Ÿæˆï¼›åŒæ—¶é€šè¿‡è‡ªç›‘ç£é€šé“ç»´åº¦åˆ’åˆ†ï¼ˆä¿¡æ¯/å†—ä½™ï¼Œå‰è€…é€tokenå»ºæ¨¡ã€åè€…å…±äº«GMMï¼‰ã€é‡é‡‡æ ·å¼CFGä¸å•æ­¥è’¸é¦åŠ é€ŸAFåå‘ï¼Œå®ç°æ›´æ˜“å»ºæ¨¡ä¸æ›´å¿«é‡‡æ ·ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ å­¦ä¹ å¼å»é‡åŒ–FARMERï¼šä»¥å¯å­¦ä¹ /å¯é€†å»é‡åŒ–æ›¿ä»£å›ºå®šé«˜æ–¯å™ªå£°ï¼Œå…¼é¡¾æ˜¾å¼ä¼¼ç„¶ä¸ç»†èŠ‚ä¿çœŸ<br>â€¢ å‡ ä½•ä¸€è‡´çš„Log-Detæ­£åˆ™åŒ–ï¼šé’ˆå¯¹ä½“ç§¯å˜åŒ–å¼•å…¥å‡ ä½•çº¦æŸä¸ç¨³å®šåŒ–é¡¹ï¼Œç¼“è§£é«˜logdeté€ æˆçš„æ¨¡ç³Šä¸é‡å»ºè¯¯å·®<br>â€¢ è‡ªé€‚åº”é€šé“åˆ†ç»„ä¸å±‚æ¬¡å…±äº«GMMï¼šä»å•ä¸€å…±äº«åˆ†å¸ƒæ‰©å±•åˆ°åŠ¨æ€/å±‚æ¬¡åˆ†ç»„ï¼Œæå‡ä¿¡æ¯-å†—ä½™è§£è€¦ä¸é‡‡æ ·æ•ˆç‡</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23581" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23581" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Audio-driven human animation models often suffer from identity drift during temporal autoregressive generation, where characters gradually lose their identity over time. One solution is to generate keyframes as intermediate temporal anchors that prevent degradation, but this requires an additional keyframe generation stage and can restrict natural motion dynamics. To address this, we propose Lookahead Anchoring, which leverages keyframes from future timesteps ahead of the current generation window, rather than within it. This transforms keyframes from fixed boundaries into directional beacons: the model continuously pursues these future anchors while responding to immediate audio cues, maintaining consistent identity through persistent guidance. This also enables self-keyframing, where the reference image serves as the lookahead target, eliminating the need for keyframe generation entirely. We find that the temporal lookahead distance naturally controls the balance between expressivity and consistency: larger distances allow for greater motion freedom, while smaller ones strengthen identity adherence. When applied to three recent human animation models, Lookahead Anchoring achieves superior lip synchronization, identity preservation, and visual quality, demonstrating improved temporal conditioning across several different architectures. Video results are available at the following link: https://lookahead-anchoring.github.io.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šåˆ†æ®µè‡ªå›å½’çš„éŸ³é¢‘é©±åŠ¨äººåƒè§†é¢‘åœ¨é•¿æ—¶ç”Ÿæˆä¸­å‡ºç°èº«ä»½æ¼‚ç§»ä¸è´¨é‡é€€åŒ–ï¼Œå°è¯¯å·®éšæ®µè½ç´¯ç§¯ï¼Œå”‡å½¢åŒæ­¥ä¸ç»†èŠ‚é€æ­¥å˜å·®ï¼›è€ŒDiTçš„äºŒæ¬¡å¤æ‚åº¦ä½¿å•æ¬¡åªèƒ½ç”ŸæˆçŸ­çª—ï¼ˆâ‰ˆ5sï¼‰ï¼Œä¸å¾—ä¸åˆ†æ®µï¼Œæ¼‚ç§»æ›´æ˜¾è‘—ã€‚<br>â€¢ å†²çªæ ¹æºï¼šç°æœ‰â€œèµ·å§‹å¸§ï¼ˆç¡¬çº¦æŸï¼‰+å‚è€ƒå›¾åƒï¼ˆè½¯å¼•å¯¼ï¼‰â€çš„åŒæ¡ä»¶ç›¸äº’å†²çªï¼Œæ¨¡å‹å¾€å¾€ä¼˜å…ˆæ»¡è¶³èµ·å§‹å¸§çš„ä¸€è‡´æ€§è€Œé€æ­¥å¿½è§†å‚è€ƒèº«ä»½ï¼Œå¯¼è‡´å¤–è§‚åç¦»ã€‚<br>â€¢ æ—¢æœ‰æ–¹æ¡ˆå±€é™ï¼šå…³é”®å¸§æ’å€¼æ³•å°†å…³é”®å¸§å½“ä½œæ®µè½è¾¹ç•Œçš„ç¡¬çº¦æŸï¼Œéœ€é¢å¤–å…³é”®å¸§ç”Ÿæˆå™¨ã€è¡¨è¾¾å—é™ä¸”æœ€ç»ˆè´¨é‡å—å…³é”®å¸§ä¸Šé™åˆ¶çº¦ï¼›çº¯æ¨ç†æ—¶çª—/å™ªå£°æŠ€å·§æˆ–è®­ç»ƒæ³•å¤šç¼ºä¹èº«ä»½ä¿æŒæœºåˆ¶ï¼Œé•¿æ—¶ä¸€è‡´æ€§æ”¹å–„æœ‰é™ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºLookahead Anchoringï¼šå°†å…³é”®å¸§æ”¾åœ¨å½“å‰ç”Ÿæˆçª—å£ä¹‹å¤–çš„æœªæ¥Då¸§å¤„ï¼Œä½œä¸ºæŒç»­â€œè¿½éšâ€çš„æ–¹å‘æ€§é”šç‚¹ï¼Œç”¨éŸ³é¢‘é©±åŠ¨å½“ä¸‹è¿åŠ¨ã€ç”¨è¿œæœŸé”šç‚¹ç»´æŒèº«ä»½ã€‚å…·ä½“åœ¨è§†é¢‘DiTä¸­è¿½åŠ ä¸€å¸§å¹²å‡€çš„æ¡ä»¶æ½œå˜é‡å¹¶èµ‹äºˆè¿œæœŸæ—¶é—´ä½ç½®ç¼–ç ï¼Œé…åˆè·¨å†…å¤–çª—çš„è·ç¦»éšæœºé‡‡æ ·å¾®è°ƒä¸â€œè‡ªå…³é”®å¸§â€ï¼ˆç›´æ¥ä»¥å‚è€ƒå›¾åƒä¸ºè¿œæœŸç›®æ ‡ï¼‰ï¼Œæ— éœ€å•ç‹¬å…³é”®å¸§æ¨¡å‹å³å¯åœ¨é•¿è§†é¢‘ä¸­å…¼é¡¾èº«ä»½ä¸€è‡´æ€§ä¸è¡¨è¾¾æ€§ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è‡ªé€‚åº”å‰è§†è·ç¦»è°ƒåº¦çš„é•¿è§†é¢‘äººåƒç”Ÿæˆï¼šåœ¨çº¿ä¼°è®¡å”‡å½¢åŒæ­¥ä¸èº«ä»½ä¸€è‡´æ€§ä¿¡å·ï¼ŒåŠ¨æ€è°ƒæ•´Dä¸é”šç‚¹å¼ºåº¦ï¼Œå®ç°è¡¨è¾¾æ€§â€”ä¸€è‡´æ€§çš„è‡ªåŠ¨æƒè¡¡ã€‚<br>â€¢ å¤šé”šç‚¹å‰è§†ä¸å™äº‹å¯æ§çš„äººåƒåŠ¨ç”»ï¼šä¸²è”/å¹¶è¡Œä½¿ç”¨ç»æ–‡æœ¬ç¼–è¾‘ç”Ÿæˆçš„å¤šå¼ é”šç‚¹å›¾ï¼Œå­¦ä¹ å¤šé”šèåˆä¸æ—¶åºåˆ‡æ¢ç­–ç•¥ï¼Œå®ç°æƒ…ç»ª/å§¿æ€/åœºæ™¯çš„å™äº‹é©±åŠ¨ã€‚<br>â€¢ è·ç¦»æ„ŸçŸ¥æ—¶é—´ä½ç½®ç¼–ç ä¸å½±å“è¡°å‡å»ºæ¨¡ï¼šè®¾è®¡è¿ç»­åŒ–ã€å¯å­¦ä¹ çš„è·ç¦»â€”å½±å“å‡½æ•°ï¼ˆä½ç½®ç¼–ç /æ³¨æ„åŠ›è°ƒåˆ¶/æŸå¤±çº¦æŸï¼‰ï¼Œç³»ç»Ÿåˆ»ç”»é”šç‚¹éšæ—¶é—´è·ç¦»çš„è½¯å¼•å¯¼å¼ºåº¦å¹¶æå‡æ³›åŒ–ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21817" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21817" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current Vision-Language-Action (VLA) models are often constrained by a rigid, static interaction paradigm, which lacks the ability to see, hear, speak, and act concurrently as well as handle real-time user interruptions dynamically. This hinders seamless embodied collaboration, resulting in an inflexible and unresponsive user experience. To address these limitations, we introduce VITA-E, a novel embodied interaction framework designed for both behavioral concurrency and nearly real-time interruption. The core of our approach is a dual-model architecture where two parallel VLA instances operate as an ``Active Model'' and a ``Standby Model'', allowing the embodied agent to observe its environment, listen to user speech, provide verbal responses, and execute actions, all concurrently and interruptibly, mimicking human-like multitasking capabilities. We further propose a ``model-as-controller'' paradigm, where we fine-tune the VLM to generate special tokens that serve as direct system-level commands, coupling the model's reasoning with the system's behavior. Experiments conducted on a physical humanoid platform demonstrate that VITA-E can reliably handle complex interactive scenarios. Our framework is compatible with various dual-system VLA models, achieving an extremely high success rate on emergency stops and speech interruptions while also successfully performing concurrent speech and action. This represents a significant step towards more natural and capable embodied assistants.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°æœ‰VLAå¤šé‡‡ç”¨é™æ€ã€é¡ºåºå¼äº¤äº’èŒƒå¼ï¼Œç¼ºä¹â€œåŒæ—¶çœ‹-å¬-è¯´-åšâ€çš„å¹¶å‘èƒ½åŠ›ï¼Œå¯¼è‡´åä½œä½æ•ˆä¸”ä¸è‡ªç„¶ï¼ˆç¬¬2é¡µä¸‰å¤§é™åˆ¶ï¼‰<br>â€¢ ä¸å¯ä¸­æ–­ï¼šä¸€æ—¦å¼€å§‹è¯´è¯æˆ–æ‰§è¡ŒåŠ¨ä½œå³è¢«é”å®šï¼Œæ— æ³•è¢«ç”¨æˆ·æ‰“æ–­æˆ–ç´§æ€¥åˆ¶åŠ¨ï¼Œéš¾ä»¥é€‚åº”å®æ—¶å˜åŒ–ï¼ˆç¬¬2é¡µï¼‰<br>â€¢ äº¤äº’åƒµåŒ–ï¼šé€šå¸¸å‡è®¾ä¸€æ¬¡æ€§ã€é™æ€æŒ‡ä»¤ï¼Œå¿…é¡»ç­‰å½“å‰åŸå­åŠ¨ä½œ/æ¨ç†å®Œæˆåæ‰èƒ½å¤„ç†æ–°æŒ‡ä»¤ï¼Œå“åº”å»¶è¿Ÿé«˜ï¼ˆç¬¬4é¡µç›¸å…³å·¥ä½œï¼‰<br>â€¢ æ¶æ„ä¸è®­ç»ƒæƒè¡¡ï¼šç«¯åˆ°ç«¯æ–¹æ³•æ˜“å‰Šå¼±VLMç†è§£ä¸æ¨ç†ï¼›åŒç³»ç»Ÿæ–¹æ³•å¤šå¿½ç•¥äº¤äº’åŠ¨æ€ï¼›é€æ­¥è€¦åˆè¯­è¨€å‘½ä»¤çš„æ–¹æ³•é™åˆ¶å¯ç”¨VLMè§„æ¨¡ä¸æ€§èƒ½ï¼ˆç¬¬3â€“4é¡µï¼‰<br>â€¢ ç¼ºå°‘ç»Ÿä¸€çš„ç³»ç»Ÿçº§æ§åˆ¶æ¥å£ï¼Œå°†é«˜å±‚æ¨ç†ä¸ç³»ç»Ÿè¡Œä¸ºç´§å¯†è€¦åˆä»¥å®ç°å¯æŠ¢å ã€å®‰å…¨çš„æ§åˆ¶ï¼ˆè¡¨1ç¬¬6é¡µæ­ç¤ºæ‰€éœ€æ§åˆ¶tokenè¯­ä¹‰ï¼‰</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºVITA-Eï¼šé€šè¿‡åŒæ¨¡å‹å¹¶è¡Œæ¶æ„ï¼ˆActive/Standbyï¼‰ä¸â€œæ¨¡å‹å³æ§åˆ¶å™¨â€èŒƒå¼ï¼Œä»¤VLMç”Ÿæˆç‰¹æ®Šæ§åˆ¶tokenï¼ˆå¦‚[RES]ã€[ACT]ã€[INST]ã€[HALT]ã€[END]ï¼‰ç›´æ¥é©±åŠ¨ç³»ç»Ÿåœ¨â€œå¬-è¡ŒåŠ¨â€çŠ¶æ€é—´åˆ‡æ¢ï¼Œå¹¶ä¸æ‰©æ•£å¼åŠ¨ä½œä¸“å®¶ååŒï¼Œä»è€Œå®ç°å¹¶å‘ã€æŠ¢å ä¸æ€¥åœï¼ˆè§å›¾3ç¬¬5é¡µã€è¡¨1ç¬¬6é¡µï¼›å›¾4ç¬¬7é¡µç¤ºå››ç§äº¤äº’æ¨¡å¼ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Hierarchical VITA-E for Long-Horizon Embodied Tasksï¼šä»¥å±‚çº§è§„åˆ’å°†é•¿ä»»åŠ¡åˆ†è§£ä¸ºå¯ä¸­æ–­çš„åŸå­æ­¥éª¤ï¼Œå¹¶ç»“åˆåœ¨çº¿äººç±»åé¦ˆç¨³å¥æ‰§è¡Œ<br>â€¢ Resource-Efficient Dual-Model via Weight Sharing and Distillationï¼šé€šè¿‡æƒé‡å…±äº«ã€å¼‚æ­¥è°ƒåº¦ä¸çŸ¥è¯†è’¸é¦ï¼Œé™ä½åŒæ¨¡å‹ç®—åŠ›ä¸æ—¶å»¶å¼€é”€è€Œä¸ç‰ºç‰²æŠ¢å èƒ½åŠ›<br>â€¢ Safe and Smooth Task Switching without Neutral Retractionï¼šä»¥è½¨è¿¹æ¥ç»­ä¸çº¦æŸä¼˜åŒ–æ›¿ä»£å›ä¸­ç«‹ä½æ’¤é€€ï¼Œå®ç°æ›´é¡ºæ»‘çš„ä»»åŠ¡åˆ‡æ¢å¹¶ç»™å‡ºå½¢å¼åŒ–å®‰å…¨/æ—¶å»¶ä¿è¯</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">ACG: Action Coherence Guidance for Flow-based VLA models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22201" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22201" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion and flow matching models have emerged as powerful robot policies, enabling Vision-Language-Action (VLA) models to generalize across diverse scenes and instructions. Yet, when trained via imitation learning, their high generative capacity makes them sensitive to noise in human demonstrations: jerks, pauses, and jitter which reduce action coherence. Reduced action coherence causes instability and trajectory drift during deployment, failures that are catastrophic in fine-grained manipulation where precision is crucial. In this paper, we present Action Coherence Guidance (ACG) for VLA models, a training-free test-time guidance algorithm that improves action coherence and thereby yields performance gains. Evaluated on RoboCasa, DexMimicGen, and real-world SO-101 tasks, ACG consistently improves action coherence and boosts success rates across diverse manipulation tasks. Code and project page are available at https://github.com/DAVIAN-Robotics/ACG and https://DAVIAN-Robotics.github.io/ACG , respectively.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šæ¨¡ä»¿å­¦ä¹ ä¸‹çš„æ‰©æ•£/æµåŒ¹é…VLAæ¨¡å‹ä¼šâ€œè®°å¿†â€äººç±»æ¼”ç¤ºä¸­çš„æŠ–åŠ¨ã€åœé¡¿ã€æŠ–é¢¤ç­‰å™ªå£°ï¼Œå¯¼è‡´åŠ¨ä½œåºåˆ—æ—¶åºä¸ä¸€è‡´ï¼ˆåŠ¨ä½œè¿è´¯æ€§ä¸‹é™ï¼‰ã€‚<br>â€¢ é‡è¦æ€§ï¼šåŠ¨ä½œä¸è¿è´¯ä¼šåœ¨æ‰§è¡Œä¸­é€ æˆä¸¤ç±»å¤±è´¥â€”â€”å…³é”®æ—¶åˆ»ä¸ç¨³å®šï¼ˆå¦‚æŠ“å–æ—¶æŠ–åŠ¨æ¨å¼€ç‰©ä½“ï¼‰ä¸å°å™ªå£°ç´¯ç§¯å¼•å‘è½¨è¿¹æ¼‚ç§»ï¼›åœ¨æŒ‰é’®æŒ‰å‹ã€æ’å…¥ç­‰ç²¾ç»†æ“ä½œä»»åŠ¡ä¸­å°¤ä¸ºè‡´å‘½ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼š<br> - æ—¶åŸŸå¹³æ»‘/å¤šæ¬¡é‡‡æ ·é›†æˆä»…å¸¦æ¥æ¸©å’Œæå‡ï¼Œæ˜“æ¨¡ç³Šç»†ç²’åº¦åŠ¨ä½œç»†èŠ‚ï¼Œç‰ºç‰²å‡†ç¡®æ€§ï¼›<br> - åŠ¨ä½œåˆ†å—/é€’è¿›è§†ç•Œå¯å‡å°ç´¯ç§¯è¯¯å·®ï¼Œä½†æ— æ³•æ¶ˆé™¤åˆ†å—å†…ï¼ˆintra-chunkï¼‰ä¸è¿è´¯ï¼›å¯¹æµç­–ç•¥åšæ—¶åºé›†æˆæ¨ç†å¼€é”€å¤§ï¼›<br> - è¯­è¨€æ¡ä»¶çš„æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰åœ¨VLAä¸­æ˜“ä¸ç¨³å®š/è¿‡åº¦å¼ºåŒ–æ–‡æœ¬æ¡ä»¶ï¼Œä¸æå‡è¿è´¯æ€§å¹¶ä¸ä¸€è‡´ï¼Œç”šè‡³æŸä¼¤æ ·æœ¬å¤šæ ·æ€§ï¼›<br> - åŸºäºå™ªå£°çš„è´Ÿå‘æ‰°åŠ¨ï¼ˆå¦‚ç™½å™ªå£°ï¼‰å­˜åœ¨â€œè¿è´¯â€”å‡†ç¡®â€æƒè¡¡ï¼šå™ªå£°è¿‡å°æ— æ•ˆã€è¿‡å¤§ç ´åå·²å­¦å…ˆéªŒã€‚<br>â€¢ éœ€æ±‚ç¼ºå£ï¼šç¼ºå°‘ä¸€ç§è®­ç»ƒå¤–ã€ä¸è¯­è¨€æ— å…³ã€ä½å¼€é”€ä¸”ç›´æ¥é¢å‘æ—¶åºè¿è´¯æ€§çš„æ¨ç†æœŸå¼•å¯¼æ–¹æ³•ï¼Œèƒ½åœ¨ä¸å†è®­ç»ƒçš„å‰æä¸‹æå‡åŠ¨ä½œè¿è´¯æ€§ä¸æˆåŠŸç‡ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºåŠ¨ä½œè¿è´¯æ€§å¼•å¯¼ï¼ˆACGï¼‰ï¼šåœ¨æ¨ç†æœŸæ„é€ â€œæ•…æ„ä¸è¿è´¯â€çš„å‘é‡åœºï¼ˆå°†è‡ªæ³¨æ„åŠ›çš„æ³¨æ„åŠ›å›¾æ›¿æ¢ä¸ºå•ä½çŸ©é˜µï¼Œåˆ‡æ–­æ—¶é—´æ­¥é€šä¿¡ï¼‰ï¼Œå†ç”¨å¼•å¯¼å‘é‡v_ACG=(1+Î»)vâˆ’Î»v_ICæ²¿ç›¸åæ–¹å‘é‡‡æ ·ï¼Œä¿ƒä½¿ç”Ÿæˆçš„åŠ¨ä½œåˆ†å—æ›´è¿è´¯ä¸”æ— éœ€å†è®­ç»ƒã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è”åˆåˆ†å—å†…å¤–ä¸€è‡´æ€§çš„è‡ªå¼•å¯¼æµç­–ç•¥ï¼šå°†ACGä¸è·¨åˆ†å—ä¸€è‡´æ€§å¼•å¯¼ï¼ˆå¦‚Self-GADï¼‰è”åˆï¼Œè‡ªé€‚åº”æƒé‡å¹³è¡¡å†…å¤–è¿è´¯æ€§<br>â€¢ è‡ªé€‚åº”å±‚é€‰æ‹©çš„æ³¨æ„åŠ›æ‰°åŠ¨å¼•å¯¼ï¼šåœ¨çº¿å­¦ä¹ é€‰æ‹©æœ€æœ‰æ•ˆçš„è‡ªæ³¨æ„åŠ›å±‚ä¸æ‰°åŠ¨å¼ºåº¦Î»ï¼Œå®ç°ä»»åŠ¡ä¸åœºæ™¯è‡ªé€‚åº”<br>â€¢ é¢å‘å®‰å…¨ä¸æ¥è§¦çš„è¿è´¯æ€§å¼•å¯¼ï¼šå¼•å…¥åŠ›/è§¦è§‰ä¸å®‰å…¨çº¦æŸï¼Œå°†æ¥è§¦ç¨³å®šæ€§çº³å…¥å¼•å¯¼ç›®æ ‡ï¼Œé™ä½ç²¾ç»†æ“ä½œå¤±è¯¯<br>â€¢ ACG-RLï¼šåŸºäºå¼ºåŒ–ä¿¡å·çš„å¼•å¯¼å¼ºåº¦å…ƒå­¦ä¹ ï¼šç”¨ç¨€ç–æˆåŠŸå¥–åŠ±å¾®è°ƒÎ»ä¸æ‰°åŠ¨ç­–ç•¥ï¼Œæ— éœ€é‡è®­ä¸»ä½“æ¨¡å‹<br>â€¢ çŸ©å½¢æµå¼•å¯¼çš„ç†è®ºåˆ†æï¼šåˆ»ç”»ACGå¯¹å‘é‡åœºåç½®ã€ç¨³å®šæ€§ä¸æ”¶æ•›æ€§çš„å½±å“ï¼Œç»™å‡ºè¿è´¯æ€§ä¸æ€§èƒ½ä¿è¯<br>â€¢ è¶…è¶Šæ³¨æ„åŠ›çš„â€œåæ¨¡å‹â€æ„é€ ï¼šæ¢ç´¢ä¸¢å¼ƒå•å…ƒã€æ—¶é—´æ´—ç‰Œã€è·¨æ³¨æ„åŠ›å±è”½ã€è§†è§‰tokenæ‰°åŠ¨ç­‰å¤šç§è´Ÿå‘å˜ä½“<br>â€¢ å®æ—¶é«˜æ•ˆACGï¼šåˆ©ç”¨ä¸­åå±‚å¤ç”¨ä¸éƒ¨åˆ†å‰å‘ï¼Œé™è‡³<1.2Ã—æ¨ç†å¼€é”€ä»¥æ»¡è¶³å®æ—¶æ§åˆ¶</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Open Multimodal Retrieval-Augmented Factual Image Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22521" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22521" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Multimodal Models (LMMs) have achieved remarkable progress in generating photorealistic and prompt-aligned images, but they often produce outputs that contradict verifiable knowledge, especially when prompts involve fine-grained attributes or time-sensitive events. Conventional retrieval-augmented approaches attempt to address this issue by introducing external information, yet they are fundamentally incapable of grounding generation in accurate and evolving knowledge due to their reliance on static sources and shallow evidence integration. To bridge this gap, we introduce ORIG, an agentic open multimodal retrieval-augmented framework for Factual Image Generation (FIG), a new task that requires both visual realism and factual grounding. ORIG iteratively retrieves and filters multimodal evidence from the web and incrementally integrates the refined knowledge into enriched prompts to guide generation. To support systematic evaluation, we build FIG-Eval, a benchmark spanning ten categories across perceptual, compositional, and temporal dimensions. Experiments demonstrate that ORIG substantially improves factual consistency and overall image quality over strong baselines, highlighting the potential of open multimodal retrieval for factual image generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°æœ‰LMM/æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­æ˜“å‡ºç°äº‹å®ä¸ä¸€è‡´ä¸å¹»è§‰ï¼Œå°¤å…¶åœ¨ç»†ç²’åº¦å±æ€§ï¼ˆå°ºå¯¸ã€æè´¨ã€ç›¸å¯¹æ¯”ä¾‹ï¼‰ä¸æ—¶é—´æ•æ„Ÿäº‹ä»¶ä¸Šåå·®æ˜¾è‘—ï¼ˆè§ç¬¬1é¡µå›¾1ï¼‰ã€‚<br>â€¢ ä»…ä¾èµ–æ¨¡å‹å‚æ•°è®°å¿†æˆ–å°é—­/é™æ€æ£€ç´¢åº“çš„RAGæ–¹æ³•æ— æ³•æ•æ‰æœ€æ–°äº‹å®ï¼Œä¸”å¤šä¸ºå•æ¨¡æ€æˆ–æµ…å±‚è¯æ®é›†æˆï¼Œéš¾ä»¥å°†å¤–éƒ¨çŸ¥è¯†è½¬åŒ–ä¸ºå¯æ§çš„ç”Ÿæˆä¿¡å·ã€‚<br>â€¢ äº‹å®å›¾åƒç”Ÿæˆéœ€è¦æ–‡æœ¬+å›¾åƒçš„äº’è¡¥è¯æ®ï¼šæ–‡æœ¬æä¾›å±æ€§/å…³ç³»ï¼Œå›¾åƒæä¾›å¤–è§‚/æ¯”ä¾‹/ç©ºé—´é…ç½®ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€åˆ†ç¦»å¤„ç†ä¸¤ç§æ¨¡æ€ï¼Œå¯¼è‡´éš¾ä»¥è”åˆå¯¹é½ä¸æ§åˆ¶ã€‚<br>â€¢ ç¼ºä¹ç³»ç»ŸåŒ–è¯„æµ‹äº‹å®ä¸€è‡´æ€§çš„åŸºå‡†ï¼Œç°æœ‰è¯„æµ‹å¤šèšç„¦è´¨é‡/å¯¹é½/ç»„åˆï¼Œå¿½è§†å¯¹æ„ŸçŸ¥ã€ç»„åˆä¸æ—¶é—´ä¸‰ç»´äº‹å®ä¸€è‡´æ€§çš„é‡åŒ–ï¼›æœ¬æ–‡æ®æ­¤æå‡ºFIG-EvalåŸºå‡†ï¼ˆç¬¬5é¡µè¡¨1ã€ç¬¬7é¡µè¡¨3ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºORIGï¼šä¸€ç§é¢å‘å¼€æ”¾ç½‘ç»œçš„å¤šè½®â€œæ–‡æœ¬+å›¾åƒâ€è”åŠ¨æ£€ç´¢-ç­›é€‰-ç´¯ç§¯-æ‰©å±•å¼æç¤ºæ„é€ æ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”å­é—®é¢˜è§„åˆ’ã€è·¨æ¨¡æ€ç²—åˆ°ç»†è¿‡æ»¤ä¸å¤šæ¨¡æ€è¦ç´ æŠ½å–ï¼Œå°†ç²¾ç‚¼è¯æ®æ³¨å…¥å¢å¼ºæç¤ºå¹¶ä¸å‚è€ƒå›¾è”åˆå¼•å¯¼ç”Ÿæˆï¼Œä»è€Œæå‡æ„ŸçŸ¥ã€ç»„åˆä¸æ—¶é—´ä¸‰ç»´çš„äº‹å®ä¸€è‡´æ€§ï¼ˆç¬¬4é¡µå›¾2ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ End-to-End ORIG with Reinforcement Learningï¼šä»¥äº‹å®ä¸€è‡´æ€§å¾—åˆ†ä¸ºå›æŠ¥ï¼Œç«¯åˆ°ç«¯è”åˆä¼˜åŒ–æ£€ç´¢è§„åˆ’ã€è¯æ®é€‰æ‹©ä¸ç”Ÿæˆå™¨æ§åˆ¶ï¼Œå‡å°‘å†—ä½™æ£€ç´¢å¹¶å¢å¼ºå¯æ§æ€§ã€‚<br>â€¢ Streaming-FIGï¼šé¢å‘æµå¼æ–°é—»/ç¤¾åª’çš„æ—¶é—´æ„ŸçŸ¥äº‹å®å›¾åƒç”Ÿæˆï¼Œæ„å»ºå¯å¢é‡æ›´æ–°çš„å¤šæ¨¡æ€çŸ¥è¯†åº“ä»¥å¤„ç†å®ä½“çŠ¶æ€ä¸äº‹ä»¶æ¼”åŒ–ã€‚<br>â€¢ Region-Level Evidence Grounding for FIGï¼šå¼•å…¥åŒºåŸŸçº§è·¨æ¨¡æ€å¯¹é½ä¸æ³¨æ„åŠ›æ§åˆ¶ï¼Œå°†æ–‡æœ¬å±æ€§ä¸å‚è€ƒå›¾åƒåŒºåŸŸç²¾ç¡®æ˜ å°„åˆ°ç”Ÿæˆè¿‡ç¨‹ï¼Œå®ç°ç»†ç²’åº¦å¤–è§‚ä¸å…³ç³»çš„å¯æ§åˆæˆã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">E^2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22733" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22733" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Text embedding models serve as a fundamental component in real-world search applications. By mapping queries and documents into a shared embedding space, they deliver competitive retrieval performance with high efficiency. However, their ranking fidelity remains limited compared to dedicated rerankers, especially recent LLM-based listwise rerankers, which capture fine-grained query-document and document-document interactions. In this paper, we propose a simple yet effective unified framework E^2Rank, means Efficient Embedding-based Ranking (also means Embedding-to-Rank), which extends a single text embedding model to perform both high-quality retrieval and listwise reranking through continued training under a listwise ranking objective, thereby achieving strong effectiveness with remarkable efficiency. By applying cosine similarity between the query and document embeddings as a unified ranking function, the listwise ranking prompt, which is constructed from the original query and its candidate documents, serves as an enhanced query enriched with signals from the top-K documents, akin to pseudo-relevance feedback (PRF) in traditional retrieval models. This design preserves the efficiency and representational quality of the base embedding model while significantly improving its reranking performance. Empirically, E^2Rank achieves state-of-the-art results on the BEIR reranking benchmark and demonstrates competitive performance on the reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also show that the ranking training process improves embedding performance on the MTEB benchmark. Our findings indicate that a single embedding model can effectively unify retrieval and reranking, offering both computational efficiency and competitive ranking accuracy.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ åµŒå…¥å¼æ£€ç´¢å™¨è™½é«˜æ•ˆï¼Œä½†æ’åä¿çœŸåº¦ä¸åŠLLMåˆ—è¡¨å¼é‡æ’å™¨ï¼Œéš¾ä»¥æ•æ‰ç»†ç²’åº¦çš„æŸ¥è¯¢-æ–‡æ¡£ä¸æ–‡æ¡£-æ–‡æ¡£äº¤äº’ï¼Œå­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼ˆå¦‚BEIRåŸºå‡†ï¼‰ã€‚<br>â€¢ ç°æœ‰LLMåˆ—è¡¨å¼é‡æ’æ¨ç†æ˜‚è´µã€æ—¶å»¶é«˜ï¼šéœ€ä¸€æ¬¡æ€§é•¿ä¸Šä¸‹æ–‡ç¼–ç å¹¶è‡ªå›å½’ç”Ÿæˆæ’åºï¼Œprefillä¸è§£ç å‡å¼€é”€å¤§ã€éš¾ä»¥å¹¶è¡Œï¼Œéš¾è½åœ°å®æ—¶åœºæ™¯ã€‚<br>â€¢ ä¸¤é˜¶æ®µâ€œæ£€ç´¢+é‡æ’â€ä½“ç³»è¯„åˆ†å‡½æ•°ä¸ç»Ÿä¸€ã€æ–‡æ¡£å‘é‡éš¾å¤ç”¨ï¼Œç³»ç»Ÿå¤æ‚åº¦ä¸æˆæœ¬åé«˜ã€‚<br>â€¢ ç”Ÿæˆå¼è§£ç å¹¶éæ’åºæ‰€å¿…éœ€ï¼Œå…³é”®åœ¨äºåˆ—è¡¨ä¸Šä¸‹æ–‡çš„äº¤äº’ä¿¡å·ï¼›ä½†å¦‚ä½•å°†å…¶æ³¨å…¥åµŒå…¥ç©ºé—´å¹¶ç”¨ç»Ÿä¸€ä½™å¼¦ç›¸ä¼¼åº¦æ‰“åˆ†ä»æœªè§£å†³ã€‚<br>â€¢ ä¼ªç›¸å…³åé¦ˆï¼ˆPRFï¼‰åœ¨å¯†é›†æ£€ç´¢ä¸­æœ‰æ•ˆï¼Œä½†é²æ£’æ€§ä¸é€‚ç”¨æ€§å—é™ï¼Œå°šæœªåœ¨LLMåˆ—è¡¨å¼é‡æ’æ¡†æ¶ä¸­è¢«ç³»ç»ŸåŒ–ã€ç»Ÿä¸€åœ°åˆ©ç”¨ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>E2RANKå°†â€œæŸ¥è¯¢+Top-Kå€™é€‰æ–‡æ¡£â€çš„åˆ—è¡¨å¼æç¤ºè§†ä½œPRFå¢å¼ºæŸ¥è¯¢ï¼Œå–å…¶[EOS]ä½ç½®å‘é‡ä¸å„æ–‡æ¡£å‘é‡åšä½™å¼¦ç›¸ä¼¼åº¦ï¼Œç»Ÿä¸€å®Œæˆæ£€ç´¢ä¸é‡æ’ã€‚è®­ç»ƒé‡‡ç”¨â€œä¸¤é˜¶æ®µ+å¤šä»»åŠ¡â€ï¼šå…ˆç”¨InfoNCEå¯¹æ¯”å­¦ä¹ è·å¾—å¼ºåµŒå…¥ï¼Œå†å¼•å…¥åŸºäºLLMæ ‡æ³¨å¯¹çš„RankNetæŸå¤±å¯¹åˆ—è¡¨å¼æç¤ºè¿›è¡Œä¼˜åŒ–ï¼Œå°†æ–‡æ¡£-æ–‡æ¡£/æŸ¥è¯¢-æ–‡æ¡£äº¤äº’æ³¨å…¥åµŒå…¥ç©ºé—´ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ AutoPRF-Rankï¼šè‡ªé€‚åº”é€‰æ‹©PRFæ–‡æ¡£ä¸åŠ¨æ€Kçš„åˆ—è¡¨å¼æç¤ºä¼˜åŒ–ï¼›ç»“åˆä¸ç¡®å®šæ€§ä¼°è®¡/å¼ºåŒ–å­¦ä¹ ï¼Œåœ¨å›ºå®šæ—¶å»¶é¢„ç®—ä¸‹æå‡é‡æ’æ•ˆæœã€‚<br>â€¢ TokenBudget-PRFï¼šé¢å‘ä½æ—¶å»¶çš„ä»¤ç‰Œé¢„ç®—PRFå‹ç¼©ä¸æç¤ºç¼–æ’ï¼›é€šè¿‡æ–‡æ¡£æ‘˜è¦/è¡¨ç¤ºæ± åŒ–/å¯å­¦ä¹ å‹ç¼©å™¨åœ¨æœ‰é™ä¸Šä¸‹æ–‡å†…æœ€å¤§åŒ–äº¤äº’ä¿¡æ¯å¯†åº¦ã€‚<br>â€¢ Click2Rankï¼šåŸºäºçœŸå®ç‚¹å‡»/åœç•™ç­‰äº¤äº’æ—¥å¿—çš„åœ¨çº¿E2RANKå¢é‡å­¦ä¹ ï¼›åˆ©ç”¨æˆå¯¹/åˆ—è¡¨åå¥½ä¿¡å·ä¸å»¶è¿Ÿçº¦æŸå®ç°æŒç»­ä¼˜åŒ–ä¸é¢†åŸŸè‡ªé€‚åº”ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22706" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22706" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Humans naturally perceive the geometric structure and semantic content of a 3D world as intertwined dimensions, enabling coherent and accurate understanding of complex scenes. However, most prior approaches prioritize training large geometry models for low-level 3D reconstruction and treat high-level spatial understanding in isolation, overlooking the crucial interplay between these two fundamental aspects of 3D-scene analysis, thereby limiting generalization and leading to poor performance in downstream 3D understanding tasks. Recent attempts have mitigated this issue by simply aligning 3D models with specific language models, thus restricting perception to the aligned model's capacity and limiting adaptability to downstream tasks. In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an end-to-end large unified transformer to unify the knowledge for both spatial reconstruction and instance-level contextual understanding. Specifically, we design a 3D-Consistent Contrastive Learning strategy that guides IGGT to encode a unified representation with geometric structures and instance-grounded clustering through only 2D visual inputs. This representation supports consistent lifting of 2D visual inputs into a coherent 3D scene with explicitly distinct object instances. To facilitate this task, we further construct InsScene-15K, a large-scale dataset with high-quality RGB images, poses, depth maps, and 3D-consistent instance-level mask annotations with a novel data curation pipeline.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç»Ÿä¸€è¡¨å¾ç¼ºå¤±ï¼šç°æœ‰â€œå‡ ä½•é‡å»ºâ†’è¯­ä¹‰ç†è§£â€çš„åˆ†æ®µæµæ°´çº¿å½¼æ­¤å‰²è£‚ï¼Œè¯¯å·®åœ¨é˜¶æ®µé—´ä¼ æ’­ï¼Œæ— æ³•è®©å‡ ä½•ä¸è¯­ä¹‰ç›¸äº’å¢å¼ºï¼Œé™åˆ¶ä¸‹æ¸¸ä»»åŠ¡ï¼ˆæœºå™¨äººã€AR/VRã€è§„åˆ’ã€ç©ºé—´QAï¼‰ã€‚<br>â€¢ VLMå¼ºè€¦åˆå¼Šç«¯ï¼šå°†å‡ ä½•ç‰¹å¾å¼ºè¡Œå¯¹é½åˆ°ç‰¹å®šVLMä¼šè¿‡åº¦å¹³æ»‘ï¼ŒæŸä¼¤é«˜é¢‘å‡ ä½•ç»†èŠ‚ä¸å¤šè§†è§’ä¸€è‡´æ€§ã€‚<br>â€¢ éš¾ä»¥å‡çº§ä¸æ³›åŒ–ï¼šä¸å•ä¸€VLMæ¶æ„å¼ºè€¦åˆï¼Œéš¾ä»¥å³æ’å³ç”¨é›†æˆæ›´å¼ºçš„æ–°æ¨¡å‹ï¼ˆå¦‚CLIPã€SigLIPï¼‰ï¼Œé™åˆ¶é€‚é…æ€§ä¸å¯æ‰©å±•æ€§ã€‚<br>â€¢ å®ä¾‹çº§åŒºåˆ†ä¸è¶³ï¼šç±»åˆ«çº§å¯¹é½éš¾ä»¥åˆ†è¾¨åŒç±»ä¸åŒå®ä¾‹ï¼Œå½±å“è·¨è§†è§’å®ä¾‹è·Ÿè¸ªä¸ç©ºé—´é—®ç­”ç­‰é«˜å±‚ä»»åŠ¡ã€‚<br>â€¢ æ•°æ®ç“¶é¢ˆï¼šç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡ã€è·¨è§†è§’3Dä¸€è‡´çš„å®ä¾‹çº§æ©ç æ ‡æ³¨ï¼Œéš¾ä»¥æœ‰æ•ˆç›‘ç£è”åˆå­¦ä¹ ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºIGGTï¼šåŸºäºå¤§å‹ç»Ÿä¸€Transformerè”åˆè®­ç»ƒå‡ ä½•å¤´ä¸å®ä¾‹å¤´ï¼Œä»å¤šè§†å›¾RGBç«¯åˆ°ç«¯é¢„æµ‹ç›¸æœºã€æ·±åº¦/ç‚¹å›¾ä¸3Dä¸€è‡´çš„å®ä¾‹ç‰¹å¾ï¼Œå¹¶é€šè¿‡çª—å£ç§»ä½çš„äº¤å‰æ³¨æ„åŠ›å°†ç»†ç²’åº¦å‡ ä½•ç»“æ„æ³¨å…¥å®ä¾‹è¡¨å¾ï¼›è¾…ä»¥3Dä¸€è‡´çš„å¤šè§†å›¾å¯¹æ¯”å­¦ä¹ çº¦æŸå®ä¾‹ç‰¹å¾ã€æ— ç›‘ç£èšç±»ç”Ÿæˆå®ä¾‹æ©ç ï¼Œå¹¶ä»¥â€œå®ä¾‹æ©ç ä¸ºæ¡¥â€å³æ’å³ç”¨è¿æ¥VLM/LMMï¼Œå®ç°å¼€æ”¾è¯æ±‡åˆ†å‰²ä¸QAåœºæ™¯å®šä½ï¼ŒåŒæ—¶æ„å»ºInsScene-15Kæä¾›é«˜è´¨é‡ç›‘ç£ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Detr-IGGT: å¯å­¦ä¹ å®ä¾‹å¤´æ›¿ä»£èšç±»çš„ç»Ÿä¸€å‡ ä½•-è¯­ä¹‰Transformerï¼šä»¥DETRå¼å®ä¾‹æŸ¥è¯¢æ›¿æ¢HDBSCANï¼Œå®ç°ç«¯åˆ°ç«¯ã€è¾¹ç•Œæ›´ç²¾ç¡®çš„å®ä¾‹åˆ†å‰²/è·Ÿè¸ªã€‚<br>â€¢ IGGT-SSL: åŸºäº3Dä¸€è‡´æ€§çš„è‡ªç›‘ç£å®ä¾‹ç‰¹å¾å­¦ä¹ ï¼šåˆ©ç”¨è·¨è§†è§’ä¸€è‡´æ€§ä¸ä¼ªæ ‡ç­¾è¿›è¡Œæ— /å¼±æ ‡æ³¨è®­ç»ƒï¼Œé™ä½å¯¹å¤§è§„æ¨¡å®ä¾‹æ ‡æ³¨çš„ä¾èµ–ã€‚<br>â€¢ Open-World IGGT: é¢å‘å®¤å¤–ä¸å¤§å°ºåº¦åœºæ™¯çš„å®ä¾‹ä¸€è‡´ä¸‰ç»´é‡å»ºï¼šæ‰©å±•è‡³æˆ·å¤–/åŸå¸‚çº§æ•°æ®ï¼Œæå‡å°ºåº¦é²æ£’æ€§ä¸é•¿åºåˆ—å¤šç›®æ ‡è·Ÿè¸ªã€‚<br>â€¢ GAUSS-IGGT: å°†å®ä¾‹æ„ŸçŸ¥å‡ ä½•è’¸é¦è¿›3DGS/NeRFè¾å°„åœºï¼šèåˆå¯æ¸²æŸ“ä¸å¯æŸ¥è¯¢çš„å®ä¾‹çº§è¯­ä¹‰-å‡ ä½•åœºï¼Œæ”¯æŒé«˜è´¨é‡æ–°è§†è§’ä¸å¼€æ”¾è¯æ±‡æ£€ç´¢ã€‚<br>â€¢ Plug-and-Play IGGT-VLM Benchmark: è·¨VLMå®ä¾‹æ¡¥æ¥çš„ç³»ç»Ÿè¯„æµ‹ï¼šæ„å»ºç»Ÿä¸€åè®®è¯„æµ‹CLIP/SigLIP/OpenSegç­‰åœ¨å®ä¾‹æ¡¥æ¥ä¸‹çš„å¼€æ”¾è¯æ±‡æ€§èƒ½ä¸ä¸€è‡´æ€§ã€‚<br>â€¢ Streaming-IGGT: åœ¨çº¿å¤šè§†å›¾å¢é‡é‡å»ºä¸å®ä¾‹è·Ÿè¸ªï¼šé¢å‘æœºå™¨äºº/ARå®æ—¶åœºæ™¯ï¼Œæ”¯æŒåœ¨çº¿ç›¸æœºä¼°è®¡ã€å‡ ä½•æ›´æ–°ä¸å®ä¾‹æŒç»­è·Ÿè¸ª/é‡è¯†åˆ«ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23451" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23451" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reward models (RMs) play a critical role in aligning AI behaviors with human preferences, yet they face two fundamental challenges: (1) Modality Imbalance, where most RMs are mainly focused on text and image modalities, offering limited support for video, audio, and other modalities; and (2) Preference Rigidity, where training on fixed binary preference pairs fails to capture the complexity and diversity of personalized preferences. To address the above challenges, we propose Omni-Reward, a step toward generalist omni-modal reward modeling with support for free-form preferences, consisting of: (1) Evaluation: We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form preferences, covering nine tasks across five modalities including text, image, video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal preference dataset comprising 248K general preference pairs and 69K instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We propose Omni-RewardModel, which includes both discriminative and generative RMs, and achieves strong performance on Omni-RewardBench as well as other widely used reward modeling benchmarks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ æ¨¡æ€ä¸å‡è¡¡ï¼šç°æœ‰å¥–åŠ±æ¨¡å‹ä¸»è¦é¢å‘æ–‡æœ¬ä¸å›¾åƒï¼Œéš¾ä»¥è¦†ç›–è§†é¢‘ã€éŸ³é¢‘ã€3Dç­‰å¼±èµ„æºæ¨¡æ€ï¼Œæ— æ³•ä¸ºä»»æ„è¾“å…¥â€”ä»»æ„è¾“å‡ºçš„å…¨æ¨¡æ€æ¨¡å‹æä¾›ä¸€è‡´å¯¹é½ä¸è¯„æµ‹ï¼ˆè§æ‘˜è¦ä¸ç¬¬1èŠ‚ï¼‰ã€‚<br>â€¢ åå¥½åˆ»æ¿ï¼šä¸»æµè®­ç»ƒä¾èµ–å›ºå®šçš„äºŒå…ƒåå¥½å¯¹ï¼Œéš¾ä»¥è¡¨è¾¾ä¸é€‚é…ç”¨æˆ·â€œè‡ªç”±è¡¨è¿°â€çš„å¤šç»´è¯„ä»·å‡†åˆ™ä¸ä¸ªæ€§åŒ–åå¥½ï¼Œå¯¼è‡´å¯¹é½çµæ´»æ€§ä¸å¯è§£é‡Šæ€§ä¸è¶³ï¼ˆç¬¬1èŠ‚ï¼‰ã€‚<br>â€¢ è¯„æµ‹ç¼ºå£ï¼šå¤šæ•°åŸºå‡†é›†ä¸­äºå•ä¸€/å°‘æ•°æ¨¡æ€ä¸å›ºå®šæ ‡å‡†ï¼Œè¾ƒå°‘æ”¯æŒåŒ…å«â€œå¹³å±€ï¼ˆtieï¼‰â€ä¸è‡ªç”±å‡†åˆ™çš„ç³»ç»Ÿæ€§è¯„æµ‹ï¼Œéš¾ä»¥å®¢è§‚è¡¡é‡RMåœ¨å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„åˆ¤åˆ«ä¸æ³›åŒ–èƒ½åŠ›ï¼ˆç¬¬2èŠ‚ï¼‰ã€‚<br>â€¢ å¯è§£é‡Šæ€§ä¸é€šç”¨æ€§ä¸è¶³ï¼šç°æœ‰åˆ¤åˆ«å¼RMæ‰“åˆ†è¿‡ç¨‹ä¸é€æ˜ã€è¯Šæ–­å›°éš¾ï¼›ä¸“ç”¨RMè·¨ä»»åŠ¡/è·¨æ¨¡æ€è¿ç§»èƒ½åŠ›æœ‰é™ï¼ˆæ‘˜è¦ä¸ç¬¬3èŠ‚ï¼‰ã€‚<br>â€¢ é‡è¦æ€§ï¼šRLHFå¯¹RMé«˜åº¦ä¾èµ–ï¼›å…¨æ¨¡æ€äº¤äº’åœºæ™¯ï¼ˆå¦‚è¯­éŸ³æŒ‡ä»¤ã€å›¾åƒç¼–è¾‘ã€è§†é¢‘ç”Ÿæˆï¼‰å¯¹â€œæœ‰ç”¨â€”æ— å®³â€”å¯ä¿¡â€çš„å¯¹é½éœ€æ±‚å¼ºçƒˆï¼ˆç¬¬1èŠ‚ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºOmni-Rewardç»Ÿä¸€æ¡†æ¶ï¼šæ„å»ºè¦†ç›–5æ¨¡æ€9ä»»åŠ¡ã€å¸¦è‡ªç”±è¡¨è¿°å‡†åˆ™ä¸å«Tieè®¾ç½®çš„Omni-RewardBenchï¼›æ±‡é›†317Kå¤šæ¨¡æ€åå¥½å¯¹ï¼ˆå«69KæŒ‡ä»¤è°ƒä¼˜ï¼‰çš„Omni-RewardDataï¼›è®­ç»ƒä¸¤ç±»é€šç”¨RMâ€”â€”åŸºäºBradleyâ€“Terryçš„åˆ¤åˆ«å¼RMä¸åŸºäºGRPOå¼ºåŒ–å­¦ä¹ ã€å¯ç”Ÿæˆâ€œè¯„è¯­+åå¥½â€çš„å¯è§£é‡Šç”Ÿæˆå¼RMã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Omni-RewardBench++ï¼šé¢å‘å¤šè½®å¯¹è¯çš„ç™¾ä¸‡çº§è‡ªç”±åå¥½å¤šæ¨¡æ€è¯„æµ‹ä¸Tieæ ¡å‡†â€”â€”æ‰©å±•è§„æ¨¡ä¸ä»»åŠ¡ç»†ç²’åº¦ï¼Œçº³å…¥å¯¹è¯ä¸Šä¸‹æ–‡ä¸æ›´ç¨³å¥çš„å¹³å±€åˆ¤å®šã€‚<br>â€¢ R1-Criticï¼šå…·å¤‡å› æœè¯æ®ä¸è¿‡ç¨‹ç›‘ç£çš„å¯è§£é‡Šç”Ÿæˆå¼å¥–åŠ±å»ºæ¨¡â€”â€”åœ¨ç”Ÿæˆå¼RMä¸­å¼•å…¥å› æœé“¾è·¯ä¸è¯æ®è¿½è¸ªï¼Œç»“åˆè¿‡ç¨‹ç›‘ç£æå‡å¯è§£é‡Šæ€§ä¸é²æ£’æ€§ã€‚<br>â€¢ Persona-RMï¼šåŸºäºç³»ç»Ÿæç¤ºæ³›åŒ–çš„ä¸ªæ€§åŒ–ä¸å¯æ§åå¥½å¯¹é½â€”â€”å°†ç”¨æˆ·ç”»åƒ/ç³»ç»Ÿæ¶ˆæ¯æ³›åŒ–ä¸åå¥½æ§åˆ¶ç»“åˆï¼Œå®ç°æŒ‰éœ€åŠ¨æ€è°ƒèŠ‚ä¸è·¨æ¨¡æ€ä¸ªæ€§åŒ–å¯¹é½ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Knocking-Heads Attention</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23052" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23052" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multi-head attention (MHA) has become the cornerstone of modern large language models, enhancing representational capacity through parallel attention heads. However, increasing the number of heads inherently weakens individual head capacity, and existing attention mechanisms - whether standard MHA or its variants like grouped-query attention (GQA) and grouped-tied attention (GTA) - simply concatenate outputs from isolated heads without strong interaction. To address this limitation, we propose knocking-heads attention (KHA), which enables attention heads to "knock" on each other - facilitating cross-head feature-level interactions before the scaled dot-product attention. This is achieved by applying a shared, diagonally-initialized projection matrix across all heads. The diagonal initialization preserves head-specific specialization at the start of training while allowing the model to progressively learn integrated cross-head representations. KHA adds only minimal parameters and FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention variants. We validate KHA by training a 6.1B parameter MoE model (1.01B activated) on 1T high-quality tokens. Compared to baseline attention mechanisms, KHA brings superior and more stable training dynamics, achieving better performance across downstream tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å¤šå¤´æ³¨æ„åŠ›å„å¤´å½¼æ­¤ç‹¬ç«‹ï¼Œåªåšæ‹¼æ¥ç¼ºå°‘å¼ºäº¤äº’ï¼›å¢åŠ å¤´æ•°ä¼šå‰Šå¼±å•å¤´ç»´åº¦ä¸è¡¨è¾¾ï¼Œå½¢æˆä½ç§©ç“¶é¢ˆä¸å†—ä½™ï¼ˆç¬¬1-2é¡µï¼‰<br>â€¢ ç°æœ‰talking-headsåœ¨æ³¨æ„åŠ›çŸ©é˜µä¸Šåšå˜æ¢ï¼Œè®¡ç®—/å†…å­˜å¼€é”€å¤§ä¸”ä¸FlashAttentionä¸å…¼å®¹ï¼›åä½œ/æ··åˆå¤´ç­‰æ–¹æ³•æˆ–ç‰ºç‰²å¤´ä¸“åŒ–ã€æˆ–äº¤äº’æœ‰é™ä¸”è®­ç»ƒå¤æ‚ï¼ˆç¬¬2é¡µï¼Œç¬¬13é¡µè¡¨6ï¼‰<br>â€¢ GQA/MQA/GTAè™½ä¼˜åŒ–KVç¼“å­˜ï¼Œä½†æœ¬è´¨ä»ç¼ºè·¨å¤´ç‰¹å¾çº§é€šä¿¡ï¼Œå¯¼è‡´æ•ˆç‡ä¸æ•ˆæœéš¾å…¼å¾—ï¼ˆç¬¬2ã€6-7é¡µè¡¨3ï¼‰<br>â€¢ é¢„è®­ç»ƒä¸­å­˜åœ¨loss spikesï¼Œéœ€è¦ä¸€ç§å…·æœ‰éšå¼æ­£åˆ™ã€èƒ½ç¨³å®šè®­ç»ƒä¸”å¢é‡æå°çš„æœºåˆ¶ï¼ˆç¬¬3ã€7-9é¡µå›¾1ã€å›¾2ï¼‰<br>â€¢ æœŸæœ›æ–¹æ¡ˆå¯å³æ’å³ç”¨ã€é€šç”¨äºå¤šç§æ³¨æ„åŠ›å˜ä½“ï¼Œå‚æ•°/FLOPså¢é‡<1%ï¼Œå¹¶èƒ½åœ¨æ¨ç†æ—¶æ— é¢å¤–å¼€é”€ï¼ˆç¬¬3-5é¡µï¼Œå¤æ‚åº¦åˆ†æç¬¬5é¡µï¼‰</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºKnocking-Heads Attentionï¼šåœ¨WQ/WK/WVä¹‹åã€æ³¨æ„åŠ›è®¡ç®—ä¹‹å‰å¯¹æ‰€æœ‰å¤´æ–½åŠ å…±äº«æŠ•å½±ï¼ˆçº¿æ€§æˆ–å¯¹Vçš„é—¨æ§MLPï¼‰ï¼Œå¹¶ä»¥å¯¹è§’åˆå§‹åŒ–ä½¿å…¶åˆæœŸè¿‘ä¼¼æ’ç­‰ã€è®­ç»ƒä¸­é€æ­¥å­¦ä¹ è·¨å¤´ç‰¹å¾äº¤äº’ï¼Œå…¶ä¸­å¯¹Vçš„å…±äº«å˜æ¢æœ€å…³é”®ã€‚çº¿æ€§ç‰ˆå¯åœ¨æ¨ç†æ—¶å¸æ”¶åˆ°åŸæŠ•å½±çŸ©é˜µä¸­ï¼Œæ€»ä½“å‚æ•°ä¸FLOPså¢é‡<1%ï¼Œå…¼å®¹MHA/GQA/GTA/MQA/MLAä¸FlashAttentionã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è‡ªé€‚åº”è·¨å¤´äº¤äº’è°ƒåº¦ä¸ç»“æ„æœç´¢ï¼šåŸºäºå±‚/å¤´ç²’åº¦è‡ªåŠ¨åŒ–æœç´¢ä¸è½¯/ç¡¬çº¦æŸï¼ŒæŒ‰è®­ç»ƒé˜¶æ®µåŠ¨æ€è°ƒèŠ‚å¯¹è§’ä¸éå¯¹è§’å¼ºåº¦<br>â€¢ é¢å‘é«˜æ•ˆæ¨ç†çš„KHAèåˆä¸é‡åŒ–ï¼šç¼–è¯‘å™¨çº§èåˆå¸æ”¶ã€ä¸KVç¼“å­˜/FlashAttentionååŒçš„ä½æ¯”ç‰¹é‡åŒ–ä¸è’¸é¦<br>â€¢ å°†KHAæ‹“å±•è‡³MoEä¸å¤šæ¨¡æ€Transformerï¼šç»“åˆä¸“å®¶è·¯ç”±çš„åŠ¨æ€è·¨å¤´äº¤äº’ï¼Œå¹¶åœ¨è§†è§‰/å¤šæ¨¡æ€ä»»åŠ¡ä¸ŠéªŒè¯ä¸ç†è®ºåŒ–åˆ†æ</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23603" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23603" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multimodal large language models (MLLMs) have demonstrated strong general-purpose capabilities in open-world visual comprehension. However, most existing MLLMs primarily focus on holistic, scene-level understanding, often overlooking the need for fine-grained, object-centric reasoning. In this paper, we present PixelRefer, a unified region-level MLLM framework that enables advanced fine-grained understanding over user-specified regions across both images and videos. Motivated by the observation that LLM attention predominantly focuses on object-level tokens, we propose a Scale-Adaptive Object Tokenizer (SAOT) to generate compact and semantically rich object representations from free-form regions. Our analysis reveals that global visual tokens contribute mainly in early LLM layers, inspiring the design of PixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion module to pre-fuse global context into object tokens. This yields a lightweight Object-Only Framework that substantially reduces computational cost while maintaining high semantic fidelity. To facilitate fine-grained instruction tuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction dataset. Extensive experiments across a range of benchmarks validate that PixelRefer achieves leading performance with fewer training samples, while PixelRefer-Lite offers competitive accuracy with notable gains in efficiency.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°æœ‰MLLMå¤šåšæ•´ä½“åœºæ™¯ç†è§£ï¼Œç¼ºä¹å¯¹ä»»æ„ç²’åº¦ï¼ˆå¯¹è±¡/éƒ¨ä»¶ï¼‰ä¸æ—¶ç©ºä¸€è‡´çš„ç²¾ç»†åŒ–å¯¹è±¡çº§æ¨ç†èƒ½åŠ›ï¼Œéš¾ä»¥æ”¯æŒHCIã€å…·èº«æ™ºèƒ½ã€åŒ»ç–—ä¸é¥æ„Ÿç­‰é«˜è¦æ±‚åº”ç”¨ï¼ˆè§ç¬¬1â€“2é¡µï¼Œå›¾1ï¼‰ã€‚<br>â€¢ ä»£è¡¨æ€§åŒºåŸŸçº§æ–¹æ³•å­˜åœ¨å±€é™ï¼šSoMæ ‡è®°æ˜“æ­§ä¹‰ä¸”ä½æ•ˆï¼›DAMéœ€é‡å¤ç¼–ç ä¸”åå•å¯¹è±¡æè¿°ï¼›PAMå¤šé™äºæè¿°ã€ç¼ºä¹å¤æ‚æ¨ç†å¹¶ä¾èµ–è¯­ä¹‰ç¼ºå¤±çš„SAM2ä¸­é—´ç‰¹å¾ã€éœ€å·¨é‡æ•°æ®ï¼Œä¸”ä»»åŠ¡ç‰¹å®šæ¶æ„å‰Šå¼±é€šç”¨æ€§ï¼ˆç¬¬2é¡µï¼‰ã€‚<br>â€¢ å°ç›®æ ‡ä¸å°ºåº¦å˜åŒ–å¯¼è‡´æ©ç åŒºåŸŸç»patchåŒ–åç‰¹å¾ä¸ç¨³ï¼›LLMå¯¹å…¨å±€è§†è§‰tokençš„æ³¨æ„ä¸»è¦é›†ä¸­åœ¨æµ…å±‚ï¼Œæ·±å±‚å‡ ä¹ç”±å¯¹è±¡tokenä¸»å¯¼ï¼Œç„¶è€Œå…¨å±€tokenæè€—ç®—åŠ›ä¸æ˜¾å­˜ï¼ˆç¬¬4â€“6é¡µï¼Œå›¾3ã€å›¾4ï¼‰ã€‚<br>â€¢ ç¼ºå°‘è¦†ç›–å¯¹è±¡çº§è¯†åˆ«ã€æè¿°ä¸å¤šè½®/å¤šå¯¹è±¡æ—¶ç©ºQAçš„é«˜è´¨é‡æŒ‡ä»¤æ•°æ®ä¸ç»Ÿä¸€è¯„æµ‹ï¼Œé™åˆ¶æ¨¡å‹å¯¹å¤æ‚è§†é¢‘æ¨ç†çš„æå‡ï¼ˆç¬¬10â€“11é¡µï¼Œå›¾8ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºPixelReferç»Ÿä¸€æ¡†æ¶ï¼ˆç¬¬6é¡µå›¾5ï¼‰ï¼Œä»¥å°ºåº¦è‡ªé€‚åº”å¯¹è±¡åˆ†è¯å™¨SAOTï¼ˆç¬¬7é¡µå›¾6ï¼‰å°†ä»»æ„åƒç´ çº§åŒºåŸŸåŠ¨æ€ç¼©æ”¾ã€ä½ç½®ç¼–ç ä¸èšç±»å‹å†—åç”Ÿæˆç´§å‡‘ä¸”è¯­ä¹‰ä¸°å¯Œçš„å¯¹è±¡tokenï¼Œå¹¶ä¸å…¨å±€è§†è§‰tokenè”åˆæ¨ç†ï¼›åŒæ—¶ç»™å‡ºé«˜æ•ˆå˜ä½“PixelReferâ€‘Liteï¼Œé€šè¿‡å¯¹è±¡ä¸­å¿ƒæ³¨å…¥OCIæ¨¡å—åœ¨LLMå‰åˆ†ä¸¤æ­¥æ³¨å…¥å±€éƒ¨ä¸å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä»…ä»¥å¯¹è±¡tokenè§£ç ï¼ˆç¬¬9é¡µç®—æ³•1ï¼‰ã€‚æ­¤å¤–æ„å»ºPixelReferâ€‘2.2Må¹¶é‡‡ç”¨â€œä¸¤é˜¶æ®µâ€è®­ç»ƒä»¥å®Œæˆç»†ç²’åº¦å¯¹é½ä¸æŒ‡ä»¤è·Ÿéšï¼ˆç¬¬10é¡µå›¾8ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è‡ªé€‚åº”å¯¹è±¡Tokené¢„ç®—ä¸æ—©å±‚è§†è§‰å‹ç¼©ç”¨äºé•¿è§†é¢‘æ¨ç†ï¼šç»“åˆå±‚é—´è§†è§‰è£å‰ªä¸åŠ¨æ€tokenåˆ†é…ï¼Œè¿›ä¸€æ­¥é™ä½æ—¶ç©ºæ¨ç†å¼€é”€ã€ä¿æŒè¯­ä¹‰å®Œæ•´ã€‚<br>â€¢ é¢å‘å¤šå¯¹è±¡äº¤äº’çš„æ—¶ç©ºå› æœæ¨ç†æ¡†æ¶ï¼šå¼•å…¥å…³ç³»å›¾å»ºæ¨¡ä¸åäº‹å®æ•°æ®å¢å¼ºï¼Œæå‡äº¤äº’ã€å› æœä¸æœªæ¥é¢„æµ‹ç­‰å¤æ‚è§†é¢‘é—®ç­”èƒ½åŠ›ã€‚<br>â€¢ æ— ç›‘ç£å¯¹è±¡åˆ†è¯å™¨çš„å¯¹é½å­¦ä¹ ï¼šæ‘†è„±å¤–éƒ¨åˆ†å‰²ä¾èµ–ï¼Œåˆ©ç”¨æ—¶ç©ºä¸€è‡´æ€§ä¸è‡ªè’¸é¦å­¦ä¹ é«˜è´¨é‡å¯¹è±¡tokenï¼Œå¢å¼ºè·¨åŸŸä¸å°ç›®æ ‡é²æ£’æ€§ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23393" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23393" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The application of Reinforcement Learning with Verifiable Rewards (RLVR) to mathematical and coding domains has demonstrated significant improvements in the reasoning and problem-solving abilities of Large Language Models. Despite its success in single generation problem solving, the reinforcement learning fine-tuning process may harm the model's exploration ability, as reflected in decreased diversity of generations and a resulting degradation of performance during Best-of-N sampling for large N values. In this work, we focus on optimizing the max@k metric, a continuous generalization of pass@k. We derive an unbiased on-policy gradient estimate for direct optimization of this metric. Furthermore, we extend our derivations to the off-policy updates, a common element in modern RLVR algorithms, that allows better sample efficiency. Empirically, we show that our objective effectively optimizes max@k metric in off-policy scenarios, aligning the model with the Best-of-N inference strategy.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜1ï¼šRLVRï¼ˆå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼‰è™½èƒ½æ˜¾è‘—æå‡å•æ¬¡ç”Ÿæˆï¼ˆpass@1ï¼‰ï¼Œå´å¸¸å¯¼è‡´å¤šæ ·æ€§ä¸‹é™ï¼Œè¿›è€Œåœ¨å¤§Nçš„Best-of-Nï¼ˆBoNï¼‰é‡‡æ ·ä¸‹å‡ºç°æ€§èƒ½åŠ£åŒ–ï¼ˆé«˜kçš„pass@kä¸‹é™ï¼‰ã€‚å®è¯ä¾æ®è§å›¾1ï¼ˆç¬¬3é¡µï¼Œä½kæå‡è€Œé«˜kä¸‹é™ï¼‰ä¸å›¾2ï¼ˆç¬¬3é¡µï¼Œç†µåˆ†å¸ƒå‘0åç§»æ˜¾ç¤ºç½®ä¿¡åº¦ä¸Šå‡ä½†å¤šæ ·æ€§ä¸‹é™ï¼‰ã€‚<br>â€¢ å…³é”®é—®é¢˜2ï¼šäºŒå…ƒå¥–åŠ±ï¼ˆå…¨æµ‹ä¾‹å…¨è¿‡=1ï¼Œå¦åˆ™=0ï¼‰ç¨€ç–ä¸”éš¾ä¼˜åŒ–ï¼Œæ˜“æŸä¼¤èƒ½åŠ›ï¼›ç›¸å¯¹åœ°ï¼Œè¿ç»­å¥–åŠ±æ›´ç¨³å®šé«˜æ•ˆä½†ä»å¯èƒ½ç‰ºç‰²é«˜kæ€§èƒ½ã€‚è¡¨1ï¼ˆç¬¬2é¡µï¼‰æ˜¾ç¤ºï¼šäºŒå…ƒå¥–åŠ±è®­ç»ƒæ˜¾è‘—é€€åŒ–ï¼Œè¿ç»­å¥–åŠ±èƒ½æå‡pass@1ä½†ç•¥é™pass@128ï¼Œæç¤ºéœ€ç›´æ¥å¯¹é½æ¨ç†æ—¶çš„BoNç›®æ ‡ã€‚<br>â€¢ å…³é”®é—®é¢˜3ï¼šç°æœ‰â€œæ¨ç†æ„ŸçŸ¥â€æ–¹æ³•å¤šå±€é™äºon-policyä¸”å¤šä¸ºäºŒå…ƒå¥–åŠ±è®¾å®šï¼Œéš¾ä»¥å…¼å®¹ç°ä»£RLVRä¸­çš„off-policyæ›´æ–°ï¼ˆå¦‚PPO/GRPOï¼‰ï¼Œå¹¶å­˜åœ¨æ ·æœ¬æ•ˆç‡ä¸ç¨³å®šæ€§ä¸è¶³çš„é—®é¢˜ã€‚éœ€è¦ä¸€ç§æ—¢æ”¯æŒè¿ç»­å¥–åŠ±ã€åˆèƒ½åœ¨off-policyä¸‹ä¼˜åŒ–ã€ä¸”ä¸BoNä¸€è‡´çš„è®­ç»ƒç›®æ ‡ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºç›´æ¥ä¼˜åŒ–max@kï¼ˆpass@kçš„è¿ç»­æ¨å¹¿ï¼‰çš„ç­–ç•¥æ¢¯åº¦ç›®æ ‡ï¼šåœ¨on-policyä¸‹ç»™å‡ºæ— åæ¢¯åº¦ä¼°è®¡å¹¶åŒ–ä¸ºå¯è®¡ç®—çš„â€œå¥–åŠ±é‡åŠ æƒâ€ï¼Œåœ¨off-policyä¸‹åŸºäºé‡è¦æ€§æ¯”ç‡Ïâ‰ˆ1è¿›è¡Œä¸€é˜¶è¿‘ä¼¼ï¼Œå¾—åˆ°é«˜æ•ˆçš„è¿‘ä¼¼æ¢¯åº¦ï¼ˆæƒé‡æ ¡æ­£ï¼‰ã€‚è¯¥ç›®æ ‡æ— ç¼é›†æˆäºGRPO/PPOå¼æ›´æ–°ï¼Œå°†è®­ç»ƒä¸BoNæ¨ç†å¯¹é½å¹¶æ˜¾è‘—æå‡å¤§kçš„max@kã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ä»BoNåˆ°å¤šèšåˆï¼šè”åˆä¼˜åŒ–max@kä¸å¤šæ•°æŠ•ç¥¨çš„ä¸€è‡´æ€§è®­ç»ƒï¼šå°†å¤šæ•°æŠ•ç¥¨ç­‰èšåˆå‡½æ•°çº³å…¥ç»Ÿä¸€ç›®æ ‡ï¼Œæ‹“å±•max@kåˆ°æ›´ä¸€èˆ¬çš„æ¨ç†ç­–ç•¥å¯¹é½ã€‚<br>â€¢ æ•°å­¦æ¨ç†çš„è¿ç»­å¯éªŒè¯å¥–åŠ±ï¼šé¢å‘max@kçš„ä¸€è‡´æ€§RLVRï¼šä¸ºæ•°å­¦é¢†åŸŸè®¾è®¡è¿ç»­å¯éªŒè¯æŒ‡æ ‡ï¼Œå¹¶ç”¨max@kä¼˜åŒ–å¯¹é½BoNä¸æ­¥éª¤éªŒè¯ã€‚<br>â€¢ åŠ¨æ€kä¸ç›®æ ‡è°ƒåº¦çš„æ¨ç†æ„ŸçŸ¥RLï¼šä»max@kåˆ°max@1çš„è¯¾ç¨‹å¼è®­ç»ƒï¼šéšè®­ç»ƒè¿›ç¨‹è‡ªé€‚åº”è°ƒæ•´kä¸ç›®æ ‡ï¼Œå¹³è¡¡æ¢ç´¢ï¼ˆé«˜kï¼‰ä¸åˆ©ç”¨ï¼ˆk=1ï¼‰ã€‚<br>â€¢ å•æ ·æœ¬-å¤šæ ·æœ¬åˆ†å¸ƒå¯¹é½ï¼šç¼©å°ä¸€æ¬¡ç”Ÿæˆä¸Best-of-Næœ€ä¼˜ä¹‹é—´çš„å·®è·ï¼šé€šè¿‡åˆ†å¸ƒåŒ¹é…/è’¸é¦ï¼Œä½¿å•æ¬¡ç”Ÿæˆæ›´æ¥è¿‘BoNçš„æœ€ä¼˜å€™é€‰ã€‚<br>â€¢ æ¡ä»¶å¼BoNç”Ÿæˆä¸è¿­ä»£æ”¹å†™ï¼šåˆ©ç”¨å…ˆå‰æ ·æœ¬è¿›è¡Œæ¡ä»¶é‡‡æ ·æå‡å¤šæ ·æ€§ï¼šåœ¨è®­ç»ƒä¸æ¨ç†ä¸­å¼•å…¥æ¡ä»¶åŒ–ä¸é‡é‡‡æ ·æœºåˆ¶ï¼ŒåŠ å¼ºå¤šæ ·æ€§ä¸å¯ä¿®æ­£æ€§ã€‚<br>â€¢ ç¨€ç–-ç¨ å¯†å¥–åŠ±æ··åˆä¼˜åŠ¿ä¼°è®¡ï¼šäºŒå…ƒé€šè¿‡ä¸æµ‹ä¾‹é€šè¿‡ç‡çš„å¤šç²’åº¦èåˆï¼šè®¾è®¡æ··åˆä¼˜åŠ¿ä¸åŸºçº¿ï¼Œå…¼é¡¾ç¨³å®šæ€§ä¸å¯¹æœ€ç»ˆé€šè¿‡ç‡çš„æ•æ„Ÿæ€§ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22946" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22946" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Unified multimodal models have recently shown remarkable gains in both capability and versatility, yet most leading systems are still trained from scratch and require substantial computational resources. In this paper, we show that competitive performance can be obtained far more efficiently by strategically fusing publicly available models specialized for either generation or understanding. Our key design is to retain the original blocks while additionally interleaving multimodal self-attention blocks throughout the networks. This double fusion mechanism (1) effectively enables rich multi-modal fusion while largely preserving the original strengths of the base models, and (2) catalyzes synergistic fusion of high-level semantic representations from the understanding encoder with low-level spatial signals from the generation encoder. By training with only ~ 35B tokens, this approach achieves strong results across multiple benchmarks: 0.91 on GenEval for compositional text-to-image generation, 82.16 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By fully releasing the entire suite of code, model weights, and datasets, we hope to support future research on unified multimodal modeling.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼ˆUMMï¼‰è™½è¡¨ç°å¼ºï¼Œä½†ä¸»æµåšæ³•éœ€ä»é›¶è®­ç»ƒã€è€—è´¹å·¨å¤§ç®—åŠ›ä¸æµ·é‡æ•°æ®ï¼Œç¼ºä¹é«˜æ•ˆå¤ç”¨ç°æœ‰å¼€æºæ¨¡å‹çš„è·¯å¾„ï¼ˆè§å›¾1ï¼Œç¬¬2é¡µï¼Œå±•ç¤ºäº†åœ¨è¾ƒå°‘tokenä¸‹è¿½å¹³/è¶…è¶Šçš„ç›®æ ‡ï¼‰ã€‚<br>â€¢ å•æ ˆç»Ÿä¸€è®­ç»ƒå°†è‡ªå›å½’ä¸æ‰©æ•£ç›®æ ‡æ··è®­ï¼Œå­˜åœ¨ä¼˜åŒ–å†²çªï¼›åç»­â€œç†è§£-ç”Ÿæˆâ€åŒé€šè·¯è™½ç¼“è§£å†²çªï¼Œä½†ä»ä¾èµ–å¤§è§„æ¨¡è®­ç»ƒä¸ç®—åŠ›ï¼ˆç¬¬1â€“3é¡µç›¸å…³å·¥ä½œï¼‰ã€‚<br>â€¢ æµ…èåˆ/è½»è¿æ¥å™¨ä»…ä½¿ç”¨ç†è§£åˆ†æ”¯æœ«å±‚è¡¨å¾æ¡ä»¶ç”Ÿæˆï¼Œä¿¡æ¯è¢«å‹ç¼©ã€è·¨æ¨¡æ€äº¤äº’ä¸è¶³ï¼Œå¯¼è‡´ä»»åŠ¡æ³›åŒ–ä¸ç¼–è¾‘ç²¾åº¦å—é™ï¼ˆå¯¹æ¯”å›¾5ï¼Œç¬¬9é¡µï¼Œæ·±èåˆä¼˜äºæµ…èåˆï¼‰ã€‚<br>â€¢ éœ€è¦åœ¨ä¸ç ´åé¢„è®­ç»ƒèƒ½åŠ›çš„å‰æä¸‹ï¼Œå®ç°ç†è§£ç«¯é«˜å±‚è¯­ä¹‰ï¼ˆViTï¼‰ä¸ç”Ÿæˆç«¯ä½å±‚ç»†èŠ‚ï¼ˆVAEï¼‰çš„æ—©æœŸã€æ·±åº¦ã€è¿ç»­äº¤äº’ï¼ŒåŒæ—¶ä¿ç•™åŸæ¨¡å‹ç»“æ„ä¸èƒ½åŠ›ï¼ˆæ¶æ„å›¾2ï¼Œç¬¬4é¡µï¼‰ã€‚<br>â€¢ å…¬å…±å¤šæºæ•°æ®è´¨é‡å‚å·®ã€ä»»åŠ¡é…æ¯”å¤±è¡¡ï¼Œå½±å“ç”Ÿæˆéµå¾ªä¸ç¼–è¾‘ä¸€è‡´æ€§ï¼ŒäºŸéœ€é«˜è´¨é‡ã€ä»»åŠ¡å‡è¡¡ä¸”å¤šæ ·çš„UMMè°ƒä¼˜æ•°æ®ï¼ˆç¬¬4é¡µæ•°æ®é›†ä¸ç¬¬5é¡µè¡¨1çš„å¤šä»»åŠ¡å¯¹æ¯”ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºâ€œåŒé‡èåˆâ€æ¡†æ¶ï¼šä¿ç•™å¹¶è¡Œçš„VLMï¼ˆQwen2.5â€‘VLâ€‘7Bï¼Œç†è§£ï¼‰ä¸DiTï¼ˆWan2.2â€‘TI2Vâ€‘5Bï¼Œç”Ÿæˆï¼‰åŸå§‹ç»“æ„ï¼Œåœ¨æ¯å±‚åæ’å…¥é›¶åˆå§‹åŒ–çš„å¤šæ¨¡æ€è‡ªæ³¨æ„åŠ›å—ï¼Œé‡‡ç”¨å¹¿ä¹‰å› æœæ³¨æ„åŠ›è®©æ–‡æœ¬/ViT/VAEä¸‰ç±»tokenè‡ªæ—©è‡³æ·±æŒç»­äº¤äº’ï¼ŒåŒæ—¶å†»ç»“ç†è§£åˆ†æ”¯ä»¥ä¿ç•™æ¨ç†èƒ½åŠ›ã€‚ç»“åˆNaViTå°ºåº¦ç­–ç•¥ä¸é˜¶æ®µåŒ–æ•°æ®é…æ¯”ï¼Œä»…âˆ¼35B seen tokenså³åœ¨GenEval/DPG/ç¼–è¾‘åŸºå‡†ä¸Šå–å¾—å¼ºç»“æœï¼ˆè§å›¾1ä¸è¡¨2â€“è¡¨5ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è‡ªé€‚åº”æ·±åº¦åŒé‡èåˆï¼šåŸºäºè·¯ç”±/é—¨æ§çš„å±‚çº§é€‰æ‹©ä¸æ¡ä»¶æ³¨å…¥å­¦ä¹ ï¼Œå®ç°æŒ‰ä»»åŠ¡ä¸tokenè‡ªé€‚åº”å†³å®šä½•å±‚ã€ä»¥ä½•å¼ºåº¦èåˆï¼Œæå‡æ³›åŒ–ä¸æ•ˆç‡ã€‚<br>â€¢ ç»Ÿä¸€å¤šç²’åº¦æ¡ä»¶ç¼–è¾‘ï¼šåœ¨ViT+VAEåŒé€šè·¯ä¸Šå¼•å…¥æ˜¾å¼ç©ºé—´æ§åˆ¶ï¼ˆæ©ç /æ·±åº¦/åˆ†å‰²/å§¿æ€ï¼‰ï¼Œå®ç°ç»“æ„å¯æ§ã€ç»†èŠ‚å¯ä¿çœŸçš„é€šç”¨ç¼–è¾‘å™¨ã€‚<br>â€¢ æ—¶ç©ºç»Ÿä¸€UMMæ‰©å±•ï¼šå°†åŒé‡èåˆæ¨å¹¿è‡³è§†é¢‘/éŸ³é¢‘-æ–‡æœ¬ä»»æ„åˆ°ä»»æ„ç”Ÿæˆï¼Œç»“åˆæ—¶åºæ³¨æ„åŠ›ä¸æ—¶ç©ºå› æœVAEï¼Œå®ç°é«˜æ•ˆçš„è·¨æ¨¡æ€æ—¶ç©ºç†è§£ä¸ç”Ÿæˆã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">LongCat-Video Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22200" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22200" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Video generation is a critical pathway toward world models, with efficient long video inference as a key capability. Toward this end, we introduce LongCat-Video, a foundational video generation model with 13.6B parameters, delivering strong performance across multiple video generation tasks. It particularly excels in efficient and high-quality long video generation, representing our first step toward world models. Key features include: Unified architecture for multiple tasks: Built on the Diffusion Transformer (DiT) framework, LongCat-Video supports Text-to-Video, Image-to-Video, and Video-Continuation tasks with a single model; Long video generation: Pretraining on Video-Continuation tasks enables LongCat-Video to maintain high quality and temporal coherence in the generation of minutes-long videos; Efficient inference: LongCat-Video generates 720p, 30fps videos within minutes by employing a coarse-to-fine generation strategy along both the temporal and spatial axes. Block Sparse Attention further enhances efficiency, particularly at high resolutions; Strong performance with multi-reward RLHF: Multi-reward RLHF training enables LongCat-Video to achieve performance on par with the latest closed-source and leading open-source models. Code and model weights are publicly available to accelerate progress in the field.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šå¦‚ä½•åœ¨åˆ†é’Ÿçº§æ—¶é•¿ä¸Šä¿æŒé«˜è´¨é‡ä¸æ—¶é—´ä¸€è‡´æ€§çš„é•¿è§†é¢‘ç”Ÿæˆï¼Œé¿å…é¢œè‰²æ¼‚ç§»ä¸è´¨é‡è¡°å‡ç­‰è¯¯å·®ç´¯ç§¯é—®é¢˜ï¼ˆè§æ‘˜è¦ä¸ç¬¬1èŠ‚ï¼‰<br>â€¢ å…³é”®é—®é¢˜ï¼šä¸åŒåº”ç”¨åœºæ™¯éœ€è¦T2Vã€I2Vã€è§†é¢‘ç»­å†™ç­‰å¤šä»»åŠ¡èƒ½åŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€åˆ†æ•£ã€éš¾ä»¥ç»Ÿä¸€å»ºæ¨¡ä¸é«˜æ•ˆè®­ç»ƒï¼ˆç¬¬1èŠ‚â€œç»Ÿä¸€æ¶æ„â€ï¼‰<br>â€¢ å…³é”®é—®é¢˜ï¼šé«˜åˆ†è¾¨ç‡ä¸é«˜å¸§ç‡å¯¼è‡´æ³¨æ„åŠ›è®¡ç®—éšæ—¶ç©ºtokenæ•°äºŒæ¬¡å¢é•¿ï¼Œæ¨ç†æˆæœ¬ä¸æ—¶å»¶éš¾ä»¥æ¥å—ï¼ˆç¬¬1èŠ‚â€œé«˜æ•ˆæ¨ç†â€ï¼‰<br>â€¢ é‡è¦æ€§ï¼šé•¿è§†é¢‘ç”Ÿæˆæ˜¯é€šå¾€ä¸–ç•Œæ¨¡å‹çš„å…³é”®è·¯å¾„ï¼Œæ”¯æ’‘æ•°å­—äººã€å…·èº«æ™ºèƒ½ã€è‡ªåŠ¨é©¾é©¶ç­‰å¤æ‚åœºæ™¯ï¼ˆç¬¬1èŠ‚ï¼‰<br>â€¢ ç°æœ‰å±€é™ï¼šå¤šæ•°æ–¹æ³•éœ€é¢å¤–å¾®è°ƒæ‰èƒ½æ”¹å–„é•¿æ—¶ä¸€è‡´æ€§ï¼›å¯¹é½æ‰‹æ®µå•ä¸€ï¼Œéš¾åŒæ—¶ä¼˜åŒ–ç”»è´¨ã€è¿åŠ¨ã€æŒ‡ä»¤éµå¾ªç­‰å¤šç»´æŒ‡æ ‡ï¼›æ¨ç†é˜¶æ®µç¼ºä¹ç³»ç»Ÿæ€§çš„é™æœ¬å¢æ•ˆæ–¹æ¡ˆï¼ˆç¬¬1èŠ‚ï¼‰</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºLongCat-Videoï¼šåŸºäºDiffusion Transformerçš„13.6Bç»Ÿä¸€è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä»¥â€œè§†é¢‘ç»­å†™â€é¢„è®­ç»ƒè·å¾—é•¿æ—¶ä¸€è‡´æ€§ï¼Œåœ¨åŒä¸€æ¨¡å‹å†…åŸç”Ÿæ”¯æŒT2V/I2V/ç»­å†™ã€‚æ¨ç†é‡‡ç”¨æ—¶ç©ºåŒè½´çš„ç²—åˆ°ç»†ç”Ÿæˆä¸å—ç¨€ç–æ³¨æ„åŠ›ï¼ˆé…åˆé«˜åˆ†è¾¨ç‡LoRAä¸“å®¶ï¼‰ï¼Œå¹¶ç”¨å¤šå¥–åŠ±GRPO-RLHFè¿›è¡Œäººç±»åå¥½å¯¹é½ï¼Œå®ç°720p@30fpsçš„é«˜æ•ˆåˆ†é’Ÿçº§è§†é¢‘ç”Ÿæˆã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ LongCat-Video-Memory: åŸºäºå±‚çº§è®°å¿†ä¸è§„åˆ’çš„è¶…é•¿è§†é¢‘ç”Ÿæˆï¼Œæ˜¾å¼å»ºæ¨¡è·¨æ®µè®°å¿†ä»¥è¿›ä¸€æ­¥æŠ‘åˆ¶è¯¯å·®ç´¯ç§¯<br>â€¢ GRPO-PhysAware: é¢å‘ç‰©ç†ä¸€è‡´æ€§ä¸äººç±»åå¥½çš„å¤šæ¨¡æ€å¤šå¥–åŠ±å­¦ä¹ ï¼Œè”åˆè¿åŠ¨ç‰©ç†ã€å®¡ç¾ä¸æŒ‡ä»¤éµå¾ªçš„å¯å­¦ä¹ å¥–åŠ±æ¨¡å‹<br>â€¢ Sparse4K-Video: é¢å‘4K/60fpsé•¿è§†é¢‘çš„å¯æ‰©å±•å—ç¨€ç–æ³¨æ„åŠ›ä¸ä¸Šä¸‹æ–‡å¹¶è¡Œæ¨ç†æ¡†æ¶ï¼Œæå‡è¶…é«˜åˆ†è¾¨ç‡ä¸è¶…é•¿æ—¶é•¿çš„æ•ˆç‡ä¸ç¨³å®šæ€§</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LimRank: Less is More for Reasoning-Intensive Information Reranking</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23544" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23544" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Existing approaches typically rely on large-scale fine-tuning to adapt LLMs for information reranking tasks, which is computationally expensive. In this work, we demonstrate that modern LLMs can be effectively adapted using only minimal, high-quality supervision. To enable this, we design LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating diverse, challenging, and realistic reranking examples. Using this synthetic data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and FollowIR for instruction-following retrieval. Our experiments demonstrate that LIMRANK achieves competitive performance, while being trained on less than 5% of the data typically used in prior work. Further ablation studies demonstrate the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization capabilities of LIMRANK across downstream tasks, including scientific literature search and retrieval-augmented generation for knowledge-intensive problem solving.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šç°æœ‰LLMé‡æ’åºå™¨åœ¨æ¨ç†å¯†é›†å‹æ£€ç´¢ä¸­ï¼Œå½“ç›¸å…³æ€§ä¾èµ–å¤šæ­¥æ¨ç†ã€éšå¼å…³ç³»ä¸ä¸Šä¸‹æ–‡æ•æ„Ÿæ€§æ—¶ï¼ŒåŸºäºè¡¨å±‚è¯­ä¹‰ç›¸ä¼¼çš„åŒ¹é…æ˜“å¤±æ•ˆï¼ˆè§p.1å›¾1ä¸Šéƒ¨ï¼‰ã€‚<br>â€¢ é‡è¦æ€§ï¼šçœŸå®æ£€ç´¢éœ€æ±‚æ¶µç›–å¤æ‚æ¨ç†ä¸æŒ‡ä»¤è·Ÿéšï¼ˆå¦‚BRIGHTä¸FOLLOWIRï¼‰ï¼Œå¹¶ç›´æ¥å½±å“ç§‘å­¦æ–‡çŒ®æ£€ç´¢ä¸RAGç­‰åº”ç”¨çš„æ•ˆæœï¼›æ–‡ä¸­åœ¨GPQA-RAGä¸LitSearchä¸Šçš„ç»“æœæ˜¾ç¤ºæå‡å…·æœ‰å®é™…ä»·å€¼ï¼ˆè§p.5è¡¨3ï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šå¤šä¾èµ–å¤§è§„æ¨¡ç›‘ç£å¾®è°ƒæˆ–å¤æ‚è®­ç»ƒï¼ˆå¦‚å¤§é‡æ¨ç†è½¨è¿¹ã€RLç­‰ï¼‰ï¼Œæˆæœ¬é«˜ã€æ•°æ®ä¾èµ–é‡ï¼›å¸¸ç”¨è®­ç»ƒé›†ï¼ˆå¦‚MS MARCOï¼‰æŸ¥è¯¢ç®€å•ã€ç¼ºä¹å¤šåŸŸä¸ç¡¬è´Ÿä¾‹ï¼Œéš¾ä»¥æ¿€æ´»LLMæ½œåœ¨æ¨ç†èƒ½åŠ›ã€‚è®ºæ–‡ä¸»å¼ ä»¥å°è€Œç²¾çš„é«˜è´¨é‡æ•°æ®æ›¿ä»£æµ·é‡æ•°æ®ï¼ˆæ‘˜è¦ä¸p.2â€“3çš„åˆæˆæ•°æ®æŒ‡å—ã€p.4è¡¨2å¯¹æ¯”ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºLIMRANK-SYNTHESIZERï¼šæŒ‰â€œé¢†åŸŸå¤šæ ·æ€§â€”çœŸå®å¯¹é½â€”éš¾åº¦å¤šæ ·æ€§â€æŒ‡å—ï¼Œç”¨Personaé©±åŠ¨ç”Ÿæˆæ—¥å¸¸/ä¸“å®¶æŸ¥è¯¢ï¼Œç»“åˆCoTå…ˆäº§å‡ºæ­£ä¾‹ææ–™å†åˆæˆç¡¬è´Ÿä¾‹ï¼Œå¹¶ä»¥DeepSeek-R1è¿›è¡Œç›¸å…³æ€§åˆ¤å®šè¿‡æ»¤ï¼Œå¾—åˆ°å°è€Œç²¾çš„è®­ç»ƒé›†ã€‚åŸºäºè¯¥æ•°æ®ï¼Œå¯¹Qwen2.5-7Bè¿›è¡Œè½»é‡å¾®è°ƒè·å¾—ç‚¹å¼é‡æ’åºå™¨LIMRANKï¼Œåœ¨ä½¿ç”¨ä¸åˆ°æ—¢æœ‰æ–¹æ³•5%æ•°æ®çš„æ¡ä»¶ä¸‹äºBRIGHTä¸FOLLOWIRä¸Šå–å¾—7Bçº§é¢†å…ˆè¡¨ç°ï¼ˆp.4è¡¨2ã€p.5è¡¨3ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ AutoLIMï¼šè‡ªåŠ¨åŒ–çš„æ¨ç†å‹é‡æ’åºæ•°æ®ç”Ÿæˆä¸éªŒè¯æ¡†æ¶â€”â€”ä»¥ä¸€è‡´æ€§æ ¡éªŒ/åˆ¤åˆ«å™¨æ›¿ä»£äººå·¥å®¡æ ¸ï¼Œé™ä½æˆæœ¬å¹¶æå‡å¯æ‰©å±•æ€§ã€‚<br>â€¢ Listwise-LIMRANKï¼šå°†â€œå°‘å³æ˜¯å¤šâ€çš„é«˜è´¨æ•°æ®è¿ç§»åˆ°åˆ—è¡¨å¼/é›†åˆå¼é‡æ’åºâ€”â€”ç³»ç»Ÿæ¯”è¾ƒä¸åŒç»“æ„åœ¨æ¨ç†å¯†é›†åœºæ™¯ä¸­çš„æ”¶ç›Šã€‚<br>â€¢ Intent-Aware LIMRANKï¼šé¢å‘æŸ¥è¯¢æ„å›¾çš„è‡ªé€‚åº”ç›¸å…³æ€§å»ºæ¨¡â€”â€”åŒºåˆ†ç›´æ¥/é—´æ¥ç›¸å…³ä¸ç¨€ç–ç›¸å…³ï¼ŒåŠ¨æ€åˆ†é…æ¨ç†ä¸è¯„åˆ†ç­–ç•¥ä»¥æå‡ç¨³å¥æ€§ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Code Aesthetics with Agentic Reward Feedback</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23272" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23272" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) have become valuable assistants for developers in code-related tasks. While LLMs excel at traditional programming tasks such as code generation and bug fixing, they struggle with visually-oriented coding tasks, often producing suboptimal aesthetics. In this paper, we introduce a new pipeline to enhance the aesthetic quality of LLM-generated code. We first construct AesCode-358K, a large-scale instruction-tuning dataset focused on code aesthetics. Next, we propose agentic reward feedback, a multi-agent system that evaluates executability, static aesthetics, and interactive aesthetics. Building on this, we develop GRPO-AR, which integrates these signals into the GRPO algorithm for joint optimization of functionality and code aesthetics. Finally, we develop OpenDesign, a benchmark for assessing code aesthetics. Experimental results show that combining supervised fine-tuning on AesCode-358K with reinforcement learning using agentic reward feedback significantly improves performance on OpenDesign and also enhances results on existing benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o and GPT-4.1, and achieves performance comparable to large open-source models with 480B-685B parameters, underscoring the effectiveness of our approach.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ LLM åœ¨å¯è§†åŒ–ç¼–ç¨‹ï¼ˆç½‘é¡µ/å›¾è¡¨ï¼‰ä¸Šç¼ºä¹â€œå®¡ç¾æ„è¯†â€ï¼Œå¸¸å‡ºç°é®æŒ¡ã€é…è‰²ä¸ä½³ã€ç»“æ„æ··ä¹±ç­‰ï¼Œå½±å“å¯è¯»æ€§ä¸ç”¨æˆ·ä½“éªŒ<br>â€¢ ç°æœ‰è®­ç»ƒå¤šä¾èµ–æ–‡æœ¬æ¨¡æ€å¥–åŠ±ï¼ˆå¯æ‰§è¡Œæ€§/å•å…ƒæµ‹è¯•/è¿‡ç¨‹å¥–åŠ±/æ–‡æœ¬åå¥½ï¼‰ï¼Œæ— æ³•è¯„ä¼°æ¸²æŸ“åçš„è§†è§‰ç¾æ„Ÿï¼Œæ›´æ— æ³•è¦†ç›–äº¤äº’æ€§<br>â€¢ ç¼ºå°‘é¢å‘â€œä»£ç ç¾å­¦â€çš„å¤§è§„æ¨¡æŒ‡ä»¤æ•°æ®ä¸æƒå¨è¯„æµ‹åŸºå‡†ï¼Œè®­ç»ƒä¸è¯„ä¼°ä¸ç³»ç»Ÿ<br>â€¢ ä»…é  SFT æ˜“è®°å¿†éš¾æ³›åŒ–ï¼Œéš¾ä»¥åœ¨å¼€æ”¾æŒ‡ä»¤å’Œå¤æ‚äº¤äº’ä¸­å…¼é¡¾åŠŸèƒ½æ­£ç¡®æ€§ä¸å®¡ç¾è´¨é‡</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºâ€œAgentic Reward Feedbackâ€å¤šæ™ºèƒ½ä½“å¥–åŠ±ç³»ç»Ÿï¼ˆæ‰§è¡Œä»£ç†ã€é™æ€ç¾å­¦ä»£ç†ã€äº¤äº’ç¾å­¦ä»£ç†ï¼‰ï¼Œå°†å¤šæºä¿¡å·åŠ æƒèšåˆå¹¶ç»“åˆ GRPO å½¢æˆ GRPO-AR å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼›é…å¥—æ„å»º AesCode-358K æ•°æ®ä¸ OpenDesign åŸºå‡†ï¼Œé‡‡ç”¨ SFT+RL ä¸¤é˜¶æ®µè”åˆä¼˜åŒ–åŠŸèƒ½ä¸ç¾å­¦ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Toward Open Aesthetic Reward Models for Codeï¼šä»¥å¼€æ”¾å¤šæ¨¡æ€ä¸äººç±»åå¥½è®­ç»ƒæ›¿ä»£ä¸“æœ‰è¯„å®¡ï¼Œé™ä½åå·®ä¸æˆæœ¬ï¼Œæå‡å¯å¤ç°æ€§<br>â€¢ Interactive-Web Agent for Aesthetic Usability Evaluationï¼šæå‡ GUI ä»£ç†çš„é²æ£’äº¤äº’ä¸åˆ¤åˆ«èƒ½åŠ›ï¼Œæä¾›æ›´ç»†ç²’åº¦ã€å¯éªŒè¯çš„äº¤äº’ç¾å­¦å¥–åŠ±<br>â€¢ Controllable Code Aesthetics via Multi-Objective RLï¼šåŸºäºå¤šç›®æ ‡ä¸åå¥½æ¡ä»¶åŒ–ï¼Œå®ç°åŠŸèƒ½/æ€§èƒ½/å¤šé£æ ¼å®¡ç¾çš„å¯æ§æƒè¡¡ä¸ä¸ªæ€§åŒ–ç”Ÿæˆ</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models with Conditional Score Distillation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21003" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21003" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Image Auto-regressive (AR) models have emerged as a powerful paradigm of visual generative models. Despite their promising performance, they suffer from slow generation speed due to the large number of sampling steps required. Although Distilled Decoding 1 (DD1) was recently proposed to enable few-step sampling for image AR models, it still incurs significant performance degradation in the one-step setting, and relies on a pre-defined mapping that limits its flexibility. In this work, we propose a new method, Distilled Decoding 2 (DD2), to further advances the feasibility of one-step sampling for image AR models. Unlike DD1, DD2 does not without rely on a pre-defined mapping. We view the original AR model as a teacher model which provides the ground truth conditional score in the latent embedding space at each token position. Based on this, we propose a novel conditional score distillation loss to train a one-step generator. Specifically, we train a separate network to predict the conditional score of the generated distribution and apply score distillation at every token position conditioned on previous tokens. Experimental results show that DD2 enables one-step sampling for image AR models with an minimal FID increase from 3.40 to 5.43 on ImageNet-256. Compared to the strongest baseline DD1, DD2 reduces the gap between the one-step sampling and original AR model by 67%, with up to 12.3times training speed-up simultaneously. DD2 takes a significant step toward the goal of one-step AR generation, opening up new possibilities for fast and high-quality AR modeling. Code is available at https://github.com/imagination-research/Distilled-Decoding-2.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ è§†è§‰è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ç”Ÿæˆè´¨é‡å¼ºä½†æ¨ç†ææ…¢ï¼Œå› éœ€é€tokené¡ºåºé‡‡æ ·ï¼Œæ— æ³•æ»¡è¶³ä½æ—¶å»¶åº”ç”¨éœ€æ±‚<br>â€¢ ç°æœ‰åŠ é€Ÿæ–¹æ³•å±€é™ï¼šé›†åˆé¢„æµ‹åœ¨ä¸€æ­¥æé™ä¸‹ç ´åtokenä¾èµ–ã€æ¨æµ‹å¼è§£ç åœ¨å›¾åƒARä¸­æé€Ÿæœ‰é™ï¼ˆ<3Ã—ï¼‰<br>â€¢ DD1è™½é¦–åˆ›ä¸€æ­¥é‡‡æ ·ä½†å­˜åœ¨æ˜æ˜¾ç²¾åº¦ä¸‹é™ã€ä¾èµ–é¢„å®šä¹‰ODEæ˜ å°„ã€è®­ç»ƒæ…¢ä¸”çµæ´»æ€§ä¸è¶³<br>â€¢ äºŸéœ€ä¸€ç§æ— éœ€é¢„å®šä¹‰æ˜ å°„ã€èƒ½åœ¨ä¸€æ­¥å†…åŒ¹é…æ•™å¸ˆARåˆ†å¸ƒçš„ç”ŸæˆèŒƒå¼ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨ä¸é«˜æ•ˆ<br>â€¢ è®­ç»ƒç¨³å®šæ€§ä¸æ”¶æ•›æ•ˆç‡å—ç¦»æ•£-è¿ç»­ç»“æ„ä¸åŒ¹é…ä¸åˆå§‹åŒ–ä¸å½“å½±å“ï¼Œéœ€ç³»ç»Ÿæ€§åˆå§‹åŒ–ä¸ä¼˜åŒ–ç­–ç•¥</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºDistilled Decoding 2ï¼ˆDD2ï¼‰ï¼šå°†æ•™å¸ˆARè§†ä½œæ¡ä»¶scoreæ¨¡å‹ï¼ˆå€ŸåŠ©Rectified Flowé—­å¼æ¡ä»¶scoreï¼‰ï¼Œä»¥æ¡ä»¶Score Distillationåœ¨æ¯ä¸ªtokenä½ç½®å¯¹é½ç”Ÿæˆå™¨åˆ†å¸ƒä¸æ•™å¸ˆåˆ†å¸ƒï¼›åŒæ—¶äº¤æ›¿è®­ç»ƒâ€œä¸€æ­¥ç”Ÿæˆå™¨â€å’Œâ€œæ¡ä»¶å¼•å¯¼ç½‘ç»œâ€ï¼Œå¹¶é€šè¿‡AR-diffusionåˆå§‹åŒ–ä¸GTSæŸå¤±ç¨³å¥å¯è®­ï¼Œæœ€ç»ˆå®ç°ä¸€æ­¥é«˜è´¨é‡é‡‡æ ·ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ æ¡ä»¶Scoreè’¸é¦ç”¨äºè¿ç»­ç©ºé—´ARï¼šæ— VQçš„æ‰©æ•£å¼ARä¸€ä½“åŒ–ä¸€æ­¥è’¸é¦æ¡†æ¶<br>â€¢ æ‰©å±•DD2è‡³æ–‡æœ¬åˆ°å›¾åƒä¸å¤šæ¨¡æ€ARï¼šå¤§è§„æ¨¡æ¡ä»¶ç”Ÿæˆçš„ä¸€æ­¥è‡ªå›å½’å­¦ä¹ <br>â€¢ å¼¥åˆä¸€æ­¥ç”Ÿæˆä¸æ•™å¸ˆå·®è·ï¼šæ¡ä»¶scoreå¯¹é½çš„ç›®æ ‡è®¾è®¡ä¸æ¶æ„å¢å¼ºç ”ç©¶</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RobotArena infty: Scalable Robot Benchmarking via Real-to-Sim Translation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23571" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23571" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The pursuit of robot generalists - instructable agents capable of performing diverse tasks across diverse environments - demands rigorous and scalable evaluation. Yet real-world testing of robot policies remains fundamentally constrained: it is labor-intensive, slow, unsafe at scale, and difficult to reproduce. Existing simulation benchmarks are similarly limited, as they train and test policies within the same synthetic domains and cannot assess models trained from real-world demonstrations or alternative simulation environments. As policies expand in scope and complexity, these barriers only intensify, since defining "success" in robotics often hinges on nuanced human judgments of execution quality. In this paper, we introduce a new benchmarking framework that overcomes these challenges by shifting VLA evaluation into large-scale simulated environments augmented with online human feedback. Leveraging advances in vision-language models, 2D-to-3D generative modeling, and differentiable rendering, our approach automatically converts video demonstrations from widely used robot datasets into simulated counterparts. Within these digital twins, we assess VLA policies using both automated VLM-guided scoring and scalable human preference judgments collected from crowdworkers, transforming human involvement from tedious scene setup, resetting, and safety supervision into lightweight preference comparisons. To measure robustness, we systematically perturb simulated environments along multiple axes, such as textures and object placements, stress-testing policy generalization under controlled variation. The result is a continuously evolving, reproducible, and scalable benchmark for real-world trained robot manipulation policies, addressing a critical missing capability in today's robotics landscape.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°å®è¯„æµ‹ä¸å¯æ‰©å±•ï¼šéœ€è¦äººå·¥æ­å»º/å¤ä½ä¸å®‰å…¨ç›‘ç®¡ï¼Œæ˜‚è´µã€ç¼“æ…¢ã€éš¾å¤ç°ï¼Œè·¨æœºæ„æ¯”è¾ƒå…¬å¹³æ€§å·®<br>â€¢ ç°æœ‰æ¨¡æ‹ŸåŸºå‡†é—­ç¯è‡ªæ´½ï¼šå¤šåœ¨åŒä¸€ä»¿çœŸåŸŸå†…è®­ç»ƒä¸æµ‹è¯•ï¼Œéš¾è¯„ä¼°ç”±çœŸå®æ¼”ç¤ºæˆ–å¼‚æºä»¿çœŸè®­ç»ƒçš„é€šç”¨ç­–ç•¥<br>â€¢ æˆåŠŸåˆ¤å®šéš¾ä»¥æ ‡å‡†åŒ–ï¼šä»»åŠ¡å®Œæˆå¸¸éœ€ç»†ç²’åº¦äººç±»åˆ¤æ–­ï¼Œç¼ºå°‘å¯æ‰©å±•ã€å¯å¤ç°çš„è‡ªåŠ¨è¯„åˆ†æ–¹æ¡ˆ<br>â€¢ æ³›åŒ–ä¸é²æ£’æ€§ç¼ºå£ï¼šç¼ºä¹ç³»ç»ŸåŒ–ã€å¯æ§çš„èƒŒæ™¯/é¢œè‰²/ç‰©ä½“ä½å§¿æ‰°åŠ¨æ¥æ£€éªŒè·¨åˆ†å¸ƒèƒ½åŠ›<br>â€¢ é›†ä¸­å¼çº¿ä¸‹æŒ‘æˆ˜æˆæœ¬é«˜ã€é¢‘æ¬¡ä½ï¼šæ— æ³•å½¢æˆå¦‚CV/NLPé‚£æ ·æŒç»­ã€å¯æ¯”è¾ƒçš„æ’è¡Œæ¦œç”Ÿæ€</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºRobotArena âˆï¼šå°†çœŸå®æ¼”ç¤ºè§†é¢‘è‡ªåŠ¨è½¬è¯‘ä¸ºå¯äº¤äº’ä»¿çœŸï¼Œéƒ¨ç½²VLAå¹¶ç”¨VLMè¿›åº¦è¯„åˆ†ä¸ä¼—åŒ…æˆå¯¹åå¥½ï¼ˆBradleyâ€“Terryï¼‰å¾—åˆ°å…¨å±€æ’åã€‚ç®¡çº¿å«å¯å¾®æ¸²æŸ“çš„æœº-ç›¸æœºæ ‡å®šã€VLMåˆ†å‰²ä¸2Dâ†’3Dèµ„äº§é‡å»º/å§¿æ€ä¼°è®¡ã€èƒŒæ™¯ä¿®å¤ã€ç³»ç»Ÿè¾¨è¯†ï¼Œå¹¶æ–½åŠ å¯æ§æ‰°åŠ¨ï¼ˆèƒŒæ™¯/é¢œè‰²/ç‰©ä½“ä½å§¿ï¼‰ä»¥å¯æ‰©å±•ã€å¯å¤ç°åœ°è¯„æµ‹è·¨åŸŸä¸é²æ£’æ€§ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Real2Sim+å¤šè§†è§’ä¸æ‰‹è…•ç›¸æœºçš„å…¨æ™¯è¯„æµ‹åŸºå‡†ï¼šæ‰©å±•åˆ°å¤šç›¸æœº/æ‰‹è…•è§†è§’ä¸æ—¶ç©ºä¸€è‡´é‡å»ºï¼Œè¦†ç›–æ›´ç»†ç²’åº¦æ“ä½œ<br>â€¢ é«˜ä¿çœŸæ¥è§¦ç‰©ç†ä¸å¾®æ’é…ä»»åŠ¡çš„ä»¿çœŸåŸºå‡†ï¼šå¼•å…¥ç²¾ç»†æ¥è§¦/æ‘©æ“¦/å…¬å·®å»ºæ¨¡ï¼Œè¯„æµ‹æ’æ‹”ç­‰é«˜éš¾ç‰©ç†ä»»åŠ¡<br>â€¢ äººç±»åå¥½é©±åŠ¨çš„è¯„æµ‹-å­¦ä¹ é—­ç¯ï¼šå°†ä¼—åŒ…æˆå¯¹åå¥½ç”¨äºåœ¨çº¿æ’åä¸åå¥½å­¦ä¹ ï¼Œè”åˆæå‡è¯„æµ‹ä¸ç­–ç•¥<br>â€¢ è·¨æ•°æ®é›†ä¸è·¨ä½“ç°çš„é²æ£’æ€§å…‰è°±ï¼šæ„å»ºæ ‡å‡†åŒ–OODä¸æ‰°åŠ¨è°±ç³»ï¼Œé‡åŒ–æ³›åŒ–è¾¹ç•Œä¸å¤±æ•ˆæ¨¡å¼<br>â€¢ ç”Ÿæˆå¼èµ„äº§ä¸åœºæ™¯çš„è‡ªç›‘ç£æ‰©å±•ï¼šç”¨2Dâ†’3D/è§†é¢‘â†’3Dä¸æè´¨æ¨æ–­å¤§è§„æ¨¡åˆæˆå¤šæ ·å¯äº¤äº’æ•°å­—å­ªç”Ÿ<br>â€¢ VLMä»»åŠ¡è¿›åº¦è¯„åˆ†çš„ç½®ä¿¡åº¦ä¸åå·®æ ¡å‡†ï¼šæ ¡å‡†VLMè¯„åˆ†ä¸äººç±»ä¸€è‡´æ€§å¹¶ç»™å‡ºä¸ç¡®å®šåº¦ä¼°è®¡</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23479" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23479" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-language alignment in multi-modal large language models (MLLMs) typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). SFT is stable and efficient but requires large-scale human annotations and cannot capture subtle preferences, while RL brings in a reward signal for training, but suffers from overhead and instability. These limitations highlight a trade-off between scalability, robustness, and alignment quality. To address this, we propose MergeMix, a training-time augmentation paradigm that bridges SFT and RL. It first applies an attention-aware image mixing via token merge with more cluster representation and spatial context, and then presents a preference-driven training paradigm for MLLMs by building preference pairs with mixed images and raw images, and optimizing via SimPO loss. As a mixup augmentation, MergeMix enhances attention consistency and efficiency, surpassing other heuristic-based methods in classification. Extensive experiments demonstrate that MergeMix achieves competitive accuracy with improved efficiency, providing a scalable approach to preference alignment in classification and MLLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å¤šæ¨¡æ€åå¥½å¯¹é½çš„â€œæ•ˆç‡â€”è´¨é‡â€çŸ›ç›¾ï¼šSFTç¨³å®šé«˜æ•ˆä½†ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨ä¸”éš¾æ˜¾å¼å»ºæ¨¡ç›¸å¯¹åå¥½ï¼›RL/RLHFå¯å¼•å…¥åå¥½ä¿¡å·ä½†è®­ç»ƒä»£ä»·é«˜ä¸”ä¸ç¨³å®šï¼Œéš¾ä»¥å…¼é¡¾å¯æ‰©å±•æ€§ã€é²æ£’æ€§ä¸å¯¹é½è´¨é‡ï¼ˆé¡µ1-2ï¼‰<br>â€¢ ç°æœ‰â€œå¢å¹¿é€ è´Ÿä¾‹â€ä¸å¯æ§ï¼šå¦‚SeVaç­‰ä»¥éšæœºè£å‰ªç­‰å¯å‘å¼ç”Ÿæˆè¾“å®¶å¹¶ç”¨DPOç­›é€‰ï¼Œæ©ç é«˜åº¦éšæœºã€ä¸æ•°æ®å¼±è€¦åˆï¼Œéš¾æ§åˆ¶â€œè¾“å®¶â€è´¨é‡ï¼Œå½±å“VQAç­‰ä»»åŠ¡å¹¶å‡å°‘æœ‰æ•ˆè®­ç»ƒæ ·æœ¬ï¼ˆé¡µ1ï¼‰<br>â€¢ mixupåœ¨ViT/MLLMä¸­çš„ä¸¤éš¾ä¸ä¿¡æ¯å¯¹é½ç¼ºå¤±ï¼šæ˜¾è‘—æ€§/æ•™å¸ˆå¼•å¯¼çš„æ©ç ä»£ä»·é«˜ã€TopKè´ªå¿ƒä¸¢å¤±ç©ºé—´å…³ç³»ï¼Œæ··åˆæ¯”ä¸æ ·æœ¬ä¿¡æ¯å¯†åº¦ä¸åŒ¹é…ï¼Œå¯¼è‡´æ•ˆç‡â€”æ€§èƒ½æƒè¡¡ä¸ä½³ã€æ ¡å‡†ä¸é²æ£’æ€§ä¸è¶³ï¼ˆå›¾1ä¸ç¬¬4.1èŠ‚ï¼‰</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºMergeMixï¼šç”¨Token Mergingï¼ˆToMeï¼‰ä¸åŒå‘è½¯åŒ¹é…æ¢å¤æ³¨æ„åŠ›ï¼Œç”Ÿæˆè¿ç»­ã€æ³¨æ„åŠ›ä¸€è‡´çš„æ··åˆæ©ç ï¼Œå¹¶ä»¥é«˜æ–¯é‡æ ‡å®šå°†åˆå¹¶æ¯”ä¸æ··åˆæ¯”å¯¹é½ï¼›åˆ†ç±»ä»»åŠ¡ç”¨ä¸€çƒ­+æ··åˆCEè®­ç»ƒï¼ŒMLLMå°†â€œæ··åˆå›¾åƒâ€“åŸå›¾â€æ„æˆåå¥½å¯¹å¹¶ä»¥SimPOæ’åæŸä¼˜åŒ–ï¼Œæ— éœ€å¥–åŠ±æ¨¡å‹ï¼Œæ¡¥æ¥SFTä¸RLï¼ˆå›¾2-3ï¼Œå¼8-12ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ TextMix-POï¼šåœ¨MLLMä¸­å¼•å…¥â€œæ–‡æœ¬ä¾§mixup+å›¾åƒä¾§MergeMixâ€çš„ååŒåå¥½ä¼˜åŒ–ï¼Œç»†ç²’åº¦æ§åˆ¶å¤šæ¨¡æ€è¾“èµ¢éš¾åº¦<br>â€¢ Learnable-ToMeMixï¼šå°†é™æ€Tokenåˆå¹¶å‡çº§ä¸ºå¯å­¦ä¹ /å¯å¾®åˆ†èšç±»çš„æ³¨æ„åŠ›é©±åŠ¨åˆå¹¶ï¼Œç«¯åˆ°ç«¯è”åˆå­¦ä¹ æ©ç ä¸æ¯”ç‡<br>â€¢ Lambda-Curriculum SimPOï¼šç”¨æ··åˆæ¯”Î»é©±åŠ¨è‡ªé€‚åº”æ’åè¾¹é™…/éš¾åº¦è¯¾ç¨‹ï¼Œç»Ÿä¸€æå‡å¯¹é½è´¨é‡ã€æ ¡å‡†ä¸é®æŒ¡é²æ£’æ€§</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23594" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23594" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce PRISM-Bench, a benchmark of puzzle-based visual challenges designed to evaluate not only whether models can solve problems, but how their reasoning unfolds. Unlike prior evaluations that measure only final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error, models must identify the first incorrect step. This setting enables fine-grained assessment of logical consistency, error detection, and visual reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric, and analogical reasoning, resisting shortcuts based on superficial pattern matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap between fluent generation and faithful reasoning: models that produce plausible CoTs often fail to locate simple logical faults. By disentangling answer generation from reasoning verification, PRISM-Bench offers a sharper lens on multimodal reasoning competence and underscores the need for diagnostic evaluation protocols in the development of trustworthy MLLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°æœ‰å¤šæ¨¡æ€è¯„æµ‹å¤šèšç„¦æœ€ç»ˆç­”æ¡ˆï¼Œæ··æ·†è§†è§‰æ„ŸçŸ¥ã€æµ…å±‚åŒ¹é…ä¸æ¨ç†ï¼Œæ— æ³•å®šä½æ¨ç†é“¾ä¸­æœ€æ—©å‡ºé”™çš„æ­¥éª¤ï¼Œéš¾ä»¥è¡¡é‡æ¨ç†å¿ å®åº¦ä¸ä¸€è‡´æ€§ï¼ˆå›¾1è§ç¬¬2é¡µå±•ç¤ºäº†æ‰€éœ€çš„åŒè½¨è¯„æµ‹æ€è·¯ï¼‰ã€‚<br>â€¢ ç¼ºä¹è¦æ±‚å¤šæ­¥ç¬¦å·ã€å‡ ä½•ã€ç±»æ¯”æ¨ç†çš„é«˜è´¨é‡è§†è§‰è°œé¢˜ï¼Œè®¸å¤šæ ·æœ¬å¯è¢«æ–‡æœ¬æ·å¾„æˆ–è¡¨é¢ç›¸ä¼¼æ€§ç ´è§£ï¼Œéš¾ä»¥å®¢è§‚è€ƒå¯ŸçœŸæ­£çš„è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ï¼ˆå›¾2ç¬¬4é¡µç¤ºä¾‹äº†å…­å¤§é¢˜å‹ï¼‰ã€‚<br>â€¢ æ¨¡å‹ä¼šç”Ÿæˆæµç•…ä½†ä¸å¯é çš„é“¾å¼æ¨ç†ï¼Œéš¾ä»¥è‡ªæˆ‘æ ¡éªŒï¼šè¡¨2ï¼ˆç¬¬7é¡µï¼‰æ˜¾ç¤ºé¦–é”™æ£€æµ‹æœ€ä½³æ¨¡å‹ä»…çº¦62%ï¼Œè¡¨3ï¼ˆç¬¬8é¡µï¼‰VQAæ•´ä½“ä¹Ÿåä½ï¼Œä¸”ä¸¤è€…ç›¸å…³æ€§ä»…ä¸­ç­‰ï¼ˆå›¾5ç¬¬10é¡µï¼ŒÏâ‰ˆ0.62ï¼›Ï„â‰ˆ0.47ï¼‰ï¼Œè¡¨æ˜ç­”æ¡ˆæ­£ç¡®ä¸ç­‰åŒäºè¿‡ç¨‹æ­£ç¡®ï¼Œè¿«åˆ‡éœ€è¦è¯Šæ–­å¼è¯„æµ‹ä¸å¯è®­ç»ƒçš„é”™è¯¯ä¿¡å·ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºPRISM-Benchï¼šæ„å»ºå«1044é¢˜ã€å…­å¤§ç±»åˆ«çš„è§†è§‰è°œé¢˜åŸºå‡†ï¼Œä¸ºæ¯é¢˜æä¾›çœŸå®æ¨ç†é“¾ä¸ä»…å«å•ä¸€é”™è¯¯çš„â€œè…åŒ–â€æ¨ç†é“¾ï¼›é€šè¿‡åœ¨éšæœºæ­¥éª¤æ³¨å…¥24ç±»é”™è¯¯å¹¶äººå·¥æ ¸éªŒï¼Œç¡®ä¿é¦–é”™å”¯ä¸€å¯åˆ¤ï¼Œå¹¶ä»¥â€œç­”æ¡ˆå‡†ç¡®ç‡+é¦–é”™æ£€æµ‹â€åŒè½¨åè®®è§£è€¦ç”Ÿæˆä¸éªŒè¯ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ é¢å‘å¤šæ¨¡æ€é¦–é”™å®šä½çš„å¯è®­ç»ƒéªŒè¯å™¨ï¼šåˆ©ç”¨æ­¥çº§æ ‡æ³¨ä¸é”™è¯¯ç±»å‹æ ‡ç­¾è®­ç»ƒåˆ¤åˆ«å¼/å¥–åŠ±æ¨¡å‹ï¼Œæå‡æ¨¡å‹è‡ªæ£€ä¸çº é”™èƒ½åŠ›<br>â€¢ éš¾åº¦å¯æ§çš„è§†è§‰è°œé¢˜è‡ªåŠ¨ç”Ÿæˆä¸æ ¡å‡†ï¼šåŸºäºç¨‹åºåŒ–ç”Ÿæˆä¸äººæœºæ ¡éªŒæ‰©å±•é¢˜åº“ï¼ŒæŒ‰é”™è¯¯ç±»å‹ä¸æ­¥éª¤æ·±åº¦è¿›è¡Œéš¾åº¦åˆ†å±‚<br>â€¢ ç”Ÿæˆ-éªŒè¯ååŒä¼˜åŒ–çš„æ¨ç†é²æ£’åŒ–ï¼šå°†é¦–é”™ä¿¡å·èå…¥è”åˆè®­ç»ƒæˆ–å¼ºåŒ–å­¦ä¹ ï¼Œä½¿æ¨¡å‹åœ¨æ±‚è§£åŒæ—¶å­¦ä¹ å®¡è®¡ä¸ä¿®æ­£æ¨ç†æ­¥éª¤</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VoMP: Predicting Volumetric Mechanical Property Fields</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22975" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22975" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Physical simulation relies on spatially-varying mechanical properties, often laboriously hand-crafted. VoMP is a feed-forward method trained to predict Young's modulus (E), Poisson's ratio (nu), and density (rho) throughout the volume of 3D objects, in any representation that can be rendered and voxelized. VoMP aggregates per-voxel multi-view features and passes them to our trained Geometry Transformer to predict per-voxel material latent codes. These latents reside on a manifold of physically plausible materials, which we learn from a real-world dataset, guaranteeing the validity of decoded per-voxel materials. To obtain object-level training data, we propose an annotation pipeline combining knowledge from segmented 3D datasets, material databases, and a vision-language model, along with a new benchmark. Experiments show that VoMP estimates accurate volumetric properties, far outperforming prior art in accuracy and speed.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ é«˜ä¿çœŸç‰©ç†ä»¿çœŸéœ€è¦ä½“ç´ çº§çš„æ¨æ°æ¨¡é‡Eã€æ³Šæ¾æ¯”Î½ã€å¯†åº¦Ïï¼Œä½†ç°æœ‰3Dæ•°æ®ä¸é‡å»ºç»“æœæ™®éç¼ºä¹æ­¤ç±»æ ‡æ³¨ï¼Œäººå·¥è®¾å®šè´¹æ—¶ä¸”ä¸»è§‚<br>â€¢ ç°æœ‰æ–¹æ³•å¤šéœ€é€å¯¹è±¡ä¼˜åŒ–æˆ–è¿è¡Œæ—¶è°ƒç”¨å¤§æ¨¡å‹ï¼ˆVLM/è§†é¢‘æ¨¡å‹ï¼‰ï¼Œé€Ÿåº¦æ…¢ã€æµç¨‹è„†å¼±ã€éš¾ä»¥è§„æ¨¡åŒ–åº”ç”¨<br>â€¢ è®¸å¤šå·¥ä½œè¾“å‡ºçš„æ˜¯æ¨¡æ‹Ÿå™¨ç‰¹å®šå‚æ•°æˆ–ç²—æè´¨ç±»åˆ«ï¼Œéš¾ä»¥åœ¨ä¸åŒä»¿çœŸæ¡†æ¶é—´ç§»æ¤ï¼Œç‰©ç†çœŸå®æ€§ä¸å¯é‡ç°æ€§ä¸è¶³<br>â€¢ å¤§å¤šä»…è¦†ç›–è¡¨é¢å±æ€§ï¼Œæ— æ³•ä¸ºç‰©ä½“å†…éƒ¨èµ‹æï¼Œå¯¼è‡´ä½“ç§¯åŠ›å­¦è¡Œä¸ºï¼ˆå¼¹æ€§ã€ç¢°æ’ã€æ³¢ä¼ æ’­ç­‰ï¼‰æ¨¡æ‹Ÿå¤±çœŸ<br>â€¢ æ–¹æ³•å¸¸å¯¹ç‰¹å®š3Dè¡¨ç¤ºï¼ˆå¦‚Splats/NeRFï¼‰å®šåˆ¶ï¼Œç¼ºä¹è·¨è¡¨ç¤ºçš„é€šç”¨æ€§ä¸ä¸€è‡´æ€§<br>â€¢ é¢„æµ‹ç»“æœæœªè¢«ç‰©ç†å¯è¡ŒåŸŸçº¦æŸï¼Œæ˜“äº§ç”Ÿä¸ç¬¦åˆçœŸå®ææ–™çš„å‚æ•°ç»„åˆ</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>VoMPæ˜¯ä¸€ä¸ªè·¨è¡¨ç¤ºçš„å‰é¦ˆæ¨¡å‹ï¼šå°†ä»»æ„å¯æ¸²æŸ“/ä½“ç´ åŒ–çš„3Dèµ„äº§çš„å¤šè§†è§’DINOv2å›¾åƒç‰¹å¾æå‡åˆ°ä½“ç´ åï¼Œç”±å‡ ä½•Transformeré¢„æµ‹æ¯ä½“ç´ çš„ææ–™æ½œå˜é‡ï¼Œå¹¶é€šè¿‡åŸºäºçœŸå®ææ–™ä¸‰å…ƒç»„è®­ç»ƒçš„MatVAEè§£ç ä¸ºç‰©ç†æœ‰æ•ˆçš„Eã€Î½ã€Ïã€‚é…å¥—çš„è‡ªåŠ¨æ ‡æ³¨ç®¡çº¿ç»“åˆåˆ†ä»¶3Dèµ„äº§ã€æè´¨æ•°æ®åº“ä¸VLMï¼Œå¼•å¯¼ç”Ÿæˆå¤§è§„æ¨¡ä½“ç´ çº§ç›‘ç£æ•°æ®ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Aniso-VoMP: ä»å„å‘åŒæ€§åˆ°å„å‘å¼‚æ€§ä½“ç§¯ææ–™åœºçš„å‰é¦ˆé¢„æµ‹ï¼šå°†ææ–™è¡¨ç¤ºæ‰©å±•ä¸ºå®Œæ•´å¼¹æ€§å¼ é‡ï¼Œå­¦ä¹ æ–¹å‘ç›¸å…³çš„åŠ›å­¦å±æ€§<br>â€¢ SimAdapt: è·¨æ¨¡æ‹Ÿå™¨çš„ææ–™åœºç­‰æ•ˆæ˜ å°„ä¸æ ‡å®šï¼šæŠŠç‰©ç†æœ‰æ•ˆ(E,Î½,Ï)è‡ªé€‚åº”æ˜ å°„åˆ°FEM/MPM/XPBDç­‰ä¸åŒå¼•æ“çš„ç­‰æ•ˆå‚æ•°åŸŸ<br>â€¢ Adaptive-Voxel VoMP: åŸºäºå…«å‰æ ‘/éšå¼è¡¨ç¤ºçš„è‡ªé€‚åº”å¤šå°ºåº¦ææ–™åœºé‡å»ºï¼šç¼“è§£å›ºå®šç½‘æ ¼å¸¦æ¥çš„è¿‡å¹³æ»‘ï¼Œç»†åŒ–å¤æ‚å†…éƒ¨ç»“æ„<br>â€¢ PhysAware Data Engine: é¢å‘ææ–™åœºçš„ä¸»åŠ¨å­¦ä¹ ä¸ä¸ç¡®å®šæ€§é©±åŠ¨æ ‡æ³¨ä½“ç³»ï¼šå‡å°‘å¯¹VLMçš„ä¾èµ–ï¼Œæå‡æ ‡ç­¾å¯ä¿¡åº¦ä¸è¦†ç›–åº¦<br>â€¢ Diff-VoMP: èåˆå¯å¾®ä»¿çœŸçš„ç«¯åˆ°ç«¯ææ–™åœºæ ¡å‡†ï¼šä»¥ä»¿çœŸé‡æŠ•å½±è¯¯å·®åé¦ˆå¾®è°ƒææ–™æ½œç©ºé—´ä¸é¢„æµ‹å™¨ï¼Œæé«˜ä»¿çœŸä¸€è‡´æ€§</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Language Server CLI Empowers Language Agents with Process Rewards</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22907" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22907" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models routinely hallucinate APIs and mislocalize edits, while language servers compute verified, IDE-grade facts about real code. We present Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language Server Protocol (LSP) server for coding agents and CI, exposing deterministic, replayable workflows. Our position is that language servers provide not only structural information (definitions, references, types, diagnostics) but also an actionable process reward: machine-checked, step-wise signals that align an agent's planning loop with program reality. In this work, Lanser-CLI contributes: (i) a robust addressing scheme beyond brittle "file:line:col" via a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a principled relocation algorithm; (ii) deterministic Analysis Bundles that normalize Language Server responses and capture environment/capability metadata with stable content hashes; (iii) a safety envelope for mutating operations (rename, code actions) with preview, workspace jails, and Git-aware, transactional apply; and (iv) a process-reward functional derived from Language Server facts (diagnostic deltas, disambiguation confidence, and safe-apply checks) that is computable online and replayable offline. We formalize determinism under frozen snapshots and establish a monotonicity property for the process reward, making it suitable for process supervision and counterfactual analysis. Project Page: https://github.com/yifanzhang-pro/lanser-cli</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç»“æ„ã€APIä¸ç¼–è¾‘å®šä½ä¸Šæ˜“äº§ç”Ÿå¹»è§‰å’Œæ¼‚ç§»ï¼Œä»£ç†çš„è®¡åˆ’-æ‰§è¡Œç¯è·¯ä¸çœŸå®ç¨‹åºçŠ¶æ€è„±èŠ‚ï¼›è€ŒåŸç”ŸLSPè™½æä¾›äº‹å®ï¼Œå´éš¾ä»¥ç›´æ¥æ”¯æ’‘ç¨³å¥çš„ä»£ç†é›†æˆã€‚<br>â€¢ åŸç”ŸLSPé›†æˆç¼ºä¹ç¡®å®šæ€§ä¸å¯å¤ç°æ€§ï¼ˆå“åº”æ’åºä¸ç¨³ã€ç¯å¢ƒ/ç‰ˆæœ¬/positionEncodingä¸ä¸€è‡´ã€UTF-16/UTF-8ç´¢å¼•é”™é…ï¼‰ï¼Œä¸”file:line:colåæ ‡è„†å¼±ï¼Œç¼–è¾‘æ˜“å¤±é…ã€‚<br>â€¢ è‡ªåŠ¨åŒ–å˜æ›´ç¼ºå°‘å®‰å…¨æŠ¤æ ï¼ˆé¢„è§ˆã€äº‹åŠ¡åŒ–åº”ç”¨ã€å·¥ä½œåŒºç›‘ç‹±ã€Gitæ•´æ´æ€§ã€ç¼–ç /ç´¢å¼•ä¸€è‡´æ€§ï¼‰ï¼Œéš¾åœ¨å¤§è§„æ¨¡ä¸CIåœºæ™¯ä¸­å¯é è½åœ°ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•ä¾§é‡ç»ˆå±€æŒ‡æ ‡ï¼Œç¼ºå°‘å¯æœºæ£€ã€å¯å›æ”¾çš„æ­¥éª¤çº§è¿‡ç¨‹ç›‘ç£ä¿¡å·ï¼Œæ— æ³•å¯¹ä»£ç†çš„ä¸­é—´å†³ç­–è¿›è¡Œå¯¹é½ä¸ä¿¡ç”¨åˆ†é…ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºLanser-CLIï¼šä¸€ä¸ªCLIä¼˜å…ˆçš„LSPç¼–æ’å±‚ï¼Œå›ºå®šæœåŠ¡å™¨ä¸ç¯å¢ƒï¼Œè¾“å‡ºå­—èŠ‚ç¨³å®šã€å¯å›æ”¾çš„Analysis Bundlesä¸è¿‡ç¨‹å¥–åŠ±ã€‚å…¶æ ¸å¿ƒåŒ…æ‹¬å¯å­˜æ´»ç¼–è¾‘çš„Selector DSLä¸ç¡®å®šæ€§é‡å®šä½ç®—æ³•ã€ç¯å¢ƒä¸ç¼–ç è§„èŒƒåŒ–å’Œç¨³å®šå“ˆå¸Œã€å¸¦é¢„è§ˆ/äº‹åŠ¡åŒ–/å·¥ä½œåŒºç›‘ç‹±çš„å®‰å…¨å˜æ›´ç®¡çº¿ï¼Œä»¥åŠåŸºäºè¯Šæ–­å¢é‡ã€å®‰å…¨æ£€æŸ¥ä¸æ­§ä¹‰ç½®ä¿¡åº¦çš„è¿‡ç¨‹å¥–åŠ±å‡½æ•°ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è·¨è¯­è¨€Lanser-CLIï¼šç»Ÿä¸€Selectorä¸è¿‡ç¨‹å¥–åŠ±åœ¨å¤šè¯­è¨€ç”Ÿæ€ä¸­çš„æ³›åŒ–ï¼šå°†DSLä¸Bundleè§„èŒƒæ‰©å±•åˆ°TypeScript/Java/C++ç­‰å¤šæœåŠ¡å™¨ï¼Œæ‰“é€šè·¨è¯­è¨€çš„ç¨³å®šæ ‡è¯†ä¸å¥–åŠ±å¯¹é½ã€‚<br>â€¢ å­¦ä¹ å¢å¼ºçš„é‡å®šä½ä¸æ­§ä¹‰æ¶ˆè§£ï¼šä»å¯å‘å¼åˆ°å¯å­¦ä¹ æ‰“åˆ†æ¨¡å‹ï¼šç”¨å†å²Bundleä¸è½¨è¿¹è®­ç»ƒå¯å­¦ä¹ çš„å€™é€‰æ‰“åˆ†/æ¶ˆæ­§æ¨¡å‹ï¼Œåœ¨å¯å®¡è®¡å‰æä¸‹æå‡å®šä½é²æ£’æ€§ä¸ç½®ä¿¡åº¦ä¼°è®¡ã€‚<br>â€¢ é™æ€-åŠ¨æ€èåˆçš„è¿‡ç¨‹å¥–åŠ±ï¼šå°†LSPäº‹å®ä¸æµ‹è¯•/è¦†ç›–/æ€§èƒ½ä¿¡å·è”åˆï¼šæŠŠè¯Šæ–­å˜åŒ–ä¸å•æµ‹é€šè¿‡ç‡ã€è¦†ç›–ç‡ã€å›å½’æ£€æµ‹ç­‰èåˆä¸ºå¯å›æ”¾çš„å¤åˆè¿‡ç¨‹å¥–åŠ±ï¼Œç”¨äºæ›´å¼ºçš„è¿‡ç¨‹ç›‘ç£ä¸CIç­–ç•¥ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">Once Upon an Input: Reasoning via Per-Instance Program Synthesis</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22849" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22849" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) excel at zero-shot inference but continue to struggle with complex, multi-step reasoning. Recent methods that augment LLMs with intermediate reasoning steps such as Chain of Thought (CoT) and Program of Thought (PoT) improve performance but often produce undesirable solutions, especially in algorithmic domains. We introduce Per-Instance Program Synthesis (PIPS), a method that generates and refines programs at the instance-level using structural feedback without relying on task-specific guidance or explicit test cases. To further improve performance, PIPS incorporates a confidence metric that dynamically chooses between direct inference and program synthesis on a per-instance basis. Experiments across three frontier LLMs and 30 benchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question answering tasks, relational reasoning tasks, and mathematical reasoning tasks show that PIPS improves the absolute harmonic mean accuracy by up to 8.6% and 9.4% compared to PoT and CoT respectively, and reduces undesirable program generations by 65.1% on the algorithmic tasks compared to PoT with Gemini-2.0-Flash.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šLLMåœ¨å¤æ‚å¤šæ­¥æ¨ç†ä¸Šæ˜“ä¸ç¨³å®šä¸”ä¸å¿ å®ï¼ŒCoT/PoTè™½æœ‰æå‡ä½†å¸¸å‡ºç°â€œä¸ºæ­£ç¡®ç­”æ¡ˆç¼–å†™ç©ºå£³ä»£ç â€ã€ç±»å‹/è¯­æ³•é”™è¯¯ä¸ä¸å¯æ‰§è¡Œé€»è¾‘ï¼Œéš¾ä»¥åœ¨ç®—æ³•ç±»ä»»åŠ¡ä¸­å¯é è½åœ°ï¼ˆè§ç¬¬3é¡µå›¾3aä¸ç¬¬4é¡µå›¾4ï¼‰ã€‚<br>â€¢ ä½•æ—¶ç”¨ä»£ç ï¼Ÿå¼€æ”¾åŸŸå®ä¾‹å±‚é¢ç¼ºå°‘â€œæ˜¯å¦åº”è¿›è¡Œç¨‹åºç»¼åˆè€Œéç›´æ¥CoTâ€çš„å¯é åˆ¤å®šï¼Œå¯¼è‡´åœ¨éç®—æ³•åŒ–å®ä¾‹ä¸Šå¾’å¢ä¸€æ¬¡ä»£ç æ‰§è¡Œä½†æ— æ”¶ç›Šï¼ˆç¬¬3èŠ‚3.1ï¼›ç¬¬3é¡µå›¾3bï¼‰ã€‚<br>â€¢ æ— ä»»åŠ¡è§„æ ¼çš„ç»¼åˆéš¾é¢˜ï¼šç¼ºä¹I/Oæ ·ä¾‹æˆ–å½¢å¼åŒ–è§„æ ¼ï¼Œæ— æ³•æŒ‡å¯¼å€™é€‰ç¨‹åºæœç´¢ä¸éªŒè¯ï¼Œæ˜“äº§ç”Ÿç¡¬ç¼–ç ç­”æ¡ˆã€å ä½ç¬¦ä»£ç ã€ç±»å‹/è¯­æ³•é”™è¯¯ï¼ˆç¬¬2.2èŠ‚ï¼Œç¬¬9é¡µè¡¨2ï¼‰ã€‚<br>â€¢ éç»“æ„åŒ–è¾“å…¥é¸¿æ²Ÿï¼šç¨‹åºåå¥½ç»“æ„åŒ–è¾“å…¥ï¼Œä½†å¤šæ•°é—®é¢˜ç»™å‡ºæ–‡æœ¬/å›¾åƒç­‰åŸå§‹å½¢æ€ï¼Œç°æœ‰æ–¹æ³•å¸¸è®©ç¨‹åºå»â€œè§£æå›¾åƒâ€è€Œéå€ŸåŠ©æ¨¡å‹æ„ŸçŸ¥ï¼Œè„†å¼±ä¸”æ˜“å¤±è´¥ï¼ˆç¬¬4-5é¡µï¼Œå›¾4bï¼‰ã€‚<br>â€¢ é‡è¦æ€§ä¸å½±å“ï¼šç¨‹åºæ‰§è¡Œå¯æä¾›å¯éªŒè¯ã€å¯å¤ç°çš„ç²¾å‡†è®¡ç®—ï¼›è®ºæ–‡åœ¨30ä¸ªåŸºå‡†ä¸Šç›¸å¯¹PoTçš„è°ƒå’Œå¹³å‡å‡†ç¡®ç‡æå‡æœ€é«˜è¾¾8.6%ï¼Œå¹¶åœ¨ç®—æ³•ç±»ä»»åŠ¡ä¸Šå°†ä¸è‰¯ç¨‹åºç”Ÿæˆé™ä½65.1%ï¼ˆç¬¬7é¡µå›¾5ï¼›ç¬¬8-9é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºPIPSï¼šå®ä¾‹çº§å¯é€‰æ‹©çš„ç¨‹åºç»¼åˆæ¡†æ¶ã€‚å…ˆç”¨è‡ªè¯„å¼ç½®ä¿¡åº¦å¼€å…³åˆ¤å®šâ€œç›´æ¥CoTâ€æˆ–â€œä»£ç ç»¼åˆâ€ï¼›è‹¥ç»¼åˆï¼Œåˆ™å…ˆå°†åŸå§‹è¾“å…¥æŠ½å–ä¸ºå®ä¾‹ä¸“å±çš„ç¬¦å·JSONï¼Œå†åœ¨ç»“æ„åŒ–æ£€æŸ¥ï¼ˆè¾“å…¥ä¾èµ–ã€è¿”å›æ ¼å¼ã€è¯­æ³•/ç±»å‹ã€å ä½ç¬¦ã€ç¬¦å·æŠ½å–é—®é¢˜ä¸sanityç­‰ï¼‰åé¦ˆä¸‹è¿­ä»£ç”Ÿæˆä¸ä¿®å¤ç¨‹åºï¼Œç›´è‡³é€šè¿‡æ£€æŸ¥å¹¶æ‰§è¡Œå¾—åˆ°ç­”æ¡ˆï¼Œæ— éœ€ä»»åŠ¡ä¸“ç”¨è§„æ ¼æˆ–æµ‹è¯•ç”¨ä¾‹ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ å¯ç»„åˆCoT-ä»£ç æ··åˆæ¨ç†ï¼šè‡ªåŠ¨åˆ†æ®µä¸æ‰§è¡Œè°ƒåº¦ï¼šå­¦ä¹ æŠŠå¤æ‚é—®é¢˜åˆ†è§£ä¸ºå­æ®µå¹¶ä¸ºæ¯æ®µè‡ªé€‚åº”é€‰æ‹©CoTæˆ–ä»£ç ï¼Œç»“åˆæ‰§è¡Œåé¦ˆå®ç°é—­ç¯çº é”™ã€‚<br>â€¢ æ— æµ‹è¯•æ ·ä¾‹çš„è¯­ä¹‰çº§åé¦ˆä¸éªŒè¯é©±åŠ¨ç»¼åˆï¼šåœ¨ç»“æ„æ£€æŸ¥ä¹‹å¤–å¼•å…¥å±æ€§æµ‹è¯•ã€å·®åˆ†æ‰§è¡Œã€å½¢å¼éªŒè¯/é™æ€åˆ†æä¸æ¨¡ç³Šæµ‹è¯•ï¼Œé™ä½â€œç­”æ¡ˆå¯¹ä½†ç†ç”±é”™â€ã€‚<br>â€¢ æ›´å¯ä¿¡çš„å®ä¾‹çº§ç¬¦å·æŠ½å–ä¸æ ¡éªŒï¼šé€šè¿‡å¯¹é½çº¦æŸã€ä¿¡æ¯ä¿æŒæ­£åˆ™ä¸å¤šæ¨¡æ€ä¸€è‡´æ€§æ£€æµ‹ï¼Œæå‡c(x)çš„å¿ å®åº¦ä¸å¯å®¡è®¡æ€§ï¼Œå¹¶è”åŠ¨ç¨‹åºè¿­ä»£ä¿®å¤æ¼æŠ½/é”™æŠ½ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DiffusionLane: Diffusion Model for Lane Detection</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22236" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22236" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In this paper, we present a novel diffusion-based model for lane detection, called DiffusionLane, which treats the lane detection task as a denoising diffusion process in the parameter space of the lane. Firstly, we add the Gaussian noise to the parameters (the starting point and the angle) of ground truth lanes to obtain noisy lane anchors, and the model learns to refine the noisy lane anchors in a progressive way to obtain the target lanes. Secondly, we propose a hybrid decoding strategy to address the poor feature representation of the encoder, resulting from the noisy lane anchors. Specifically, we design a hybrid diffusion decoder to combine global-level and local-level decoders for high-quality lane anchors. Then, to improve the feature representation of the encoder, we employ an auxiliary head in the training stage to adopt the learnable lane anchors for enriching the supervision on the encoder. Experimental results on four benchmarks, Carlane, Tusimple, CULane, and LLAMAS, show that DiffusionLane possesses a strong generalization ability and promising detection performance compared to the previous state-of-the-art methods. For example, DiffusionLane with ResNet18 surpasses the existing methods by at least 1\% accuracy on the domain adaptation dataset Carlane. Besides, DiffusionLane with MobileNetV4 gets 81.32\% F1 score on CULane, 96.89\% accuracy on Tusimple with ResNet34, and 97.59\% F1 score on LLAMAS with ResNet101. Code will be available at https://github.com/zkyntu/UnLanedet.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°æœ‰åŸºäºå¯å­¦ä¹ é”šç‚¹çš„è½¦é“çº¿æ£€æµ‹åœ¨åˆ†å¸ƒè¿ç§»ï¼ˆåŸŸåç§»ï¼‰åœºæ™¯ä¸‹æ³›åŒ–å·®ï¼Œå¾€å¾€éœ€è¦é’ˆå¯¹æ–°åŸŸé‡æ–°è®­ç»ƒï¼Œé™ä½éƒ¨ç½²ä¾¿æ·æ€§ä¸é²æ£’æ€§ï¼ˆè§è®ºæ–‡å¯¹CLRNetç­‰æ–¹æ³•çš„è®¨è®ºï¼‰ã€‚<br>â€¢ è‹¥å°†æ‰©æ•£ç›´æ¥æ–½åŠ åœ¨æ•´æ¡è½¦é“çš„æ‰€æœ‰é‡‡æ ·ç‚¹ä¸Šï¼Œè®¡ç®—ä¸ä¼˜åŒ–è´Ÿæ‹…è¿‡é‡ã€è®­ç»ƒå›°éš¾ï¼›éœ€è¦ä¸€ç§åœ¨å‚æ•°ç©ºé—´æ›´è½»é‡çš„æ‰©æ•£å»ºæ¨¡æ–¹å¼ã€‚<br>â€¢ ä»éšæœº/ä½è´¨é”šç‚¹å‡ºå‘ä¼šå‰Šå¼±ç¼–ç å™¨çš„ç‰¹å¾è¡¨å¾ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹æ»‘ï¼›éœ€è¦æ–°çš„è§£ç ä¸ç›‘ç£ç­–ç•¥å¼¥è¡¥éšæœºé”šç‚¹å¸¦æ¥çš„è¡¨å¾ä¸è¶³ä¸æ­£æ ·æœ¬ç¨€ç–é—®é¢˜ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>å°†è½¦é“æ£€æµ‹å»ºæ¨¡ä¸ºå‚æ•°ç©ºé—´ï¼ˆèµ·ç‚¹x/yä¸è§’åº¦ï¼‰çš„æ‰©æ•£-å»å™ªï¼šè®­ç»ƒæ—¶å¯¹GTå‚æ•°åŠ é«˜æ–¯å™ªå£°å¹¶å­¦ä¹ é€æ­¥å¤åŸï¼Œæ¨ç†æ—¶ä»é«˜æ–¯é‡‡æ ·çš„éšæœºé”šç‚¹ç»å°‘é‡DDIMæ­¥è¿­ä»£ç²¾åŒ–å¾—åˆ°è½¦é“ã€‚ä¸ºç¼“è§£éšæœºé”šç‚¹å¸¦æ¥çš„è¡¨å¾ä¸è¶³ï¼Œæå‡ºæ··åˆæ‰©æ•£è§£ç å™¨ï¼ˆå…¨å±€RoIGather+å±€éƒ¨è‡ªæ³¨æ„åŠ›/åŠ¨æ€å·ç§¯ï¼‰ä¸ä»…è®­ç»ƒæœŸçš„è¾…åŠ©å¤´ï¼Œå¹¶é…åˆé”šç‚¹é‡é‡‡æ ·å¯¹é½è®­æ¨åˆ†å¸ƒã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ FastDiffLane: è½»é‡æ··åˆè§£ç ä¸æ­¥é•¿è‡ªé€‚åº”çš„é«˜é€Ÿè½¦é“æ‰©æ•£æ£€æµ‹â€”â€”é€šè¿‡è§£ç å™¨è’¸é¦ã€ç¨€ç–é”šç‚¹ä¸è‡ªé€‚åº”é‡‡æ ·æ­¥æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦<br>â€¢ TemporalDiffLane: å¼•å…¥æ—¶åºä¸€è‡´æ€§çš„æ‰©æ•£å¼è§†é¢‘è½¦é“æ£€æµ‹â€”â€”è·¨å¸§ä¸€è‡´æ€§æŸå¤±ä¸æ—¶åºæ‰©æ•£æ›´æ–°æå‡ç¨³å®šæ€§ä¸é²æ£’æ€§<br>â€¢ CrossDomainDiffLane: é¢å‘æ— ç›‘ç£åŸŸè‡ªé€‚åº”çš„æ‰©æ•£è½¦é“æ£€æµ‹â€”â€”ç»“åˆåŸŸä¸å˜å™ªå£°è°ƒåº¦ã€é£æ ¼/ç‰¹å¾å¯¹é½ä¸ä¼ªæ ‡ç­¾è‡ªè®­ç»ƒå¼ºåŒ–è·¨åŸŸæ³›åŒ–</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20512" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20512" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in accelerating text-to-image (T2I) diffusion models have enabled the synthesis of high-fidelity images even in a single step. However, personalizing these models to incorporate novel concepts remains a challenge due to the limited capacity of one-step models to capture new concept distributions effectively. We propose a bidirectional concept distillation framework, EchoDistill, to enable one-step diffusion personalization (1-SDP). Our approach involves an end-to-end training process where a multi-step diffusion model (teacher) and a one-step diffusion model (student) are trained simultaneously. The concept is first distilled from the teacher model to the student, and then echoed back from the student to the teacher. During the EchoDistill, we share the text encoder between the two models to ensure consistent semantic understanding. Following this, the student model is optimized with adversarial losses to align with the real image distribution and with alignment losses to maintain consistency with the teacher's output. Furthermore, we introduce the bidirectional echoing refinement strategy, wherein the student model leverages its faster generation capability to feedback to the teacher model. This bidirectional concept distillation mechanism not only enhances the student ability to personalize novel concepts but also improves the generative quality of the teacher model. Our experiments demonstrate that this collaborative framework significantly outperforms existing personalization methods over the 1-SDP setup, establishing a novel paradigm for rapid and effective personalization in T2I diffusion models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°æœ‰ä¸ªæ€§åŒ–æ–¹æ³•åœ¨ä¸€æ­¥æ‰©æ•£æ¨¡å‹ä¸Šæ™®éå¤±æ•ˆï¼šå­¦ç”Ÿæ¨¡å‹éš¾ä»¥é€šè¿‡ä»…è°ƒæ–‡æœ¬ç¼–ç å™¨å­¦ä¹ æ–°æ¦‚å¿µï¼Œç›´æ¥å¾®è°ƒéª¨å¹²ä¼šç ´åå…ˆéªŒï¼ˆå¦‚Textual Inversionã€Custom Diffusionã€IP-Adapteråœ¨SD-Turbo/TCDä¸Šæ•ˆæœä¸ä½³ï¼‰ã€‚<br>â€¢ ä¼ ç»Ÿâ€œteacher-firstâ€ä¸¤é˜¶æ®µè’¸é¦ä½æ•ˆä¸”ä¸å¯é ï¼šæ•™å¸ˆéœ€å¤šæ­¥é‡‡æ ·ç›‘ç£ã€è®­ç»ƒä»£ä»·é«˜ï¼Œä¸”æ•™å¸ˆæœ¬èº«å¯¹å°‘æ ·æœ¬æ¦‚å¿µå­¦ä¹ å¯èƒ½å¤±è´¥ï¼Œç”Ÿæˆçš„ç›‘ç£æ ·æœ¬ä¼šè¯¯å¯¼å­¦ç”Ÿã€‚<br>â€¢ è¿«åˆ‡éœ€æ±‚æ˜¯åœ¨ä¿æŒä¸€æ­¥/å°‘æ­¥é«˜é€Ÿç”Ÿæˆçš„åŒæ—¶ï¼Œå®ç°å¯¹æ–°æ¦‚å¿µçš„é«˜ä¿çœŸä¸ªæ€§åŒ–ï¼ˆ1-SDPï¼‰ï¼Œç¼©å°â€œé€Ÿåº¦â€”ä¸ªæ€§åŒ–â€é¸¿æ²Ÿã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºEchoDistillï¼šç«¯åˆ°ç«¯è”åˆè®­ç»ƒå¤šæ­¥æ•™å¸ˆä¸ä¸€æ­¥å­¦ç”Ÿï¼Œå…±äº«æ–‡æœ¬ç¼–ç å™¨ï¼Œå­¦ç”Ÿç”¨å¯¹é½æŸå¤±ï¼ˆIPAèº«ä»½ç‰¹å¾ã€MSEã€MSâ€‘SWDï¼Œå«æ—¶é—´æƒé‡ï¼‰å¯¹é½æ•™å¸ˆè¾“å‡ºï¼Œå¹¶é€šè¿‡å¤šåˆ¤åˆ«å™¨ï¼ˆDINOv1/DINOv2/CLIPï¼‰å¯¹æŠ—æŸå¤±è´´è¿‘çœŸå®æ¦‚å¿µåˆ†å¸ƒï¼›éšåä»¥å­¦ç”Ÿå¿«é€Ÿç”Ÿæˆçš„æ ·æœ¬ä½œä¸ºâ€œå›å£°â€åå‘ä¼˜åŒ–æ•™å¸ˆä¸å­¦ç”Ÿï¼Œå½¢æˆåŒå‘æ¦‚å¿µè’¸é¦ã€‚ä»…å¾®è°ƒä¸¤ç«¯UNetçš„K/VæŠ•å½±ï¼Œå‚æ•°é«˜æ•ˆå¹¶å¯å…¼å®¹å°‘æ­¥æ¨ç†ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ EchoDistill-Liteï¼šåŸºäºè‡ªè’¸é¦ä¸è‡ªé€‚åº”å•åˆ¤åˆ«å™¨çš„ä¸€æ­¥ä¸ªæ€§åŒ–åŠ é€Ÿæ¡†æ¶ï¼Œé™ä½å¤šåˆ¤åˆ«å™¨å¸¦æ¥çš„è®­ç»ƒå¼€é”€ä¸ä¸ç¨³å®šæ€§ã€‚<br>â€¢ One-Shot EchoDistillï¼šç»“åˆå¯¹æ¯”å­¦ä¹ ä¸å…ƒå­¦ä¹ çš„å•æ ·æœ¬ä¸ªæ€§åŒ–è’¸é¦ï¼Œæå‡æä½æ ·æœ¬ä¸‹çš„ç¨³å®šæ€§ä¸æ¦‚å¿µä¿çœŸåº¦ã€‚<br>â€¢ Compositional EchoDistillï¼šé¢å‘å¤šä¸»ä½“/å¤šæ¦‚å¿µçš„å¯ç»„åˆä¸æŒç»­ä¸ªæ€§åŒ–å­¦ä¹ ï¼Œåœ¨ä¸€æ­¥/å°‘æ­¥æ‰©æ•£ä¸­å®ç°è·¨æ¦‚å¿µèåˆä¸é—å¿˜æŠ‘åˆ¶ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">Scaling Laws for Deepfake Detection</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.16320" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.16320" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This paper presents a systematic study of scaling laws for the deepfake detection task. Specifically, we analyze the model performance against the number of real image domains, deepfake generation methods, and training images. Since no existing dataset meets the scale requirements for this research, we construct ScaleDF, the largest dataset to date in this field, which contains over 5.8 million real images from 51 different datasets (domains) and more than 8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we observe power-law scaling similar to that shown in large language models (LLMs). Specifically, the average detection error follows a predictable power-law decay as either the number of real domains or the number of deepfake methods increases. This key observation not only allows us to forecast the number of additional real domains or deepfake methods required to reach a target performance, but also inspires us to counter the evolving deepfake technology in a data-centric manner. Beyond this, we examine the role of pre-training and data augmentations in deepfake detection under scaling, as well as the limitations of scaling itself.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç¼ºä¹å¯é¢„æµ‹çš„â€œæ•°æ®è§„æ¨¡â†’æ€§èƒ½â€å…³ç³»ï¼Œå¯¼è‡´æ·±ä¼ªæ£€æµ‹éš¾ä»¥è§„åˆ’æ•°æ®é‡‡é›†ä¸è®­ç»ƒèµ„æºï¼›åŒæ—¶ç”ŸæˆæŠ€æœ¯ä¸æ–­æ¼”è¿›ï¼Œæ¨¡å‹å¯¹æ–°ä¼ªé€ æ–¹æ³•ä¸æ–°çœŸå®åŸŸçš„æ³›åŒ–ä¸è¶³ï¼ˆè§ç¬¬1é¡µâ€œIntroductionâ€ä¸å›¾1(d)ï¼‰ã€‚<br>â€¢ ç°æœ‰æ•°æ®é›†è§„æ¨¡ä¸å¤šæ ·æ€§ä¸è¶³ï¼šå¤šæ•°ä»…1â€“2ä¸ªçœŸå®åŸŸã€ä¼ªé€ æ–¹æ³•è¦†ç›–æœ‰é™ï¼Œæ— æ³•æ”¯æ’‘è·¨åŸŸ/è·¨æ–¹æ³•çš„ç³»ç»Ÿæ ‡åº¦ç ”ç©¶ä¸ç¨³å¥è¯„ä¼°ï¼ˆè§ç¬¬2é¡µå›¾1(c)ã€ç¬¬5é¡µâ€œ3.2 Comparing ScaleDFâ€¦â€ï¼‰ã€‚<br>â€¢ æ—¢æœ‰æ–¹æ³•å¤šä¾èµ–å°è§„æ¨¡æ•°æ®ä¸Šçš„æ¶æ„/ç‰¹å¾æŠ€å·§ï¼Œç¼ºå°‘åœ¨å¤§è§„æ¨¡ä¸‹å¯¹é¢„è®­ç»ƒã€æ•°æ®å¢å¼ºä¸é¥±å’Œç‚¹/å±€é™æ€§çš„é‡åŒ–åˆ†æï¼ˆè§ç¬¬3é¡µâ€œContributionsâ€ä¸ç¬¬1â€“3é¡µæ‘˜è¦/å›¾1(d)ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æ„å»ºScaleDFï¼ˆ5.8MçœŸå®å›¾åƒ/51ä¸ªçœŸå®åŸŸï¼Œ8.8Mä¼ªé€ /102ç§æ–¹æ³•ï¼Œè¦†ç›–FS/FR/FF/FE/TFäº”ç±»ï¼Œè·¨åŸŸ+è·¨æ–¹æ³•åˆ’åˆ†ï¼‰ï¼Œå¹¶ä»¥ViTå°†ä»»åŠ¡å»ºæ¨¡ä¸ºäºŒåˆ†ç±»ï¼Œç³»ç»Ÿæ”¹å˜â€œçœŸå®åŸŸæ•°â€â€œä¼ªé€ æ–¹æ³•æ•°â€â€œè®­ç»ƒå›¾åƒæ•°â€ã€‚é€šè¿‡æ‹Ÿåˆä¸éªŒè¯æ£€æµ‹è¯¯å·®çš„å¹‚å¾‹1âˆ’AUC=AÂ·N^{-Î±}ä¸åŒé¥±å’Œå¹‚å¾‹1âˆ’AUC=c+KÂ·(N+N0)^{-Î³}ï¼ˆå›¾1(d)ï¼Œç¬¬2â€“3é¡µï¼‰ï¼ŒåŒæ—¶åˆ†æé¢„è®­ç»ƒä¸æ•°æ®å¢å¼ºåœ¨æ ‡åº¦ä¸‹çš„ä½œç”¨ä¸å±€é™ï¼ˆå¦‚åœ¨46åŸŸ/88æ–¹æ³•ä¸‹ï¼Œå›¾åƒæ•°>10^7åå‡ºç°é¥±å’Œè¶‹åŠ¿ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ é¢å‘è§†é¢‘çš„æ—¶åºæ·±åº¦ä¼ªé€ æ£€æµ‹æ ‡åº¦å¾‹ï¼šå°†æ ‡åº¦ç ”ç©¶ä»å›¾åƒæ‰©å±•åˆ°æ—¶åºçº¿ç´¢ä¸æ—¶ç©ºæ¨¡å‹ï¼Œé‡åŒ–å¸§ç‡ã€åºåˆ—é•¿åº¦ä¸æ—¶åºå»ºæ¨¡å¯¹å¹‚å¾‹å‚æ•°ä¸å¤–æ¨æ€§çš„å½±å“ã€‚<br>â€¢ å¼€æ”¾ä¸–ç•Œä¸åˆ†å¸ƒæ¼‚ç§»ä¸‹çš„æ ‡åº¦å¾‹ï¼šç³»ç»Ÿè€ƒå¯Ÿå‹ç¼©/é‡ç¼–ç ã€å¯¹é½ä¸è£å‰ªã€æ··åˆè¾¹ç•Œã€éšæœºæ‰°åŠ¨ç­‰éƒ¨ç½²å› ç´ å¯¹æ ‡åº¦æ›²çº¿ä¸è·¨åŸºå‡†æ³›åŒ–çš„å½±å“ï¼Œå¹¶å»ºç«‹é²æ£’æ€§æ ‡åº¦æ¨¡å‹ã€‚<br>â€¢ æ•°æ®-æ¨¡å‹-ç®—åŠ›ä¸‰ç»´è”åˆæ ‡åº¦ä¸ä¸»åŠ¨é‡‡æ ·ï¼šè”åˆä¼˜åŒ–æ•°æ®å¤šæ ·æ€§ã€æ¨¡å‹è§„æ¨¡/é¢„è®­ç»ƒç­–ç•¥ä¸è®­ç»ƒé¢„ç®—ï¼Œåˆ©ç”¨æ ‡åº¦å¾‹åšé¢„ç®—åˆ†é…ä¸æ–°åŸŸ/æ–°æ–¹æ³•çš„ä¸»åŠ¨è·å–ä¸å¤–æ¨ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view Human Reconstruction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.07723" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.07723" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Photorealistic 3D full-body human reconstruction from a single image is a critical yet challenging task for applications in films and video games due to inherent ambiguities and severe self-occlusions. While recent approaches leverage SMPL estimation and SMPL-conditioned image generative models to hallucinate novel views, they suffer from inaccurate 3D priors estimated from SMPL meshes and have difficulty in handling difficult human poses and reconstructing fine details. In this paper, we propose SyncHuman, a novel framework that combines 2D multiview generative model and 3D native generative model for the first time, enabling high-quality clothed human mesh reconstruction from single-view images even under challenging human poses. Multiview generative model excels at capturing fine 2D details but struggles with structural consistency, whereas 3D native generative model generates coarse yet structurally consistent 3D shapes. By integrating the complementary strengths of these two approaches, we develop a more effective generation framework. Specifically, we first jointly fine-tune the multiview generative model and the 3D native generative model with proposed pixel-aligned 2D-3D synchronization attention to produce geometrically aligned 3D shapes and 2D multiview images. To further improve details, we introduce a feature injection mechanism that lifts fine details from 2D multiview images onto the aligned 3D shapes, enabling accurate and high-fidelity reconstruction. Extensive experiments demonstrate that SyncHuman achieves robust and photo-realistic 3D human reconstruction, even for images with challenging poses. Our method outperforms baseline methods in geometric accuracy and visual fidelity, demonstrating a promising direction for future 3D generation models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ä»»åŠ¡ä¸é‡è¦æ€§ï¼šä»å•å¼ RGBå›¾é‡å»ºé«˜ä¿çœŸç©¿è¡£å…¨èº«äººä½“ï¼ŒæœåŠ¡äºAR/VRã€è™šæ‹Ÿè¯•è¡£ã€æ¸¸æˆä¸å½±è§†ç­‰åº”ç”¨ï¼ˆæ‘˜è¦ã€1èŠ‚ï¼‰<br>â€¢ å…³é”®æŒ‘æˆ˜ï¼šå•è§†è§’å›ºæœ‰æ­§ä¹‰ä¸å¼ºè‡ªé®æŒ¡ï¼Œå¯¼è‡´å‡ ä½•æ¢å¤ä¸å¤–è§‚é‡å»ºå›°éš¾ï¼ˆæ‘˜è¦ã€1èŠ‚ï¼‰<br>â€¢ ç°æœ‰2Då¤šè§†å›¾ç”Ÿæˆå±€é™ï¼šä¾èµ–SMPLå…ˆéªŒï¼Œä½†SMPLåœ¨é®æŒ¡/å¤§å§¿æ€ä¸‹æ˜“ä¸å‡†ï¼Œä¸”åªè¡¨ç¤ºè£¸ä½“ï¼Œéš¾è¦†ç›–å®½æ¾è¡£ç‰©ä¸å¤æ‚æ‹“æ‰‘ï¼Œé€ æˆè·¨è§†ä¸ä¸€è‡´ä¸ç»“æ„ä¼ªå½±ï¼ˆå›¾2a-bï¼Œ1èŠ‚ï¼‰<br>â€¢ åŸç”Ÿ3Dç”Ÿæˆå±€é™ï¼šç»“æ„ä¸€è‡´ä½†ç»†èŠ‚ä¸è¾“å…¥å¿ å®åº¦ä¸è¶³ï¼Œç”Ÿæˆå½¢çŠ¶åç²—ã€å¤–è§‚ç¼ºä¹çœŸå®æ€§ï¼ˆå›¾2cï¼Œ1èŠ‚ï¼‰<br>â€¢ éœ€æ±‚ç¼ºå£ï¼šæœŸæœ›ä¸€ç§è”åˆ2Dä¸3Dä¼˜åŠ¿ã€æå‡è·¨è§†ä¸€è‡´æ€§ä¸ç»†èŠ‚ä¿çœŸã€å¹¶å°½é‡æ‘†è„±ä¸å¯é SMPLä¾èµ–çš„å•è§†å›¾äººä½“é‡å»ºæ¡†æ¶ï¼ˆå›¾3ï¼Œ1èŠ‚ï¼‰</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºSyncHumanï¼šé¦–æ¬¡è”åˆ2Då¤šè§†å›¾æ‰©æ•£ä¸åŸç”Ÿ3Dç”Ÿæˆï¼Œé€šè¿‡2D-3DåŒæ­¥æ³¨æ„åŠ›åœ¨è®­ç»ƒä¸­åŒå‘å¯¹é½ï¼ˆ3DæŒ‡å¯¼2Dæå‡è·¨è§†ä¸€è‡´æ€§ï¼Œ2Dåå“º3Dæå‡ç»†èŠ‚ä¸å¿ å®åº¦ï¼‰ï¼›å¹¶ä»¥å¤šè§†å›¾å¼•å¯¼è§£ç å™¨å°†DINOv2ç‰¹å¾åƒç´ å¯¹é½æ³¨å…¥ç»“æ„æ½œå˜é‡ï¼Œç›´æ¥è§£ç ä¸ºé«˜ä¿çœŸçº¹ç†ç½‘æ ¼/3DGSï¼ˆå›¾3ã€å›¾4ã€å¼(4)ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Illumination-Aware SyncHumanï¼šå¯åˆ†è§£å…‰ç…§ä¸æè´¨çš„2D-3DåŒæ­¥ç”Ÿæˆï¼Œç¼“è§£éå‡åŒ€å…‰ç…§ä¼ªå½±å¹¶å®ç°é‡å…‰ç…§ï¼ˆé’ˆå¯¹å›¾10çš„é™åˆ¶ï¼‰<br>â€¢ SDF-SyncHumanï¼šä»¥SDFåŸç”Ÿ3Dç”Ÿæˆä¸æ‹“æ‰‘/æ°´å¯†çº¦æŸæ›¿ä»£ä½“ç´ åˆ†æ”¯ï¼Œæ¶ˆé™¤ç½‘æ ¼ç ´æ´å¹¶æå‡å‡ ä½•å¯ç¼–è¾‘æ€§ï¼ˆé’ˆå¯¹é™„å½•C.1çš„é™åˆ¶ï¼‰<br>â€¢ ScaleSyncHumanï¼šç»“åˆè§†é¢‘æ‰©æ•£ä¸å¤§è§„æ¨¡å¤šè§†è§’äººä½“æ•°æ®çš„è”åˆé¢„è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡è·¨è§†ä¸€è‡´æ€§ã€ç»†èŠ‚ä¸æ³›åŒ–å¹¶åŠ é€Ÿæ¨ç†</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23605" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23605" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity of the subject. However, personalized 3D/4D generation is still largely underexplored. In this work, we introduce TIRE (Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation. It takes an initial 3D asset produced by an existing 3D generative model as input and uses video tracking to identify the regions that need to be modified. Then, we adopt a subject-driven 2D inpainting model for progressively infilling the identified regions. Finally, we resplat the modified 2D multi-view observations back to 3D while still maintaining consistency. Extensive experiments demonstrate that our approach significantly improves identity preservation in 3D/4D generation compared to state-of-the-art methods. Our project website is available at https://zsh2000.github.io/track-inpaint-resplat.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å½“å‰3D/4Dç”Ÿæˆåœ¨è·¨è§†è§’çš„â€œèº«ä»½ä¿æŒâ€ä¸Šè¡¨ç°æ¬ ä½³ï¼Œæœªè§‚æµ‹åŒºåŸŸï¼ˆä¾§/åè§†ï¼‰å¸¸è¢«é”™è¯¯â€œå¹»è§‰â€ï¼Œå‡ºç°ç³»ç»Ÿæ€§è‰²åä¸çº¹ç†æ¼‚ç§»ï¼Œéš¾ä»¥æ»¡è¶³ä¸ªæ€§åŒ–ç”Ÿæˆéœ€æ±‚ã€‚<br>â€¢ å•å›¾/å•è§†é¢‘é©±åŠ¨çš„æ–¹æ³•å¯¹æœªè§è§†è§’ç¼ºä¹çº¿ç´¢ï¼ŒSDSç±»ä¼˜åŒ–æ—¢è€—æ—¶åˆæ˜“å¯¼è‡´å¤–è§‚/è¿åŠ¨è¢«å¹³å‡åŒ–ï¼Œéš¾ä»¥å®ç”¨éƒ¨ç½²ã€‚<br>â€¢ å¤šè§†å›¾æ‰©æ•£è™½å¯å¿«é€Ÿå¤–æ¨ï¼Œä½†å—è®­ç»ƒæ•°æ®åå·®ä¸ä¸€è‡´æ€§ç¼ºé™·å½±å“ï¼Œéš¾ä»¥å¯é è¿˜åŸé®æŒ¡åŒºåŸŸçš„çœŸå®å¤–è§‚ã€‚<br>â€¢ åŸç”Ÿ3D/4Då¿«é€Ÿç”Ÿæˆï¼ˆfeed-forwardï¼‰æ–¹æ³•èšç„¦æ•ˆç‡ä¸ç¾è§‚ï¼Œå´æ™®éå¿½è§†ä¸ªæ€§åŒ–ä¸èº«ä»½ä¸€è‡´æ€§ï¼Œéœ€ä¸€ç§å¯ä¸ä¹‹äº’è¡¥ä¸”æ¨¡å‹æ— å…³çš„å¢å¼ºæ–¹æ¡ˆã€‚<br>â€¢ åœ¨è¡¥å…¨çº¹ç†çš„åŒæ—¶ä¿æŒè·¨è§†è§’ä¸€è‡´æ€§ä¸å‡ ä½•è´¨é‡ä»å…·æŒ‘æˆ˜ï¼Œå¸¸å‡ºç°é¬¼å½±ä¸ç»“æ„é”™ä¹±ç­‰é—®é¢˜ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºTIREä¸‰é˜¶æ®µæµæ°´çº¿ï¼šå…ˆç”¨è§†é¢‘ç‚¹è·Ÿè¸ªçš„â€œåå‘è·Ÿè¸ªâ€ä»ç›®æ ‡è§†è§’å›åˆ°æºè§†è§’ï¼Œç²¾å‡†å®šä½éœ€è¡¥çº¹ç†çš„é®æŒ¡åŒºåŸŸï¼›å†ä»¥ä¸ªæ€§åŒ–LoRAå¾®è°ƒçš„2Dæ‰©æ•£å¼ä¿®å¤è¿›è¡Œâ€œæ¸è¿›å¼â€è§†è§’æ‰©å±•è¡¥çº¹ç†ï¼›æœ€åç”¨æ©ç æ„ŸçŸ¥çš„å¤šè§†å›¾æ‰©æ•£åªåœ¨æœªè§åŒºåŸŸå»å™ªæ ¡æ­£ä¸€è‡´æ€§ï¼Œå¹¶å°†å¤šè§†å›¾ç»“æœé‡æŠ•å›3Dï¼ˆresplatï¼‰ï¼Œæ•´ä½“æ¨¡å‹æ— å…³ã€å¯å åŠ åˆ°ä»»æ„3D/4Dç”Ÿæˆå™¨ä¸Šã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Tuning-free TIRE: æ— éœ€æ¯ç›®æ ‡å¾®è°ƒçš„å…è®­ç»ƒä¸ªæ€§åŒ–3D/4Dæ¸è¿›è¡¥çº¹ç†æ¡†æ¶ï¼šä»¥é€‚é…å™¨/æç¤ºæ³¨å…¥æ›¿ä»£LoRAå¾®è°ƒï¼Œæ˜¾è‘—æå‡æ•ˆç‡<br>â€¢ Time-aware TIRE-4D: é¢å‘æ—¶é—´å˜å¤–è§‚çš„æ—¶åºä¸€è‡´ä¸ªæ€§åŒ–4Dç”Ÿæˆï¼šæ˜¾å¼å»ºæ¨¡éšæ—¶é—´å˜åŒ–çš„æè´¨/èŠ±çº¹å¹¶ä¸è¿åŠ¨ç»‘å®š<br>â€¢ Geo-Perceptual Identity Metric: é¢å‘3D/4Dä¸ªæ€§åŒ–çš„å‡ ä½•æ„ŸçŸ¥èº«ä»½ä¸€è‡´æ€§æŒ‡æ ‡ï¼šèåˆVLMè¯„åˆ†ä¸å¯è§æ€§/æ³•çº¿/æ·±åº¦ä¸€è‡´æ€§<br>â€¢ End-to-End TIRE: å¯å¾®åˆ†çš„ç«¯åˆ°ç«¯â€œè·Ÿè¸ª-ä¿®å¤-é‡æŠ•â€è”åˆå­¦ä¹ ï¼šç”¨å•ä¸€æŸå¤±è”åˆä¼˜åŒ–é®æŒ¡ä¼°è®¡ã€è¡¥çº¹ç†ä¸3Dä¸€è‡´æ€§<br>â€¢ Learned Visibility over Tracking: ç”¨å¯å­¦ä¹ å¯è§æ€§åœºæ›¿ä»£è§†é¢‘è·Ÿè¸ªçš„é®æŒ¡åŒºåŸŸé¢„æµ‹ï¼šç»“åˆä½“æ¸²æŸ“æˆ–å¯è§æ€§ç½‘ç»œç¨³å¥è·¨è§†è§’<br>â€¢ Representation-agnostic Resplat: é¢å‘é«˜ä¿çœŸç½‘æ ¼/NeRF/é«˜æ–¯çš„ç»Ÿä¸€é‡æŠ•ä¸ä¸€è‡´æ€§æ•´åˆæ¨¡å—ï¼šæå‡ä¸åŒ3Dè¡¨ç¤ºçš„å¯ç”¨æ€§</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-11">

    <div class="paper">
        <h2 class="paper-title">Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22603" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22603" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) have recently advanced auditory speech recognition (ASR), visual speech recognition (VSR), and audio-visual speech recognition (AVSR). However, understanding of their internal dynamics under fine-tuning remains limited. In natural language processing, recent work has revealed attention sinks, tokens that attract disproportionately high attention, and associated massive activations in which some features of sink tokens exhibit huge activation in LLMs. In this work, we are the first to study these phenomena in multimodal speech recognition. Through a detailed analysis of audio-visual LLMs, we identify attention sinks and massive activations not only at the BOS token but also at intermediate low-semantic tokens across ASR, VSR, and AVSR. We show that massive activations originate in the MLP layers and correspond to fixed feature indices across all sink tokens. We further show that intermediate sink tokens exhibit high cosine similarity to the BOS token, thereby amplifying attention and activation. Building on these insights, we introduce a simple decorrelation loss that reduces cosine similarity between BOS and other tokens, effectively mitigating intermediate sinks and massive activations. Furthermore, our method improves word error rate (WER) under high audio-visual feature downsampling while remaining stable at lower downsampling rates.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šLLMé©±åŠ¨çš„ASR/VSR/AVSRåœ¨å¾®è°ƒåå‡ºç°æ³¨æ„åŠ›æ±‡èšï¼ˆattention sinksï¼‰ä¸è¶…å¤§æ¿€æ´»ï¼ˆmassive activationsï¼‰ï¼Œå…¶ä¸­é™¤BOSå¤–çš„â€œä¸­é—´ä½è¯­ä¹‰tokenâ€ä¹Ÿæˆä¸ºsinkï¼Œè½¬ç§»æ³¨æ„åŠ›ã€ç ´åéŸ³è§†å¯¹é½ï¼Œå°¤å…¶åœ¨é«˜å‹ç¼©ç‡ä¸‹æ¶åŒ–WERï¼ˆè§é¡µ2å›¾1(aâ€“d)ã€é¡µ3å›¾2(a)ï¼‰ã€‚<br>â€¢ é‡è¦æ€§ï¼šè¿™äº›ç°è±¡åœ¨å¤šæ¨¡æ€è¯­éŸ³è¯†åˆ«é¢†åŸŸå°šæœªè¢«ç³»ç»Ÿç ”ç©¶ï¼Œå½±å“æ¨¡å‹ç¨³å¥æ€§ä¸æ•ˆç‡ï¼›ä½œè€…å®è¯æ˜¾ç¤ºå¤§æ¿€æ´»èµ·æºäºMLPç¬¬2å±‚çš„GLUé—¨æ§ï¼Œä¸”sinkå…±äº«å›ºå®šæ¿€æ´»ç‰¹å¾ï¼Œæ ¹å› æ˜¯ä¸BOSéšè—æ€çš„é«˜ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆè§é¡µ3å›¾2(b)ã€å›¾3(a)(b)ï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šç°æœ‰BOS-sinkç¼“è§£ç­–ç•¥ï¼ˆå¦‚Softmaxå˜ä½“ã€å ä½tokenï¼‰å¤šéœ€å…¨é‡é¢„è®­ç»ƒä¸”é¢å‘é•¿ä¸Šä¸‹æ–‡ï¼›ACTä»…æ¨ç†æœŸé‡åˆ†é…æ³¨æ„åŠ›ã€å¢åŠ å¼€é”€ä¸”æ— æ³•æ¶ˆé™¤å¤§æ¿€æ´»ï¼Œä¸”ä¸LoRAå¾®è°ƒèŒƒå¼ä¸å…¼å®¹ï¼ˆé¡µ4è¡¨2ï¼‰ï¼Œç¼ºä¹è®­ç»ƒæœŸã€è½»é‡ã€åŒæ—¶ç¼“è§£sinkä¸å¤§æ¿€æ´»çš„æ–¹æ¡ˆã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºå»ç›¸å…³æŸå¤±ï¼šåœ¨ä¸­é—´å±‚æƒ©ç½šBOSä¸å…¶ä»–tokenéšè—æ€çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå¹³æ–¹ï¼‰ï¼Œä¸äº¤å‰ç†µè”åˆä¼˜åŒ–ï¼Œæ—¢ä¸æ”¹æ¨¡å‹ç»“æ„ä¹Ÿæ— æ¨ç†å¼€é”€ã€å…¼å®¹LoRAï¼Œä»æºå¤´å‰Šå¼±ä¸­é—´sinkä¸å¤§æ¿€æ´»å¹¶åœ¨é«˜å‹ç¼©ä¸‹æ˜¾è‘—æå‡WERï¼ˆè§é¡µ4è¡¨1ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Sink-æ„ŸçŸ¥çš„è‡ªé€‚åº”å»ç›¸å…³æ­£åˆ™ç”¨äºæµå¼AVSRï¼šåŠ¨æ€è°ƒèŠ‚Î»ä¸å±‚é€‰æ‹©ï¼Œé€‚é…é•¿ä¸Šä¸‹æ–‡ä¸å®æ—¶åœºæ™¯ï¼Œæƒè¡¡ç¨³å¥æ€§ä¸å»¶è¿Ÿã€‚<br>â€¢ ä»MLPé—¨æ§åˆ°æ³¨æ„åŠ›çš„ç»Ÿä¸€æ­£åˆ™ï¼šé¢å‘massive activationsçš„é—¨æ§ç»Ÿè®¡/è°±çº¦æŸä¸å»ç›¸å…³è”åˆè®­ç»ƒï¼Œä»æºå¤´æŠ‘åˆ¶æ”¾å¤§é“¾è·¯ã€‚<br>â€¢ æ— SinkåŒ–çš„è·¨æ¨¡æ€æŠ•å½±ä¸æç¤ºè®¾è®¡ï¼šé€šè¿‡æ­£äº¤å­ç©ºé—´/å¯å­¦ä¹ æ—‹è½¬ä¸promptå·¥ç¨‹ï¼Œé¿å…ä½è¯­ä¹‰tokenæˆä¸ºsinkå¹¶å¼ºåŒ–éŸ³è§†å¯¹é½ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22317" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22317" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present memory-based language modeling as an efficient, eco-friendly alternative to deep neural network-based language modeling. It offers log-linearly scalable next-token prediction performance and strong memorization capabilities. Implementing fast approximations of k-nearest neighbor classification, memory-based language modeling leaves a relatively small ecological footprint both in training and in inference mode, as it relies fully on CPUs and attains low token latencies. Its internal workings are simple and fully transparent. We compare our implementation of memory-based language modeling, OLIFANT, with GPT-2 and GPT-Neo on next-token prediction accuracy, estimated emissions and speeds, and offer some deeper analyses of the model.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ é™ä½å¤§æ¨¡å‹è®­ç»ƒ/æ¨ç†æˆæœ¬ä¸ç¢³è¶³è¿¹ï¼šä¸»æµTransformer LMè®­ç»ƒä¸æ¨ç†è€—èƒ½é«˜ã€ä¾èµ–GPUï¼Œéš¾ä»¥åœ¨èµ„æºå—é™ä¸å¯æŒç»­åœºæ™¯éƒ¨ç½²ï¼›è®ºæ–‡æå‡ºåŸºäºCPUçš„è®°å¿†å¼LMï¼Œå®æµ‹æ¨ç†ç¢³æ’æ˜¾è‘—æ›´ä½ï¼ˆå¦‚å›¾7ç¬¬9é¡µã€è¡¨1ç¬¬8é¡µï¼‰ã€‚<br>â€¢ æå‡å¯è§£é‡Šæ€§ä¸å¯æº¯æºæ€§ï¼šç¥ç»LMå†…éƒ¨æœºç†ä¸é€æ˜ï¼Œéš¾ä»¥è§£é‡Šé¢„æµ‹æ¥æºï¼›æœ¬æ–‡æ–¹æ³•å¯è¿”å›æœ€è¿‘é‚»/è·¯å¾„åŠå…¶è®¡æ•°åˆ†å¸ƒï¼Œæ”¯æŒæºçº§æº¯æºï¼ˆç¬¬10â€“11é¡µï¼‰ã€‚<br>â€¢ è§£å†³è®°å¿†å¼LMæ¨ç†æ•ˆç‡ç“¶é¢ˆï¼šæœ´ç´ kNNæ¨ç†å¤æ‚åº¦é«˜ï¼ˆO(nd)ï¼‰ï¼Œéš¾ä»¥å®ç”¨ï¼›è®ºæ–‡ä»¥å‰ç¼€Trieä¸ä¿¡æ¯å¢ç›Šæ’åºæ„å»ºé«˜æ•ˆç´¢å¼•ï¼Œå¹¶é€šè¿‡TRIBL2/IGTreeå®ç°ä½å»¶è¿Ÿæ¨ç†ï¼ˆå›¾6ç¬¬8é¡µï¼‰ã€‚<br>â€¢ æ¾„æ¸…ä¸åˆ©ç”¨å¯æ‰©å±•æ€§ï¼šTransformerè®­ç»ƒæ¶‰åŠæ•°æ®/æ¨¡å‹/ç®—åŠ›ä¸‰ç»´è€¦åˆï¼Œç¼©æ”¾è§„å¾‹å¤æ‚ï¼›è®°å¿†å¼LMâ€œæ¨¡å‹å³æ•°æ®â€ï¼Œè®­ç»ƒå•ç»´çº¿æ€§æ‰©å±•ï¼Œå‘ˆè¿‘ä¼¼å¯¹æ•°çº¿æ€§å­¦ä¹ æ›²çº¿ï¼Œå¹¶åœ¨â‰ˆ10^9 tokenså¤„å‡ºç°æ›´é™¡å¢ç›Šï¼ˆå›¾5ç¬¬7é¡µï¼‰ã€‚<br>â€¢ è¦†ç›–é•¿å°¾ä¸å¯æ§è®°å¿†ï¼šç¥ç»LMå¯¹é•¿å°¾è¯ä¸ç²¾ç¡®èƒŒè¯µå—é™ï¼›æœ¬æ–‡æ¨¡å‹å¤©ç„¶å¼ºåŒ–é•¿å°¾ï¼ˆç¬¬12é¡µã€å›¾11ï¼‰å¹¶èƒ½åœ¨ä¸Šä¸‹æ–‡4çš„æ¡ä»¶ä¸‹è¾ƒé«˜æ¯”ä¾‹å¤è¿°è®­ç»ƒæ•°æ®ï¼ˆç¬¬9â€“10é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºOLIFANTï¼šä»¥ä¿¡æ¯å¢ç›Šï¼ˆgain ratioï¼‰æ’åºçš„å‰ç¼€Trieä¸ºç´¢å¼•ï¼Œåœ¨å›ºå®šå°ä¸Šä¸‹æ–‡ï¼ˆé»˜è®¤4 tokenï¼‰ä¸Šè¿›è¡Œä¸‹ä¸€è¯é¢„æµ‹ï¼Œå¹¶æä¾›ä¸‰ç§æ¨ç†æ¨¡å¼ï¼ˆIB1-IGçº¯kNNã€TRIBL2æ ‘+å±€éƒ¨kNNã€IGTreeçº¯å†³ç­–æ ‘ï¼‰å®ç°ä¸€æ¬¡éå†çš„å¢é‡è®­ç»ƒä¸CPUä½å»¶è¿Ÿã€ç¨€ç–åˆ†å¸ƒè¾“å‡ºï¼ŒåŒæ—¶ç³»ç»Ÿè¯„ä¼°å‡†ç¡®ç‡ã€å»¶è¿Ÿä¸ç¢³æ’ï¼ˆå›¾5â€“7ã€è¡¨1ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Exemplar-Weighted OLIFANTï¼šé¢å‘åœ¨çº¿ä¸ªæ€§åŒ–ä¸é—å¿˜çš„æ³¨æ„åŠ›åŒ–æŠ•ç¥¨æœºåˆ¶â€”â€”ä»¥ç¤ºä¾‹æƒé‡ä¸æ—¶é—´è¡°å‡ä¼˜å…ˆè¿‘æœŸ/ç‰¹å®šåŸŸæ•°æ®ï¼Œå®ç°å¿«é€Ÿå¾®è°ƒä¸å¯æ§é—å¿˜ã€‚<br>â€¢ èåˆMVDMä¸éä½ç½®å…¨å±€ç‰¹å¾çš„è®°å¿†è¯­è¨€æ¨¡å‹â€”â€”å°†å¤šå€¼å·®å¼‚åº¦é‡ä¸BoW/ä¸»é¢˜/å®ä½“ç­‰å…¨å±€ä¿¡å·èå…¥ï¼Œçªç ´ä»…é ä½ç½®ç‰¹å¾ä¸4-tokenä¸Šä¸‹æ–‡çš„é™åˆ¶ã€‚<br>â€¢ æŠ•æœºè§£ç ä¸Transformeræ’å€¼çš„ä½ç¢³æ··åˆNN-LMâ€”â€”å°†OLIFANTä¸TransformeråškNNæ’å€¼/æŸæœç´¢/æŠ•æœºè§£ç ï¼Œå…¼é¡¾é•¿ä¸Šä¸‹æ–‡ä¼˜åŠ¿ä¸é•¿å°¾è¦†ç›–ï¼Œåœ¨CPUä¸Šå®ç°é«˜èƒ½æ•ˆæ¨ç†ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">FlowOpt: Fast Optimization Through Whole Flow Processes for Training-Free Editing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22010" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22010" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The remarkable success of diffusion and flow-matching models has ignited a surge of works on adapting them at test time for controlled generation tasks. Examples range from image editing to restoration, compression and personalization. However, due to the iterative nature of the sampling process in those models, it is computationally impractical to use gradient-based optimization to directly control the image generated at the end of the process. As a result, existing methods typically resort to manipulating each timestep separately. Here we introduce FlowOpt - a zero-order (gradient-free) optimization framework that treats the entire flow process as a black box, enabling optimization through the whole sampling path without backpropagation through the model. Our method is both highly efficient and allows users to monitor the intermediate optimization results and perform early stopping if desired. We prove a sufficient condition on FlowOpt's step-size, under which convergence to the global optimum is guaranteed. We further show how to empirically estimate this upper bound so as to choose an appropriate step-size. We demonstrate how FlowOpt can be used for image editing, showcasing two options: (i) inversion (determining the initial noise that generates a given image), and (ii) directly steering the edited image to be similar to the source image while conforming to a target text prompt. In both cases, FlowOpt achieves state-of-the-art results while using roughly the same number of neural function evaluations (NFEs) as existing methods. Code and examples are available on the project's webpage.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ è¿­ä»£é‡‡æ ·å¯¼è‡´ç«¯åˆ°ç«¯æ§åˆ¶å›°éš¾ï¼šæ‰©æ•£/æµåŒ¹é…æ¨¡å‹éœ€å¤šæ¬¡NFEï¼Œç›´æ¥å¯¹æœ€ç»ˆç”Ÿæˆå›¾åƒåšæ¢¯åº¦ä¼˜åŒ–åœ¨ç®—åŠ›ä¸å†…å­˜ä¸Šä¸å¯è¡Œï¼ˆè§ç¬¬1é¡µæ‘˜è¦ã€ç¬¬2é¡µå¼•è¨€ï¼‰ã€‚<br>â€¢ åˆ†æ—¶æ­¥æ–¹æ³•è¯¯å·®ç´¯ç§¯ï¼šç°æœ‰ç¼–è¾‘/æ¢å¤/å‹ç¼©å¤šåœ¨æ¯ä¸ªæ—¶é—´æ­¥ç‹¬ç«‹å¹²é¢„ï¼Œç¼ºä¹å¯¹æœ€ç»ˆç»“æœçš„ç›´æ¥ç›‘ç£ï¼Œè¯¯å·®æ²¿æ—¶é—´æ­¥ç´¯ç§¯ï¼Œé‡å»ºä¸ç¼–è¾‘ç²¾åº¦å—é™ï¼ˆç¬¬1â€“2é¡µï¼‰ã€‚<br>â€¢ å…¨æµç¨‹æ¢¯åº¦ä¼˜åŒ–ä¸å…·æ‰©å±•æ€§ï¼šå¦‚D-Flowéœ€å¯¹æ•´æ¡å»å™ªå™¨é“¾åä¼ ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤§æ¨¡å‹ä¸é«˜åˆ†è¾¨ç‡ã€äº¤äº’å¼ç¼–è¾‘åœºæ™¯ï¼ˆç¬¬3é¡µç›¸å…³å·¥ä½œï¼‰ã€‚<br>â€¢ ç°æœ‰â€œæ— åä¼ â€ä¼˜åŒ–ä»æ˜¯åˆ†æ—¶æ­¥ï¼šSDS/DDS/PDS/iRFDSæŒ‰éšæœºæ—¶åˆ»æ›´æ–°ï¼Œæœªå®ç°è´¯ç©¿æ•´æ¡æµçš„è”åˆä¼˜åŒ–ï¼ˆç¬¬3é¡µï¼‰ã€‚<br>â€¢ éœ€è¦è®­ç»ƒå…ã€é»‘ç›’å¯ç”¨ä¸”é«˜æ•ˆçš„æ–¹æ¡ˆï¼šæ—¢èƒ½å¯¹æœ€ç»ˆå›¾åƒç›´æ¥ä¼˜åŒ–ï¼Œåˆèƒ½åœ¨æœ‰é™NFEå†…è¿è¡Œã€å¯é€æ­¥ç›‘æ§ä¸æ—©åœï¼ˆç¬¬1â€“2é¡µï¼Œå›¾3â€“4ï¼‰ã€‚<br>â€¢ éœ€è¦å¯è¯æ”¶æ•›ä¸å¯è°ƒæ­¥é•¿ï¼šä»¥å¾€é›¶é˜¶æ³•Î·=1åœ¨ç°ä»£æ¨¡å‹ä¸Šä¸æ”¶æ•›ï¼Œéœ€ç»™å‡ºå……åˆ†æ¡ä»¶ä¸ç»éªŒä¸Šç•Œä¼°è®¡ï¼ˆç¬¬5â€“6é¡µï¼Œå®šç†1ä¸è¡¨1ã€å›¾5ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºFlowOptï¼šå°†æ•´æ¡æµé‡‡æ ·è¿‡ç¨‹å°è£…ä¸ºé»‘ç›’æ˜ å°„fï¼Œå¯¹æœ€ç»ˆå›¾åƒçš„L2ç­‰æŸå¤±æ‰§è¡Œé›¶é˜¶è¿­ä»£z_{t}^{(i+1)}=z_{t}^{(i)}âˆ’Î·(f(z_{t}^{(i)},c)âˆ’y)ï¼Œæ— é¡»åä¼ å³å¯è·¨å…¨æµç¨‹è”åˆä¼˜åŒ–ï¼›å¹¶ç»™å‡ºåŸºäºå‹ç¼©æ˜ å°„çš„æ­¥é•¿å……åˆ†æ¡ä»¶ï¼ˆå¼7ï¼‰åŠç»éªŒä¼°è®¡ï¼ˆè¡¨1ï¼‰ï¼Œåœ¨FLUXä¸SD3ä¸Šä»¥å°‘é‡æ­¥æ•°å®ç°SOTAçš„é‡å»ºä¸ç›´æ¥ç¼–è¾‘ï¼Œä¸”å¯ä¸­é€”ç›‘æ§ä¸æ—©åœï¼ˆè§ç¬¬4â€“6é¡µï¼Œå›¾3â€“6ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Adaptive-FlowOptï¼šè‡ªé€‚åº”/çº¿æœç´¢æ­¥é•¿ä¸æ—¶åˆ»è°ƒåº¦çš„é›¶é˜¶å…¨æµç¨‹ä¼˜åŒ–ï¼ŒåŠ é€Ÿæ”¶æ•›å¹¶æå‡ç¨³å¥æ€§ã€‚<br>â€¢ FlowOpt-RestoreCompressï¼šé›¶é˜¶å…¨æµç¨‹ä¼˜åŒ–åœ¨å›¾åƒæ¢å¤ä¸å‹ç¼©ä¸­çš„ç»Ÿä¸€æ¡†æ¶ä¸ç†è®ºè¯¯å·®ç•Œã€‚<br>â€¢ Jacobian-Light FlowOptï¼šå¼•å…¥ä½ç§©/å±€éƒ¨é›…å¯æ¯”è¿‘ä¼¼çš„åŠé›¶é˜¶æ³•ï¼Œå…¼é¡¾æ•ˆç‡ä¸æ›´ç´§çš„æ”¶æ•›ä¿è¯ã€‚<br>â€¢ Video/Audio-FlowOptï¼šé¢å‘è§†é¢‘/éŸ³é¢‘çš„è®­ç»ƒå…é›¶é˜¶å…¨æµç¨‹ç¼–è¾‘ï¼Œå¢å¼ºæ—¶åºä¸€è‡´ä¸è·¨æ¨¡æ€é€‚é…ã€‚<br>â€¢ Masked-FlowOpt for Large-Region Editsï¼šç»“åˆæ©è†œä¸ç»“æ„å…ˆéªŒï¼Œæ”»å…‹è®ºæ–‡æŒ‡å‡ºçš„å¤§èŒƒå›´åŒºåŸŸç¼–è¾‘éš¾ä¾‹ã€‚<br>â€¢ Tight-Bounds for FlowOptï¼šé’ˆå¯¹å…·ä½“æ±‚è§£å™¨ä¸æ¨¡å‹çš„æ­¥é•¿ä¸Šç•Œç´§åŒ–ä¸è¯¯å·®ä¼ æ’­ç†è®ºï¼Œæ”¹è¿›ç¨³å®šæ€§ä¸é€Ÿåº¦ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-12">

    <div class="paper">
        <h2 class="paper-title">Sprint: Sparse-Dense Residual Fusion for Efficient Diffusion Transformers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21986" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21986" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion Transformers (DiTs) deliver state-of-the-art generative performance but their quadratic training cost with sequence length makes large-scale pretraining prohibitively expensive. Token dropping can reduce training cost, yet na\"ive strategies degrade representations, and existing methods are either parameter-heavy or fail at high drop ratios. We present SPRINT, Sparse--Dense Residual Fusion for Efficient Diffusion Transformers, a simple method that enables aggressive token dropping (up to 75%) while preserving quality. SPRINT leverages the complementary roles of shallow and deep layers: early layers process all tokens to capture local detail, deeper layers operate on a sparse subset to cut computation, and their outputs are fused through residual connections. Training follows a two-stage schedule: long masked pre-training for efficiency followed by short full-token fine-tuning to close the train--inference gap. On ImageNet-1K 256x256, SPRINT achieves 9.8x training savings with comparable FID/FDD, and at inference, its Path-Drop Guidance (PDG) nearly halves FLOPs while improving quality. These results establish SPRINT as a simple, effective, and general solution for efficient DiT training.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ è®­ç»ƒå¼€é”€é«˜ï¼šDiffusion Transformerè®­ç»ƒæˆæœ¬éšåºåˆ—é•¿åº¦å‘ˆäºŒæ¬¡å¢é•¿ï¼Œå¯¼è‡´å¤§è§„æ¨¡é¢„è®­ç»ƒè®¡ç®—ä¸æ˜¾å­˜ä¸å¯æ‰¿å—ï¼ˆè§ç¬¬1é¡µæ‘˜è¦/å¼•è¨€ï¼‰ã€‚<br>â€¢ ç®€å•ä¸¢tokenå¤±æ•ˆï¼šæœ´ç´ æˆ–éšæœºçš„token droppingä¼šå‰Šå¼±è¡¨ç¤ºã€åœ¨æ¨ç†ç”¨å…¨tokenæ—¶å‡ºç°æ˜¾è‘—çš„trainâ€“inference gapï¼Œè´¨é‡ä¸‹æ»‘ï¼ˆç¬¬1é¡µï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šè®¸å¤šæ–¹æ³•è¦ä¹ˆå‚æ•°/æ¨¡å—å¼€é”€å¤§ï¼Œè¦ä¹ˆåªèƒ½åœ¨ä¸­ç­‰æ©ç ï¼ˆâ‰¤50%ï¼‰ä¸‹ç¨³å®šï¼›åœ¨æ¿€è¿›æ¯”ä¾‹ï¼ˆå¦‚75%ï¼‰æ—¶æ€§èƒ½å´©æºƒï¼Œä¸”ä¸å¯¹é½ç±»æŸå¤±ï¼ˆå¦‚REPAï¼‰å¾€å¾€ä¸å…¼å®¹æˆ–ä¸ç¨³å®šï¼ˆç¬¬2é¡µç›¸å…³å·¥ä½œï¼‰ã€‚<br>â€¢ æ¶æ„å†—ä½™ï¼šåŒè´¨DiTè®©æ·±å±‚å¯¹æ‰€æœ‰ç¨ å¯†tokené‡å¤è®¡ç®—å±€éƒ¨ç»†èŠ‚ï¼Œæ·±å±‚åº”èšç„¦å…¨å±€è¯­ä¹‰å´æµªè´¹å¤§é‡FLOPsï¼Œå¯¼è‡´æ”¶æ•›æ…¢ï¼ˆç¬¬4é¡µâ€œç“¶é¢ˆâ€ï¼‰ã€‚<br>â€¢ æ¨ç†æˆæœ¬é«˜ï¼šæ ‡å‡†CFGéœ€ä¸¤æ¬¡å‰å‘ä½¿FLOPsè¿‘ä¹ç¿»å€ï¼Œé™åˆ¶éƒ¨ç½²æ•ˆç‡ï¼ˆç¬¬5é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>SPRINTå°†DiTåˆ’åˆ†ä¸ºç¼–ç å™¨â€”ä¸­é—´å—â€”è§£ç å™¨ï¼šç¼–ç å™¨åœ¨å…¨éƒ¨tokenä¸Šæå–ç¨ å¯†æµ…å±‚å±€éƒ¨ç‰¹å¾ï¼›åœ¨è¿›å…¥ä¸­é—´å—å‰ä»¥é«˜æ¯”ä¾‹ï¼ˆå¦‚75%ï¼‰ä¸¢å¼ƒtokenï¼Œä»…åœ¨ç¨€ç–å­é›†ä¸Šå»ºæ¨¡å…¨å±€è¯­ä¹‰ï¼Œå¹¶ç”¨[MASK]å›å¡«è‡³åŸé•¿åº¦åä¸ç¨ å¯†æµ…å±‚ç‰¹å¾é€šé“æ‹¼æ¥ã€çº¿æ€§æŠ•å½±èåˆï¼Œå†äº¤ç”±è§£ç å™¨é¢„æµ‹å…¨tokenã€‚è®­ç»ƒé‡‡ç”¨â€œä¸¤é˜¶æ®µâ€ï¼šé•¿æ—¶é«˜æ©ç é¢„è®­ç»ƒ+çŸ­æš‚å…¨tokenå¾®è°ƒä»¥å¼¥åˆè®­ç»ƒ-æ¨ç†å·®è·ï¼›æ¨ç†ç«¯æå‡ºPath-Drop Guidanceï¼Œç”¨æµ…è·¯å¾„è¿‘ä¼¼æ— æ¡ä»¶åˆ†æ”¯ï¼ŒåŸºæœ¬å°†é‡‡æ ·FLOPså‡åŠå¹¶æå‡è´¨é‡ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è‡ªé€‚åº”ç¨€ç–è·¯ç”±çš„SPRINTï¼šå¼•å…¥å¯å­¦ä¹ çš„tokené‡è¦æ€§/æ—¶åˆ»è‡ªé€‚åº”drop ratioï¼Œæ›¿ä»£éšæœºä¸¢å¼ƒä»¥è¿›ä¸€æ­¥æå‡é«˜æ©ç ç¨³å®šæ€§ä¸è´¨é‡ã€‚<br>â€¢ æ—¶ç©ºSPRINTç”¨äºè§†é¢‘æ‰©æ•£ï¼šå°†ç¨ å¯†-ç¨€ç–æ®‹å·®èåˆæ‰©å±•åˆ°æ—¶ç©ºè½´ä¸æ—¶åºæ³¨æ„åŠ›ï¼Œé™ä½è§†é¢‘DiTçš„è®¡ç®—å¤æ‚åº¦å¹¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ã€‚<br>â€¢ PDG++ä½æˆæœ¬å¼•å¯¼æ¡†æ¶ï¼šç»“åˆæµ…è·¯å¾„è‡ªè’¸é¦ä¸å¯å­¦ä¹ èåˆæƒé‡ï¼Œç³»ç»Ÿæ¯”è¾ƒ/ç»Ÿä¸€PDGã€CFGä¸Auto-Guidanceåœ¨ä¸åŒå™ªå£°æ—¥ç¨‹ä¸‹çš„æ•ˆç‡â€”è´¨é‡æƒè¡¡ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MARS-M: When Variance Reduction Meets Matrices</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21800" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21800" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Matrix-based preconditioned optimizers, such as Muon, have recently been shown to be more efficient than scalar-based optimizers for training large-scale neural networks, including large language models (LLMs). On the other hand, recent benchmarks on optimizers for LLM pre-training have demonstrated that variance-reduction techniques such as MARS can achieve substantial speedups over standard optimizers that do not employ variance reduction. In this paper, to achieve the best of both worlds, we introduce MARS-M, a new optimizer that integrates the variance reduction technique in MARS with Muon. Under standard regularity conditions, we prove that Muon-M converges to a first-order stationary point at a rate of mathcal{O}(T^{-1/3}), which improves upon mathcal{O}(T^{-1/4}) rate attained by Muon. Our empirical results on language modeling and computer vision tasks demonstrate that MARS-M consistently yields lower losses and improved performance across various downstream benchmarks. The implementation of MARS-M is available at https://github.com/AGI-Arena/MARS/MARS_M.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šå¦‚ä½•æŠŠå¯¹LLMæœ‰æ•ˆçš„â€œçŸ©é˜µå‹â€é¢„æ¡ä»¶ä¼˜åŒ–ï¼ˆå¦‚Muon/Moonlightï¼‰ä¸èƒ½æ˜¾è‘—é™å™ªæé€Ÿçš„æ–¹å·®ç¼©å‡æ–¹æ³•ï¼ˆå¦‚MARSï¼‰æœ‰æœºç»“åˆï¼Œå…¼å¾—äºŒè€…ä¼˜åŠ¿ã€‚<br>â€¢ é‡è¦æ€§ï¼šçŸ©é˜µå‹ä¼˜åŒ–å™¨åœ¨å¤§æ¨¡å‹è®­ç»ƒä¸­å·²ä¼˜äºæ ‡é‡å‹æ–¹æ³•ï¼Œè€Œæ–¹å·®ç¼©å‡åœ¨è¿‘æœŸé¢„è®­ç»ƒåŸºå‡†ä¸­å¸¦æ¥é¢å¤–åŠ é€Ÿï¼›è‹¥èƒ½èåˆï¼Œå°†ç›´æ¥æå‡LLMä¸è§†è§‰å¤§æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦ä¸ç¨³å®šæ€§ã€‚<br>â€¢ ç°æœ‰å±€é™1ï¼šä¼ ç»Ÿæ–¹å·®ç¼©å‡ï¼ˆSAG/SVRG/SARAH/SPIDER/STORMç­‰ï¼‰ä¸æ·±åº¦å­¦ä¹ å®è·µã€é¢„æ¡ä»¶ä¼˜åŒ–å­˜åœ¨ä¸å…¼å®¹ï¼Œéš¾ä»¥åœ¨å¤§è§„æ¨¡ç¥ç»ç½‘ç»œä¸­è½åœ°ç¨³å®šæœ‰æ•ˆã€‚<br>â€¢ ç°æœ‰å±€é™2ï¼šå·²æœ‰èåˆå°è¯•ï¼ˆå¦‚MARS-Shampooï¼‰åœ¨å®è·µä¸­ä»ä¸å¦‚å‘é‡å‹å˜ä½“ï¼Œè¡¨æ˜â€œå¦‚ä½•ä¸çŸ©é˜µé¢„æ¡ä»¶å™¨æ­£ç¡®è€¦åˆâ€ä»æœªè§£å†³ã€‚<br>â€¢ ç°æœ‰å±€é™3ï¼šMuonåœ¨ç†è®ºä¸Šä»…æœ‰O(T^{-1/4})æ”¶æ•›ç‡ï¼›ä¸”ç²¾ç¡®æ–¹å·®æ ¡æ­£éœ€åŒæ¬¡æ¢¯åº¦è®¡ç®—ï¼Œè®¡ç®—/å·¥ç¨‹ä»£ä»·åé«˜ï¼Œéœ€è¦é«˜æ•ˆè¿‘ä¼¼æ–¹æ¡ˆã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºMARS-Mï¼šåœ¨Moonlightï¼ˆMuonå˜ä½“ï¼‰çš„çŸ©é˜µåŠ¨é‡ä¸Šå¼•å…¥MARSå¼â€œç¼©æ”¾çš„æ¢¯åº¦æ ¡æ­£+è£å‰ªâ€ï¼Œå¹¶ç”¨Newtonâ€“Schulzè¿‘ä¼¼UtVt^Tè¿›è¡ŒçŸ©é˜µé¢„æ¡ä»¶æ›´æ–°ï¼ŒåŒæ—¶ä¿ç•™Moonlightçš„0.2Â·âˆšmax(m,n)å°ºåº¦ä¸æƒé‡è¡°å‡ï¼›ç»™å‡ºç²¾ç¡®ä¸è¿‘ä¼¼ä¸¤ç§å®ç°ï¼Œç†è®ºä¸Šè¾¾åˆ°O~(T^{-1/3})æ”¶æ•›ç‡ï¼Œä¸”è¿‘ä¼¼ç‰ˆå¯ç­‰ä»·ä¸ºâ€œåŠ¨é‡å‚æ•°é‡è®¾â€çš„Moonlightä»¥é™ä½è®¡ç®—å¼€é”€ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ MARS-M++ï¼šè‡ªé€‚åº”Î³ä¸åˆ†å±‚åŠ¨é‡çš„çŸ©é˜µæ–¹å·®ç¼©å‡ä¼˜åŒ–å™¨â€”â€”æŒ‰å±‚/æŒ‰å½¢çŠ¶è‡ªé€‚åº”è°ƒèŠ‚Î³ä¸åŠ¨é‡ï¼Œæå‡ç¨³å¥æ€§å¹¶ç»™å‡ºç»Ÿä¸€æ”¶æ•›åˆ†æä¸å¤§æ¨¡å‹å®è¯<br>â€¢ å¤§æ‰¹é‡ä¸å¼‚æ­¥å¹¶è¡Œä¸‹çš„MARS-Mï¼šç†è®ºä¸ç³»ç»ŸååŒä¼˜åŒ–â€”â€”é¢å‘åˆ†å¸ƒå¼/æµæ°´çº¿/å¼‚æ­¥è®­ç»ƒè®¾è®¡é€šä¿¡é«˜æ•ˆçš„æ ¡æ­£ç­–ç•¥ä¸è¯¯å·®è¡¥å¿æœºåˆ¶<br>â€¢ å¼ é‡/ç¨€ç–ç»“æ„ä¸Šçš„MARS-Mï¼šä¸Shampoo/SOAP/PolarGradçš„ç»Ÿä¸€æ¡†æ¶â€”â€”å°†æ–¹å·®ç¼©å‡æ¨å¹¿åˆ°Kronecker/å¼ é‡é¢„æ¡ä»¶ä¸ç¨€ç–/ä½ç§©ç»“æ„ï¼Œå…¼é¡¾ç†è®ºä¿è¯ä¸å·¥ç¨‹å¯æ‰©å±•æ€§</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Multi-Agent Evolve: LLM Self-Improve through Co-evolution</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23595" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23595" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement Learning (RL) has demonstrated significant potential in enhancing the reasoning capabilities of large language models (LLMs). However, the success of RL for LLMs heavily relies on human-curated datasets and verifiable rewards, which limit their scalability and generality. Recent Self-Play RL methods, inspired by the success of the paradigm in games and Go, aim to enhance LLM reasoning capabilities without human-annotated data. However, their methods primarily depend on a grounded environment for feedback (e.g., a Python interpreter or a game engine); extending them to general domains remains challenging. To address these challenges, we propose Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in solving diverse tasks, including mathematics, reasoning, and general knowledge Q&A. The core design of MAE is based on a triplet of interacting agents (Proposer, Solver, Judge) that are instantiated from a single LLM, and applies reinforcement learning to optimize their behaviors. The Proposer generates questions, the Solver attempts solutions, and the Judge evaluates both while co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves an average improvement of 4.54% on multiple benchmarks. These results highlight MAE as a scalable, data-efficient method for enhancing the general reasoning abilities of LLMs with minimal reliance on human-curated supervision.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°æœ‰ç”¨äºæå‡LLMæ¨ç†èƒ½åŠ›çš„RLæ–¹æ³•é«˜åº¦ä¾èµ–äººå·¥æ ‡æ³¨æ•°æ®ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆå¦‚æ ‡å‡†ç­”æ¡ˆ/å¯æ‰§è¡Œç¯å¢ƒï¼‰ï¼Œéš¾ä»¥è§„æ¨¡åŒ–ä¸”é€šç”¨æ€§æœ‰é™ï¼ˆç¬¬1é¡µå¯¼è¨€ï¼‰ã€‚<br>â€¢ ç°æœ‰LLMè‡ªåšå¼ˆ(Self-Play)å¤šä¾èµ–å…·è±¡åŒ–ç¯å¢ƒï¼ˆPythonè§£é‡Šå™¨ã€æ¸¸æˆå¼•æ“ï¼‰æä¾›å¯éªŒè¯åé¦ˆï¼Œéš¾è¿ç§»åˆ°å¼€æ”¾é¢†åŸŸï¼›é€šç”¨æ¨ç†ä»»åŠ¡çš„å¥–åŠ±è®¾è®¡æœ¬è´¨å«ç³Šï¼ˆç¬¬1é¡µï¼‰ã€‚<br>â€¢ ä¼ ç»Ÿä¸¤äººé›¶å’Œå¯¹å¼ˆèŒƒå¼ä¸é€šç”¨ä»»åŠ¡ä¸å®Œå…¨å¥‘åˆï¼Œæ¨¡å‹é—´äº¤äº’å—é™ä¸”è®­ç»ƒæ˜“ä¸ç¨³ã€æ•°æ®é›†æ˜“åŠ£åŒ–ï¼Œéœ€è¦ä¸€ç§ç¨³å®šçš„è‡ªè¿›åŒ–é—­ç¯ä¸æ•°æ®è´¨é‡æ§åˆ¶ï¼ˆç¬¬3é¡µé›¶å’Œè®¨è®ºï¼›ç¬¬9-11é¡µç¨³å®šæ€§ä¸è´¨é‡è¿‡æ»¤ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>å°†åŒä¸€LLMå®ä¾‹åŒ–ä¸ºProposerâ€“Solverâ€“Judgeä¸‰è§’è‰²ï¼Œå½¢æˆâ€œæå‡º-æ±‚è§£-è¯„åˆ¤â€çš„é—­ç¯ï¼›ä»¥Judgeçš„é€šç”¨è¯„åˆ†ä¸ºè‡ªå¥–åŠ±ï¼Œç»“åˆéš¾åº¦å¥–åŠ±ï¼ˆSolverå¤±è¯¯è¶Šå¤šè¶Šé«˜ï¼‰ã€è´¨é‡/æ ¼å¼å¥–åŠ±ä¸é¢˜ç›®è´¨é‡è¿‡æ»¤ï¼Œå¹¶ç”¨Task-Relative REINFORCE++å¯¹ä¸‰è§’è‰²åŒæ­¥æ›´æ–°ï¼Œå®ç°æ— éœ€äººå·¥æ ‡æ³¨ä¸å¤–éƒ¨éªŒè¯å™¨çš„è·¨é¢†åŸŸè‡ªè¿›åŒ–ï¼ˆè§å›¾1ç¬¬2é¡µã€ç®—æ³•1ç¬¬6é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Scaling Multi-Agent Evolve to Larger Backbones and Multi-Role Systemsï¼šç³»ç»Ÿæ€§ç ”ç©¶è§„æ¨¡å¾‹ä¸ç¨³å®šæ€§ï¼Œåœ¨æ›´å¤§åŸºåº§ä¸æ–°å¢è§’è‰²ï¼ˆå¦‚Planner/Critic/Verifierï¼‰ä¸‹çš„æ”¶ç›Šä¸å¤±ç¨³è¾¹ç•Œã€‚<br>â€¢ Hybrid Verifiable-and-Judge Rewards for Open-Domain Self-Evolutionï¼šå°†å¯éªŒè¯ç¯å¢ƒï¼ˆä»£ç æ‰§è¡Œ/å•å…ƒæµ‹è¯•/æ£€ç´¢æ ¡éªŒï¼‰ä¸LLM-as-a-Judgeèåˆï¼Œé™ä½å¥–åŠ±åå·®ä¸â€œæŠ•æœºå–å·§â€ï¼Œæå‡å¯æ³›åŒ–æ€§ã€‚<br>â€¢ Difficulty-Aware Curriculum and Convergence Theory for Co-evolutionary LLMsï¼šå»ºç«‹ååŒè¿›åŒ–çš„éš¾åº¦æ§åˆ¶ä¸æ”¶æ•›ç†è®ºï¼Œè®¾è®¡è‡ªé€‚åº”éš¾åº¦è°ƒåº¦å™¨ä¸åå¥–åŠ±æ”»å‡»é˜²æŠ¤ï¼Œä¿è¯é•¿æœŸç¨³å®šå¢ç›Šã€‚</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 12</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button><button class="page-btn" onclick="goToPage(11)">11</button><button class="page-btn" onclick="goToPage(12)">12</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-10-28 23:18:08 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 12;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>