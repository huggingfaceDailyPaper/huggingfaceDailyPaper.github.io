<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 07, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 07, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.04570" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.04570" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>"Thinking with Text" and "Thinking with Images" paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce "Thinking with Video", a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2's performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions "thinking with video" as a unified multimodal reasoning paradigm.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Static-image limitation: prior ‚ÄúThinking with Text/Images‚Äù paradigms cannot depict temporal dynamics (processes like drawing, motion, reflection, or progressive construction)<br>‚Ä¢ Modality separation: text and vision are handled as distinct channels, preventing a unified, temporally coherent multimodal chain-of-thought<br>‚Ä¢ Lack of unified evaluation: earlier video reasoning studies rely on qualitative inspection, lack verifiable datasets, and offer limited comparisons with strong VLMs<br>‚Ä¢ Need for human-like reasoning: models should imagine, simulate, draw, and write within one temporally grounded medium to better mirror human problem solving<br>‚Ä¢ Practical gap: no benchmark to systematically test both vision-centric (spatial/inductive) and text-centric reasoning in video generation models</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce ‚ÄúThinking with Video,‚Äù prompting video generation models (e.g., Sora-2) to draw, write, and speak within a single temporal chain-of-thought, and propose VideoThinkBench to evaluate vision-centric and text-centric reasoning with verifiable, multi-modal (audio/frames) metrics. Analyze test-time self-consistency and few-shot in-context learning effects, and standardize evaluation (LLM-as-judge, frame-based voting, deviation metrics) for video reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ RLVR for Thinking with Video: Scaling Verifiable Video Reasoning via Reinforcement Learning with Verifiable Rewards on VideoThinkBench<br>‚Ä¢ Test-Time Self-Consistency for Video Reasoners: Majority-over-Frames and Multi-Sample Voting to Boost Verifiable Video CoT Accuracy<br>‚Ä¢ Unified Multimodal Pretraining by Frame-by-Frame Text Synthesis: Converting Text Corpora into Handwriting-Style Video to Teach Video Models World Knowledge<br>‚Ä¢ Disentangling Prompt Rewriting from Generation: Auditing and Controlling Internal Rewriters to Attribute and Improve Text-Centric Reasoning in Video Models<br>‚Ä¢ Open-Source Video Reasoning Suite: Reproducible Benchmarks, Judges, and Protocols for Vision- and Text-Centric Video CoT Evaluation</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">V-Thinker: Interactive Thinking with Images</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.04460" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.04460" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Empowering Large Multimodal Models (LMMs) to deeply integrate image interaction with long-horizon reasoning capabilities remains a long-standing challenge in this field. Recent advances in vision-centric reasoning explore a promising "Thinking with Images" paradigm for LMMs, marking a shift from image-assisted reasoning to image-interactive thinking. While this milestone enables models to focus on fine-grained image regions, progress remains constrained by limited visual tool spaces and task-specific workflow designs. To bridge this gap, we present V-Thinker, a general-purpose multimodal reasoning assistant that enables interactive, vision-centric thinking through end-to-end reinforcement learning. V-Thinker comprises two key components: (1) a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies interactive reasoning datasets across three dimensions-diversity, quality, and difficulty; and (2) a Visual Progressive Training Curriculum that first aligns perception via point-level supervision, then integrates interactive reasoning through a two-stage reinforcement learning framework. Furthermore, we introduce VTBench, an expert-verified benchmark targeting vision-centric interactive reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently outperforms strong LMM-based baselines in both general and interactive reasoning scenarios, providing valuable insights for advancing image-interactive reasoning applications.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LMMs generate long chain-of-thoughts but often detach from visual grounding, leading to hallucinations and over-reliance on linguistic priors.<br>‚Ä¢ Existing interactive approaches offer narrow, task-specific visual tool spaces and depend on precise spatial localization, limiting generalization.<br>‚Ä¢ Image-to-code editing pipelines struggle to represent spatial relations faithfully and introduce noise during visual interaction.<br>‚Ä¢ Data scarcity: few large-scale, diverse, high-quality, and difficulty-graded datasets for image-interactive reasoning; limited standardized benchmarks.<br>‚Ä¢ Training gap: lack of curricula that first align fine-grained perception/localization and then integrate interactive reasoning with executable visual tools.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>V-Thinker is a code-driven multimodal reasoner trained via a two-part framework: a Data Evolution Flywheel that auto-synthesizes diverse, verified, and progressively harder interactive reasoning data (yielding V-Interaction-400K), and a Visual Progressive Training Curriculum that first aligns perception with point-level supervision (V-Perception-40K) then performs SFT+RL in a sandboxed executor to generate and execute visual-editing code during reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Compositional Visual Tool Learning for General-Purpose Interactive Reasoning: Formalize and learn a unified, compositional tool algebra that scales the visual action space beyond task-specific operations.<br>‚Ä¢ Differentiable Visual Executors for End-to-End Trainable Image-Interactive LMMs: Replace purely code-based executors with differentiable visual operators to enable gradient flow, reduce execution errors, and improve spatial precision.<br>‚Ä¢ VTBench++: An Open-World Benchmark for Long-Horizon Vision-Centric Interaction: Extend VTBench to real images, videos, and 3D scenes with standardized protocols, safety constraints, and metrics for multi-step interactive reasoning.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Scaling Agent Learning via Experience Synthesis</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.03773" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.03773" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While reinforcement learning (RL) can empower large language model (LLM) agents by enabling self-improvement through interaction, its practical adoption remains challenging due to costly rollouts, limited task diversity, unreliable reward signals, and infrastructure complexity, all of which obstruct the collection of scalable experience data. To address these challenges, we introduce DreamGym, the first unified framework designed to synthesize diverse experiences with scalability in mind to enable effective online RL training for autonomous agents. Rather than relying on expensive real-environment rollouts, DreamGym distills environment dynamics into a reasoning-based experience model that derives consistent state transitions and feedback signals through step-by-step reasoning, enabling scalable agent rollout collection for RL. To improve the stability and quality of transitions, DreamGym leverages an experience replay buffer initialized with offline real-world data and continuously enriched with fresh interactions to actively support agent training. To improve knowledge acquisition, DreamGym adaptively generates new tasks that challenge the current agent policy, enabling more effective online curriculum learning. Experiments across diverse environments and agent backbones demonstrate that DreamGym substantially improves RL training, both in fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in RL-ready but costly settings, it matches GRPO and PPO performance using only synthetic interactions. When transferring a policy trained purely on synthetic experiences to real-environment RL, DreamGym yields significant additional performance gains while requiring far fewer real-world interactions, providing a scalable warm-start strategy for general-purpose RL.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ RL for LLM agents is bottlenecked by costly, slow, and sample-inefficient real-environment rollouts (long horizons, high per-step compute, sparse/delayed rewards).<br>‚Ä¢ Existing benchmarks provide limited and static task sets; scaling diverse, verifiable tasks is hard and expensive, blocking effective goal-conditioned RL and exploration.<br>‚Ä¢ Reward signals in real interactive settings are noisy/unstable (dynamic web/GUI), many actions are irreversible, and environments often lack reliable reset mechanisms, causing unsafe and unstable training.<br>‚Ä¢ Infrastructure for large-scale rollouts is heterogeneous and heavy (Docker/VMs), making high-throughput sampling engineering-intensive and costly.<br>‚Ä¢ Prior synthetic data approaches depend on real-environment collection (still unscalable), yield static non-adaptive trajectories, or build data-hungry world models focused on raw fidelity rather than learning-centric signals.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DreamGym trains agents via a reasoning-based experience model that operates in an abstract textual state space to synthesize consistent multi-turn transitions and reward signals with chain-of-thought, guided by an active replay buffer seeded with offline data and a curriculum task generator that selects high-entropy tasks. Policies are trained entirely on these synthetic experiences (with PPO/GRPO) and optionally transferred to real environments (DreamGym-S2R) for efficient sim-to-real fine-tuning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Universal DreamGym: A Multi-Domain Reasoning Experience Model for Cross-Environment Transfer: Unify experience models across diverse domains to enable shared abstractions and zero-shot transfer, reducing per-environment data needs.<br>‚Ä¢ Reward-Calibrated Experience Synthesis for Reliable Sim-to-Real RL: Learn uncertainty-aware, verifiable reward and transition models with automatic validators to further bound sim-to-real gaps and stabilize training.<br>‚Ä¢ Multimodal DreamGym: Extending Reasoning-Based Experience Synthesis Beyond Text: Incorporate visual, GUI, and tool-interaction modalities to synthesize richer trajectories for web/OS/embodied agents under a unified abstraction.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">Cambrian-S: Towards Spatial Supersensing in Video</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.04670" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.04670" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We argue that progress in true multimodal intelligence calls for a shift from reactive, task-driven systems and brute-force long context towards a broader paradigm of supersensing. We frame spatial supersensing as four stages beyond linguistic-only understanding: semantic perception (naming what is seen), streaming event cognition (maintaining memory across continuous experiences), implicit 3D spatial cognition (inferring the world behind pixels), and predictive world modeling (creating internal models that filter and organize information). Current benchmarks largely test only the early stages, offering narrow coverage of spatial cognition and rarely challenging models in ways that require true world modeling. To drive progress in spatial supersensing, we present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial recall) and VSC (continual visual spatial counting). These tasks require arbitrarily long video inputs yet are resistant to brute-force context expansion. We then test data scaling limits by curating VSI-590K and training Cambrian-S, achieving +30% absolute improvement on VSI-Bench without sacrificing general capabilities. Yet performance on VSI-SUPER remains limited, indicating that scale alone is insufficient for spatial supersensing. We propose predictive sensing as a path forward, presenting a proof-of-concept in which a self-supervised next-latent-frame predictor leverages surprise (prediction error) to drive memory and event segmentation. On VSI-SUPER, this approach substantially outperforms leading proprietary baselines, showing that spatial supersensing requires models that not only see but also anticipate, select, and organize experience.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Video MLLMs over-rely on sparse frames and language priors/captions, lacking implicit 3D spatial understanding and long-horizon memory.<br>‚Ä¢ Existing benchmarks mostly probe early stages (semantic perception) and are often solvable without strong visual grounding, leaving predictive world modeling untested.<br>‚Ä¢ Brute-force long-context strategies do not scale to arbitrarily long streams and fail to selectively organize information needed for spatial reasoning.<br>‚Ä¢ There is no benchmark that stresses continual spatial cognition (e.g., long-horizon recall and counting across viewpoint changes), hindering progress.<br>‚Ä¢ Data scaling alone is insufficient: even with a large spatially focused corpus (VSI-590K) and a strong model (Cambrian-S), performance on supersensing tasks remains limited, signaling the need for a new paradigm.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces VSI-SUPER (VSR for long-horizon spatial recall and VSC for continual counting) to stress-test spatial supersensing, trains Cambrian-S on the curated spatial corpus VSI-590K to push spatial cognition, and shows scale alone is inadequate. It then proposes predictive sensing via self-supervised next-latent-frame prediction that uses surprise (prediction error) to drive memory allocation and event segmentation, substantially improving performance on VSI-SUPER versus long-context baselines.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Surprise-Gated World Models for Continual Video Agents: Build end-to-end agents where prediction error governs attention, memory writes/reads, and learning over unbounded video streams.<br>‚Ä¢ Implicit 3D Scene Graphs for Multimodal LLMs: Fuse latent 3D geometry estimation with language models to form persistent scene graphs that support metric reasoning, occlusion handling, and viewpoint changes.<br>‚Ä¢ Active Perception and Memory Budgeting under Predictive Uncertainty: Develop controllers that proactively select frames/views and dynamically allocate memory/compression based on uncertainty and task objectives for long-horizon video understanding.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.04307" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.04307" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce GUI-360^circ, a large-scale, comprehensive dataset and benchmark suite designed to advance computer-using agents (CUAs). CUAs present unique challenges and is constrained by three persistent gaps: a scarcity of real-world CUA tasks, the lack of automated collection-and-annotation pipelines for multi-modal trajectories, and the absence of a unified benchmark that jointly evaluates GUI grounding, screen parsing, and action prediction. GUI-360^circ addresses these gaps with an LLM-augmented, largely automated pipeline for query sourcing, environment-template construction, task instantiation, batched execution, and LLM-driven quality filtering. The released corpus contains over 1.2M executed action steps across thousands of trajectories in popular Windows office applications, and includes full-resolution screenshots, accessibility metadata when available, instantiated goals, intermediate reasoning traces, and both successful and failed action trajectories. The dataset supports three canonical tasks, GUI grounding, screen parsing, and action prediction, and a hybrid GUI+API action space that reflects modern agent designs. Benchmarking state-of-the-art vision--language models on GUI-360^circ reveals substantial out-of-the-box shortcomings in grounding and action prediction; supervised fine-tuning and reinforcement learning yield significant gains but do not close the gap to human-level reliability. We release GUI-360^circ and accompanying code to facilitate reproducible research and accelerate progress on robust desktop CUAs. The full dataset has been made public on https://huggingface.co/datasets/vyokky/GUI-360.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Scarcity of realistic, large-scale desktop CUA data grounded in authentic user intents; most existing datasets are handcrafted or LLM-synthesized and miss real-world frequency/diversity.<br>‚Ä¢ Lack of automated, scalable pipelines to collect and annotate multimodal trajectories (screenshots, a11y, actions, reasoning); manual logging/labeling is costly, error-prone, and unscalable.<br>‚Ä¢ Absence of a unified benchmark that jointly evaluates GUI grounding, screen parsing, and action prediction, with standardized splits, metrics, and both success and failure cases.<br>‚Ä¢ Desktop environments are uniquely hard: high-resolution mixed-content screens, heterogeneous widgets, multi-window layouts, and often missing accessibility metadata; tasks are longer-horizon and compositional.<br>‚Ä¢ State-of-the-art VLMs underperform out-of-the-box on grounding and stepwise action prediction, underscoring the need for stronger training data and comprehensive evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>An LLM-augmented, largely automated pipeline sources real user queries, builds reusable environment templates, instantiates concrete tasks, and executes them via a multi-agent executor (TrajAgent) that records full-resolution screenshots, accessibility metadata (SoM), thoughts, and hybrid GUI+API actions. Trajectories are validated by an LLM judge and structured into a unified dataset/benchmark supporting GUI grounding, screen parsing, and action prediction with standardized splits and metrics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Hybrid GUI+API Policies via Offline-to-Online RL on GUI-360: Leverage both successful and failed trajectories to train policies that choose between GUI and API actions, then refine with online RL or value-based improvement.<br>‚Ä¢ Pixel-to-A11y: Synthesizing Accessibility Trees from Screenshots for Robust Desktop Agents: Train models to infer element lists and bounds (SoM) from pixels to operate reliably when accessibility metadata is absent or incomplete.<br>‚Ä¢ Hierarchical Planning for Long-Horizon Desktop Tasks: Develop hierarchical/option-based planners that align with the dataset‚Äôs multi-step thoughts and trajectories to improve compositional task completion.<br>‚Ä¢ Cross-Application and Cross-OS Generalization of GUI Agents: Explore domain adaptation and meta-learning to transfer skills from Office apps to unseen applications and operating systems using template-driven environments.<br>‚Ä¢ Safety-Aware CUAs with Structured Constraints and Reversible Actions: Integrate constraint checking, risk modeling, and rollback strategies to prevent harmful operations while maintaining task efficiency on GUI-360.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">NVIDIA Nemotron Nano V2 VL</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.03929" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.03929" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Nemotron Nano V2 VL, the latest model of the Nemotron vision-language series designed for strong real-world document understanding, long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers significant improvements over our previous model, Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major enhancements in model architecture, datasets, and training recipes. Nemotron Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and innovative token reduction techniques to achieve higher inference throughput in long document and video scenarios. We are releasing model checkpoints in BF16, FP8, and FP4 formats and sharing large parts of our datasets, recipes and training code.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Real-world document and OCR understanding at scale is still challenging, especially across tables, charts, forms, and multilingual text, limiting reliability on benchmarks like OCRBench and DocVQA.<br>‚Ä¢ Long-context reasoning over multi-page documents and long videos is bottlenecked by context length and throughput; prior VLMs typically top out near 16K tokens and are inefficient on long sequences.<br>‚Ä¢ Video inference wastes compute on redundant frames/regions, causing slow TTFT and low throughput for long videos without accuracy gains.<br>‚Ä¢ Multimodal SFT often erodes the base LLM‚Äôs text/code skills (e.g., code reasoning), indicating catastrophic interference between modalities.<br>‚Ä¢ Practical deployment is constrained by latency/memory and a train‚Äìserve gap (dynamic FP8 training vs static quantized inference), hindering efficient serving on common stacks.<br>‚Ä¢ Reasoning verbosity can hurt accuracy and cost; most models lack fine-grained control over ‚Äúhow much‚Äù thinking to perform at inference time.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A 12B VLM that fuses a RADIOv2.5 vision encoder with a hybrid Mamba‚ÄëTransformer LLM via an MLP connector, using dynamic 512√ó512 tiling with pixel‚Äëshuffle token reduction and Efficient Video Sampling to cut visual tokens. A multi-stage FP8 SFT pipeline (warm‚Äëup + 4 stages) progressively extends context to ~300K tokens, preserves text/code skills via targeted SFT and context parallelism/sequence packing, and ships deployable FP8/NVFP4 checkpoints via PTQ/QAD with optional reasoning budget control.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Vision Tokenization for OCR‚ÄëHeavy Images: Learn content‚Äëaware tiling and token reduction that preserve tiny text and layout while minimizing visual tokens for faster, more accurate document OCR.<br>‚Ä¢ End‚Äëto‚ÄëEnd Learnable Efficient Video Sampling: Jointly train a differentiable temporal/spatial redundancy pruner with the VLM to maximize long‚Äëvideo accuracy per token under latency budgets.<br>‚Ä¢ Interference‚ÄëFree Multimodal SFT for Code‚ÄëSafe VLMs: Develop modular adapters or orthogonal gradient constraints to add vision skills without degrading text/code reasoning, eliminating the need for post‚Äëhoc recovery stages.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Contamination Detection for VLMs using Multi-Modal Semantic Perturbation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.03774" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.03774" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in Vision-Language Models (VLMs) have achieved state-of-the-art performance on numerous benchmark tasks. However, the use of internet-scale, often proprietary, pretraining corpora raises a critical concern for both practitioners and users: inflated performance due to test-set leakage. While prior works have proposed mitigation strategies such as decontamination of pretraining data and benchmark redesign for LLMs, the complementary direction of developing detection methods for contaminated VLMs remains underexplored. To address this gap, we deliberately contaminate open-source VLMs on popular benchmarks and show that existing detection approaches either fail outright or exhibit inconsistent behavior. We then propose a novel simple yet effective detection method based on multi-modal semantic perturbation, demonstrating that contaminated models fail to generalize under controlled perturbations. Finally, we validate our approach across multiple realistic contamination strategies, confirming its robustness and effectiveness. The code and perturbed dataset will be released publicly.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Test-set leakage in Vision‚ÄìLanguage Models (VLMs) trained on internet-scale, proprietary corpora inflates benchmark scores and obscures true generalization vs. memorization.<br>‚Ä¢ Practitioners need a practical, black-box way to detect contamination without access to the training corpus; exhaustive decontamination is expensive and often unreported.<br>‚Ä¢ Existing LLM-oriented detectors overlook multi-modality: text perturbations can leave visual cues unchanged, leading to failures or inconsistent signals on VLMs.<br>‚Ä¢ Prior VLM-specific heuristics (e.g., color-channel shuffling, masking/option shuffling) introduce confounds and fail to reliably flag memorization.<br>‚Ä¢ A robust detector must satisfy three unmet requirements: practicality (no leaked-data knowledge), reliability (works across fine-tuning regimes like standard/LoRA/full), and consistency (signal correlates with contamination degree).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Generate multi-modal semantic perturbations by preserving the image‚Äôs global composition (via diffusion + ControlNet guided by Canny edges) while minimally altering visual content to change the correct answer, using an LLM to produce question- and answer-conditioned dense captions. Detect contamination by the model‚Äôs accuracy drop from original to perturbed items‚Äîclean models maintain or improve, while contaminated models that memorized original pairs fail to generalize‚Äîyielding a black-box, training-agnostic, and contamination-degree‚Äìsensitive detector.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Fully Automated Multi-Modal Perturbation Audits for VLMs: Replace manual filtering with automated QA validation and stronger controllable editors to scale black-box contamination detection.<br>‚Ä¢ Beyond Multiple Choice: Contamination Detection for Free-Form VQA and Generative Tasks: Extend perturbation-based probes to open-ended outputs using string matching, likelihoods, or LLM-as-judge.<br>‚Ä¢ Natural Counterfactuals at Scale for Memorization Auditing: Build large real-world counterfactual benchmarks to complement synthetic edits and stress-test contamination in the wild.<br>‚Ä¢ Estimating Contamination Degree via Perturbation Sensitivity Curves: Calibrate performance-drop vs. exposure epochs to quantify contamination strength under black-box access.<br>‚Ä¢ Adversarial Contamination and Defense in Multimodal Pretraining: Model stealth leakage strategies and develop training-time defenses informed by semantic-perturbation diagnostics.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.04217" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.04217" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The strong lottery ticket hypothesis (SLTH) conjectures that high-performing subnetworks, called strong lottery tickets (SLTs), are hidden in randomly initialized neural networks. Although recent theoretical studies have established the SLTH across various neural architectures, the SLTH for transformer architectures still lacks theoretical understanding. In particular, the current theory of the SLTH does not yet account for the multi-head attention (MHA) mechanism, a core component of transformers. To address this gap, we introduce a theoretical analysis of the existence of SLTs within MHAs. We prove that, if a randomly initialized MHA of H heads and input dimension d has the hidden dimension O(dlog(Hd^{3/2})) for the key and value, it contains an SLT that approximates an arbitrary MHA with the same input dimension with high probability. Furthermore, by leveraging this theory for MHAs, we extend the SLTH to transformers without normalization layers. We empirically validate our theoretical findings, demonstrating that the approximation error between the SLT within a source model (MHA and transformer) and an approximate target counterpart decreases exponentially by increasing the hidden dimension of the source model.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ The lack of a theoretical foundation for the Strong Lottery Ticket Hypothesis (SLTH) in transformers, specifically for multi-head attention (MHA), despite proofs for fully-connected, convolutional, and equivariant networks.<br>‚Ä¢ Existing SLTH techniques (two-layers-for-one approximations) do not account for the query‚Äìkey inner-product structure of attention and cannot independently approximate Q and K via pruning a single random matrix.<br>‚Ä¢ Naive error propagation through softmax yields bounds that can scale with sequence length T; a tighter, T-independent analysis is needed for attention.<br>‚Ä¢ Practical need for principled initialization/pruning strategies to uncover SLTs in real transformers, informing model compression and understanding of overparameterized models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Merge query‚Äìkey and value‚Äìoutput projections (W_QK and W_VO), then approximate each merged matrix by pruning two randomly initialized matrices whose product matches the target (a variant of two-layers-for-one via subset-sum). A new softmax perturbation bound ensures the end-to-end approximation error is independent of sequence length T, enabling SLTs in MHAs and extending to transformers (without normalization) with logarithmic hidden-dimension overhead.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Proving the Strong Lottery Ticket Hypothesis for Transformers with Normalization Layers: Extend the theory to include LayerNorm and other normalization/rescaling effects, analyzing their impact on error propagation and required widths.<br>‚Ä¢ Constructive Algorithms for Discovering Attention SLTs at Scale: Replace mixed-integer subset-sum with efficient, theoretically grounded mask-finding methods (e.g., greedy/convex relaxations) and optimized initializations to uncover SLTs in large LLMs.<br>‚Ä¢ Beyond Softmax: SLTs in Alternative and Structured Attention Mechanisms: Generalize the framework to linear/efficient attention, rotary/relative positional encodings, cross- and multi-query attention, and study tightened width bounds under these variants.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Benchmark Designers Should "Train on the Test Set" to Expose Exploitable Non-Visual Shortcuts</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.04655" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.04655" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Robust benchmarks are crucial for evaluating Multimodal Large Language Models (MLLMs). Yet we find that models can ace many multimodal benchmarks without strong visual understanding, instead exploiting biases, linguistic priors, and superficial patterns. This is especially problematic for vision-centric benchmarks that are meant to require visual inputs. We adopt a diagnostic principle for benchmark design: if a benchmark can be gamed, it will be. Designers should therefore try to ``game'' their own benchmarks first, using diagnostic and debiasing procedures to systematically identify and mitigate non-visual biases. Effective diagnosis requires directly ``training on the test set'' -- probing the released test set for its intrinsic, exploitable patterns. We operationalize this standard with two components. First, we diagnose benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology. Our primary diagnostic tool involves fine-tuning a powerful Large Language Model via k-fold cross-validation on exclusively the non-visual, textual inputs of the test set to reveal shortcut performance and assign each sample a bias score s(x). We complement this with a lightweight Random Forest-based diagnostic operating on hand-crafted features for fast, interpretable auditing. Second, we debias benchmarks by filtering high-bias samples using an ``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive non-visual biases. As a case study, we apply our full framework to create VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider vision-blind performance gap than the original.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Multimodal LLMs can achieve high scores on vision-centric benchmarks by exploiting non-visual shortcuts (linguistic priors, world knowledge, and statistical artifacts), inflating perceived visual understanding.<br>‚Ä¢ Common ‚Äúblind‚Äù ablations reveal that vision can be unnecessary but do not diagnose why specific samples are exploitable or how to fix them; they also lack sample-level guidance.<br>‚Ä¢ Existing approaches focus on training-time debiasing or use separate in-distribution data, missing idiosyncratic test-set artifacts introduced by sampling, templates, or filtering.<br>‚Ä¢ Statistical shortcuts are pervasive (e.g., skewed answer distributions, category-position correlations, predictable log-normal size priors), enabling high accuracy without images across benchmarks (VSI-Bench, CV-Bench, MMMU, VideoMME).<br>‚Ä¢ The field lacks a principled, quantitative definition/measure of ‚Äúexploitability‚Äù and a systematic, actionable pipeline to audit and mitigate non-visual biases in the test set itself.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>They propose Test-set Stress-Testing (TsT): k-fold cross-validation that trains diagnostic models on test-set text only to quantify non-visual solvability and produce per-sample bias scores s(x). Two realizations‚ÄîTsT-LLM (LoRA-tuned LLM on questions/choices) and TsT-RF (Random Forest on hand-crafted non-visual features)‚Äîfeed an Iterative Bias Pruning (IBP) procedure that removes high-s(x) samples to create debiased benchmarks (e.g., VSI-Bench-Debiased).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ TST-in-the-Loop Benchmark Synthesis: Automatic Question Rewriting and Answer Rebalancing Guided by s(x): Replace pruning with generative rewriting/rebalancing that transforms high-bias samples into low-bias variants while preserving task intent.<br>‚Ä¢ Generalizing Test-Set Stress-Testing to Audio‚ÄìVideo‚Äì3D: A Unified Framework for Multimodal Shortcut Auditing: Extend TsT to additional modalities (audio, 3D, sensor data) with modality-specific non-visual features and diagnostics for broader benchmark ecosystems.<br>‚Ä¢ Bias-Aware Evaluation Metrics for Multimodal Benchmarks: Incorporating Exploitability into Scoring: Develop standardized metrics that weight samples by (1 ‚àí s(x)), report TsT accuracy and vision‚Äìblind gaps, and provide confidence-calibrated exploitability reports.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.03996" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.03996" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Humanoid soccer poses a representative challenge for embodied intelligence, requiring robots to operate within a tightly coupled perception-action loop. However, existing systems typically rely on decoupled modules, resulting in delayed responses and incoherent behaviors in dynamic environments, while real-world perceptual limitations further exacerbate these issues. In this work, we present a unified reinforcement learning-based controller that enables humanoid robots to acquire reactive soccer skills through the direct integration of visual perception and motion control. Our approach extends Adversarial Motion Priors to perceptual settings in real-world dynamic environments, bridging motion imitation and visually grounded dynamic control. We introduce an encoder-decoder architecture combined with a virtual perception system that models real-world visual characteristics, allowing the policy to recover privileged states from imperfect observations and establish active coordination between perception and action. The resulting controller demonstrates strong reactivity, consistently executing coherent and robust soccer behaviors across various scenarios, including real RoboCup matches.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Humanoid soccer demands tight perception‚Äìaction coupling under fast dynamics, yet common modular pipelines (separate perception, planning, control) introduce latency and incoherent behaviors.<br>‚Ä¢ Onboard vision is noisy, delayed, and narrow-FOV; observations are partial and often wrong during motion, so controllers must be robust to perceptual uncertainty and disturbances.<br>‚Ä¢ Prior unified RL policies often omit vision or depend on environment-specific rendering (e.g., NeRF) and show weak reactivity and poor generalization to real matches.<br>‚Ä¢ Feature-based imitation (e.g., DeepMimic) requires strict temporal alignment to references, limiting adaptability to task needs and dynamic environments; multi-stage pipelines are cumbersome.<br>‚Ä¢ GAN-based motion learning had focused on proprioception in static settings, not leveraging exteroceptive sensing needed for soccer.<br>‚Ä¢ Sim-to-real gaps from raw RGB (lighting, texture) hinder transfer; there is a need for compact, task-relevant visual abstractions that align across sim and real.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A unified RL controller extends Adversarial Motion Priors to vision by coupling a history-based encoder‚Äìdecoder (recovering privileged states from noisy detections) with a compact BEV perception pipeline and a virtual perception model (noise, latency, frequency, detection probability) for sim-to-real transfer. Training uses asymmetric actor‚Äìcritic with multi-critic PPO to reduce reward interference, yielding a single policy that maps egocentric detections and proprioception directly to joint commands for reactive searching, chasing, and multi-directional kicking.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Multi-Agent Vision-Driven AMP for Competitive Humanoid Soccer: Extend the unified controller to learn opponent-aware and team-coordinated skills with shared perceptual spaces and adversarial multi-robot training.<br>‚Ä¢ Hierarchical Tactics Meets Reactive Control: Integrate a high-level tactical planner (world-model or VLA) with the low-level AMP policy to decide when to dribble, pass, or shoot under game context.<br>‚Ä¢ Learning Contact-Rich Dribbling and Trapping with Perception-Action Reconstruction: Generalize beyond kicking to continuous ball control using the encoder‚Äìdecoder to predict ball state through intermittent visibility and contacts.<br>‚Ä¢ Active Perception and Self-Supervised Denoising for Humanoid Soccer: Train auxiliary objectives for predictive state estimation and learn head/torso gaze control policies that maximize ball observability under occlusion.<br>‚Ä¢ End-to-End Geometry-Aware Visual Abstractions for Sim-to-Real: Replace hand-engineered BEV with a learned, geometry-consistent detector that jointly optimizes perception and control across lighting and clutter.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">How to Evaluate Speech Translation with Source-Aware Neural MT Metrics</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.03295" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.03295" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Source-aware MT metrics (e.g., COMET, MetricX) correlate better with human judgments than reference-only metrics, but they require a textual source, which is missing in speech translation (ST) where the source is audio.<br>‚Ä¢ Real-world ST often lacks manual transcripts and sentence-level alignments; segmentation mismatches between speech and text further hinder reliable evaluation.<br>‚Ä¢ Existing ST evaluation borrows MT practices (e.g., BLEU) that underutilize source information and can mis-rank systems; end-to-end speech-based metrics (e.g., BLASER 2.0) currently underperform metrics using high-quality transcripts.<br>‚Ä¢ There is no systematic guidance on whether synthetic textual proxies (ASR transcripts vs. back-translation of references) can safely stand in for the missing source, nor on when each is preferable.<br>‚Ä¢ Standard re-segmentation (L-Segmenter) misplaces boundary words and cannot handle cross-lingual alignment needed to pair source-side text with target-side references.<br>‚Ä¢ Practical constraints (language coverage, computational cost, neutrality/bias when sharing components with evaluated systems) are not addressed by existing methods.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Generate source text proxies via either ASR transcripts or back-translations (BT) of the references, then align them to reference segments with a novel two-stage cross-lingual re-segmentation algorithm (XLR-Segmenter) that first aligns via intra-lingual WER minimization over BT and then refines boundaries using SimAlign on multilingual embeddings to minimize cross-alignments. Validate across two ST corpora, six diverse ST systems, and two top source-aware metrics by correlating synthetic-source scores with scores using gold transcripts; key finding: ASR is superior when WER ‚â§ 20%, else BT is preferable.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond Proxies: End-to-End Source-Aware Speech Translation Metrics That Directly Consume Audio: Design multimodal metrics that bypass text proxies, leveraging speech-text joint representations to surpass transcript-dependent metrics.<br>‚Ä¢ Adaptive Proxy Selection for ST Evaluation via Reference-Free Quality Signals: Predict when to use ASR or BT sources based on reference-free WER/quality estimation and confidence modeling, enabling per-corpus or per-segment switching.<br>‚Ä¢ Cross-Lingual Boundary Alignment with Large Multilingual Models for ST Evaluation: Replace SimAlign with stronger multilingual encoders or LLMs and integrate prosody/timestamps to further reduce boundary errors and computation.<br>‚Ä¢ Debiasing Source-Aware Metrics in Cascaded ST: Methods to Remove ASR/MT Component Leakage: Detect and mitigate metric inflation when the evaluation source shares components or data with the system under test.<br>‚Ä¢ Stress-Testing Source-Aware ST Evaluation in Noisy, Overlapped, and Low-Resource Conditions: Establish benchmarks and protocols to assess metric robustness across multi-speaker, accented, dysarthric, and non-standard language scenarios.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.04668" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.04668" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite impressive high-level video comprehension, multimodal language models struggle with spatial reasoning across time and space. While current spatial training approaches rely on real-world video data, obtaining diverse footage with precise spatial annotations remains a bottleneck. To alleviate this bottleneck, we present SIMS-V -- a systematic data-generation framework that leverages the privileged information of 3D simulators to create spatially-rich video training data for multimodal language models. Using this framework, we investigate which properties of simulated data drive effective real-world transfer through systematic ablations of question types, mixes, and scales. We identify a minimal set of three question categories (metric measurement, perspective-dependent reasoning, and temporal tracking) that prove most effective for developing transferable spatial intelligence, outperforming comprehensive coverage despite using fewer question types. These insights enable highly efficient training: our 7B-parameter video LLM fine-tuned on just 25K simulated examples outperforms the larger 72B baseline and achieves competitive performance with proprietary models on rigorous real-world spatial reasoning benchmarks. Our approach demonstrates robust generalization, maintaining performance on general video understanding while showing substantial improvements on embodied and real-world spatial tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Multimodal LLMs underperform at spatial reasoning in videos (metric estimation, perspective-taking, temporal tracking), despite good general comprehension.<br>‚Ä¢ Real-world video data with precise 3D spatial annotations is costly and scarce; web-scale pretraining lacks explicit spatial supervision.<br>‚Ä¢ Simulators offer perfect ground truth but sim-to-real transfer is hard, and it is unclear which simulated data properties (question types, mixes, scales) best transfer to real videos.<br>‚Ä¢ Existing benchmarks and training often emphasize action recognition or single-image spatial tasks and can contain non-visual shortcuts, masking true spatial reasoning deficits.<br>‚Ä¢ There is a need for data-efficient training that builds robust spatial intelligence with minimal, targeted supervision.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SIMS-V is a simulated instruction-tuning framework that uses procedurally generated 3D environments (AI2-THOR/ProcTHOR + Objaverse), agent trajectories, and privileged simulator metadata (global 3D layout + per-frame observations) to programmatically create quality-controlled, spatially rich video QA pairs. Controlled ablations identify a minimal, transferable 3-question mix‚Äîabsolute distance (metric), relative direction (perspective), and appearance order (temporal tracking)‚Äîfor efficiently fine-tuning video LLMs to real-world spatial reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Frame-Sampling-Aware Simulated Data Co-Design for Video LLMs: Optimize trajectories and QA generation so each example remains answerable under the model‚Äôs frame subsampling, leveraging simulator ground truth to maximize data efficiency.<br>‚Ä¢ Mixed-Corpus Spatial Instruction Tuning to Prevent Forgetting: Jointly train on SIMS-V and broad multimodal instruction data with curricula/ratio schedules to preserve general video understanding while amplifying spatial gains.<br>‚Ä¢ From 3Q to Action: Extending SIMS-V to Route Planning and Embodied Control: Incorporate action-conditioned route-planning/navigation QAs to develop decision-centric spatial reasoning beyond perception.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">RDMA Point-to-Point Communication for LLM Systems</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.27656" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.27656" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Emerging Large Language Model (LLM) system patterns, such as disaggregated inference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement fine-tuning, require flexible point-to-point communication beyond simple collectives. Existing implementations are locked to specific Network Interface Controllers (NICs), hindering integration into inference engines and portability across hardware providers. We present TransferEngine, which bridges the functionality of common NICs to expose a uniform interface. TransferEngine exposes one-sided WriteImm operations with a ImmCounter primitive for completion notification, without ordering assumptions of network transport, transparently managing multiple NICs per GPU. We demonstrate peak throughput of 400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We showcase TransferEngine through three production systems: (1) KvCache transfer for disaggregated inference with dynamic scaling, (2) RL weight updates achieving 1.3 seconds for trillion-parameter models, and (3) MoE dispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7, with the first viable latencies on EFA. We demonstrate that our portable point-to-point communication complements collectives while avoiding lock-in.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Emerging LLM patterns (disaggregated inference, MoE routing, asynchronous RL fine-tuning) require flexible, low-latency point-to-point (P2P) communication that collectives cannot provide.<br>‚Ä¢ Collective libraries (e.g., NCCL/torch.distributed/MPI) impose fixed membership, synchronized initialization, global operation ordering, and uniform buffer sizes‚Äîhurting dynamic, sparse, and irregular transfers and latency.<br>‚Ä¢ Vendor lock-in and hardware heterogeneity: NVIDIA ConnectX (RC, in-order) vs AWS EFA (SRD, unordered); many existing solutions depend on ConnectX-only features (e.g., IBGDA), while EFA is unsupported or slow‚Äîno portable cross-cloud P2P solution.<br>‚Ä¢ Practical need to aggregate multiple NICs per GPU (e.g., 4√ó100 Gbps on EFA) transparently, while sustaining peak bandwidth and low latency in production deployments.<br>‚Ä¢ Lack of an RDMA completion mechanism that works without transport ordering assumptions, complicating correctness and performance across heterogeneous NICs.<br>‚Ä¢ Production requirements (elastic scaling, cancellation/failure handling, CUDA Graph compatibility) are poorly supported by current P2P or collective stacks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces TransferEngine, a portable RDMA P2P library that exposes SEND/RECV and one-sided WRITEIMM with a novel IMMCOUNTER completion primitive that does not rely on message ordering, while transparently sharding traffic across multiple NICs per GPU. Implemented over libibverbs (ConnectX) and libfabric (EFA), it achieves up to 400 Gbps and powers three systems: disaggregated KvCache transfer, P2P RL weight updates, and proxy-based MoE dispatch/combine.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Portable GPU-Initiated RDMA Abstractions Across Heterogeneous NICs: Design a vendor-agnostic GPU-initiated P2P API that emulates or extends IBGDA semantics over unordered transports like EFA SRD.<br>‚Ä¢ DPU/SmartNIC Offload for IMMCOUNTER and P2P Primitives: Offload completion counting, scatter/barrier, and WR templating to DPUs to reduce CPU proxy overhead and further lower end-to-end latency.<br>‚Ä¢ Cross-Cloud RDMA Autotuning and Performance Modeling for LLM P2P: Build an online model + autotuner that selects message sizes, sharding, and WR chaining strategies per NIC/topology to maximize throughput/latency.<br>‚Ä¢ Consistency-Aware Disaggregated KV Cache Store on TransferEngine: Provide a portable, fault-tolerant KV store with page-level writes, ordering-free completion, and cancellation safety across RC/SRD transports.<br>‚Ä¢ An Unordered-Reliable RDMA Programming Model for LLM Systems: Formalize correctness patterns (e.g., IMMCOUNTER-based sync, remote semaphores) and provide verified libraries for P2P pipelines (MoE, RL, disaggregated inference).</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02280" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02280" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce SAIL-RL, a reinforcement learning (RL) post-training framework that enhances the reasoning capabilities of multimodal large language models (MLLMs) by teaching them when and how to think. Existing approaches are limited by outcome-only supervision, which rewards correct answers without ensuring sound reasoning, and by uniform thinking strategies, which often lead to overthinking on simple tasks and underthinking on complex ones. SAIL-RL addresses these challenges with a dual reward system: the Thinking Reward, which evaluates reasoning quality through factual grounding, logical coherence, and answer consistency, and the Judging Reward, which adaptively determines whether deep reasoning or direct answering is appropriate. Experiments on the state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal understanding benchmarks at both 4B and 8B scales, achieving competitive performance against commercial closed-source models such as GPT-4o, and substantially reduces hallucinations, establishing it as a principled framework for building more reliable and adaptive MLLMs. The code will be available at https://github.com/BytedanceDouyinContent/SAIL-RL.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Outcome-only supervision in RL post-training rewards only final answers, ignoring reasoning quality, which leads to incoherent or fabricated chains, spurious alignments (‚Äúfalse correctness‚Äù), and increased hallucinations (see Fig. 2).<br>‚Ä¢ Uniform, always-on chain-of-thought causes overthinking on simple tasks and underthinking on hard ones, wasting compute and harming accuracy and reliability (Fig. 2).<br>‚Ä¢ Lack of explicit supervision for factual grounding, logical coherence, and answer‚Äìreasoning consistency undermines robustness and trustworthiness in MLLMs.<br>‚Ä¢ Existing additive reward designs are vulnerable to reward hacking (e.g., lucky guesses or unnecessary but high-quality thoughts), lacking a principled coupling between judgment, reasoning, and outcome.<br>‚Ä¢ There is no mechanism to adaptively allocate cognitive effort (when to think) to match task difficulty and efficiency constraints.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SAIL-RL is a dual-reward RL post-training framework for MLLMs that teaches both what to think and when to think. It combines a Thinking Reward (logical coherence, factual grounding, and answer‚Äìreasoning consistency) with a Judging Reward (deciding to think vs. answer directly), cascaded with answer and format rewards in an AND-style product, following a two-stage pipeline: LongCoT SFT with <judge>/<think>/\boxed{} formatting, then RL tuning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning to Judge Without Labels: Self-Supervised Difficulty Estimation for When-to-Think in MLLMs: Replace supervised judging labels with uncertainty- or calibration-based self-assessment to trigger reasoning adaptively without external annotations.<br>‚Ä¢ Token-Level Credit Assignment for Thinking Rewards in Multimodal RL: Move from binary, sequence-level reasoning scores to step/token-level credit assignment using counterfactuals to precisely reward (or penalize) specific reasoning steps.<br>‚Ä¢ Budget-Aware Reasoning Policies for Compute-Constrained MLLMs: Integrate latency/energy budgets into the Judging Reward to optimize accuracy‚Äìefficiency trade-offs under real-world deployment constraints.<br>‚Ä¢ Open-Reward Modeling for Multimodal Reasoning Without Proprietary Judges: Distill and ensemble open-source multimodal reward models to replace closed-source judges while maintaining robust logical, factual, and consistency evaluations.<br>‚Ä¢ Extending SAIL-RL to Tool- and Agent-Augmented MLLMs: Generalize judging to decide among direct answering, thinking, or invoking tools/agents, with composite rewards that evaluate tool selection, usage, and final outcomes.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">EVTAR: End-to-End Try on with Additional Unpaired Visual Reference</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.00956" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.00956" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We propose EVTAR, an End-to-End Virtual Try-on model with Additional Reference, that directly fits the target garment onto the person image while incorporating reference images to enhance try-on accuracy. Most existing virtual try-on approaches rely on complex inputs such as agnostic person images, human pose, densepose, or body keypoints, making them labor-intensive and impractical for real-world applications. In contrast, EVTAR adopts a two-stage training strategy, enabling simple inference with only the source image and the target garment inputs. Our model generates try-on results without masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional reference images of different individuals wearing the same clothes to preserve garment texture and fine-grained details better. This mechanism is analogous to how humans consider reference models when choosing outfits, thereby simulating a more realistic and high-quality dressing effect. We enrich the training data with supplementary references and unpaired person images to support these capabilities. We evaluate EVTAR on two widely used benchmarks and diverse tasks, and the results consistently validate the effectiveness of our approach.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing try-on pipelines rely on multiple external predictors (pose/densepose/segmentation/masks), making them labor‚Äëintensive, brittle to upstream errors, and impractical for real applications.<br>‚Ä¢ Garment-only inputs are ambiguous for materials and fine details (e.g., translucent fabrics, lace collars); users actually rely on model photos‚Äîyet current methods/datasets do not leverage reference people wearing the same garment.<br>‚Ä¢ Diffusion-based try-on methods still require auxiliary conditions or lack support for reference images; mask-free, simple-inference models are scarce.<br>‚Ä¢ Public datasets lack both unpaired person‚Äìgarment pairs (same person in different clothes) and ‚Äúreference-person‚Äù images (different people wearing the same garment), hindering end-to-end training and detail preservation.<br>‚Ä¢ There is a need for robust, in-the-wild try-on that preserves garment structure and fine textures while aligning with the target person‚Äôs pose and body.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>EVTAR is an end-to-end, mask-optional virtual try-on framework trained in two stages: a mask-based model first synthesizes unpaired person‚Äìgarment pairs, then a Flux-Kontext (flow-matching DiT) with LoRA fine-tuning learns to directly map a person image and target garment to the dressed result, optionally conditioned on reference images. A multi-input positional index extends Flux-Kontext to handle multiple visual conditions, and a data engine (Qwen2.5-VL‚Äìguided prompts + Flux-Kontext editing) generates high-quality reference images to build the VFR dataset.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Reference-Aware Try-On from Real E-commerce Catalogs Without Synthetic Stages: Replace synthetic unpaired/reference generation with weakly supervised mining of real product‚Äìmodel images to reduce domain gaps and labeling cost.<br>‚Ä¢ Universal Multi-Condition Flux-DiTs for Mask-Free Try-On: Extend the multi-input positional indexing to jointly exploit pose/depth/body parsing when available, improving occlusion handling and cross-category robustness while remaining optional at inference.<br>‚Ä¢ 4D Virtual Try-On: Geometry-Aware, Temporally Consistent Outfit Transfer in Videos: Incorporate 3D/body priors and temporal constraints into the reference-aided framework for smooth, detail-preserving video try-on across diverse motions.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 5</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-07 23:08:03 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 5;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>