<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 24, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 24, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16334" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16334" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of transparent, reproducible data curation and training pipelines for multimodal reasoning (both SFT and RL), which hampers scaling and understanding of what drives performance.<br>â€¢ Existing methods often address only one stage (SFT or RL), focus on text-only reasoning, or omit critical details, limiting generalization across tasks and modalities.<br>â€¢ Instability and inefficiency in RL for multimodal reasoning with unclear best practices across algorithms and filtering strategies, leading to brittle training.<br>â€¢ Data quality gaps: insufficient diversity in sources and in answer traces, and limited step-by-step validation, making it hard to build a strong cold-start reasoning foundation.<br>â€¢ Limited open-sourcing of pipelines, datasets, and model weights in prior work, restricting reproducibility and fair comparison.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>OpenMMReasoner is a fully open two-stage recipe: an 874k-sample SFT stage that uses cross-domain sourcing, teacher-guided rejection sampling, repeated answer sampling, and step-by-step validation to establish strong reasoning. A subsequent 74k-sample RL stage with verifiable rewards compares and selects stable RL strategies (e.g., GSPO/GRPO/DAPO) with data filtering to sharpen and stabilize multimodal reasoning, with all code, data, and weights released.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Curriculum RLVR for Multimodal Reasoning at Scale: Adaptive Algorithm and Data Schedules: Design curriculum policies that dynamically select RL algorithms, reward strengths, and domains to maximize stability and sample efficiency across tasks.<br>â€¢ Automated Answer-Diversity Optimization for SFT of LMRMs: Develop active-learning and uncertainty-driven sampling to generate/select diverse, validated rationales, balancing source and answer diversity for improved data efficiency.<br>â€¢ Beyond Images: Extending OpenMMReasoner to Video, Audio, and 3D with Unified Reward Models: Generalize the open recipe to additional modalities by building verifiers and RL rewards that handle temporal and multi-sensory reasoning.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.15210" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.15210" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text "representationally simple" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively "easy", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Geometric vs predictive complexity conflation: Existing evaluations rely on entropy/perplexity, leaving the geometric complexity of representations (intrinsic dimension, ID) underexplored and ungrounded in interpretable text properties.<br>â€¢ Unclear textual determinants of ID: No comprehensive account of which linguistic/semantic features drive ID, limiting the interpretability and practical use of ID in NLP.<br>â€¢ Lack of causal evidence: Prior work is largely correlational; there is no causal validation linking semantic features (e.g., narrative, emotion) to shifts in ID.<br>â€¢ Domain/genre stratification unknown: It is unclear how ID varies across genres (scientific, encyclopedic, creative/opinion) and what this implies for model capability and evaluation.<br>â€¢ Practical guidance missing: Without a grounded understanding of ID, practitioners lack guidelines for dataset design (balancing low-/high-ID text) and for interpreting ID-based analyses.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A multi-faceted analysis that estimates intrinsic dimension on LLM embeddings across genres, formally and empirically disentangles ID from entropy (controlling for length), and links ID to linguistic features. Sparse autoencoders discover interpretable, steerable features (e.g., formal tone, personalization, emotion), and steering experiments causally test their effects on ID.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning to Balance Intrinsic Dimensions: Curriculum and Data Mixture Design for LLM Training: Develop training strategies that balance low-ID (scientific/encyclopedic) and high-ID (narrative/opinion) data to improve robustness and evaluative fidelity.<br>â€¢ Layer-wise Intrinsic Dimension Profiling for Genre-Aware Adaptation: Map ID across layers and genres to guide targeted fine-tuning or adapters that specialize layers for genre-specific representational complexity.<br>â€¢ Intrinsic Dimension-Guided Decoding and Prompting for Controlled Creativity: Design prompting and decoding schemes that steer ID (via SAE features) to control narrative richness, personalization, and emotional tone in generation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.15705" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.15705" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Agentic multimodal reasoning models overemphasize image manipulation and lack seamless integration of web search, making them rely solely on inherent knowledge and limiting general-purpose reasoning.<br>â€¢ Existing geolocalization benchmarks do not provide high-resolution imagery or sufficiently challenging settings to evaluate deep agentic reasoning.<br>â€¢ Geolocalization inherently requires nuanced visual grounding plus iterative hypothesis confirmation/refinement via external information retrieval, necessitating tool-integrated, multi-turn reasoning loops.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>GeoVista is an agentic VLM that embeds an image zoom-in tool and a web-search tool directly into its reasoning loop, trained via a cold-start SFT stage to learn reasoning and tool-use priors, followed by RL with hierarchical rewards leveraging multi-level geographic information to improve geolocalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ GeoVista-3D: Integrating 3D GIS layers and street-view panoramas for richer spatial context in agentic geolocalization.<br>â€¢ Budget-Aware Tool-Use RL for GeoAgents: Optimizing the sequence and frequency of web search and zoom actions under latency and cost constraints.<br>â€¢ Cross-Lingual Web-Augmented GeoReasoner: Leveraging multilingual web signals and regional knowledge bases to improve global geolocalization robustness.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">SAM 3: Segment Anything with Concepts</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16719" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16719" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., "yellow school bus"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing SAM/PVS systems segment a single object per prompt and cannot find and track all instances of a concept across images/videos; many applications require exhaustive concept-level segmentation with identity tracking.<br>â€¢ Open-vocabulary concept detection is challenging because recognition and localization are entangled; current methods (e.g., OWLv2) underperform, especially with hard negatives and diverse noun phrases.<br>â€¢ Noun-phrase grounding is intrinsically ambiguous (polysemy, subjective descriptors, boundary ambiguity), calling for interactive refinement and ambiguity-aware evaluation/modeling.<br>â€¢ Available datasets lack scale and diversity in concepts and exhaustive masks; there is no comprehensive PCS benchmark with hard negatives across images and videos.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SAM 3 couples a DETR-based, prompt-conditioned detector (text and image exemplars) with a SAM 2â€“style memory tracker on a shared backbone, introducing a global presence head to decouple recognition from localization for open-vocabulary PCS. A scalable human/AI-in-the-loop data engine (diverse media, ontology-driven noun phrases, hard negatives, MLLM verification) trains the model and produces the SA-Co benchmark.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Beyond Noun Phrases: Compositional and Relational Promptable Concept Segmentation: Extend PCS to complex referring expressions with attributes, relations, and logical composition via deeper integration with MLLMs.<br>â€¢ Ambiguity-Aware Learning for Open-Vocabulary Segmentation: Design objectives and inference modules that model multiple valid interpretations and uncertainty to handle polysemy and boundary ambiguity.<br>â€¢ Long-Form Video Concept Tracking at Scale: Develop efficient memory and re-identification strategies to track hundreds of objects over hour-long videos with shot changes and concept drift.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13593" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13593" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.67% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLM agents struggle with long-term contextual consistency and dynamic personalization in complex environments, reducing coherence and user-awareness over extended interactions.<br>â€¢ Semantic grouping/retrieval in existing memory systems overlooks semantically irrelevant but crucial user characteristics and situational context, while introducing retrieval noise that increases redundancy, latency, and token consumption.<br>â€¢ Prevailing architectures rely on static historical embeddings and lack dynamic, hierarchical, user-centric modeling to continuously connect and update memory fragments for comprehensive understanding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>O-Mem performs active user profiling that continuously extracts and updates structured persona attributes and episodic event records from ongoing interactions, and employs hierarchical user-centric retrieval that prioritizes persona cues before topic-specific context to reduce noise and improve efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Multi-Modal Omni Memory: Extending Active User Profiling with Vision and Audio to Enrich Persona and Event Context<br>â€¢ Privacy-Preserving Personalized Memory: Differential Privacy and Controllable Forgetting for Safe Long-Horizon User Modeling<br>â€¢ Learning to Retrieve: Reinforcement-Learned Hierarchical Retrieval Policies for Noise-Resilient, Efficient Personalized Agents</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RynnVLA-002: A Unified Vision-Language-Action and World Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.17502" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.17502" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ VLA models lack internal representations of action dynamics because actions appear only as outputs, limiting understanding of causeâ€“effect in control.<br>â€¢ VLA models cannot imagine future states (no video prediction), hindering foresight and counterfactual reasoning.<br>â€¢ VLA models have no explicit physics modeling, weakening grasp of object interactions, contact, and stability.<br>â€¢ World models forecast observations but cannot generate actions, leaving a gap for explicit action planning and control.<br>â€¢ Autoregressive discrete action generation suffers error accumulation and weak action generalization (MLLMs are not pretrained on actions).<br>â€¢ Discrete autoregressive policies are data-inefficient and slow at inference in real robots; generated action chunks can be non-smooth and poorly generalized.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>RynnVLA-002 unifies a VLA policy and a world model in a single shared-token LLM (built on Chameleon), jointly training on mixed sequences to both generate action chunks from observations and predict next-frame images from actions. It introduces an action-attention mask to prevent autoregressive error propagation and augments discrete tokens with a lightweight continuous Action Transformer head for parallel, smooth, and fast action chunk generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Scaling Unified Vision-Language-Action World Models with Cross-Embodiment Pretraining: Pretrain RynnVLA-style models across diverse robots, viewpoints, and tasks to improve zero-shot generalization and transfer.<br>â€¢ Uncertainty-Aware Action World Models for Safe Robot Planning: Incorporate probabilistic video prediction and action uncertainty to enable risk-sensitive, robust planning under model error.<br>â€¢ Hierarchical Chunked Policies with Imagination-Based Lookahead: Combine multi-scale action chunking with world-model rollouts to plan over long horizons while ensuring smooth, coherent trajectories.<br>â€¢ Self-Improving Robots via World-Model-Guided Data Generation: Use counterfactual and synthesized trajectories from the world model to mine hard cases and continually refine the VLA policy.<br>â€¢ Learning Semantic Action Tokenizers for Hybrid Control: Replace fixed binning with learned, semantics-aware action tokenization and continuousâ€“discrete hybrids to reduce quantization error and data requirements.<br>â€¢ Real-Time Multiview Consistent World Modeling for Closed-Loop Control: Enforce cross-view (front/wrist) and proprioceptive consistency with latency-aware fusion for accurate online prediction and control.<br>â€¢ Online RL with Unified VLAâ€“World Models: Interleave model-based RL/policy improvement with the unified architecture for continual adaptation in the real world.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.17220" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.17220" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low "follow rates" (leq 11%, GPT-5: 4\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\%, Qwen 2.5-1.5B: 94\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of "resistance to overfitting pressure" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLMs exhibit sycophancyâ€”shifting answers to agree with authoritative but false user claimsâ€”causing accuracy and calibration collapse in high-stakes applications.<br>â€¢ Existing evaluations largely use binary accuracy, narrow domains, and ignore confidence dynamics, masking distinct failure modes under social pressure.<br>â€¢ Practitioners lack a reproducible, production-ready framework to measure robustness to persuasion across models and domains, and alignment objectives (e.g., RLHF) may conflict with epistemic integrity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>PARROT runs dual-path evaluations per question (neutral vs. manipulated with a domain-specific authoritative false claim), extracts token-level log-probabilities to compute calibrated class confidences via anchored summation, and quantifies confidence shifts and calibration degradation while classifying outcomes into an eight-state behavioral taxonomy across 22 models and 13 domains.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Beyond Multiple Choice: Open-Ended Sycophancy Stress Tests Under Multi-Turn Social Pressure: Build open-ended, multi-turn, emotionally charged persuasion scenarios to track answer drift, confidence reliability, and safety outcomes beyond MMLU-style items.<br>â€¢ Mechanisms of Authority Representation in LLMs: Causal Analyses and Training Interventions: Use activation probing and causal interventions to identify how models encode authority signals and test targeted training (counter-preference data, constitutional objectives) to reduce sycophancy.<br>â€¢ Calibration Under Social Influence: Semantic-Level Confidence Estimation for Robust Assistants: Develop semantic confidence metrics (disagreement probes, rephrasing consistency, self-reported uncertainty models) that outperform token logprobs for detecting epistemic collapse under social pressure.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Loomis Painter: Reconstructing the Painting Process</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.17344" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.17344" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Step-by-step painting tutorials are vital for learning artistic techniques, but existing video resources (e.g., YouTube) lack interactivity and personalization. While recent generative models have advanced artistic image synthesis, they struggle to generalize across media and often show temporal or structural inconsistencies, hindering faithful reproduction of human creative workflows. To address this, we propose a unified framework for multi-media painting process generation with a semantics-driven style control mechanism that embeds multiple media into a diffusion models conditional space and uses cross-medium style augmentation. This enables consistent texture evolution and process transfer across styles. A reverse-painting training strategy further ensures smooth, human-aligned generation. We also build a large-scale dataset of real painting processes and evaluate cross-media consistency, temporal coherence, and final-image fidelity, achieving strong results on LPIPS, DINO, and CLIP metrics. Finally, our Perceptual Distance Profile (PDP) curve quantitatively models the creative sequence, i.e., composition, color blocking, and detail refinement, mirroring human artistic progression.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Step-by-step painting tutorials are passive and lack interactivity and personalization, limiting effective skill acquisition.<br>â€¢ Existing generative models struggle with process-level modeling, showing temporal discontinuities, structural jumps, and weak generalization across artistic media.<br>â€¢ Prior neural painting methods often rely on parametric strokes or synthetic/narrow datasets, diverging from authentic human artistic workflows.<br>â€¢ Image-to-video diffusion models are temporally misaligned with natural painting order, leading to poor coherence when reconstructing processes from blank canvas to finished work.<br>â€¢ Real tutorial videos contain occlusions (hands, tools) and overlays that hinder learning accurate stroke-level transformations.<br>â€¢ There is no principled, quantitative metric to characterize the progression of artistic creation over time.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>LoRA-tuned video diffusion (image-to-video) model with medium-aware semantic conditioning and cross-media structural alignment, trained via a reverse-painting strategy (temporally reversed videos) to ensure coherent, human-aligned process reconstruction and controllable media transfer. A curated, occlusion-free multi-media dataset supports training, and a Perceptual Distance Profile metric quantifies progression from composition to detail.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Interactive Loomis Painter: Real-Time, User-in-the-Loop Guidance for Personalized Painting Workflows: Add controllable constraints and feedback to adapt steps to user skill and preferences during generation.<br>â€¢ Stroke-Aware Diffusion: Hybrid Explicit Stroke Parameterization and Pixel-Space Video Diffusion for Faithful Process Reconstruction: Combine differentiable brush/stroke primitives with video diffusion to capture both procedural structure and realistic textures.<br>â€¢ Multi-View and 3D Artistic Process Reconstruction: Learning Consistent Painting Workflows from Multi-Camera Tutorials: Fuse multi-angle videos to recover view-consistent, 3D-aware painting sequences.<br>â€¢ Beyond PDP: Comprehensive Pedagogical Metrics for Evaluating Artistic Process Quality and Learnability: Design human-aligned metrics that assess step granularity, cognitive load, and teachability alongside perceptual progression.<br>â€¢ Few-Shot Personalization of Artistic Media and Style in Process Generation: Adapter-based or meta-learning approaches to tailor workflows to individual artists or niche media from limited examples.<br>â€¢ Physics-Informed Painting Diffusion: Incorporating Material and Tool Dynamics into Process Generation: Model medium-specific layering, drying, and brushâ€“canvas interactions to improve procedural realism.<br>â€¢ Branching and Editable Process Generation: Enabling Alternative Paths and Mid-Sequence Editing in Painting Tutorials: Support interactive branching, correction, and re-planning while preserving temporal coherence.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">WorldGen: From Text to Traversable and Interactive 3D Worlds</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16825" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16825" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce WorldGen, a system that enables the automatic creation of large-scale, interactive 3D worlds directly from text prompts. Our approach transforms natural language descriptions into traversable, fully textured environments that can be immediately explored or edited within standard game engines. By combining LLM-driven scene layout reasoning, procedural generation, diffusion-based 3D generation, and object-aware scene decomposition, WorldGen bridges the gap between creative intent and functional virtual spaces, allowing creators to design coherent, navigable worlds without manual modeling or specialized 3D expertise. The system is fully modular and supports fine-grained control over layout, scale, and style, producing worlds that are geometrically consistent, visually rich, and efficient to render in real time. This work represents a step towards accessible, generative world-building at scale, advancing the frontier of 3D generative AI for applications in gaming, simulation, and immersive social environments.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Bridging text prompts to functional, traversable 3D scenes (not just single objects), ensuring coherent layout, style, and gameplay viability.<br>â€¢ Guaranteeing navigability via explicit constraints (navmesh) while preserving creative diversity and stylistic richness.<br>â€¢ Overcoming the lack of large-scale datasets that directly map text to full 3D scenes; leveraging image intermediates and modular conditioning instead.<br>â€¢ Addressing limitations of pure text-to-image and monolithic scene generation (occlusions, non-traversable geometry, low global coherence, hard-to-edit single meshes).<br>â€¢ Improving scalability and editability through compositional scene outputs; prior part/scene segmentation methods are slow and fail to generalize to complex scenes.<br>â€¢ Enhancing per-object geometric and texture detail beyond holistic low-res reconstructions, which struggle due to fixed latent capacity over complex scenes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>WorldGen is a modular text-to-3D pipeline that uses LLM-driven procedural blockouts and navmesh extraction, depth-conditioned reference image generation, navmesh-conditioned image-to-3D holistic reconstruction, AutoPartGen-based fast scene decomposition, and per-object image/mesh/texture enhancement to produce coherent, traversable, and editable 3D worlds.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Differentiable Navmesh-Constrained Text-to-3D Scene Generation: End-to-end training that integrates differentiable navigation constraints to jointly optimize layout, geometry, and traversability from text.<br>â€¢ SceneBench: A Large-Scale Dataset of Text, Blockouts, Navmeshes, Reference Images, and Meshes for World Generation: Public benchmark to catalyze research on conditioned scene synthesis and compositional reconstruction.<br>â€¢ Physics- and Gameplay-Aware WorldGen: Incorporating physics simulation and interaction affordances (climb/jump/collide) during generation for ready-to-play levels with agent-based validation.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11007" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11007" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a "visual processing bottleneck": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.8% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ VLMs exhibit a visual processing bottleneck: they lose grounding in visual evidence during long autoregressive generation, harming fine-grained perception, multi-step reasoning, and long-sequence fidelity.<br>â€¢ Deficit in contextualized visual memory: models prioritize accumulated text over visual cues and lack mechanisms to retain and recall perceptual details alongside semantic context.<br>â€¢ Existing paradigms are limited: direct training (SFT/RL) boosts task-specific performance but causes catastrophic forgetting; image-level interventions are computationally expensive and tool-dependent; token-level methods are non-generative and can only resurface existing representations; latent-space approaches often operate purely in language space or require auxiliary visual data, making them ill-suited for VLM visual memory.<br>â€¢ Need for cognitively aligned memory (short-term visually dominant, long-term semantically dominant) to preserve perceptual fidelity and semantic consistency throughout thinking and generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>VisMem augments VLMs with dynamic latent vision memoryâ€”short-term (fine-grained perceptual retention) and long-term (abstract semantic consolidation)â€”invoked via special tokens during autoregressive inference. A query builder converts hidden states into context-aware queries for lightweight memory formers that synthesize latent memory tokens, which are inserted into the generation stream; a two-stage reinforcement learning regime first learns effective memory contents and then optimizes invocation patterns.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Scheduling of Short- and Long-Term Latent Vision Memory in VLMs: Learn task-aware controllers that decide when and which memory to invoke under accuracyâ€“latency trade-offs.<br>â€¢ Episodic Multimodal Memory for Lifelong Vision-Language Learning: Extend VisMem to accumulate and consolidate cross-session episodic memories to improve continual learning and reduce catastrophic forgetting.<br>â€¢ Interpretable and Grounded Latent Vision Tokens: Develop decoding/visualization tools and alignment objectives to make latent memory tokens transparent, traceable to visual evidence, and safe.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16175" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16175" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms Ï€_{0.5}, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Low-dimensional action supervision is too sparse to effectively train large VLA models processing high-dimensional visual inputs, leading to underutilized capacity and suboptimal performance.<br>â€¢ Pixel-level visual foresight (future-frame prediction) imposes high training cost, slows convergence, and can distract action learning by conflating appearance changes with motion.<br>â€¢ Compressing visual states into compact control signals (e.g., tracks/keypoints) creates information bottlenecks that lose fine-grained motion cues and suffer from tracking inaccuracies.<br>â€¢ Robot-specific training often neglects language supervision, degrading instruction-following and reasoning abilities learned by VLMs.<br>â€¢ Naive joint fusion of vision, language, and action causes cross-modal competition and unstable optimization.<br>â€¢ Temporal ensemble methods improve motion stability at inference but incur significant computational overhead, motivating efficiencyâ€“stability trade-offs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Mantis introduces Disentangled Visual Foresight by decoupling future-state prediction into a diffusion transformer head driven by meta queries that become latent-action queries via a residual connection from the current frame, allowing the VLM backbone to focus on language grounding and reasoning. A DiT-based action head consumes latent-action and action queries (with multi-gap supervision), trained via a progressive modality-fusion schedule and deployed with an Adaptive Temporal Ensemble that dynamically adjusts smoothing to balance stability and efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Scaling DVF to 3D Multi-View Robotics: A Disentangled Visual Foresight Framework for Depth and Multi-Camera Embodiments: Extend DVF to 3D by fusing multi-view/depth inputs to better model object-centric dynamics in cluttered manipulation.<br>â€¢ Online Adaptive Latent-Action Discovery for Continual Robot Learning: Enable latent-action queries to adapt online via self-supervised/RL signals, improving transfer to new tasks and embodiments without retraining the backbone.<br>â€¢ Uncertainty-Aware Adaptive Temporal Ensemble for Safe Manipulation: Integrate predictive uncertainty to modulate ensemble strength, trading off smoothness and responsiveness under safety constraints.<br>â€¢ Language-Grounded Hierarchical Planning with DVF: Coupling LLM Task Decomposition with Latent-Action Control: Combine high-level language planners with DVF-driven low-level controllers for long-horizon, multi-stage tasks.<br>â€¢ Efficient Edge Deployment of VLA-DVF via Token Pruning and Distillation: Compress the connector/DiT and prune dynamic patches for real-time inference on onboard compute with minimal performance loss.<br>â€¢ Interpretable and Controllable Latent Actions: Causal Probing of DVF Queries into Human-Readable Motion Primitives: Map latent-action queries to semantic primitives and enable user editing for debugging and safety-aware control.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.14899" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.14899" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Achieve multi-view consistent image editing from sparse input views, where users often have only a few photos rather than dense captures.<br>â€¢ Overcome limitations of per-scene neural fields (NeRF/3DGS) that require dense coverage and tend to overfit or produce artifacts under sparse-view regimes.<br>â€¢ Address failures of temporal/extended-attention 2D or video editors that maintain consistency only for small viewpoint changes and break under large baselines.<br>â€¢ Bridge the gap between multi-view diffusion models (which have strong 3D priors but lack editing capabilities) and powerful 2D instruction editors.<br>â€¢ Provide an automated, scalable solution important for product imagery, real estate/Interior design, AR/VR, and post-production, where cross-view coherence is critical.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>I-Mix2Mix distills edits from a frozen 2D instruction editor (InstructPix2Pix) into a pretrained multi-view diffusion student (SEVA) via an adapted SDS pipeline, replacing the neural field consolidator with a multi-view model to leverage its data-driven 3D prior. Key adaptations include incremental distillation across student timesteps, a specialized teacher noise scheduler to avoid degeneration, and a cross-view attention modification (random cross-view attention) that strengthens consistency without additional cost.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Unified Edit-GEN: Joint 2Dâ€“Multi-View Diffusion for End-to-End Consistent Editing: Build a single model that natively supports instruction-based editing and multi-view synthesis, removing the need for teacherâ€“student distillation.<br>â€¢ Geometry-Aware I-Mix2Mix: Integrating Explicit Depth/Surface Priors into Multi-View Editing: Augment the student with lightweight geometry estimation to further stabilize cross-view consistency under extreme sparsity.<br>â€¢ Temporal I-Mix2Mix: Consistent Sparse-View Editing Across Time for Multi-View Video: Extend the framework to dynamic sequences, coupling cross-view and temporal coherence with schedule-aware attention and noise control.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.14806" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.14806" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Modeling genomic sequences faces two unsolved challenges: the information density varies widely across different regions, while there is no clearly defined minimum vocabulary unit. Relying on either four primitive bases or independently designed DNA tokenizers, existing approaches with naive masked language modeling pre-training often fail to adapt to the varying complexities of genomic sequences. Leveraging Token Merging techniques, this paper introduces a hierarchical architecture that jointly optimizes a dynamic genomic tokenizer and latent Transformers with context-aware pre-training tasks. As for network structures, the tokenization module automatically chunks adjacent bases into words by stacking multiple layers of the differentiable token merging blocks with local-window constraints, then a Latent Encoder captures the global context of these merged words by full-attention blocks. Symmetrically employing a Latent Decoder and a Local Decoder, MergeDNA learns with two pre-training tasks: Merged Token Reconstruction simultaneously trains the dynamic tokenization module and adaptively filters important tokens, while Adaptive Masked Token Modeling learns to predict these filtered tokens to capture informative contents. Extensive experiments show that MergeDNA achieves superior performance on three popular DNA benchmarks and several multi-omics tasks with fine-tuning or zero-shot evaluation, outperforming typical tokenization methods and large-scale DNA foundation models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Genomic information density is highly uneven across coding and non-coding regions; fixed-length or base-level tokens waste capacity and dilute signal in repetitive DNA.<br>â€¢ DNA lacks inherent word boundaries; fixed k-mers/BPE or hand-crafted tokenizers cannot adapt token length to context, missing motifs of variable size.<br>â€¢ Genomic sequences are extremely long and naive, uniform MLM pretraining and single-base long-range models allocate attention uniformly, failing to prioritize informative regions; prior work optimizes tokenization, architecture, and objectives in isolation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MergeDNA is a hierarchical autoencoder-style Transformer that learns a dynamic, context-aware DNA tokenizer via local-window differentiable token merging (Local Encoder) and models global dependencies with a full-attention Latent Encoder. It is trained with Merged Token Reconstruction and Adaptive Masked Token Modeling that select, reconstruct, and predict salient tokens, jointly optimizing tokenization and context modeling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Motif-Aware Token Merging for Interpretable Genomic Tokenizers: Incorporate known motifs and biochemical priors into the merge criterion to yield biologically meaningful variable-length tokens.<br>â€¢ Linear-Time MergeDNA for Million-Base Contexts: Replace full-attention latent blocks with SSM/hybrid modules to scale dynamic tokenization and global modeling to chromosome-scale inputs.<br>â€¢ Cross-Omics Dynamic Tokenization for the Central Dogma: Jointly learn tokenization across DNA, RNA, and protein with shared latent spaces to improve transfer and zero-shot performance.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16931" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16931" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>With the rapid development of Large Language Models (LLMs), AI agents have demonstrated increasing proficiency in scientific tasks, ranging from hypothesis generation and experimental design to manuscript writing. Such agent systems are commonly referred to as "AI Scientists." However, existing AI Scientists predominantly formulate scientific discovery as a standalone search or optimization problem, overlooking the fact that scientific research is inherently a social and collaborative endeavor. Real-world science relies on a complex scientific infrastructure composed of collaborative mechanisms, contribution attribution, peer review, and structured scientific knowledge networks. Due to the lack of modeling for these critical dimensions, current systems struggle to establish a genuine research ecosystem or interact deeply with the human scientific community. To bridge this gap, we introduce OmniScientist, a framework that explicitly encodes the underlying mechanisms of human research into the AI scientific workflow. OmniScientist not only achieves end-to-end automation across data foundation, literature review, research ideation, experiment automation, scientific writing, and peer review, but also provides comprehensive infrastructural support by simulating the human scientific system, comprising: (1) a structured knowledge system built upon citation networks and conceptual correlations; (2) a collaborative research protocol (OSP), which enables seamless multi-agent collaboration and human researcher participation; and (3) an open evaluation platform (ScienceArena) based on blind pairwise user voting and Elo rankings. This infrastructure empowers agents to not only comprehend and leverage human knowledge systems but also to collaborate and co-evolve, fostering a sustainable and scalable innovation ecosystem.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing AI Scientist systems frame discovery as standalone search/optimization, ignoring the inherently social and collaborative nature of science.<br>â€¢ They lack explicit modeling of scientific infrastructureâ€”structured knowledge networks (citations, concepts), peer review, collaboration protocols, and contribution attributionâ€”hindering context-aware reasoning and scientific rigor.<br>â€¢ There is no governed, open ecosystem enabling humanâ€“AI co-evolution and robust evaluation; current tools are isolated from human workflows and lack community-facing assessment mechanisms.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>OmniScientist is an end-to-end, multi-agent LLM framework that encodes human research infrastructure into the AI research lifecycleâ€”spanning data foundation, literature review, ideation, experiment automation, scientific writing, and paper review. It integrates a structured knowledge system over citation/concept graphs, a collaboration protocol (OSP) enabling humanâ€“AI co-work with contribution provenance, and an open evaluation platform (ScienceArena) using blind pairwise voting with Elo rankings.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Protocol-Grounded Credit Markets for Humanâ€“AI Scientific Collaboration: Formalize and evaluate incentive and provenance mechanisms within OSP to align contributions, deter gaming, and reward novelty and rigor.<br>â€¢ Robustness and Fairness of Elo-Based Peer Review for AI-Generated Research: Analyze ScienceArenaâ€™s blind pairwise voting and Elo rankings under bias, adversarial behavior, and domain shifts, with theory and large-scale experiments.<br>â€¢ From Simulated to Physical Labs: Closed-Loop Multi-Agent Experiment Automation: Extend OmniScientistâ€™s experiment module to control real laboratory hardware with safety constraints, sensor-driven feedback, and self-correction.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.17490" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.17490" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Text-rich videos contain small, transient textual cues that demand repeated inspection; existing models miss fine-grained, frame-specific evidence.<br>â€¢ Dominant single-pass perception over fixed frames prevents revisiting frames or zooming into regions, causing brittle reasoning and hallucinations.<br>â€¢ Text-only chain-of-thought can improve logic but often amplifies ungrounded claims when pixel evidence is incomplete.<br>â€¢ Coordinate-grounded methods (predicting frame indices/boxes/layout) treat coordinates as static endpoints rather than actionable steps to re-read pixels, leaving evidence unintegrated.<br>â€¢ Scaling LMMs to long, complex videos and UI/slide walkthroughs requires pixel-grounded, iterative reasoning that current systems lack.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Video-R4 is a 7B LMM that performs iterative visual ruminationâ€”selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state in a closed-loop readâ€“retrieveâ€“refocusâ€“reinforce cycleâ€”trained via a multi-stage curriculum using Video-R4-CoT-17k for supervised atomic operations and Video-R4-RL-30k for GRPO/PPO-based reinforcement learning with diversity/representativeness- and curiosity-inspired rewards.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Autonomous Visual Rumination Agents for Long-Form Video: Hierarchical policies and memory to schedule frame selection and zooming over hours-long videos while maintaining pixel-grounded evidence.<br>â€¢ Cross-Modal Rumination: Integrating Audio and Subtitles into Pixel-Grounded Video Reasoning: Fuse ASR, speaker cues, and on-screen text to guide when and where to re-read visual regions.<br>â€¢ Uncertainty-Guided Rumination for Reliable Video QA: Confidence estimation and Bayesian policies to trigger targeted reinspection and pixel re-encoding when evidence is ambiguous.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.17487" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.17487" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Understand how downscaling LLM size affects multimodal performance, especially which capabilities degrade and why, for on-device efficient models.<br>â€¢ Resolve inconsistencies in prior findings on whether LLM size impacts perception, and identify if failures stem from visual reasoning or fundamental perceptual deficits.<br>â€¢ Address the disproportionate drop in visually demanding tasks when using small MLLMs by isolating perception vs reasoning effects.<br>â€¢ Overcome limitations of existing instruction tuning that require diverse, under-specified visual extraction skills, and reduce reliance on scaling parameters/data.<br>â€¢ Provide a principled, decoupled analysis to pinpoint bottlenecks and guide targeted improvements in small multimodal systems.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A decoupled perceptionâ€“reasoning framework with visual extraction tuning trains the model to consistently extract instruction-relevant visual details, followed by step-by-step reasoning over these details (EXTRACT+THINK). This two-stage approach yields parameter- and data-efficient performance improvements in small MLLMs, surpassing prior baselines while using substantially smaller modules and fewer visual samples.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Curriculum-based Visual Extraction Tuning for Robust Perception in Small MLLMs: Design curricula and task-conditioned prompts to unify and strengthen instruction-relevant visual extraction skills across diverse tasks.<br>â€¢ Distilling Perceptual Priors from Specialist Vision Models into EXTRACT+THINK Pipelines: Transfer fine-grained detection/OCR/spatial cues from expert vision systems to enhance small-model perception without increasing LLM capacity.<br>â€¢ A Standardized Decoupled Benchmark for Perception vs Reasoning in Multimodal Downscaling: Build datasets and metrics that separately evaluate visual extraction and reasoning to guide design and training of compact multimodal models.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Diversity Has Always Been There in Your Visual Autoregressive Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.17074" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.17074" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Visual Autoregressive (VAR) models have recently garnered significant attention for their innovative next-scale prediction paradigm, offering notable advantages in both inference efficiency and image quality compared to traditional multi-step autoregressive (AR) and diffusion models. However, despite their efficiency, VAR models often suffer from the diversity collapse i.e., a reduction in output variability, analogous to that observed in few-step distilled diffusion models. In this paper, we introduce DiverseVAR, a simple yet effective approach that restores the generative diversity of VAR models without requiring any additional training. Our analysis reveals the pivotal component of the feature map as a key factor governing diversity formation at early scales. By suppressing the pivotal component in the model input and amplifying it in the model output, DiverseVAR effectively unlocks the inherent generative potential of VAR models while preserving high-fidelity synthesis. Empirical results demonstrate that our approach substantially enhances generative diversity with only neglectable performance influences. Our code will be publicly released at https://github.com/wangtong627/DiverseVAR.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Visual autoregressive (VAR) models exhibit diversity collapseâ€”multiple samples for the same prompt look highly similarâ€”despite being efficient and high-quality.<br>â€¢ Existing diversity fixes in diffusion/AR (teacherâ€“student distillation, parallel decoders, feature amplification) typically require extra training, additional models, or prompt engineering and are not directly transferable to VAR.<br>â€¢ Structure and diversity are formed at early scales in VAR; current inference does not exploit this, leading to redundancy and limited variability.<br>â€¢ Naive interventions (e.g., zeroing pivotal tokens) can harm semantics and fidelity; a training-free, controllable approach that preserves textâ€“image alignment is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DiverseVAR is a training-free inference scheme that applies SVD-based soft suppression of dominant singular values in early-scale input features (SSR) to unlock diversity, followed by soft amplification of output features (SAR) to guide logits and preserve textâ€“image alignment. It operates at selected early scales across VAR blocks to increase diversity with negligible impact on fidelity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Pivotal Component Discovery for VAR Diversity Control: Learn prompt- and scale-dependent suppression/amplification schedules (Î±, Î²) and automatic scale selection to replace fixed SVD parameters.<br>â€¢ Semantic- and Numeracy-Constrained Diversity Guidance in Visual AR: Integrate text parsers/CLIP/LLM-based constraints to enforce attribute and count correctness while maintaining diversity during SAR.<br>â€¢ A Theory of Diversity Emergence in Next-Scale AR via Spectral Dynamics: Provide a formal analysis linking singular value spectra evolution across scales to diversity and fidelity, yielding provable schedules for SSR/SAR.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Insights from the ICLR Peer Review and Rebuttal Process</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.15462" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.15462" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Peer review is a cornerstone of scientific publishing, including at premier machine learning conferences such as ICLR. As submission volumes increase, understanding the nature and dynamics of the review process is crucial for improving its efficiency, effectiveness, and the quality of published papers. We present a large-scale analysis of the ICLR 2024 and 2025 peer review processes, focusing on before- and after-rebuttal scores and reviewer-author interactions. We examine review scores, author-reviewer engagement, temporal patterns in review submissions, and co-reviewer influence effects. Combining quantitative analyses with LLM-based categorization of review texts and rebuttal discussions, we identify common strengths and weaknesses for each rating group, as well as trends in rebuttal strategies that are most strongly associated with score changes. Our findings show that initial scores and the ratings of co-reviewers are the strongest predictors of score changes during the rebuttal, pointing to a degree of reviewer influence. Rebuttals play a valuable role in improving outcomes for borderline papers, where thoughtful author responses can meaningfully shift reviewer perspectives. More broadly, our study offers evidence-based insights to improve the peer review process, guiding authors on effective rebuttal strategies and helping the community design fairer and more efficient review processes. Our code and score changes data are available at https://github.com/papercopilot/iclr-insights.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Understand the dynamics and effectiveness of the ML conference peer review process at scale amid rapidly growing submission volumes<br>â€¢ Quantify how rebuttals affect reviewer scores and decisions, especially for borderline papers, and determine when rebuttals are most impactful<br>â€¢ Identify associations between reviewer comments/ratings and score changes, and characterize common strengths/weaknesses across rating groups<br>â€¢ Analyze temporal patterns of review and rebuttal activity and their relation to engagement and outcomes<br>â€¢ Measure co-reviewer influence (herding) and disagreement convergence to inform fairness and process design<br>â€¢ Address limitations of prior work that found marginal rebuttal impact but lacked large-scale, transparent, longitudinal analyses of temporal and co-reviewer effects and offered limited evidence-based guidance for authors</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Construct a large-scale corpus from ICLR 2024â€“2025 via OpenReview, reconstruct pre- and post-rebuttal scores, and combine statistical analysis with LLM-based categorization of review and rebuttal texts to model predictors of score changes, temporal dynamics, and engagement. Quantify co-reviewer influence and disagreement convergence and identify effective rebuttal strategies (evidence-backed clarifications vs. generic defenses).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Causal Effects of Rebuttals in Peer Review: A Randomized Controlled Trial at Scale: Randomize rebuttal visibility and timing to estimate causal impacts on score changes and acceptance, with special focus on borderline papers.<br>â€¢ Mitigating Co-Reviewer Influence in ML Conferences: Algorithms and Policy Interventions: Design and evaluate mechanisms (e.g., independent scoring, blinded aggregation, calibration protocols) to reduce herding and improve fairness.<br>â€¢ LLM-Augmented Rebuttal Assistant for Fair and Effective Author-Reviewer Dialogue: Develop and field-test tools that help authors craft evidence-backed clarifications and track engagement, measuring effects on reviewer convergence and score shifts.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">Planning with Sketch-Guided Verification for Physics-Aware Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.17450" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.17450" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Modern I2V diffusion models struggle to produce physically realistic, temporally consistent motion and to follow fine-grained motion instructions.<br>â€¢ Single-shot LLM/MLLM planning is brittle; errors in a one-pass trajectory propagate to final videos, yielding implausible motions.<br>â€¢ Iterative refinement methods require repeated full video generation, incurring high computational cost and long runtimes.<br>â€¢ Existing physics-aware approaches often depend on heavy simulators, specialized datasets, or finetuning, limiting practicality and zero-shot generalization.<br>â€¢ There is a need for efficient test-time verification that ensures semantic alignment and physics plausibility before expensive synthesis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SketchVerify is a training-free, test-time framework that samples multiple object trajectories, renders them as lightweight video sketches by compositing segmented objects onto a static background, and ranks them using a multimodal verifier assessing instruction alignment and physics plausibility (e.g., Newtonian consistency, non-penetration, gravity coherence, shape stability). The best per-step trajectories are fused into a unified plan and fed to a trajectory-conditioned diffusion model for final video synthesis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Jointly Learned Plannerâ€“Verifier for Physics-Aware Video Synthesis: Co-train the trajectory planner and multimodal verifier to reduce test-time search and improve end-to-end consistency.<br>â€¢ Multi-Object Interaction-Aware Sketch Verification: Extend sketch-guided verification to multiple moving agents with contact, occlusion, and camera motion, enforcing collision handling and interaction dynamics.<br>â€¢ Differentiable Sketch Verification with Simulator-Guided Priors: Integrate lightweight differentiable physics modules or simulators into the verifier to provide gradients and stronger physical guarantees during plan selection.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16110" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16110" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The growing misuse of Vision-Language Models (VLMs) has led providers to deploy multiple safeguards, including alignment tuning, system prompts, and content moderation. However, the real-world robustness of these defenses against adversarial attacks remains underexplored. We introduce Multi-Faceted Attack (MFA), a framework that systematically exposes general safety vulnerabilities in leading defense-equipped VLMs such as GPT-4o, Gemini-Pro, and Llama-4. The core component of MFA is the Attention-Transfer Attack (ATA), which hides harmful instructions inside a meta task with competing objectives. We provide a theoretical perspective based on reward hacking to explain why this attack succeeds. To improve cross-model transferability, we further introduce a lightweight transfer-enhancement algorithm combined with a simple repetition strategy that jointly bypasses both input-level and output-level filters without model-specific fine-tuning. Empirically, we show that adversarial images optimized for one vision encoder transfer broadly to unseen VLMs, indicating that shared visual representations create a cross-model safety vulnerability. Overall, MFA achieves a 58.5% success rate and consistently outperforms existing methods. On state-of-the-art commercial models, MFA reaches a 52.8% success rate, surpassing the second-best attack by 34%. These results challenge the perceived robustness of current defense mechanisms and highlight persistent safety weaknesses in modern VLMs. Code: https://github.com/cure-lab/MultiFacetedAttack</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Real-world robustness of stacked defenses (alignment tuning, system prompts, input/output moderation) in VLMs is underexplored and likely overestimated.<br>â€¢ Existing jailbreak studies are fragmented (text-only or image-only), often ignore deployed content filters, and rarely evaluate proprietary models.<br>â€¢ There is no formal theoretical explanation for why benign-looking prompts can bypass alignment, hindering principled defense design.<br>â€¢ Safety vulnerabilities may transfer across models due to shared visual encoders, but cross-model transferability is not well characterized.<br>â€¢ Many attacks rely on model-specific fine-tuning and do not jointly evade both input- and output-level filters, limiting practicality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes the Multi-Faceted Attack (MFA), combining an Attention-Transfer Attack that hides harmful instructions inside benign meta tasks (theoretically framed as reward hacking), a lightweight transfer-enhancement algorithm with a repetition strategy to jointly evade input/output filters, and vision-encoderâ€“targeted adversarial images that embed malicious system prompts in pixel space to achieve broad cross-model transfer.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Toward Robust Reward Models: Preventing Meta-Task Reward Hacking in Vision-Language Systems: Redesign reward modeling and alignment objectives to resist attention-transfer exploits that dilute safety signals.<br>â€¢ Certifying Safety Under Stacked Defenses: Formal Guarantees for Defense-Equipped Vision-Language Models: Develop theoretical and empirical certification methods for VLM safety with alignment, system prompts, and content filters.<br>â€¢ Breaking the Monoculture: Diversified Vision Encoders to Reduce Cross-Model Adversarial Transfer: Investigate encoder heterogeneity and ensemble strategies to mitigate shared-representation vulnerabilities.<br>â€¢ Adaptive Multimodal Moderation: Joint Inputâ€“Output Filters Resilient to Repetition-Based Evasion: Build dynamic, context-aware filters that detect and counter repetition and transfer-enhanced attacks across modalities.<br>â€¢ Detecting Benign-Looking Meta Tasks: Automated Identification of Attention-Transfer Jailbreaks in VLMs: Create classifiers and attention-pattern monitors to spot adversarial meta tasks embedded in prompts or images.<br>â€¢ Pixel-Space Prompt Immunization: Training VLMs to Ignore Embedded Malicious System Prompts in Images: Develop robust training and preprocessing to neutralize pixel-level prompt injections without harming utility.<br>â€¢ Benchmarking Cross-Model Safety Transfer: A Standardized Suite for Evaluating Defense-Equipped VLMs: Establish benchmarks and protocols to measure transferability and robustness across commercial and open-source VLMs.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Taming Generative Synthetic Data for X-ray Prohibited Item Detection</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.15299" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.15299" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Labeled X-ray security images are scarce; collecting and annotating them is time-consuming, labor-intensive, and costly.<br>â€¢ Existing TIP- and GAN-based synthesis pipelines are two-stage and require extra labor (e.g., foreground threat image collection, masks/trimaps/semantic labels), limiting scalability.<br>â€¢ Pure layout-to-image generation from natural image methods produces mismatched backgrounds for X-ray data and yields misaligned bounding boxes, hurting detector training.<br>â€¢ Synthetic images often lack realistic background occlusion seen in baggage, leading to overfitting and reduced robustness in detection.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Xsyn is a one-stage, text-grounded inpainting pipeline that fine-tunes a diffusion model (GLIGEN) to synthesize prohibited items directly within real X-ray backgrounds, and introduces Cross-Attention Refinement (CAR) to auto-correct bounding boxes using diffusion cross-attention + SAM with median point sampling, and Background Occlusion Modeling (BOM) to inject realistic occlusion via latent-space recombination with background regions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Generative CT Security Image Synthesis via Text-Grounded 3D Diffusion: Extend Xsyn to volumetric CT data with 3D cross-attention refinement and cross-modality alignment between X-ray and CT to reduce 3D annotation cost.<br>â€¢ Detector-in-the-Loop Synthetic Curriculum for X-ray Prohibited Item Detection: Use detector feedback to adapt generation prompts, placement, and BOM occlusion levels, targeting failure modes to maximize mAP via active/reinforcement learning.<br>â€¢ Physics-Aware Latent Occlusion Modeling for Realistic X-ray Imaging: Integrate material-dependent X-ray attenuation models into BOM to learn occluder selection and blending coefficients that mimic true transmission and scattering patterns.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13081" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13081" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods. We address this gap by introducing the Reference-Frame times Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise ("Why this prediction?") and contrastive ("Why this and not an alternative?") explanations. Granularity: Ranging from fine-grained class-level (e.g., "Why Husky?") to coarse-grained group-level (e.g., "Why Dog?") interpretations. Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets. By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Saliency maps lack alignment with usersâ€™ actual explanatory questions, which often require contrastive reasoning (e.g., â€œWhy A and not B?â€) rather than purely pointwise explanations.<br>â€¢ Current explanations are ambiguous because they omit explicit reference-frame (pointwise vs. contrastive) and semantic granularity (class vs. group) context, leading to misinterpretation.<br>â€¢ Existing evaluation metrics predominantly test pointwise faithfulness and ignore contrastive validity and granularity coherence, undermining trust and practical utility.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes the RFxG taxonomy (Reference-Frame Ã— Granularity) and introduces four faithfulness metrics based on structured perturbations and score comparisons to evaluate both pointwise vs. contrastive and class vs. group explanations; it also provides group-level ImageNet labels (via WordNet) and a comprehensive benchmark across 10 methods, 4 model architectures, and 3 datasets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Human-Aligned Contrastive Explanations: A User Study Benchmark for RFxG Validity: Develop standardized human evaluations to measure how well RFxG explanations match user intent across contrastive and granularity levels.<br>â€¢ RFxG for Multimodal Models: Extending Reference-Frame Ã— Granularity to Vision-Language Explanations: Generalize taxonomy and metrics to imageâ€“text transformers, assessing contrastive and group-aware reasoning in multimodal settings.<br>â€¢ Intent-Adaptive Explainers: Learning to Generate RFxG-Aligned Saliency Maps from User Queries: Train models to dynamically produce explanations tailored to explicit user-specified reference frames and granularities, optimizing new RFxG metrics end-to-end.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.17199" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.17199" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ VLAs struggle with spatiotemporally coherent manipulationâ€”actions are spatially imprecise and temporally discontinuous (idle pauses, jitter).<br>â€¢ Mismatch between 2D image coordinates and 3D robot/world coordinates, and between static frames and time-varying action trajectories.<br>â€¢ Existing 3D VLAs improve spatial precision but lack explicit temporal control; prior 4D VLAs add temporal cues to visuals but do not represent or plan temporal action variables.<br>â€¢ Coarse visual reasoning (often single-image) limits fine-grained planning; datasets lack temporal action annotations needed to learn coherent timing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>VLA-4D fuses 3D positions and 1D time into visual features via Fourier spatiotemporal embeddings and cross-attention, and extends the action space to [Î”x, Î”Î¸, Grip, Î”t] with multimodal alignment to an LLM that predicts spatiotemporal actions. A two-stage pipeline pre-aligns 4D vision-language features and fine-tunes on a temporally annotated LIBERO dataset using an L1 loss.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Unified 4D World Models for Closed-Loop VLA Manipulation: Integrate predictive spatiotemporal dynamics and uncertainty into VLA-4D for feedback-based, long-horizon coherent control.<br>â€¢ Automatic Temporal Labeling for VLA Datasets via Multimodal Alignment: Self-supervise Î”t and action phase discovery from video, depth, and robot logs to scale 4D training without manual timing annotations.<br>â€¢ Cross-Robot Spatiotemporal Transfer in VLA-4D through Proprioceptive Calibration: Develop adaptation layers that map proprioceptive spaces and time constants across embodiments to preserve spatial smoothness and temporal coherence.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 8</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-24 23:06:26 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 8;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>