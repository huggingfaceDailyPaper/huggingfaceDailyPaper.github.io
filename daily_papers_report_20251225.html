<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - December 25, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">December 25, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.16093" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.16093" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations. We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ End-to-end video diffusion inference is prohibitively slow (minutes to hours per 5-second clip on a single GPU), preventing interactive and real-time applications.<br>‚Ä¢ Quadratic full attention dominates compute and memory; naive sparsity or caching typically hurts quality or fails to scale with long sequences/resolutions.<br>‚Ä¢ Reducing sampling steps is necessary for speed, but prior distillation often degrades visual fidelity/temporal consistency or is hard to integrate with other accelerations.<br>‚Ä¢ Large video models at 480P‚Äì720P strain memory/throughput; existing quantization often targets weights only and misses activation/Tensor Core efficiency.<br>‚Ä¢ Existing systems (e.g., FastVideo) achieve smaller speedups and show quality regressions; there is a lack of an end-to-end, co-designed pipeline that composes low-bit attention, trainable sparsity, step distillation, and INT8 linear acceleration.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TurboDiffusion co-trains Sparse-Linear Attention (SLA) and rCM step distillation, merges their updates, and at inference uses SageSLA (low-bit SageAttention2++ fused with SLA), W8A8 block-wise INT8 quantization for both weights and activations in Linear layers, plus fused CUDA/Triton norms to maximize Tensor Core utilization. This algorithm‚Äìsystem co-design yields 100‚Äì200√ó end-to-end speedups on a single RTX 5090 while maintaining video quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive SageSLA: Dynamic Quality-Aware Sparse‚ÄìDense Switching for Video Diffusion: Learn to adjust Top‚ÄëK sparsity per layer/frame conditioned on noise level and scene complexity to optimize speed‚Äìquality trade-offs.<br>‚Ä¢ Ultra-Low-Bit Video Diffusion with Error-Compensated W4A8/W4A4: Push quantization beyond W8A8 using per-block/per-channel scaling and calibration to preserve quality at even higher throughput.<br>‚Ä¢ rCM-XL: Content-Adaptive Step Scheduling for Long-Form, High-Resolution Video Generation: Extend step distillation with variable step counts across timesteps and shots to sustain quality on longer clips and 1080P+ outputs.<br>‚Ä¢ Cross-Modal TurboDiffusion: A Unified Acceleration Stack for Audio-Video and 3D Diffusion Models: Generalize SageSLA + rCM + INT8 pipeline to multimodal and 3D tasks with modality-specific sparsity patterns and kernels.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.20557" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.20557" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Vision-language models (VLMs) are weak at dynamic spatial reasoning (DSR)‚Äîunderstanding how object geometry and relationships evolve in 3D over time‚Äîdue to scarce, scalable 4D-aware training resources.<br>‚Ä¢ Existing data/benchmarks emphasize static scenes or short-horizon motion, narrow domains (e.g., driving/HOI), lack viewpoint transformations and multi-object interactions, and provide coarse, non-procedural answers; training data for DSR is largely missing.<br>‚Ä¢ Naive fusion of large 3D priors into VLMs (e.g., direct cross-attention/addition) introduces noisy, task-specific signals that degrade general video understanding; there is no targeted, question-guided selection of relevant geometry.<br>‚Ä¢ Realistic, in-the-wild videos require egocentric‚Äìallocentric transformations and fine-grained temporal reasoning; monocular 3D lacks metric scale, complicating faithful supervision without qualitative, trend-based formulations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DSR Suite couples an automated pipeline that converts in-the-wild videos into 4D-grounded multiple-choice QA (DSR-Train and the human-refined DSR-Bench) using camera poses, point clouds, masks, orientations, and 3D trajectories, with a lightweight Geometry Selection Module (GSM). GSM stacks two Q-Formers to condense the question and retrieve only question-relevant geometry tokens from pretrained 4D priors, then fuses them with vision tokens to boost DSR while preserving general video understanding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ GSM++: End-to-End Question-Guided 4D Geometry Selection for Video VLMs: Jointly train the selector, video encoder, and 3D backbone to learn adaptive, compact geometry tokens and further reduce noise from in-the-wild priors.<br>‚Ä¢ DSR-Agent: Embodied 4D Spatial Reasoning for Real-World Robotics and AR: Deploy DSR-enhanced VLMs in closed-loop agents for dynamic navigation/manipulation; study action-conditioned, safety-aware reasoning in changing scenes.<br>‚Ä¢ Metric-Scale DSR: Toward Quantitative 4D Reasoning from In-the-Wild Videos: Integrate self-calibration or auxiliary sensors to recover metric scale, enabling numeric distance/speed judgments and longer-horizon prediction and counterfactuals.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21252" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21252" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The "one-shot" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing approaches stitch independently generated clips or rely only on first/last-frame conditioning, often causing disjointed transitions and weak temporal coherence; precise temporal control over intermediate moments is missing.<br>‚Ä¢ 3D VideoVAE encoders are causal with temporal downsampling, so an intermediate condition latent encodes multiple neighboring frames, breaking exact frame-level correspondence and hindering arbitrary-frame control.<br>‚Ä¢ Super-resolution stages amplify mismatches between conditions and generated content, yielding flicker and cross-frame color shifts, especially under intermediate conditioning.<br>‚Ä¢ Large semantic/visual gaps between conditioning points trigger abrupt cuts and physically implausible subject motion; base models lack mechanisms to prefer smooth transitions and realistic dynamics.<br>‚Ä¢ Long one-shot videos exceed the memory/compute envelope of DiT models, making single-pass generation impractical without specialized, memory-efficient inference.<br>‚Ä¢ Addressing these issues enables controllable, coherent, and cinematic one-shot videos from fragmented visual inputs‚Äîvaluable for filmmaking, pre-viz, advertising, and creative storytelling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DreaMontage adds lightweight arbitrary frame/video conditioning to a DiT-based video generator via channel-wise latent concatenation, an Interm-Cond Adaptive Tuning scheme to align training with inference, and a Shared-RoPE sequence-wise conditioning in the SR DiT to suppress flicker. It further applies a progressive pipeline‚ÄîVisual Expression SFT and Tailored DPO (for cut avoidance and realistic motion)‚Äîand uses a Segment-wise Auto-Regressive inference strategy to generate long, seamless one-shot videos efficiently.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Non-Causal VideoVAE for Precise Arbitrary-Frame Conditioning: Replace causal, temporally downsampled encoders with bidirectional or localized VAEs that produce single-frame-consistent latents, eliminating re-encode/resample approximations.<br>‚Ä¢ Planning-Guided Transition Synthesis Between Heterogeneous Conditions: Introduce a high-level planner that predicts camera/object trajectories and transition styles between conditions to improve narrative coherence and controllability.<br>‚Ä¢ Cross-Scale Consistent Super-Resolution for Arbitrary-Conditioned Video Diffusion: Train SR with shared positional priors and cross-scale temporal alignment losses to further reduce flicker and color drift under intermediate conditioning.<br>‚Ä¢ Interactive Streaming One-Shot Generation with Online Condition Insertion: Enable real-time keyframe/clip insertion and prompt edits during generation using causal decoding plus SAR for low-latency, controllable outputs.<br>‚Ä¢ 3D-Aware One-Shot Video Generation with View-Consistent Camera Control: Integrate 3D scene representations (e.g., NeRF/Gaussians) to maintain geometric consistency across large camera motions and enable novel-view one-shots.<br>‚Ä¢ Scalable Preference Alignment for Video via Multimodal RLHF: Generalize Tailored DPO to large-scale reward modeling and RLHF for smoothness, physics realism, and style adherence while minimizing human labeling.<br>‚Ä¢ Script-to-Timeline Compiler for Arbitrary-Frame Conditioning: Use LLM/VLM tools to convert scripts and storyboards into optimal timed conditions and prompts, jointly optimizing conditioning schedules and transitions.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21094" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21094" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Evaluation of text-to-audio-video (T2AV) generation is fragmented and often unimodal, failing to assess cross-modal semantic alignment and temporal synchronization.<br>‚Ä¢ Existing benchmarks have limited coverage of fine-grained audiovisual coupling (e.g., multi-source mixing, off-screen sound, physical causality), struggle with long and compositional prompts, and rely on narrow metric sets without interpretable diagnostics.<br>‚Ä¢ There is no unified, scalable protocol that combines objective signal-level metrics with high-level instruction following and perceptual realism assessment.<br>‚Ä¢ Realistic T2AV requires simultaneous success in video/audio quality, cross-modal alignment, temporal sync, and physically grounded realism; without comprehensive evaluation, progress is hard to measure and compare.<br>‚Ä¢ State-of-the-art models exhibit persistent failures‚Äîespecially an ‚ÄúAudio Realism Bottleneck,‚Äù weak fine-grained synchronization, and imperfect instruction following‚Äîhighlighting the need for a challenging, diagnostic benchmark.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>T2AV-Compass proposes a taxonomy-driven benchmark of 500 complex prompts (curated via semantic clustering, LLM rewriting, and real-video inversion) plus a dual-level evaluation suite. It fuses objective metrics for video/audio quality and cross-modal alignment/synchrony with a reasoning-first MLLM-as-a-Judge that scores instruction following and perceptual realism using granular checklists and violation checks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Breaking the Audio Realism Bottleneck: Joint Audiovisual Diffusion for Physically Grounded Foley and Synchrony: Develop end-to-end architectures that learn cross-modal physical correlations to improve material‚Äìtimbre consistency and event-level A‚ÄìV sync.<br>‚Ä¢ Long-Form T2AV-Compass: Benchmarking and Modeling Minute-Scale Narratives with Stable Cross-Modal Coherence: Extend evaluation and modeling to long-duration (>10s) videos with multi-event audio mixing, narrative continuity, and robust temporal consistency.<br>‚Ä¢ LightJudge: Distilled and Debiased MLLM-as-a-Judge for Scalable T2AV Evaluation: Train compact, interpretable evaluators from reasoning-first judges with human-in-the-loop feedback to reduce cost and bias while preserving diagnostic power.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21337" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21337" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of a global, open, continuous-label benchmark for building-age estimation; prior datasets are geographically narrow, temporally shallow, lack photos, or are closed-source (Table 1, page 2).<br>‚Ä¢ Accurate dating matters for sustainability audits, heritage preservation, and disaster assessment, yet ages for most of the world‚Äôs buildings are unknown (Introduction and Figure 1, page 1).<br>‚Ä¢ Prior methods often cast age prediction as classification, ignoring temporal ordinality; licensing issues hinder reproducibility (pages 1‚Äì2).<br>‚Ä¢ SOTA VLMs may memorize famous landmarks rather than reason architecturally; strong popularity bias observed (e.g., Gemini2.0-Flash gains +34.18% IA5 on high-pageview buildings; Table 2, page 6).<br>‚Ä¢ Models show geographic and temporal biases, with large errors in early periods and uneven performance across continents (Table 3, page 7).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>They introduce YearGuessr (55,546 CC BY-SA Wikipedia-sourced facade images with GPS and pageviews, 1001‚Äì2024) and YearCLIP, a CLIP-based ordinal regression model that fuses image and GPS embeddings via a learnable zero-convolution and uses style tokens plus reasoning prompts to output year estimates and human-verifiable rationales (Figure 4, page 5). They also define popularity-aware evaluation (Interval Accuracy and stratified IA5/MAE) to quantify memorization bias.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Popularity-Debiasing for VLMs on YearGuessr: Counterfactual prompts, reweighting by pageviews, and adversarial training to reduce landmark memorization.<br>‚Ä¢ Renovation-Aware Building Dating: Add temporal-segmentation/renovation labels and a multi-stage ordinal regressor that separates original vs. rebuilt phases.<br>‚Ä¢ Geo-Diverse Expansion of YearGuessr: Active learning and diffusion-based synthesis to boost pre-1600 and underrepresented regions‚Äô coverage.<br>‚Ä¢ Uncertainty-Aware OrdinalCLIP: Probabilistic ordinal losses with calibrated prediction intervals and risk-sensitive evaluation.<br>‚Ä¢ Multi-View and 3D Cues for Architectural Age: Fuse street-level, aerial, and open-vocabulary 3D features for robust dating across periods.<br>‚Ä¢ Explainable Age Prediction with Human-in-the-Loop: Curate and refine reasoning prompts using expert feedback for trustworthy rationales.<br>‚Ä¢ Metadata-as-Language Fusion for Dating: Integrate EXIF, address, and climate priors (e.g., AddressCLIP/EXIF-as-language) to improve fairness and robustness.<br>‚Ä¢ Stress-Testing Memorization in VLMs: A standardized suite to benchmark closed- vs. open-source VLMs under popularity-controlled splits.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.20848" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.20848" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Improve inference efficiency without sacrificing accuracy by activating only a small fraction of parameters per token (sparse MoE) to outperform similarly sized open models in throughput-heavy regimes.<br>‚Ä¢ Enable reliable ultra-long context (up to 1M tokens) without degrading short-context performance, which prior long-context training often harms.<br>‚Ä¢ Avoid capability regressions from single-environment RL by training across multiple verifiable environments simultaneously with a principled curriculum.<br>‚Ä¢ Replace brittle Bradley‚ÄìTerry preference models susceptible to reward hacking with a stronger, reasoning-based generative reward model for RLHF.<br>‚Ä¢ Provide controllable reasoning (on/off and token-budget control) to reduce unnecessary chain-of-thought verbosity and inference cost, which most models lack.<br>‚Ä¢ Preserve accuracy under aggressive post-training quantization; naive full-model quantization can significantly degrade performance.<br>‚Ä¢ Expand and decontaminate pretraining data (code, STEM, high-quality web text) to overcome scarcity/duplication and improve downstream reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A 31.6B-parameter Mixture-of-Experts hybrid Mamba‚ÄìTransformer (GQA) model that activates ~3.2B params per token, pretrained on 25T tokens with a two-phase curriculum and long-context CPT to 1M, then post-trained via unified multi-environment RL from verifiable rewards and RLHF using a large generative reward model with group-relative length control; finally, selective FP8 PTQ keeps attention/preceding Mamba layers in BF16 to retain accuracy while boosting throughput.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Off-Policy Multi-Environment RL for Generalist LLMs: Scale beyond synchronous GRPO by adding replay, masked importance corrections, and adaptive curricula to further improve stability, sample-efficiency, and cross-domain retention.<br>‚Ä¢ Learning to Budget: Dynamic Reasoning-Length Control via Reward Shaping: Generalize group-relative length control to instance-adaptive, tool-aware budgets that jointly optimize answer quality, latency, and cost across tasks.<br>‚Ä¢ Beyond One Million Tokens: Mixture-of-Length CPT with Retrieval for Extreme Contexts: Combine mixture-of-length continuous pretraining, retrieval augmentation, and stability regularizers to push reliable context windows to >1M without short-context regressions.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21338" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21338" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ High-resolution video diffusion has quadratic spatio-temporal complexity, making 1080p and long-duration generation impractical in latency and memory.<br>‚Ä¢ Sampling acceleration (e.g., timestep distillation) reduces the number of steps but per-step cost at high resolution remains prohibitively high.<br>‚Ä¢ Sparse/sliding attention reduces quadratic cost, yet KV caches and memory still grow with video length, causing unstable speed and memory blow-up.<br>‚Ä¢ There is inherent redundancy across axes: early denoising steps only set coarse structure (spatial redundancy), far-past frames contribute little (temporal redundancy), and later chunks need fewer steps (timestep redundancy).<br>‚Ä¢ Maintaining temporal coherence without attending to the full history is challenging; current autoregressive designs suffer from growing context and drift.<br>‚Ä¢ Two-stage pipelines with external super-resolution are cheap but often lose fine details; DiT-based models also face high-res positional encoding misalignment, yielding blur.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>HiStream eliminates spatial, temporal, and timestep redundancy via Dual-Resolution Caching (early steps at low resolution followed by high-resolution refinement with aligned dual KV caches) and an Anchor-Guided Sliding Window (a persistent first-frame anchor plus a small neighbor cache for fixed-size context). Optionally, Asymmetric Denoising uses fewer steps for subsequent chunks, achieving up to 76.2√ó‚Äì107.5√ó speedups at 1080p with minimal quality loss.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive HiStream: Content-Aware Scheduling for Streaming Video Diffusion: Dynamically allocate resolution, chunk size, and denoising steps based on motion/texture complexity to further reduce compute while preserving fidelity.<br>‚Ä¢ Multi-Anchor Streaming Attention for Long-Form and Scene-Change Robustness: Learn anchor selection/refresh and use multi-anchor caches or anchor compression to handle scene transitions and prevent drift without growing memory.<br>‚Ä¢ HiStream-4K: Hierarchical Dual-Resolution Caching for Ultra-High-Resolution, Long-Duration Video: Extend to multi-scale pyramids with quantized/low-rank KV caches and joint distillation to scale to 4K+ resolution and hour-long videos with stable latency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">NVIDIA Nemotron 3: Efficient and Open Intelligence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.20856" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.20856" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ High inference latency and poor throughput in Transformer-based MoE for reasoning workloads due to attention KV-cache growth and all-to-all expert routing bottlenecks (see Fig. 2, p.2).<br>‚Ä¢ Limited long-context capability; RoPE-based Transformers degrade or fail beyond training length, hindering 512k‚Äì1M token tasks like code repositories and RAG (Table 3, p.8).<br>‚Ä¢ Existing MoE layers are memory-bandwidth‚Äìbound at low batch and communication‚Äìbound at high throughput, reducing accuracy per byte under fixed latency/compute (¬ß2.2).<br>‚Ä¢ Training cost/efficiency constraints for trillion-token pretraining; current FP8/BF16 regimes underutilize new hardware throughput (NVFP4 offers up to 3√ó FP8 peak on GB300; ¬ß2.4).<br>‚Ä¢ Post-training fragility: staged RL can cause reward hacking and capability regressions across tasks (¬ß2.6), and models lack controllable reasoning budgets at inference (¬ß2.7).<br>‚Ä¢ Fragmented or closed releases (weights/recipes/data) limit reproducibility and adoption; the paper aims to provide open, end-to-end assets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Nemotron 3 uses a hybrid Mamba‚ÄìTransformer Mixture-of-Experts with LatentMoE (expert routing/compute in a lower-dimensional latent space) and minimal attention to cut communication and KV-cache costs while improving accuracy per byte. It trains with NVFP4 and Multi-Token Prediction, extends context to 1M tokens without RoPE, and applies multi-environment RL plus inference-time reasoning-budget control.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive LatentMoE: Learning Per-Layer Latent Dimensions and Top-K for Hardware-Aware Expert Routing: Jointly optimize latent size and active experts per token/layer to maximize accuracy-throughput under bandwidth constraints.<br>‚Ä¢ Auto-Hybridization of Mamba, Attention, and MoE for 1M-Token Contexts: Neural architecture search that places attention vs. Mamba vs. MoE under target latency, memory, and long-context retrieval objectives.<br>‚Ä¢ Policy-Driven Reasoning Budgets for Agentic LLMs: Reinforcement Learning of Dynamic Think-Token Allocation: Train controllers to allocate thinking tokens online to meet accuracy/latency SLAs across tasks and users.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.20757" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.20757" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ It is hard to isolate the impact of tokenization because existing models confound tokenizer effects with differences in architecture, training data, and budgets‚Äîthere is no open suite of models that are identical except for the tokenizer.<br>‚Ä¢ Current benchmarks largely evaluate on clean text and underrepresent real-world, tokenizer-sensitive perturbations (e.g., Unicode styling, OCR noise, romanization, diacritics, keyboard constraints, morphology, LaTeX/STEM formatting); see Figure 1 on page 2 for illustrative perturbation examples.<br>‚Ä¢ Multilingual settings expose tokenization inefficiency and unfairness (high subword fertility, poor parity, high continued-word rates), leading to cost and performance gaps across languages (Appendix C, Table 7).<br>‚Ä¢ Preprocessing choices (normalization, whitespace handling, contraction/number rules) and OOV strategies vary widely across tokenizers, but their downstream effects on robustness are poorly understood.<br>‚Ä¢ Universal vulnerabilities persist‚Äîparticularly Unicode styling and structural formatting‚Äîwhile simply scaling model size or training longer provides only modest robustness gains (Tables 1, 18, 21, 22).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TokSuite trains 14 language models that are identical in architecture, data, training budget, and initialization but differ only in their tokenizer; a super-vocabulary creates bijective mappings so shared tokens receive identical embedding initializations. It introduces a multilingual robustness benchmark (~5k examples across EN/TR/IT/FA/ZH) with real-world perturbation families, and evaluates models via byte-length‚Äìnormalized log-likelihood and relative accuracy drop, complemented by intrinsic tokenizer efficiency metrics (fertility, parity, PCW).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Boundless Tokenization for Robust Multilingual LMs: Allow cross pre-tokenization merges and morphology-aware segmentation to reduce fragmentation under noise and agglutinative morphology; evaluate on TokSuite.<br>‚Ä¢ Unicode-Native, Reversible Normalization Pipelines for LLMs: Design lossless normalization and tokenizer vocabularies that preserve styled/compatibility characters while eliminating performance cliffs from Unicode styling and homoglyphs.<br>‚Ä¢ Decoupling Input and Output Vocabularies in Technical Domains: Use byte-level or STEM-specialized input tokenization with compact output vocabularies to improve robustness on LaTeX, diagrams, and structured ASCII without sacrificing efficiency.<br>‚Ä¢ Robustness Scaling Laws with Controlled Tokenization: Systematically scale parameters and data with fixed tokenizers (and vice versa) to quantify how robustness curves change, isolating effects beyond what Table 22 suggests.<br>‚Ä¢ Inference-Time Tokenization Repair via Exact Byte-Level Probabilities: Integrate exact byte-level probability conversion and token healing into decoding to mitigate perturbation-induced segmentation errors without retraining; benchmark on TokSuite.<br>‚Ä¢ Vocabulary Optimization under Fixed Token Budgets: Jointly optimize vocabulary size, composition, and normalization to minimize fertility/parity gaps across languages while preserving downstream accuracy.<br>‚Ä¢ Adversarial Tokenization and Defenses in Safety Pipelines: Build non-canonical segmentation attacks (e.g., styled Unicode, zero-width, diacritics) and develop training-time and inference-time defenses for alignment-critical applications.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21004" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21004" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Visual generative pretraining is dominated by BERT-style masked modeling that underutilizes temporal dependencies critical for video understanding (pages 1‚Äì2; Fig. 1a).<br>‚Ä¢ Existing visual autoregressive (AR) pretraining often embeds semantics deep in intermediate layers and needs layer-wise probing; semantic localization is inaccurate (page 2).<br>‚Ä¢ Direct regression objectives for generating patches/frames struggle with diversity, leading to blurry or averaged outputs and weak semantics (page 2).<br>‚Ä¢ Video AR models can trivially copy previous frames due to temporal redundancy; masked next-frame prediction is needed to make the task non-trivial (page 3, Sec. 3.1).<br>‚Ä¢ Representation and decoding are entangled in end-to-end AR models, causing encoder features to be altered by decoding dynamics; decoupling is needed for stable, strong semantics (pages 3‚Äì4; Fig. 2).<br>‚Ä¢ Typical conditioning injection (e.g., AdaLN or sequence concat used in text-to-image) is mismatched for dense, spatially structured conditions; spatially aligned conditioning is required (page 2).<br>‚Ä¢ Preventing information leakage across frames during generation is crucial; custom attention masks are needed (page 4; Fig. 3).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>NExT-Vid pretrains via masked next-frame prediction with a context-isolated autoregressive predictor that forecasts next-frame latent features from ViT encoder outputs, and a conditioned flow-matching decoder that generates VAE latents using spatially aligned concatenation. An EMA-updated reference encoder provides alignment regularization, while custom attention masks (frame-wise causal, autoregressive, frame-isolated) prevent leakage and decouple representation from decoding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Mask-Free Autoregressive Video Pretraining at GPT Efficiency: Design objectives/architectures that avoid trivial copying without masking to recover GPT-like training efficiency for video.<br>‚Ä¢ Joint Generation-and-Representation Optimization for Video Foundation Models: Multi-objective or curriculum strategies to simultaneously reach high-fidelity generation and strong downstream representations, resolving the noted trade-off (page 15, Limitations).<br>‚Ä¢ Long-Context Autoregressive Pretraining for Minute-Scale Video Understanding: Hierarchical predictors and memory-efficient attention to extend frame-isolated decoding to long videos and complex temporal reasoning.<br>‚Ä¢ Unified Multimodal Next-Frame Prediction with Audio/Text Conditioning: Incorporate audio and language conditions into the flow-matching decoder to enrich temporal semantics and cross-modal grounding.<br>‚Ä¢ Video-Native Tokenizers for Flow-Matching Targets: Develop improved video VAEs or tokenizers (beyond image VAEs and current video VAEs) that yield stable, precise latents and better pretraining signals.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">From Word to World: Can Large Language Models be Implicit Text-based World Models?</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.18832" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.18832" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer a potential way to improve learning efficiency through simulated experience, but it remains unclear whether large language models can reliably serve this role and under what conditions they meaningfully benefit agents. We study these questions in text-based environments, which provide a controlled setting to reinterpret language modeling as next-state prediction under interaction. We introduce a three-level framework for evaluating LLM-based world models: (i) fidelity and consistency, (ii) scalability and robustness, and (iii) agent utility. Across five representative environments, we find that sufficiently trained world models maintain coherent latent state, scale predictably with data and model size, and improve agent performance via action verification, synthetic trajectory generation, and warm-starting reinforcement learning. Meanwhile, these gains depend critically on behavioral coverage and environment complexity, delineating clear boundry on when world modeling effectively supports agent learning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Agentic RL faces an experience bottleneck: real environments are non-adaptive, limited in coverage, costly, and hard to scale, hindering experience-driven scaling.<br>‚Ä¢ It is unclear whether and when LLMs can reliably serve as world models that maintain coherent latent state and improve agents‚Äô learning and safety.<br>‚Ä¢ Prior LLM-as-simulator approaches often rely on domain-specific, structured outputs or zero/few-shot prompting, yielding limited accuracy and poor transfer to open-ended settings.<br>‚Ä¢ Existing evaluations focus on single-step prediction; they rarely test long-horizon rollout consistency, distribution shift robustness, or practical agent utility.<br>‚Ä¢ There is no unified framework connecting next-token prediction to next-state prediction for multi-turn interaction, nor clear data/model scaling laws for text-based world models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Reformulate world modeling as multi-turn next-state prediction in text and train LLMs via supervised fine-tuning on large interaction trajectories across five environments, evaluating short-term fidelity (EM/F1) and long-horizon rollout fidelity using a WM‚ÜíReal consistency protocol (Real/WM/W2R/CR). Use the trained world models for pre-execution action verification, synthetic trajectory generation, and early-experience warm-start RL, while analyzing scalability (data/model size), OOD generalization, cross-environment joint training, and mixed-agent behavioral coverage.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Grounded Text World Models: Reducing Simulation Drift with Partial Real Observations: Incorporate real-environment anchors (e.g., initial search results) during rollouts to stabilize open-ended domains like WebShop.<br>‚Ä¢ Generalist World Models via Cross-Environment Pretraining: Jointly pretrain on diverse text environments to discover transferable dynamics and serve multiple tasks with a single model.<br>‚Ä¢ Behaviorally Diverse Training for Robust WM-to-Real Transfer: Curate mixed-agent trajectory corpora to broaden behavioral coverage and improve consistency under policy shift.<br>‚Ä¢ Uncertainty-Aware Action Verification for Safe Agents: Calibrate world models (ensembles/MC-dropout) to gate irreversible actions with confidence-aware pass/fail decisions.<br>‚Ä¢ Retrieval- and Tool-Augmented World Models for Open-Ended Tasks: Fuse retrieval or structured tool signals to handle long-tail, compositional dynamics beyond fixed schemas.<br>‚Ä¢ From Text to Multimodal World Models: Extend next-state prediction from text to vision and embodied domains for richer, grounded dynamics.<br>‚Ä¢ Co-Training Agents and World Models with Shared Objectives: Jointly optimize planning agents and simulators to reduce simulator-agent mismatch and improve sample efficiency.<br>‚Ä¢ Benchmarks and Metrics for Long-Horizon Consistency: Develop standardized datasets and measures beyond EM (e.g., executable rollouts, CR variants, repairability) for robust evaluation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Streaming Video Instruction Tuning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21334" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21334" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Offline video LLMs process complete, bounded clips and cannot handle continuous, unbounded video streams or decide when to respond under real-time latency constraints.<br>‚Ä¢ Existing streaming approaches split perception/decision with external controllers, creating accuracy‚Äìefficiency trade-offs, higher latency, and weak coupling between perception and response.<br>‚Ä¢ Prior streaming methods often focus narrowly on real-time narration using special tokens, failing to balance silence/standby/response states or generalize to diverse tasks (grounding, event/action captioning, time-sensitive QA).<br>‚Ä¢ Heterogeneous, temporally inconsistent datasets hinder precise temporal alignment and unified instruction-following across tasks.<br>‚Ä¢ Severe class imbalance in streaming supervision (Silence dominates) causes models to over-predict silence and miss correct response timing.<br>‚Ä¢ Current streaming benchmarks are predominantly QA-style and do not adequately test broader instruction-following across mixed task types.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Streamo is an end-to-end streaming video LLM that predicts response states as special tokens (<Silence>, <Standby>, <Response>) within a time-stamped, multi-turn dialogue and immediately generates outputs when <Response> is triggered. It is trained with focal and frequency-balanced losses on Streamo-Instruct-465K, a large, unified, temporally annotated, multi-task instruction dataset (narration, action/event captioning, temporal grounding, time-sensitive QA), and evaluated with the mixed-task Streamo-Bench.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Infinite-Horizon Streamo: KV-Cache Management and Token Pruning for Real-Time Video LLMs: Integrate sliding-window attention, cache eviction, and visual token pruning to scale to unbounded streams with low latency and memory.<br>‚Ä¢ Learning When to Speak: Reinforcement Learning for Response Timing and Proactive Streaming Interaction: Optimize silence/standby/response decisions and proactive alerts with task- and latency-aware rewards beyond supervised state labeling.<br>‚Ä¢ Label-Efficient Temporal Supervision: Self-/Weakly-Supervised Streaming Instruction Tuning for Event Boundaries: Automatically mine temporal boundaries and response states from large unlabeled streams to expand multi-task streaming datasets.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">Multi-hop Reasoning via Early Knowledge Alignment</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.20144" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.20144" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at https://github.com/yxzwang/EarlyKnowledgeAlignment{Github}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Single-step RAG struggles with multi-hop questions because relevant evidence is rarely retrievable in one shot, leading to frequent failures on knowledge-intensive queries.<br>‚Ä¢ Iterative RAG often plans first without awareness of what the retriever can actually fetch, causing plan failure in the initial think step and cascading retrieval errors.<br>‚Ä¢ RL-based iterative RAG wastes budget on high-entropy, unfocused exploration when initial reasoning is ungrounded, degrading both efficiency and final answer quality.<br>‚Ä¢ Many RL pipelines rely heavily on innate LLM reasoning or SFT data quality; misalignment with the retrieval corpus yields redundant or suboptimal reasoning paths.<br>‚Ä¢ Lack of early contextual grounding amplifies compounding errors during multi-hop reasoning, hurting both retrieval precision and downstream accuracy and efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Early Knowledge Alignment (EKA) injects a small set of top-k retrieved passages before the first planning step, grounding the initial think so the agent iteratively Think-Search-Answer with lower-entropy, retrieval-aware trajectories. It is training-free or RL-compatible (GRPO/PPO), retriever-agnostic, and empirically improves retrieval precision, reduces turns, and raises end-to-end accuracy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ EKA for Deep Research Agents in Open Web Environments: Extend early knowledge alignment to long-horizon, browsing-based research with budgeted, multi-session planning and dynamic web sources.<br>‚Ä¢ Entropy-Guided RL Objectives for Iterative RAG: Design RL losses that explicitly regulate exploration entropy conditioned on retrieved context to focus search and minimize cascading plan errors.<br>‚Ä¢ Co-Training Retrievers and Policies with Early Knowledge Signals: Jointly optimize retriever and generator using EKA-derived signals to align retrieval distributions with policy needs and maximize information gain per token.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.18470" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.18470" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, Tool comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on Tool, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing benchmarks emphasize isolated, single-issue fixes and underrepresent long-horizon software evolution that spans many files, commits, and versions (Figure 2 p.3; Section 1).<br>‚Ä¢ Real-world SE requires interpreting high-level requirements (SRS), planning multi-step modifications, coordinating cross-module changes, and preserving functionality under heavy regression testing; maintenance dominates industry work and AI adoption is high (Section 1).<br>‚Ä¢ SWE-Bench shows signs of saturation and may inflate performance via incomplete fixes, limited test coverage, or data contamination; its binary scoring obscures partial progress (Sections 1, 3.2).<br>‚Ä¢ Current agents struggle with sustained, multi-file reasoning and instruction following; strong models drop from ~65% on SWE-Bench Verified to ~21% on SWE-EVO (Tables 2‚Äì3, pp.10‚Äì11).<br>‚Ä¢ There is a need for a benchmark with longer specifications and broader changes: SWE-EVO tasks average 2390-word specs, 21 files edited, 610 lines changed, and 874 tests per instance across 48 tasks and 7 repos (Table 1 p.6; Figure 4 p.9), with difficulty correlated to multiple PRs per instance (Figure 5 p.10; Figure 7 p.14).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce SWE-EVO, a benchmark built from release notes and versioned snapshots of 7 Python projects where agents must evolve a codebase from one release tag to the next, validated by large FAIL_TO_PASS and PASS_TO_PASS test suites while remaining SWE-Bench-compatible. Propose Fix Rate, a soft metric that credits the fraction of failing tests fixed without regressions, complementing Resolved Rate and Patch Apply Rate (Section 3).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Soft-Score RL for Regression-Safe Code Evolution: Train agents to optimize Fix Rate with strict no-regression constraints using verifier feedback and trajectory signals on SWE-EVO.<br>‚Ä¢ SRS-Grounded Planning for Multi-PR Codebase Upgrades: Parse release notes into structured intermediate plans that map requirements to code changes across files and commits to improve instruction following.<br>‚Ä¢ Difficulty-Aware Multi-Agent Orchestration for Long-Horizon SE: Adapt agent roles and search depth based on difficulty estimators (e.g., PR count) to better coordinate navigation, patching, and verification at scale.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PhononBench:A Large-Scale Phonon-Based Benchmark for Dynamical Stability in Crystal Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21227" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21227" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In this work, we introduce PhononBench, the first large-scale benchmark for dynamical stability in AI-generated crystals. Leveraging the recently developed MatterSim interatomic potential, which achieves DFT-level accuracy in phonon predictions across more than 10,000 materials, PhononBench enables efficient large-scale phonon calculations and dynamical-stability analysis for 108,843 crystal structures generated by six leading crystal generation models. PhononBench reveals a widespread limitation of current generative models in ensuring dynamical stability: the average dynamical-stability rate across all generated structures is only 25.83%, with the top-performing model, MatterGen, reaching just 41.0%. Further case studies show that in property-targeted generation-illustrated here by band-gap conditioning with MatterGen--the dynamical-stability rate remains as low as 23.5% even at the optimal band-gap condition of 0.5 eV. In space-group-controlled generation, higher-symmetry crystals exhibit better stability (e.g., cubic systems achieve rates up to 49.2%), yet the average stability across all controlled generations is still only 34.4%. An important additional outcome of this study is the identification of 28,119 crystal structures that are phonon-stable across the entire Brillouin zone, providing a substantial pool of reliable candidates for future materials exploration. By establishing the first large-scale dynamical-stability benchmark, this work systematically highlights the current limitations of crystal generation models and offers essential evaluation criteria and guidance for their future development toward the design and discovery of physically viable materials. All model-generated crystal structures, phonon calculation results, and the high-throughput evaluation workflows developed in PhononBench will be openly released at https://github.com/xqh19970407/PhononBench</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing evaluations of generative crystal models emphasize thermodynamic stability (e.g., Ehull) but neglect dynamical stability, which determines whether a structure can physically exist (imaginary phonon modes indicate instability)<br>‚Ä¢ First-principles phonon calculations (DFPT/supercell) are too costly for large-scale assessment, so models are rarely tested for phonon stability at scale<br>‚Ä¢ Current models frequently generate structures that appear stable thermodynamically but are dynamically unstable, undermining reliability and synthesizability<br>‚Ä¢ Lack of a standardized, scalable benchmark and metric to compare models on dynamical stability across architectures, datasets, and conditioning settings<br>‚Ä¢ Limited understanding of how symmetry constraints and property conditioning (e.g., band gap) affect dynamical stability and novelty</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>PhononBench conducts large-scale, standardized phonon-based dynamical stability evaluation of AI-generated crystals by coupling the MatterSim uMLIP (DFT-level phonon accuracy) with Phonopy to screen 108,843 relaxed structures from six generative models, using absence of imaginary modes as the stability criterion. It reports a unified dynamical-stability rate metric, analyzes property- and symmetry-conditioned generation, and releases 28,119 phonon-stable structures and all workflows for reproducibility.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Phonon-Guided Diffusion Models for Crystal Generation: Integrate differentiable phonon predictors or guidance into the generative loop to directly enforce dynamical stability during sampling<br>‚Ä¢ Soft-Mode-Aware Auto-Relaxation for AI-Generated Crystals: Follow unstable phonon modes to symmetry-broken minima to systematically convert dynamically unstable outputs into stable polymorphs<br>‚Ä¢ Finite-Temperature PhononBench: Benchmarking Dynamical Stability under Temperature and Pressure with MLIPs: Extend the benchmark to quasi-harmonic/anharmonic regimes to assess stability landscapes beyond 0 K and ambient conditions</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21010" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21010" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapid proliferation of Large Language Models (LLMs) and diverse specialized benchmarks necessitates a shift from fragmented, task-specific metrics to a holistic, competitive ranking system that effectively aggregates performance across multiple ability dimensions. Primarily using static scoring, current evaluation methods are fundamentally limited. They struggle to determine the proper mix ratio across diverse benchmarks, and critically, they fail to capture a model's dynamic competitive fitness or its vulnerability when confronted with sequential, high-stakes tasks. To address this, we introduce the novel Competitive Swiss-System Dynamics (CSD) framework. CSD simulates a multi-round, sequential contest where models are dynamically paired across a curated sequence of benchmarks based on their accumulated win-loss record. And Monte Carlo Simulation (N=100,000 iterations) is used to approximate the statistically robust Expected Win Score (E[S_m]), which eliminates the noise of random pairing and early-round luck. Furthermore, we implement a Failure Sensitivity Analysis by parameterizing the per-round elimination quantity (T_k), which allows us to profile models based on their risk appetite--distinguishing between robust generalists and aggressive specialists. We demonstrate that CSD provides a more nuanced and context-aware ranking than traditional aggregate scoring and static pairwise models, representing a vital step towards risk-informed, next-generation LLM evaluation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Fragmented multi-benchmark landscape makes model selection hard; practitioners need a single, holistic ranking for deployment while manual inspection is infeasible (pp. 1‚Äì2).<br>‚Ä¢ Arbitrary weighting in static aggregation: rankings depend on heuristic task weights across diverse benchmarks, lacking an objective mix ratio (pp. 2‚Äì3).<br>‚Ä¢ Static averages hide path dependence; early foundational failures block downstream capabilities (illustrated in Figure 2), so "excel later" cannot compensate for "fail early".<br>‚Ä¢ Static pairwise models (Elo/Bradley‚ÄìTerry) ignore sequential dynamics and failure risk; they do not model elimination pressure or path-dependent pairing (Sec. 3).<br>‚Ä¢ Tournament/pairing luck introduces high variance; need a statistically robust estimate that removes random pairing effects (Abstract; Sec. 2.4).<br>‚Ä¢ Lack of risk profiling: current methods cannot distinguish robust generalists from aggressive specialists under varying failure penalties (Abstract; Sec. 2.5).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Competitive Swiss-System Dynamics (CSD): simulate a multi-round Swiss tournament over a sequenced set of benchmarks using a precomputed pairwise win‚Äërate tensor, with zero-point byes and per‚Äëround elimination to enforce path dependence and failure penalties. Use Monte Carlo (many runs) to estimate each model‚Äôs Expected Win Score and vary the elimination parameter to conduct Failure Sensitivity Analysis that profiles robustness versus fragility.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Agentic CSD: Predicting Real-World Agent Performance via Sequential Benchmark Curricula: Map multi-step agent workflows to ordered benchmark sequences and test how CSD rankings predict agent task success while controlling for contamination.<br>‚Ä¢ Risk‚ÄëCalibrated CSD: Learning Elimination Schedules and Benchmark Orders from Deployment Logs: Fit elimination intensity (Tk) and sequencing/weights from real failure data to align Expected Win Scores with observed operational reliability.<br>‚Ä¢ Uncertainty‚ÄëAware CSD: Bayesian Estimation of Win‚ÄëRate Tensors and Confidence Intervals for E[Sm]: Model uncertainty in pairwise win rates, propagate it through the Monte Carlo, and report calibrated intervals and variance‚Äëreduced estimators.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 6</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-12-25 23:07:39 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 6;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>