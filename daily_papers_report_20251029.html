<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - October 29, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "SF Pro Display", -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Helvetica Neue", "Arial", "Noto Sans", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", sans-serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">October 29, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">InteractComp: Evaluating Search Agents With Ambiguous Queries</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24668" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24668" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Language agents have demonstrated remarkable potential in web search and information retrieval. However, these search agents assume user queries are complete and unambiguous, an assumption that diverges from reality where users begin with incomplete queries requiring clarification through interaction. Yet most agents lack interactive mechanisms during the search process, and existing benchmarks cannot assess this capability. To address this gap, we introduce InteractComp, a benchmark designed to evaluate whether search agents can recognize query ambiguity and actively interact to resolve it during search. Following the principle of easy to verify, interact to disambiguate, we construct 210 expert-curated questions across 9 domains through a target-distractor methodology that creates genuine ambiguity resolvable only through interaction. Evaluation of 17 models reveals striking failure: the best model achieves only 13.73% accuracy despite 71.50% with complete context, exposing systematic overconfidence rather than reasoning deficits. Forced interaction produces dramatic gains, demonstrating latent capability current strategies fail to engage. Longitudinal analysis shows interaction capabilities stagnated over 15 months while search performance improved seven-fold, revealing a critical blind spot. This stagnation, coupled with the immediate feedback inherent to search tasks, makes InteractComp a valuable resource for both evaluating and training interaction capabilities in search agents. The code is available at https://github.com/FoundationAgents/InteractComp.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 现实查询存在歧义且信息不完整，但现有搜索代理普遍假设查询完整，导致在未澄清的情况下武断作答、产生错误与资源浪费（图1，第2页显示：完整查询基准成绩快速提升，而交互消歧任务长期停滞）。<br>• 评测缺口突出：搜索类基准（如BrowseComp、GAIA）默认信息充分，无法衡量“识别歧义并主动澄清”的能力；交互类基准（如IN3、Tau-Bench）又缺少可验证的检索落地任务，难以量化交互对答案正确性的贡献（第3页相关工作）。<br>• 关键症结是策略而非能力：在可用交互条件下，模型仍系统性过度自信、很少发问，导致性能显著低于“提供完整上下文”时的上限；强制交互可大幅提升准确率，表明能力是潜在的但未被有效触发（表2第7页最佳仅13.73%；表3第8页给足上下文可达71.50%；强制交互实验第8-9页可翻倍）。<br>• 训练信号机会：搜索任务答案易验证且反馈清晰，适合用作强化学习等训练交互策略（第9页结论），填补“何时问、问什么”的策略学习空白。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出INTERACTCOMP基准：采用目标-干扰项构造，用共享属性生成不可直接判定的歧义问句，将区分性属性作为隐藏上下文，需通过与模拟用户的封闭式问答（yes/no/I don't know）来消歧。配合ReAct式代理与两阶段数据校验与自动评分协议，系统评估并诊断“识别歧义—主动提问—检索—作答”的全流程能力。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Ask-or-Answer: 基于不确定性校准的澄清触发学习：以校准损失与置信估计训练“何时提问”的门控策略，降低过度自信。<br>• Interact-RL: 用可验证搜索回报优化主动交互代理：将答对率与提问成本纳入奖励，强化学习最优提问数量与顺序。<br>• Ask-Search Co-Planning: 提问与检索的联合规划框架：把发问与搜索统一本体化为行动空间，学习协同策略以最小化无效检索。<br>• Robust Clarification under Noisy Users: 面向噪声与不确定反馈的稳健交互：扩展模拟器到含噪/延迟/非二元回答，并学习对抗性稳健策略。<br>• Multilingual & Multimodal InteractComp: 跨语言与多模态的歧义消解基准：将目标-干扰项扩展到图像/视频/表格与多语场景，研究跨模态澄清提问。<br>• From Forced to Free Interaction: 强制交互到自发交互的策略蒸馏：用强制交互产生的高性能轨迹做离线RL与策略蒸馏，提升自发交互质量。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Tongyi DeepResearch Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24701" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24701" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：深度研究任务需要多步推理、跨源检索与长时程规划，现有系统大多闭源、过程不可见，缺乏开放、可复现的方法与模型<br>• 能力缺口：通用基础模型缺少“代理性”归纳偏置，仅做后训练同时学习对齐与代理行为易产生优化冲突、效果次优<br>• 数据瓶颈：研究级问题与代理轨迹天然稀缺，人工标注成本高且难验证，难以依赖自然数据实现可扩展训练<br>• 环境挑战：真实Web环境非平稳、交互成本高且易失败，导致训练/评测难以稳定复现、难以归因<br>• 上下文限制：超长推理与多轮工具调用易导致上下文拥塞与注意力涣散，传统ReAct难在超长序列中保持聚焦和记忆</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出统一的端到端代理化训练范式：中训练用Agentic CPT（32K→128K）与大规模自动合成“规划—推理—决策—函数调用”数据注入代理性偏置；后训练以高质量可验证合成QA做SFT冷启动，并在沙箱化工具与异步rollout框架下进行严格on-policy的RLVR强化学习，配套“先验世界—模拟—真实”分阶段环境与模型参数加权融合。核心执行采用ReAct结合马尔可夫式上下文管理，兼顾长程任务的记忆压缩与稳态推理。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Beyond 128K：自适应上下文管理与可扩展记忆用于长程深度研究：研究更强的压缩与检索记忆、层级摘要与可学习状态重构，突破超长上下文瓶颈<br>• Partial-Rollout Off-Policy RL：提升代理化强化学习的效率与稳定性：探索部分轨迹学习、重要性采样与方差降低技术，缓解分布偏移并降低交互成本<br>• 通用代理基础模型：从深度研究到跨域工具使用的统一框架：扩展环境与工具族、制定环境调度与sim-to-real迁移策略，并引入偏好对齐提升报告忠实度与可用性</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AgentFold: Long-Horizon Web Agents with Proactive Context Management</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24699" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24699" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>LLM-based web agents show immense promise for information seeking, yet their effectiveness on long-horizon tasks is hindered by a fundamental trade-off in context management. Prevailing ReAct-based agents suffer from context saturation as they accumulate noisy, raw histories, while methods that fixedly summarize the full history at each step risk the irreversible loss of critical details. Addressing these, we introduce AgentFold, a novel agent paradigm centered on proactive context management, inspired by the human cognitive process of retrospective consolidation. AgentFold treats its context as a dynamic cognitive workspace to be actively sculpted, rather than a passive log to be filled. At each step, it learns to execute a `folding' operation, which manages its historical trajectory at multiple scales: it can perform granular condensations to preserve vital, fine-grained details, or deep consolidations to abstract away entire multi-step sub-tasks. The results on prominent benchmarks are striking: with simple supervised fine-tuning (without continual pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or matches open-source models of a dramatically larger scale, such as the DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like OpenAI's o4-mini.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：长程网页代理在“信息完整性 vs. 上下文简洁性”之间两难；追加式日志会累积大量噪声与冗余，固定频率的全史摘要又易丢失关键细节且不可逆。<br>• 重要性：上下文膨胀与噪声直接导致推理偏差、成本飙升和可交互步数受限，严重制约在 BrowseComp、WideSearch 等长周期信息检索/综合任务中的性能与可扩展性。<br>• 现有局限：ReAct 的 append-only 策略造成“上下文饱和”；MEM/固定摘要类方法每步压缩全历史，关键细节随压缩次数累积性流失；缺少把“记忆管理”纳入策略空间、可在推理中主动操作上下文的代理与相应高质量训练数据。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出 AgentFold：将上下文设计为“多尺度状态摘要 + 最近交互”的动态工作台，并在每步同时输出动作与折叠指令，通过细粒度凝练（单步）与深度合并（多步）主动雕琢历史，既保细节又控体量；配套 Fold-Generator 生成结构化轨迹并进行 SFT（基于 Qwen3-30B-A3B），在长程任务中实现高效、稳健的上下文管理与SOTA性能。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• FoldRL: 面向长程Web代理的强化学习驱动多尺度记忆折叠：以任务成功率与上下文成本为奖励，学习何时/多大范围折叠与何种摘要粒度。<br>• CertFold: 具有信息保真度保证的可验证上下文折叠：引入不确定性评估与误差上界，提供关键证据不丢失的概率性或形式化保证。<br>• CrossTask-Fold: 跨任务迁移与外部长期记忆协同的主动折叠代理：将折叠策略与向量检索/外部记忆结合，提升跨领域、跨语言与GUI环境的可迁移性与可扩展性。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">RoboOmni: Proactive Robot Manipulation in Omni-modal Context</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23763" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23763" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision-Language-Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively. In this work, we introduce cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands. To address this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction. To address the absence of training data for proactive intention recognition in robotic manipulation, we build OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 日常人机互动中的指令多为隐式，机器人需从语音语调、环境声与视觉线索中主动推断并在执行前确认意图，而非被动等待显式命令。<br>• 现有方法多依赖文本或ASR转写语音，丢失副语言信息（语调、情感、说话人身份）且忽略环境声音，导致理解歧义、时序对齐困难与额外延迟。<br>• 架构与数据双重缺口：模块化规划-控制存在接口割裂与语义漂移，端到端VLA仍围绕显式指令缺乏跨模态时空融合的意图识别；同时缺少包含语音/环境声/视觉并支持“上下文指令”推断的大规模训练与评测数据。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出RoboOmni：基于Perceiver–Thinker–Talker–Executor的一体化端到端全模态LLM，将语音（含环境声）、视觉与文本统一编码并自回归地产生对话与离散化动作（FAST+ token），在同一序列中完成意图识别、语音交互确认与动作执行；并构建含14万条样本的OmniAction数据集以训练与评测跨模态上下文指令。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• RoboOmni-Stream: 实时流式跨模态意图跟踪与低延迟执行——面向连续语音与视频的增量对齐与在线动作解码。<br>• OmniAction++: 跨语言、多设备与真实噪声环境的上下文指令基准——扩展多语种、真实录音与声学场景以评测鲁棒泛化。<br>• SafeRoboOmni: 融合形式化约束与符号规划的可验证端到端操控——在动作token解码中引入安全约束与可证明的执行保障。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23691" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23691" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Game-TARS, a generalist game agent trained with a unified, scalable action space anchored to human-aligned native keyboard-mouse inputs. Unlike API- or GUI-based approaches, this paradigm enables large-scale continual pre-training across heterogeneous domains, including OS, web, and simulation games. Game-TARS is pre-trained on over 500B tokens with diverse trajectories and multimodal data. Key techniques include a decaying continual loss to reduce causal confusion and an efficient Sparse-Thinking strategy that balances reasoning depth and inference cost. Experiments show that Game-TARS achieves about 2 times the success rate over the previous sota model on open-world Minecraft tasks, is close to the generality of fresh humans in unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet in FPS benchmarks. Scaling results on training-time and test-time confirm that the unified action space sustains improvements when scaled to cross-game and multimodal data. Our results demonstrate that simple, scalable action representations combined with large-scale pre-training provide a promising path toward generalist agents with broad computer-use abilities.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 动作空间碎片化与环境耦合：现有API/GUI/游戏内高层指令需为每个环境单独适配，难以跨游戏复用与规模化预训练，限制开放世界泛化能力。<br>• 因果对齐与训练信号失真：高频重复动作导致标准交叉熵被易样本主导并产生因果混淆；多模态录制的时间戳偏移破坏观测-动作因果链，削弱政策学习有效性。<br>• 推理与记忆低效：离线“补思考”难以还原真实认知且成本高、易幻觉；缺少可控的稀疏思考与长程记忆机制；指令遵循易受预训练动作先验和绑定习惯（键位）干扰。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出Game-TARS：以键鼠为人-原生统一动作空间，进行大规模连续预训练，结合视觉锚点因果对齐与衰减损失缓解重复动作主导，并用在线think-aloud采集的稀疏ReAct轨迹训练。随后通过动作空间自动增广+逆动力学辅助的指令遵循、RFT强化的稀疏思考、双层（上下文+摘要）记忆与跨域Agent轨迹后训练，实现跨游戏/网页/OS的通用代理。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 跨设备统一人-原生动作空间：将键鼠统一动作扩展到触控/手柄/语音输入，系统评估跨平台泛化与数据采集可拓展性<br>• 自适应稀疏思考调度：基于不确定性与推理代价的思考触发与深度控制，联合法奖励的在线RFT以降低幻觉与延迟<br>• 因果一致的连续预训练理论：面向非马尔可夫轨迹设计带收敛保证的衰减损失与采样策略，提升关键转移与多域泛化</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Uniform Discrete Diffusion with Metric Path for Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24717" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24717" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency. In this work, we revisit discrete generative modeling and present Uniform discRete diffuSion with metric pAth (URSA), a simple yet powerful framework that bridges the gap with continuous approaches for the scalable video generation. At its core, URSA formulates the video generation task as an iterative global refinement of discrete spatiotemporal tokens. It integrates two key designs: a Linearized Metric Path and a Resolution-dependent Timestep Shifting mechanism. These designs enable URSA to scale efficiently to high-resolution image synthesis and long-duration video generation, while requiring significantly fewer inference steps. Additionally, we introduce an asynchronous temporal fine-tuning strategy that unifies versatile tasks within a single model, including interpolation and image-to-video generation. Extensive experiments on challenging video and image generation benchmarks demonstrate that URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods. Code and models are available at https://github.com/baaivision/URSA</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 离散视觉生成（自回归/掩码扩散）相较连续扩散存在明显性能差距：属于“不可再细化的局部生成”，易产生误差累积，长时空一致性差，尤其在视频任务上表现不足。<br>• 长序列/高分辨率生成需要对扰动强度与时间路径进行精细控制；现有离散概率路径对不同长度与分辨率缺乏自适应，训练与采样不稳定、步数多、效率低。<br>• 离散扩散的采样误差较大，缺少类似连续扩散的全局迭代细化机制，导致画面质量与运动自然性不足。<br>• 多任务统一难：同步时步为所有帧施加相同噪声，难以兼顾局部重建与全局时序，限制了图生视频、插帧、外推等任务在单一模型中的统一。<br>• 现有视频tokenizer时空压缩与重建质量有限，进一步放大离散方法在长上下文下的脆弱性，亟需算法侧弥合与连续方法的性能差距。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>URSA提出“带度量路径的统一离散扩散”，从类别噪声出发对离散时空token进行全局迭代细化：以嵌入距离驱动的线性化度量概率路径为核心，并结合分辨率相关的时步偏移与帧级异步时刻调度，实现高分辨率与长视频的少步数高效生成及多任务统一。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Learnable Metric Path for Discrete Diffusion：将βt与距离度量设为可学习/自适应，依据内容与序列长度动态调整扰动轨迹，进一步降低采样步数与误差。<br>• End-to-End Co-Training of Tokenizer and URSA：联合优化离散视频tokenizer与度量路径/生成器，使重建保真度与时空压缩率与生成质量协同提升。<br>• Hierarchical Asynchronous Scheduling for Long-Form Video：引入镜头/片段/帧的分层异步时步与跨段一致性约束，稳定扩展至分钟级长视频并支持可控镜头语法。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Repurposing Synthetic Data for Fine-grained Search Agent Supervision</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24694" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24694" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. This critical limitation renders them unable to distinguish informative "near-miss" samples-those with substantially correct reasoning but a flawed final answer-from complete failures, thus discarding valuable learning signals. We address this by leveraging the very entities discarded during training. Our empirical analysis reveals a strong positive correlation between the number of ground-truth entities identified during an agent's reasoning process and final answer accuracy. Building on this insight, we introduce Entity-aware Group Relative Policy Optimization (E-GRPO), a novel framework that formulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to effectively learn from these "near-misses". Experiments on diverse question-answering (QA) and deep research benchmarks show that E-GRPO consistently and significantly outperforms the GRPO baseline. Furthermore, our analysis reveals that E-GRPO not only achieves superior accuracy but also induces more efficient reasoning policies that require fewer tool calls, demonstrating a more effective and sample-efficient approach to aligning search agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：基于GRPO的搜索代理采用稀疏的结果式奖励，无法区分“近似正确”的推理与“完全失败”，导致大量有价值的中间学习信号被丢弃（见图2，第5页）。<br>• 原因重要性：合成数据是实体中心生成的，但训练阶段把这些实体信息丢弃，错失可复用的细粒度、低成本监督来源；而在开放网络搜索中，轨迹长、工具调用多，稀疏奖励尤为致命，影响样本与计算效率。<br>• 现有方法局限：过程奖励模型与树搜索虽可细粒度监督，但在开放域网页环境中标注成本高、采样与计算不可扩展，难以实际落地。<br>• 关键证据：实体匹配率与正确率显著正相关，正确轨迹的实体匹配率显著高于错误轨迹（约4:1问题占比优势；见图1右上与右下，第3页）；E-GRPO在多基准上显著优于GRPO，且学到更少工具调用的高效策略（表1，第8页；表2，第9页；图3中间，第10页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出E-GRPO：将合成数据中的真值实体重用为细粒度监督，在GRPO中为错误样本按归一化实体匹配率给予部分奖励（正确为1，错误为α·γ̂，异常为0），以密集的实体感知奖励计算组相对优势并进行裁剪优化。实体匹配在思考块内做精确字符串匹配并组内归一化，配合KL-free与clip-higher策略，稳定且低开销地区分“近似正确”和“完全失败”。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 从实体匹配到关系一致性的E-GRPO：将实体关系图与推理链的一致性纳入奖励，强化多跳推理的关系链对齐与组合泛化。<br>• 自适应α调度的E-GRPO：基于样本难度与不确定性、训练进度与组内分布动态调节实体奖励权重，平衡探索效率与最终答案正确性。<br>• 过程级轻量PRM与E-GRPO协同优化：以步骤级命中实体与证据对齐为伪标签训练轻量PRM，与实体感知优势联合驱动高效、稳定的过程监督。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24563" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24563" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>With advances in decision-making and reasoning capabilities, multimodal agents show strong potential in computer application scenarios. Past evaluations have mainly assessed GUI interaction skills, while tool invocation abilities, such as those enabled by the Model Context Protocol (MCP), have been largely overlooked. Comparing agents with integrated tool invocation to those evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP, the first comprehensive and fair benchmark for assessing computer-use agents' tool invocation, GUI operation, and decision-making abilities in a real-world environment. We design a novel automated code-generation pipeline to create tools and combine them with a curated selection from existing tools. Rigorous manual validation yields 158 high-quality tools (covering 7 common applications), each verified for correct functionality, practical applicability, and versatility. Extensive evaluations of state-of-the-art multimodal agents on OSWorld-MCP show that MCP tools generally improve task success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1% to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of assessing tool invocation capabilities. However, even the strongest models have relatively low tool invocation rates, Only 36.3%, indicating room for improvement and highlighting the benchmark's challenge. By explicitly measuring MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents and sets a new standard for evaluating performance in complex, tool-assisted environments. Our code, environment, and data are publicly available at https://osworld-mcp.github.io.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 现有GUI基准忽略MCP等外部工具调用，无法公平、全面评估计算机使用代理在“工具+GUI”混合场景下的真实能力。<br>• 现实任务常可通过工具更高效稳健地完成（如一键安装扩展），仅靠GUI动作既低效又脆弱，导致评测与应用脱节。<br>• 已有MCP评测覆盖的工具数量少、功能重叠多、任务设计受可用工具束缚，缺乏对开放复杂任务与视觉GUI理解的综合考查。<br>• 缺少统一的高质量、跨应用（7类软件）且可复用的工具集与自动化构建流程，难以形成可比性与可扩展的评测基座。<br>• 缺乏衡量“何时用工具/何时用GUI”的决策能力与工具利用度的标准化指标，现有评测无法分解分析工具使用策略与效率。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出OSWorld-MCP基准：在OSWorld真实环境中将158个高质量MCP工具与GUI操作融合，代理每步可自适应选择调用工具或执行GUI动作，并以准确率、工具调用率TIR与平均完成步数ACS进行评估。工具集通过“代码生成-可用性过滤-工具封装”的自动流水线与人工甄选获得，并以RAG按应用检索相关工具以控制上下文长度与干扰。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 面向GUI–MCP混合代理的鲁棒工具链规划：层次化/课程学习与执行监督，提升多轮、多工具组合下的规划与故障恢复能力。<br>• 顺序不敏感的MCP工具提示与上下文压缩：设计顺序不敏感提示、结构化检索与压缩策略，缓解工具描述顺序与长上下文对调用率和性能的影响。<br>• 面向精确工具调用的强化学习方法：以TIR/ACS/成功率为多目标奖励，结合离线-在线RL与反事实回放，提升“何时用/用哪个/如何串联”的决策质量。<br>• 大规模自动化MCP工具合成与验证：扩展到更多应用与OS，结合单测/形式化验证与人审闭环，持续生成高质通用工具集。<br>• 面向真实OS的安全感知MCP代理：引入权限约束与安全惩罚，联动OS-Harm等基准评测，降低误操作与越权风险并提升稳健性。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24698" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24698" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel thinking faces two key challenges in this setting: inefficiency from repeatedly rolling out from scratch, and difficulty in integrating long-horizon reasoning trajectories during answer generation, as limited context capacity prevents full consideration of the reasoning process. To address these issues, we propose ParallelMuse, a two-stage paradigm designed for deep IS agents. The first stage, Functionality-Specified Partial Rollout, partitions generated sequences into functional regions and performs uncertainty-guided path reuse and branching to enhance exploration efficiency. The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information relevant to answer derivation and synthesize a coherent final answer. Experiments across multiple open-source agents and benchmarks demonstrate up to 62% performance improvement with a 10--30% reduction in exploratory token consumption.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 传统并行思考在深度信息检索代理中低效：多次从头展开重复探索，无法复用已确定的前缀，导致采样与计算预算浪费。<br>• 长程交互–推理轨迹冗长且高度冗余，受上下文窗口限制难以在生成阶段整合全部有效中间推理；多数投票与基于置信度的选择在引入外部信息后易失准、易偏置。<br>• 现有部分展开默认“令牌功能同质”，未区分推理与工具调用（探索）等功能区的不同不确定性模式（如早期探索不确定性更高、推理峰值稍后出现），无法进行按功能区的定向分支与高效扩展。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出PARALLELMUSE，两阶段：1）功能区指定的部分展开，以步骤级PPL分别度量推理/探索区不确定性，选择高不确定性步骤做异步分支并复用KV缓存以定向扩展；2）压缩式推理聚合，将每条轨迹压缩为包含计划、方法与最终合成的结构化报告，在不追加工具调用的前提下跨轨迹聚合生成答案。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Func-Branch++: 面向多工具/多行为代理的功能区不确定性驱动并行展开——将功能区从“推理/探索”扩展到更多工具行为，学习式选择分支与预算分配。<br>• GraphFuse: 基于实体–关系图的无损轨迹压缩与跨模型聚合——显式重建信息状态图，结合强聚合器进行结构化证据融合与一致性检验。<br>• Calib-Seek: 深度信息代理的置信度校准与稳健答案选择——在外部检索干扰下进行置信度校准、反多数偏置的聚合准则与不确定性估计改进。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24697" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24697" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Model (LLM)-based agents have emerged as a transformative approach for open-ended problem solving, with information seeking (IS) being a core capability that enables autonomous reasoning and decision-making. While prior research has largely focused on improving retrieval depth, we observe that current IS agents often suffer from low search efficiency, which in turn constrains overall performance. A key factor underlying this inefficiency is the sparsity of target entities in training tasks, which limits opportunities for agents to learn and generalize efficient search behaviors. To address these challenges, we propose WebLeaper, a framework for constructing high-coverage IS tasks and generating efficient solution trajectories. We formulate IS as a tree-structured reasoning problem, enabling a substantially larger set of target entities to be embedded within a constrained context. Leveraging curated Wikipedia tables, we propose three variants for synthesizing IS tasks, Basic, Union, and Reverse-Union, to systematically increase both IS efficiency and efficacy. Finally, we curate training trajectories by retaining only those that are simultaneously accurate and efficient, ensuring that the model is optimized for both correctness and search performance. Extensive experiments on both basic and comprehensive settings, conducted on five IS benchmarks, BrowserComp, GAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method consistently achieves improvements in both effectiveness and efficiency over strong baselines.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• LLM 信息检索代理检索低效：如图2（第2页）所示，有效动作比例峰值仅约0.04，普遍存在冗余查询、无关检索与过长链条。<br>• 训练数据目标实体稀疏：导致在有限上下文下难以学到高效搜索策略，并引入效率度量偏差；命题1证明 ISE 方差随目标实体数 n 增长而降至 O(1/n)。<br>• 现有方法偏重“检索深度/多步管线”，忽视搜索效率与实体覆盖度，且易被关键词捷径绕过真实合成推理。<br>• 奖励与评测不匹配：二元成功奖励在实体密集任务中过于稀疏；EM/F1 脆弱不稳，LLM 裁判成本高且一致性差，难以支撑RL优化。<br>• 结果是计算/时间成本高、训练信号弱，限制了开环信息寻址与整体表现。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出WebLeaper：将信息寻址建模为树状推理，从维基表格合成实体密集任务（Basic/Union/Reverse-Union），并以ISR与ISE筛选“既正确又高效”的轨迹进行SFT与RL训练。RL阶段使用基于软精确率/召回率的加权F-score混合奖励，稳定提升信息覆盖与搜索效率。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Auto-Scaling Entity-Rich Curricula for Web Agents：基于学习进度自适应调节实体数量与跨源难度，提升稳定性与泛化<br>• Online ISR/ISE-Aware Planning and Early-Stopping：在推理时动态估计ISR/ISE以引导查询、访问与停止策略，减少冗余动作<br>• Cross-Modal Reverse-Union beyond Wikipedia：将Union/Reverse-Union扩展到多模态与非结构化网站，验证鲁棒性与可迁移性</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24695" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24695" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Training large language model agents on tasks at the frontier of their capabilities is key to unlocking advanced reasoning. We introduce a data synthesis approach inspired by the educational theory of the Zone of Proximal Development (ZPD), which defines this frontier as tasks an LLM cannot solve alone but can master with guidance. To operationalize this, we present the AgentFrontier Engine, an automated pipeline that synthesizes high-quality, multidisciplinary data situated precisely within the LLM's ZPD. This engine supports both continued pre-training with knowledge-intensive data and targeted post-training on complex reasoning tasks. From the same framework, we derive the ZPD Exam, a dynamic and automated benchmark designed to evaluate agent capabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on our synthesized data, which achieves state-of-the-art results on demanding benchmarks like Humanity's Last Exam, even surpassing some leading proprietary agents. Our work demonstrates that a ZPD-guided approach to data synthesis offers a scalable and effective path toward building more capable LLM agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• LLM在跨文档/跨领域的知识融合与深度研究任务上能力不足，RAG在多源异构信息综合时性能显著下降，难以支撑复杂推理<br>• 训练语料缺乏系统性支持代理式能力（工具使用、自反、规划、多步推理）的高质量数据；现有专家基准（如HLE）昂贵且不可扩展，评测逐渐饱和<br>• 现有数据合成范式（query-centric与document-centric）多考察局部理解，难以生成需要多文献综合、抽象归纳与可计算验证的复杂任务<br>• 缺少精确的难度校准机制，难以将任务对齐到模型的“最近发展区（ZPD）”；启发式叠约束与自生成常停留在模型表达上限内，升级噪声大、不可持续</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出AgentFrontier Engine：一个基于ZPD的三阶段数据合成与校准框架——(1) 从多文档复合单元生成种子问答；(2) 以具备搜索/学术/浏览/代码工具的代理迭代升级问题的知识广度、抽象层次、事实校验与可计算性；(3) 通过“LKP–MKO”对抗校准筛出位于ZPD的数据（简单样本用于CPT，前沿样本用于后训练）。同时基于同一流程构建自进化的ZPD Exam，并对Qwen3-30B进行CPT+RFT以显著提升跨学科深度研究能力。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• ZPD-分级支架学习：从全轨迹示范到层级提示的自适应辅助框架——将MKO由“全解”扩展为从策略提示到子目标分解的渐进式支架<br>• 从模仿到探索：基于ZPD信号的强化学习优化研究代理——以RFT为先验、用BoN/可验证反馈做RL，弥合pass@1与pass@N性能鸿沟<br>• 代理从工具使用到工具创造：面向复杂研究任务的层级工具组合与程序合成——动态生成“元工具”与按需程序以扩展问题可解空间</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Group Relative Attention Guidance for Image Editing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24657" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24657" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recently, image editing based on Diffusion-in-Transformer models has undergone rapid development. However, existing editing methods often lack effective control over the degree of editing, limiting their ability to achieve more customized results. To address this limitation, we investigate the MM-Attention mechanism within the DiT model and observe that the Query and Key tokens share a bias vector that is only layer-dependent. We interpret this bias as representing the model's inherent editing behavior, while the delta between each token and its corresponding bias encodes the content-specific editing signals. Based on this insight, we propose Group Relative Attention Guidance, a simple yet effective method that reweights the delta values of different tokens to modulate the focus of the model on the input image relative to the editing instruction, enabling continuous and fine-grained control over editing intensity without any tuning. Extensive experiments conducted on existing image editing frameworks demonstrate that GRAG can be integrated with as few as four lines of code, consistently enhancing editing quality. Moreover, compared to the commonly used Classifier-Free Guidance, GRAG achieves smoother and more precise control over the degree of editing. Our code will be released at https://github.com/little-misfit/GRAG-Image-Editing.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 现有DiT/多模态注意力编辑方法缺乏对编辑强度的连续、细粒度控制，难以在“遵循编辑指令”与“保持原图一致性”之间取得可调节的平衡。<br>• 模型内部的MM-Attention中Query/Key嵌入存在层依赖的共享偏置向量，导致token间语义差异被稀释，降低了对具体编辑信号的灵敏度与可控性。<br>• 用户常需依赖提示工程或多次推理反复试错；常用的Classifier-Free Guidance调节不够平滑、精确，难以得到用户期望的渐进式编辑效果。<br>• 文本与图像在统一嵌入空间中呈现不同的RoPE频率分布，跨模态对齐不足也限制了精确可控的编辑表现。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出Group Relative Attention Guidance（GRAG）：在MM-Attention中以token组为单位计算组均值Kbias作为共享偏置，并用K' = λ·Kbias + δ·(K−Kbias)重加权偏置与偏差，连续地调节模型对原图与指令的侧重与聚焦强度。该方法无需训练、仅需极少代码即可集成到现有DiT编辑器中，相比CFG提供更平滑、精细的编辑强度控制。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 基于偏置感知的逐层调度用于DiT图像编辑：学习/搜索逐层λ、δ曲线，自适应平衡各层的共享偏置与token差分以提升可控性与保真度。<br>• 面向相对注意力引导的语义分组机制：以语义分割/注意力聚类替代连续token切片，实现对局部区域与对象级编辑的更精确调控。<br>• 混合CFG与GRAG的鲁棒图像编辑引导：结合全局强度控制（CFG）与相对注意力细化（GRAG），在多场景下获得更稳定、可预测的编辑轨迹。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">VisCoder2: Building Multi-Language Visualization Coding Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23642" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23642" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) have recently enabled coding agents capable of generating, executing, and revising visualization code. However, existing models often fail in practical workflows due to limited language coverage, unreliable execution, and lack of iterative correction mechanisms. Progress has been constrained by narrow datasets and benchmarks that emphasize single-round generation and single-language tasks. To address these challenges, we introduce three complementary resources for advancing visualization coding agents. VisCode-Multi-679K is a large-scale, supervised dataset containing 679K validated and executable visualization samples with multi-turn correction dialogues across 12 programming languages. VisPlotBench is a benchmark for systematic evaluation, featuring executable tasks, rendered outputs, and protocols for both initial generation and multi-round self-debug. Finally, we present VisCoder2, a family of multi-language visualization models trained on VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4.1, with further gains from iterative self-debug, reaching 82.4% overall execution pass rate at the 32B scale, particularly in symbolic or compiler-dependent languages.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 可靠性与迭代修复缺失：现有可视化代码代理常在单轮生成中出现崩溃、渲染错误或语义偏差，缺少基于执行日志与渲染结果的自我纠错机制，难以符合真实“生成—运行—修正”的工作流。<br>• 多语言覆盖不足且符号/编译型语言最脆弱：多数方法集中于 Python/Vega‑Lite，难以泛化到 LaTeX、LilyPond、Asymptote 等语法与编译严格的语言，跨生态一致性差、容错低。<br>• 数据与评测资源匮乏：公开数据多不可执行或缺少渲染输出与多轮交互，既有基准偏重单语言、单回合，无法系统衡量跨语言与多轮修复能力，制约训练与客观评估。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出三位一体技术路线：构建含 12 语言、679K 条可执行样本与 66K 多轮纠错对话的 VisCode‑Multi‑679K；发布覆盖 8 语言、888 任务并支持“执行—渲染—评分—多轮自调试”的 VisPlotBench；训练具备“生成—执行—渲染—自纠错”闭环的 VisCoder2 模型家族（基于 Qwen2.5‑Coder 微调），在 32B 规模自调试下整体执行通过率达 82.4%。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Grammar-Aware Self-Debugging for Visualization DSLs：以语法树/编译器反馈约束解码与训练，针对 LaTeX/LilyPond/Asymptote 降低结构与接口错误，稳固符号语言执行率。<br>• Execution-Grounded Reward Learning for Chart Agents：将执行日志、任务评分与视觉相似度作为奖励信号，进行RL/DPO以优化可执行性与语义-感知一致性。<br>• VisPlotBench 2.0: Multimodal and Interactive Evaluation：扩展到交互式图表、图到码/码到图、多前端渲染差异与更细粒度判分器，覆盖更多语言与库以提升外推与公平评测。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24711" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24711" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model capacity while preserving computational efficiency. Despite its notable success in large language models (LLMs), existing attempts to apply MoE to Diffusion Transformers (DiTs) have yielded limited gains. We attribute this gap to fundamental differences between language and visual tokens. Language tokens are semantically dense with pronounced inter-token variation, while visual tokens exhibit spatial redundancy and functional heterogeneity, hindering expert specialization in vision MoE. To this end, we present ProMoE, an MoE framework featuring a two-step router with explicit routing guidance that promotes expert specialization. Specifically, this guidance encourages the router to partition image tokens into conditional and unconditional sets via conditional routing according to their functional roles, and refine the assignments of conditional image tokens through prototypical routing with learnable prototypes based on semantic content. Moreover, the similarity-based expert allocation in latent space enabled by prototypical routing offers a natural mechanism for incorporating explicit semantic guidance, and we validate that such guidance is crucial for vision MoE. Building on this, we propose a routing contrastive loss that explicitly enhances the prototypical routing process, promoting intra-expert coherence and inter-expert diversity. Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives. Code and models will be made publicly available.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 现有DiT中的MoE增益有限：视觉token具有高空间冗余与功能异质性（CFG引入有/无条件两类token），导致专家难以专精、路由失衡，DiT-MoE/EC-DiT/DiffMoE提升有限或不稳定。<br>• 稠密DiT激活全部参数、计算开销大，亟需在不增加激活参数的前提下提升模型容量与生成质量的稀疏化方案。<br>• 缺乏显式语义/功能路由指导，难以同时实现“专家内一致性（相似token归一类）”与“专家间多样性（专家功能差异化）”，限制可扩展性与上限。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出ProMoE：采用带显式路由指导的两步路由。先进行条件路由，硬划分无条件token到无条件专家、有条件token进入标准专家池；再以可学习原型的余弦相似度执行原型路由（Top-K），并引入路由对比损失强化语义聚合与专家分离，同时配合共享专家学习共性。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 时空自适应原型路由：在不同扩散时刻/网络层动态调整原型与温度，提升细粒度专家分工与稳定性。<br>• 跨模态与多任务ProMoE：将条件+原型路由推广到文本-图像、视频生成与编辑任务，构建统一稀疏路由框架。<br>• 可解释与稳健的路由对比学习：加入原型可视化与鲁棒正则，提升对分布偏移与对抗噪声的路由稳定性。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24693" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24693" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning. We formalize audio 4D intelligence that is defined as reasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to measure it. STAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories. Our data curation pipeline uses two methods to ensure high-quality samples. For foundational tasks, we use procedurally synthesized and physics-simulated audio. For holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance. Unlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5\% temporal, -35.2\% spatial), evidencing its focus on linguistically hard-to-describe cues. Evaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning. Our STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 现有音频基准主要考查可由文本字幕恢复的粗粒度语义，难以衡量细粒度声学感知与深度时空推理；用“音频字幕”作答的性能几乎不下降，而在STAR-Bench上降幅显著（图1左，第2页），暴露出基准与模型对非语言化声学线索的缺口。<br>• 实际应用（embodied/机器人、安全预警、场景理解）需要“4D音频智能”（时间+三维空间）来理解声源随时间与空间的动态变化，人类可凭难以言表的声学线索完成此类判断（如水位、来车轨迹）（第2页引言）。<br>• 现有方法与评测的局限：多为单音频、静态空间任务，缺失多音频对比/整合与动态轨迹推理；缺少对六大基础属性（音高、响度、时长、方位、俯仰、距离）的定量测评；常见预处理将多通道平均为单声道，导致空间线索丢失（图3，第6页）；评测鲁棒性与人工核验不足（表1，第3页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出STAR-Bench分层基准：以程序化/物理仿真构建“基础声学感知”六属性的绝对范围与相对敏感度测评，结合真实世界音频上的“整体时空推理”（片段重排的时间推理与静态/多源关系/动态轨迹的空间推理），并配套AI+人工四阶段数据筛选与AA/ACR鲁棒评测、原生/分通道两种立体声输入策略。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Beyond-Mono: 原生多通道音频编码器用于4D空间推理：设计端到端保真处理ITD/ILD/HRTF线索的多通道LALM，避免通道平均造成的空间信息丢失，支持动态轨迹建模。<br>• Dense-AAC Pretraining for Reasoning: 面向细粒度声学线索的稠密音频字幕预训练：构建稠密音频描述数据与训练目标，桥接感知与知识，提升时间片段重排与多源空间关系推理。<br>• Multi-Audio Grounding Transformers: 面向多音频对比与对齐的跨片段推理模型：引入跨音频对齐、检索与一致性约束，提升对多段音频的比较、整合与时空定位能力。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24514" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24514" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. Inspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad. The internal visual representations of MLLMs have traditionally been confined to perceptual understanding. We repurpose them to support generative visual thought without compromising reasoning ability. Building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process. It allows the model to interleave textual reasoning with the generation of visual latents. These latents guide the internal thought process and can be translated into sketch images for interpretability. To realize this, we introduce two components: a Context-Aware Vision Head autoregressively produces visual representations, and a pretrained Sketch Decoder renders these into human-interpretable images. We evaluate the framework on our new dataset MazePlanning. Experiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance to their backbone. It further generalizes across distinct frontier MLLMs, including Gemma3 and Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our framework opens new opportunities for richer human-computer interaction and broader applications. More details and resources are available on our project page: https://latent-sketchpad.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 现有MLLM在需要精确空间推理与动态视觉定位的多步场景（如路径规划、持续状态跟踪）中表现欠佳，单纯文本CoT难以承载空间结构与时序约束。<br>• 人类推理常借助“心智草图”进行想象与规划；将视觉思维内化到模型中有望显著提升复杂多模态推理能力与可解释性。<br>• 依赖外部工具（检测器、可执行程序）的方案受工具能力与外部环境约束，闭环难、鲁棒性受限，且不易端到端一体化。<br>• 统一文生图生成模型更侧重像素逼真度而非抽象结构表达，训练代价高、与强感知型MLLM衔接成本大，不利于推理中的即时视觉想象。<br>• 现有强预训练MLLM（如 Gemma3、Qwen2.5-VL）虽具强视觉理解力，却缺乏原生视觉内容生成；其内部视觉表征未被“再利用”为生成式视觉思考通道。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出Latent Sketchpad：在预训练MLLM中加入上下文感知的Vision Head，使模型在自回归推理中可交替生成文本与视觉潜变量（通过因果交/自注意力融合全局历史图像与当前局部上下文），并用独立预训练的Sketch Decoder（AlignerNet映射至VAE潜空间、接入冻结的SDXL-VAE解码器）将潜变量可视化为草图以提升可解释性。训练仅回归潜变量且冻结骨干，实现即插即用地为Gemma3、Qwen2.5-VL等赋予“视觉思考”能力。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Scaling Visual Thought to Real-World Tasks：将潜在草图从迷宫规划扩展到室内导航、机器人抓取与多目标交互等复杂场景，验证跨任务泛化与数据合成策略。<br>• When and Where to Sketch: Learning Triggers for Visual Thought：学习式策略决定何时生成/更新视觉潜变量与可视化步数，在推理预算下实现代价–收益最优。<br>• Structure-Preserving Latent Training for Visual Reasoning：引入拓扑/几何一致性与物理可达性约束，提升潜在草图对环境结构与动力学的遵循度与OOD鲁棒性。<br>• Joint Tuning for Language–Vision Latent Alignment：从只训练头部扩展为对连接器/少量骨干的轻量联合微调，强化语言—视觉潜空间的语义与几何对齐。<br>• Uncertainty-Aware and Branching Visual Thoughts：在潜变量与草图上建模不确定性与多假设分支，支持歧义场景中的分布式规划与人机协作决策。<br>• Tool-Augmented Visual Sketching Loop：将内生草图与外部检索/仿真工具协同，使“内在可视化—外部验证—再规划”形成闭环，提高稳定性与可扩展性。<br>• Lightweight and High-Fidelity Sketch Decoding：探索扩散/Rectified Flow解码器、蒸馏与量化以兼顾可视化保真度与在线推理时延，用于实时应用。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24320" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24320" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Training critiquing language models to assess and provide feedback on model outputs is a promising way to improve LLMs for complex reasoning tasks. However, existing approaches typically rely on stronger supervisors for annotating critique data. To address this, we propose Critique-RL, an online RL approach for developing critiquing language models without stronger supervision. Our approach operates on a two-player paradigm: the actor generates a response, the critic provides feedback, and the actor refines the response accordingly. We first reveal that relying solely on indirect reward signals from the actor's outputs for RL optimization often leads to unsatisfactory critics: while their helpfulness (i.e., providing constructive feedback) improves, the discriminability (i.e., determining whether a response is high-quality or not) remains poor, resulting in marginal performance gains. To overcome this, Critique-RL adopts a two-stage optimization strategy. In stage I, it reinforces the discriminability of the critic with direct rule-based reward signals; in stage II, it introduces indirect rewards based on actor refinement to improve the critic's helpfulness, while maintaining its discriminability via appropriate regularization. Extensive experiments across various tasks and models show that Critique-RL delivers substantial performance improvements. For example, it achieves a 9.02% gain on in-domain tasks and a 5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：在没有更强监督与测试时无oracle验证的现实条件下，如何训练既能“判断好坏”（判别性）又能“给出可操作反馈”（帮助性）的批判模型，用于可扩展监督复杂推理与决策任务。<br>• 重要性：LLM在数学推理、代码与决策等高难任务上急需可靠监督；批判模型可显著提升推理准确率、抬高性能上限并更高效利用推理计算（见文中图1、表1与OOD结果）。<br>• 现有方法局限：① 依赖更强监督的SFT/数据构造成本高、难扩展，且标注分布与模型输出分布不匹配；② 仅靠提示工程常假设测试时存在oracle判别器，绕过“判别”能力，脱离oracle即遇性能瓶颈；③ 仅用结果型间接奖励（如基于改写是否正确）做RL会导致帮助性提升而判别性不足，进而出现过度保守/过度激进的失败模式，训练易陷入瓶颈甚至崩塌（文中图3分析）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出Critique-RL：在两角色（Actor生成—Critic批判—Actor改写）的在线RL框架下，对“批判模型”进行两阶段强化。Stage I用直接、基于规则的判别奖励强化Critic的判别性；Stage II引入基于Actor改写正确性的间接奖励提升帮助性，并以判别奖励与KL正则保持判别能力稳定。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 从判别到共训：联合优化Actor–Critic以提升协同演化：让Actor不再冻结，与Critic进行双向RL共训，研究稳定性、收敛性与整体收益。<br>• 开放式任务的Critique-RL-RM：引入学习型奖励模型的两阶段批判RL：以奖励模型/AI反馈替代规则判别扩展到摘要、对话等开放式任务。<br>• 不确定性感知的校准批判模型：在奖励与损失中引入置信度与校准约束，缓解保守/激进失衡并提升Acc@Dis与总体效益。<br>• 多批判者辩论与集成的两阶段RL：训练多样Critic并在推理时做辩论/集成，研究计算-性能权衡与鲁棒性提升。<br>• 跨规模弱到强的可扩展监督：用较小Critic监督更强Actor（weak-to-strong），系统分析迁移效果与泛化边界。<br>• 两阶段Critique-RL的理论保证：对判别-帮助性的目标权衡给出收敛与稳定性分析，阐明训练崩塌的避免条件。<br>• 自适应推理计算分配的Critique-RL：学习式控制response–critique–refine的预算分配与采样策略，提升计算效率与性能上限。<br>• 面向安全与对抗场景的鲁棒批判监督：设计抗分布外/对抗攻击的判别奖励与训练流程，提升实际部署可靠性。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large Reasoning Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21978" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21978" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement learning with verifiable rewards (RLVR) has delivered impressive gains in mathematical and multimodal reasoning and has become a standard post-training paradigm for contemporary language and vision-language models. However, the RLVR recipe introduces a significant risk of capability regression, where models forget foundational skills after prolonged training without employing regularization strategies. We empirically confirm this concern, observing that open-source reasoning models suffer performance degradation on core capabilities such as perception and faithfulness. While imposing regularization terms like KL divergence can help prevent deviation from the base model, these terms are calculated on the current task, thus they do not guarantee broader knowledge. Meanwhile, commonly used experience replay across heterogeneous domains makes it nontrivial to decide how much training focus each objective should receive. To address this, we propose RECAP-a replay strategy with dynamic objective reweighting for general knowledge preservation. Our reweighting mechanism adapts in an online manner using short-horizon signals of convergence and instability, shifting the post-training focus away from saturated objectives and toward underperforming or volatile ones. Our method is end-to-end and readily applicable to existing RLVR pipelines without training additional models or heavy tuning. Extensive experiments on benchmarks based on Qwen2.5-VL-3B and Qwen2.5-VL-7B demonstrate the effectiveness of our method, which not only preserves general capabilities but also improves reasoning by enabling more flexible trade-offs among in-task rewards.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：推理导向的RLVR/GRPO后训练会导致通用能力遗忘（感知、OCR、鲁棒性、事实性/安全），开源推理模型在非推理基准上普遍退步；如第2页图1展示了Qwen2.5-VL系列在A-OKVQA、VisOnly、OCRBench等上的下降，图2显示仅数学强化训练使LISA感知任务迅速劣化。<br>• 重要性：实际系统需要在不牺牲预训练获得的基础通用技能前提下提升推理，否则会带来幻觉增多与越狱脆弱性上升，影响可靠性与安全性。<br>• 现有方法局限：KL正则通常在目标任务上计算，无法保证跨域知识保留；不少推理RL管线为增塑性而减弱/移除KL，进一步加剧遗忘；经验回放/多域静态混合难以设定各目标权重，不同奖励收敛速度差异大（第7页图4显示格式奖励易快收敛并饱和），手工调权既昂贵又不可泛化（如MoDoMoDo需训练代理模型并事先获知目标性能，仍依赖手调权重）。<br>• 评测偏差：严格格式奖励与格式敏感评测易把“会按模板作答”误判为“会推理”，掩盖真实能力退化；仅结果型奖励稀疏且不稳定，易出现探索/多样性塌缩。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出RECAP（Replay-Enhanced Capability Preservation）：在RLVR训练中回放通用能力数据，并依据每个目标（奖励/损失）的短时收敛率与不稳定度，在线计算优先级并通过温度软最大化得到动态权重，对多目标联合损失进行自适应重加权。该方法端到端、量纲无关、无需额外模型，能在格式信号饱和后自动把训练重心转向欠学或高方差目标，从而同时保留通用能力并提升推理正确性。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• RECAP++：将动态重加权扩展到偏好与过程监督的统一后训练框架——把DPO/ORPO等偏好优化与PRM过程奖励纳入同一在线收敛-不稳定度调度器，系统比较不同监督粒度下的能力保留与推理增益。<br>• AutoReplay：基于不确定性与表现退化信号的跨域自适应回放采样——联合样本难度/不确定性与短窗收敛诊断，动态决定各域采样率与奖励退火策略，减少人工配比与代理模型开销。<br>• ThinkLessRL：面向高效推理的思维长度自适应控制与奖励整形——检测格式/思维奖励饱和后自适应减弱长链奖励，约束冗长CoT以降耗提速，同时保持或提升最终正确率（呼应第10页思维长度缩短的实证）。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22037" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22037" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Scaling laws research has focused overwhelmingly on English -- yet the most prominent AI models explicitly serve billions of international users. In this work, we undertake the largest multilingual scaling laws study to date, totaling 774 multilingual training experiments, spanning 10M-8B model parameters, 400+ training languages and 48 evaluation languages. We introduce the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual pretraining, which outperforms existing scaling laws' out-of-sample generalization often by more than 0.3 R^2. Our analyses of the experiments shed light on multilingual learning dynamics, transfer properties between languages, and the curse of multilinguality. First, we derive a cross-lingual transfer matrix, empirically measuring mutual benefit scores between 38 x 38=1444 language pairs. Second, we derive a language-agnostic scaling law that reveals how to optimally scale model size and data when adding languages without sacrificing performance. Third, we identify the computational crossover points for when to pretrain from scratch versus finetune from multilingual checkpoints. We hope these findings provide the scientific foundation for democratizing scaling laws across languages, and enable practitioners to efficiently scale models -- beyond English-first AI.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 英语中心的缩放律偏见：现有研究几乎只在英语上拟合与验证，但主流模型要服务数百语言，缺乏可靠的多语种缩放规律<br>• 现有方法局限：Chinchilla不考虑多轮数据重复，DCSL需跨一轮前后大量观测且难以统一拟合，MSL只按语系聚合无法建模具体语言间转移与干扰<br>• 实践痛点未解：如何量化语言两两之间的正迁移/干扰、如何设计最优多语混合比例、如何在扩大覆盖语言时保持单语性能<br>• 能力与算力权衡不清：多语词表/多语训练带来的“效率税”、模型容量导致的“多语诅咒”如何随N、D、K变化并给出可操作的扩展法则<br>• 训练路径选择困难：面对既有多语基座，何时应从零预训练，何时应从多语检查点继续单语预训练（finetune）缺少可泛化的决策准则</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出ATLAS缩放律：在标准L(N,D)框架中引入“重复感知”的有效数据Deff，将目标语言、若干可迁移语言及其它语言分项建模，并以饱和函数Sλ处理多轮数据重复，学习语言间转移权重τ以自适应跨语种迁移。基于大规模实证，构建跨语迁移矩阵，进一步给出刻画多语诅咒的K–N–D缩放律与K→rK扩语的等损失与算力最优闭式解，同时提供预训练vs微调的计算交叉点启发式。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• ATLAS-Plus：面向子词与词表自适应的一体化多语缩放律——将词表大小/脚本共享纳入Deff与τ学习，统一建模词表选择对跨语迁移的影响<br>• AutoMix-ATLAS：基于跨语转移矩阵的自适应数据混合优化——用BTS/FAS先验与ATLAS拟合，在线决策多语采样率<br>• Capacity-Aware MoE for Multilinguality：面向多语诅咒的容量分配与专家路由——在ATLAS等损约束下学习专家数与路由策略<br>• ATLAS for Low-Resource Onboarding：低资源语言增量接入的最小算力扩展法则——K→rK闭式解指导主动数据收集与训练日程<br>• From Language to Multimodality：将ATLAS扩展到多模态预训练的迁移与容量缩放律——统一文本与视觉/语音源的Deff与τ参数化<br>• Pretrain-or-Finetune 2.0：跨基座与混合差异的训练路径选择定律——推广交叉点模型，纳入不同基座训练时长与混合分布的影响</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24702" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24702" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an "interlingua" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 碎片化与异构格式阻碍大规模SFT：多源代理数据分散在不同格式、工具与接口中，难以直接合并与复用，传统做法需为每个数据集×每个代理框架定制转换，工程成本呈O(D×A)（见图2，第6页）。<br>• 数据整理复杂且成本高：人工、合成与rollout三类采集方式各有不足，质量验证困难，难以在规模、质量与多样性间取得平衡（§2.2）。<br>• 跨数据集分析与对比困难：结构与表示差异大，难以系统评估覆盖度与质量，限制了数据驱动的选择与改进（§2.2）。<br>• 缺乏通用标准导致公开大规模代理SFT稀缺：瓶颈不在于数据缺乏而在于缺标准，现有统一尝试多为任务/系统特定，难以社区级复用（摘要、引言）。<br>• 需要促进跨任务迁移与泛化：单域微调易产生负迁移；统一语料可提升跨域泛化（见表6，第10页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出Agent Data Protocol（ADP），以简洁而具表达力的Pydantic模式将代理轨迹统一为Action–Observation序列：Action涵盖API/Code/Message，Observation涵盖Text/Web，并配套自动验证与质检。通过两段式Raw→ADP→SFT枢纽辐射式管线，将多对多转换降为O(D+A)（见图1第2页、图2第6页），支持13个数据集与多代理框架无缝训练转换（见表2与表3–5）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• ADP-M：面向多模态的代理数据协议扩展——将ADP扩展至图像/视频/屏幕录制与GUI视觉语义，统一多模态Action–Observation并验证在Web/UI/桌面任务中的收益。<br>• ADP-Eval：评测与环境工件的协议化标准——将环境状态、重放脚本、快照与标注统一为可组合的评测协议，打通“数据—训练—评测”的可复现流水线。<br>• AutoADP：异构数据到ADP的自动转换与质量控制——利用LLM与程序合成自动完成Raw→ADP映射，内置一致性检查、推理覆盖率与错误检测，降低转换成本并提升数据质量。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20661" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20661" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable progress. However, two key challenges remain : 1) the absence of a large-scale high-quality UHR T2I dataset, and (2) the neglect of tailored training strategies for fine-grained detail synthesis in UHR scenarios. To tackle the first challenge, we introduce UltraHR-100K, a high-quality dataset of 100K UHR images with rich captions, offering diverse content and strong visual fidelity. Each image exceeds 3K resolution and is rigorously curated based on detail richness, content complexity, and aesthetic quality. To tackle the second challenge, we propose a frequency-aware post-training method that enhances fine-detail generation in T2I diffusion models. Specifically, we design (i) Detail-Oriented Timestep Sampling (DOTS) to focus learning on detail-critical denoising steps, and (ii) Soft-Weighting Frequency Regularization (SWFR), which leverages Discrete Fourier Transform (DFT) to softly constrain frequency components, encouraging high-frequency detail preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks demonstrate that our approach significantly improves the fine-grained detail quality and overall fidelity of UHR image generation. The code is available at https://github.com/NJU-PCALab/UltraHR-100k{here}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 缺乏开源、规模足够且质量可控的UHR文本-图像数据集，现有模型多在≤1024训练，直接扩展到4K会出现细节缺失与结构伪影，难以满足真实应用对高频细节与高保真需求（见第1页摘要与第2页引言；表2第4页对比）。<br>• 训练自由方法通过网络/推理改动实现UHR，但普遍过度平滑、细节不可信且推理耗时长，且依赖未见过UHR数据的基座模型，先天能力不足（第2页引言；相关工作2.2，第4页）。<br>• 训练型方法侧重效率而忽视细粒度细节，公开数据如Aesthetic-4K规模小且筛选标准不严，限制泛化与高质量合成（第2页对Aesthetic-4K的讨论；图3第3页、图4第5页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出UltraHR-100K（10万+张、最小3K、基于细节丰富度/内容复杂度/美学三维度严选并配Gemini 2.0长描述；参见表1第3页、图3-4）与频率感知后训练FAPT：以DOTS用Beta分布偏向后期去噪时间步强化细节（图5第5页），并用基于DFT的SWFR对频谱施加软权重约束以保留高频细节（第6页），从而显著提升4K生成的细节与保真。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 自适应频率课程学习的8K文本到图像生成：将DOTS与SWFR扩展为动态频带/时间步权重调度，实现端到端8K生成并降低训练与推理成本。<br>• 人机协同的UHR语义-美学对齐与偏好优化：在UltraHR-100K上引入人类偏好与VLM校对形成高质量多粒度标注，结合RLHF/DPO优化细节与审美。<br>• 跨域频率自适应UHR生成与评测：面向医学、遥感、工业设计等领域，提出频率自适应微调与UltraHR-eval4K++基准，系统评估跨域细节迁移与鲁棒性。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.17439" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.17439" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• VLAs在3D物理世界中执行却多基于2D编码器，存在显著的空间推理缺口，导致对新场景/背景/尺度/高度变化的泛化与适应性不足（第1–3页）<br>• 显式3D集成需专用传感器（点云/深度），获取难、成本高，与缺乏3D标注的大规模数据不匹配，且在RGB-only、RGB-D、位姿等不同模态间可迁移性差（第2页图1与相关工作2.1）<br>• 隐式3D线索（伪深度、可学习空间嵌入）空间表征弱，难以捕获可靠几何先验，且将空间嵌入拼接进VLM易破坏视觉-语言对齐，导致嵌入漂移与零样本推理退化（第2–3页）<br>• 迫切需要一种既能从RGB获得强几何先验、在可用时无缝吸收深度/位姿，又不破坏语言推理与对齐的空间信息注入范式（摘要与第3页）</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出FALCON：以空间基础模型从RGB提取稠密空间token，并通过Embodied Spatial Model可选融合深度与位姿；将空间token在Spatial-Enhanced Action Head内与VLM产生的语义动作token进行轻量融合，从动作头而非VLM主干注入空间先验，兼顾3D推理、跨模态可迁移性与语义对齐（第1–5页，图2）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 空间-语义自适应融合的动作头：引入门控/动态路由与对比-重建联合约束，自动平衡空间先验与语言语义，提升跨任务与长时序泛化<br>• 大规模无3D标注的空间先验蒸馏：从空间基础模型与合成多视角数据蒸馏几何先验到VLA，覆盖多机器人/多相机，强化零样本跨域能力<br>• 不确定性感知的ESM与主动视角选择：在ESM中建模深度/位姿不确定性与遮挡，并联动相机位姿/视角规划，实现风险敏感的稳健操纵</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24081" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24081" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>To date, there exist almost no culturally-specific evaluation benchmarks for large language models (LLMs) that cover a large number of languages and cultures. In this paper, we present Global PIQA, a participatory commonsense reasoning benchmark for over 100 languages, constructed by hand by 335 researchers from 65 countries around the world. The 116 language varieties in Global PIQA cover five continents, 14 language families, and 23 writing systems. In the non-parallel split of Global PIQA, over 50% of examples reference local foods, customs, traditions, or other culturally-specific elements. We find that state-of-the-art LLMs perform well on Global PIQA in aggregate, but they exhibit weaker performance in lower-resource languages (up to a 37% accuracy gap, despite random chance at 50%). Open models generally perform worse than proprietary models. Global PIQA highlights that in many languages and cultures, everyday knowledge remains an area for improvement, alongside more widely-discussed capabilities such as complex reasoning and expert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA provides a glimpse into the wide diversity of cultures in which human language is embedded.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：现有多语言评测多由英文数据翻译而来，缺乏覆盖本地食物、习俗、日常物件与规范的文化特定基准，导致模型在真实本地语境中的常识能力不可见。<br>• 重要性：物理常识与日常知识强依赖文化与环境（不同于数学/逻辑的跨文化统一性）；缺少文化特定评测会削弱全球可用性与公平性，并掩盖低资源语言的大幅性能落差（如西欧 vs. 撒哈拉以南非洲最高模型达95.6% vs. 80.2%，机会水平50%）。<br>• 现有方法局限：翻译基准易引入英文化偏见与翻译伪影、题目不自然；已有文化基准语言覆盖有限或偏重学科知识而非日常常识；开源与闭源模型在多语言文化常识上差距未被系统量化（最佳闭源与最强开源平均存在~9%精度差）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出Global PIQA：以参与式方式由335位研究者手工编写116种语言的非平行PIQA题（每语种100题，59.9%为文化特定），经统一清洗、英译审查、文化性标注与去重采样形成官方分割；并给出completion与prompted两种评测协议，对开源/闭源模型进行跨语言对比。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Global PIQA-Parallel：用平行数据分离“语言能力”与“文化知识”对模型表现的相对贡献<br>• 缩小低资源语言常识差距：参与式小样本扩展与高质量合成过滤的指令调优框架<br>• 开源模型的文化常识对齐：跨语言LoRA/Adapter与区域词汇增强的系统评估与方法</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Latent Chain-of-Thought for Visual Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23925" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23925" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Chain-of-thought (CoT) reasoning is critical for improving the interpretability and reliability of Large Vision-Language Models (LVLMs). However, existing training algorithms such as SFT, PPO, and GRPO may not generalize well across unseen reasoning tasks and heavily rely on a biased reward model. To address this challenge, we reformulate reasoning in LVLMs as posterior inference and propose a scalable training algorithm based on amortized variational inference. By leveraging diversity-seeking reinforcement learning algorithms, we introduce a novel sparse reward function for token-level learning signals that encourage diverse, high-likelihood latent CoT, overcoming deterministic sampling limitations and avoiding reward hacking. Additionally, we implement a Bayesian inference-scaling strategy that replaces costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank optimal rationales and answers. We empirically demonstrate that the proposed method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in terms of effectiveness, generalization, and interpretability.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 现有LVLM的CoT训练（SFT/PPO/GRPO）在跨任务泛化不足：SFT依赖教师强制仅复述参考轨迹，PPO/GRPO依赖偏置的奖励/评论器，难以建模完整的推理多样性与不确定性。<br>• 强KL约束与剪切目标抑制探索，导致模式塌缩，难以发现新颖且高质量的隐式推理链，同时存在“奖励黑客”倾向。<br>• 视觉推理链很长（~1k tokens），逐token奖励难以高效获取，现有AVI/GFlowNet在多模态长序列上的效率与稳定性受限。<br>• 推理时的BoN/Beam Search成本高、依赖外部判别器且易受偏置，缺乏概率学上的最优性与可解释性。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>将视觉链式思维视为潜变量后验推断：用GFlowNet实现摊销变分推断，训练理由采样器qθ(Z|X)使轨迹概率与联合似然成比例；提出参考引导的GFlowNet微调（RGFN）+ 稀疏-插值的token级奖励近似以提升探索与稳定性，并在推理端用贝叶斯BiN对N条潜在理由进行长度归一化边际化选择答案，无需外部判别器与BoN。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 层级化潜变量视觉CoT：从视觉锚点到步骤计划的后验分解：将Z分解为视觉定位/步骤计划/公式三层潜变量，提升可解释性与跨任务迁移。<br>• 联合训练的LaCoT：理由采样器与回答器的边际似然最大化：端到端最大化P(Y|X)并共享表示，缓解奖励偏置与幻觉，稳定采样与作答协同。<br>• 置信自适应的贝叶斯推理时扩展：动态候选与重要采样：在BiN中加入重要/分层采样与不确定性校准，按置信度动态调整N与温度，进一步降本增效。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">SPICE: Self-Play In Corpus Environments Improves Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24684" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24684" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Self-improving systems require environmental interaction for continuous adaptation. We introduce SPICE (Self-Play In Corpus Environments), a reinforcement learning framework where a single model acts in two roles: a Challenger that mines documents from a large corpus to generate diverse reasoning tasks, and a Reasoner that solves them. Through adversarial dynamics, the Challenger creates an automatic curriculum at the frontier of the Reasoner's capability, while corpus grounding provides the rich, near-inexhaustible external signal necessary for sustained improvement. Unlike existing ungrounded self-play methods that offer more limited benefits, SPICE achieves consistent gains across mathematical (+8.9%) and general reasoning (+9.8%) benchmarks on multiple model families. Our analysis reveals how document grounding is a key ingredient in SPICE to continuously generate its own increasingly challenging goals and achieve them, enabling sustained self-improvement.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 纯自博弈语言模型缺乏外部可验证信号，易出现性能平台化或训练崩塌的问题<br>• 不接地自博弈会放大幻觉：模型在自生问题与答案上训练，错误会被循环放大<br>• 出题者与解题者信息对称，导致缺乏真正挑战与自动课程，问题逐渐简单且模式化<br>• 现有RLVR多依赖人工策划题库与领域专用验证器（数学/代码），难以覆盖通用推理，受验证瓶颈制约<br>• 需要一种利用大规模文档语料作为“外部环境”，持续提供多样且可验证的挑战以实现自我提升</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>SPICE提出语料环境中的自博弈RL：同一模型在出题者与解题者两种角色间切换，出题者从大规模文档生成带语料支撑的MCQ或自由格式题目与金答案，解题者在无文档条件下作答以形成信息不对称。以方差驱动的难度奖励为出题者构建自动课程、以二元正确性奖励训练解题者，采用按角色分开的DrGRPO优势估计与通用规则验证器实现无人工标注的持续自改进。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• SPICE-M：多模态语料自博弈提升跨模态推理——将图像/表格/视频等纳入语料环境并设计统一验证器<br>• Retrieval-SPICE：检索-出题联合优化的自适应语料挖掘——端到端联训检索器与出题者，动态对齐难度前沿<br>• VerifierGym：通用可执行验证器驱动的语料自博弈——引入执行器与弱符号工具，覆盖物理、因果与知识查询<br>• Tri-Player SPICE：出题-审题-解题的多代理对抗协同——加入审题裁判代理做质量控制与鲁棒性对抗<br>• Online-SPICE：面向动态网页的在线持续学习——处理语料流式更新下的稳定-可塑平衡与灾难性遗忘<br>• Safety-SPICE：语料接地自博弈中的安全与偏差控制——构建安全奖励与过滤以抑制有害内容与数据污染</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22768" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22768" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As Large Vision-Language Models (LVLMs) are increasingly deployed in domains such as shopping, health, and news, they are exposed to pervasive persuasive content. A critical question is how these models function as persuadees-how and why they can be influenced by persuasive multimodal inputs. Understanding both their susceptibility to persuasion and the effectiveness of different persuasive strategies is crucial, as overly persuadable models may adopt misleading beliefs, override user preferences, or generate unethical or unsafe outputs when exposed to manipulative messages. We introduce MMPersuade, a unified framework for systematically studying multimodal persuasion dynamics in LVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs images and videos with established persuasion principles across commercial, subjective and behavioral, and adversarial contexts, and (ii) an evaluation framework that quantifies both persuasion effectiveness and model susceptibility via third-party agreement scoring and self-estimated token probabilities on conversation histories. Our study of six leading LVLMs as persuadees yields three key insights: (i) multimodal inputs substantially increase persuasion effectiveness-and model susceptibility-compared to text alone, especially in misinformation scenarios; (ii) stated prior preferences decrease susceptibility, yet multimodal information maintains its persuasive advantage; and (iii) different strategies vary in effectiveness across contexts, with reciprocity being most potent in commercial and subjective contexts, and credibility and logic prevailing in adversarial contexts. By jointly analyzing persuasion effectiveness and susceptibility, MMPersuade provides a principled foundation for developing models that are robust, preference-consistent, and ethically aligned when engaging with persuasive multimodal content.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 缺少系统性框架研究LVLM作为“被说服者”的多模态劝服易感性；现有研究多为文本单模态、单轮或短对话，难以反映真实多模态传播格局。<br>• 多模态劝服在购物、健康、新闻等场景普遍存在，过度易感会导致采纳错误信念、覆盖用户偏好并触发不安全输出，尤其在错误信息与对抗性场景下风险更大。<br>• 缺少大规模、理论支撑且可控变量（策略/场景/模态）的多模态数据集和基准，难以比较不同劝服策略与模态的效力差异。<br>• 评估方法与指标不足：多依赖粗粒度的最终同意度或二元成败，忽视“隐式信念变化”与“显性口头同意”的区分，也无法同时刻画劝服的时机与强度；对先验偏好（顽固度）与系统提示词等因素的影响缺乏系统检验。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出MMPERSUADE统一框架：在商业、主观/行为与对抗性场景中，将文本对话扩展为配套图像/视频的多轮多模态数据集（策略映射到Cialdini六原则与Aristotle三诉求），并以静态劝服者在三种模态设定（纯文本/文本+图像描述/真多模态）下与LVLM对话。结合第三方同意评分与自估token概率两条评估线，并以PDCG统一量化“越早越强”的劝服效果，同时控制偏好（顽固度）与策略变量进行对比分析。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 动态博弈式多模态劝服与防御：引入自适应劝服者与反劝服助手的对抗训练与评测，刻画与提升LVLM在真实交互中的鲁棒性。<br>• 面向偏好一致性的多模态对齐：将用户偏好与安全约束注入隐式/显性层（token概率与口头同意），通过校准与训练确保模型在强劝服下保持偏好一致。<br>• 利用隐式信念信号的多模态误导检测与校准：以token概率漂移与PDCG为早期信号，联合法证式可视证据分析与反事实解释，在线识别并抑制错误信息劝服。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24645" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24645" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Function calling (FC) empowers large language models (LLMs) and autonomous agents to interface with external tools, a critical capability for solving complex, real-world problems. As this ability becomes increasingly central to advanced AI systems, the need for high-quality, multi-turn training data to develop and refine it cannot be overstated. Existing data synthesis methods, such as random environment sampling or multi-agent role-playing, are not powerful enough to generate high-quality data in real-world environments. Practical challenges come in three folds: targeted model training, isolation of tool architecture, and multi-turn logical dependency. To address these structural deficiencies, we present FunReason-MT, a novel data synthesis framework for real-world multi-turn tool use. FunReason-MT resolves the complexity barrier in multi-turn FC data by employing 1) Environment-API Graph Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query Synthesis to simplify hard query construction, and 3) Guided Iterative Chain for sophisticated CoT generation. Evaluations on Berkeley Function-Calling Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built upon FunReason-MT generated data achieves state-of-the-art performance among comparable-sized models, outperforming most close-source models. Further performance improvements on BFCLv4 confirm that FunReason-MT provides a reliable and robust source for agentic learning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：缺乏可控、可信且高复杂度的多轮函数调用训练数据，导致模型在真实多工具协作与长链推理上出现性能“天花板”。<br>• 重要性：函数调用是LLM对接外部工具与构建自主代理的核心能力，高质量多轮轨迹是解锁复杂真实任务的基础。<br>• 现有局限1：随机环境采样难以覆盖长尾与稀有复杂事件，无法定向训练对目标复杂工具的协同掌握。<br>• 现有局限2：多智能体角色扮演倾向“幸福路径”，缺乏多样性与困难度，难以生成带逻辑跳跃的难查询。<br>• 现有局限3：工具体系模块化且相互隔离，直接采样难以整合前置依赖；RLLM在未知环境下生成CoT易不完整/不一致，导致多轮轨迹不稳定。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出FunReason-MT三阶段框架：先在工具-环境API关系图上做依赖合法且指向目标工具的定向采样，得到可执行多步轨迹；再将多步轨迹抽象为“高级工具”，反向合成含逻辑跳跃的硬查询；最后以“推理-校验-批判-重试”的引导式迭代链提升CoT一致性与函数调用正确性，并扩展至多轮对话。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 面向开放世界的API图数据生成：扩展Environment-API图到大规模动态工具生态，研究跨域依赖建模与在线拓展策略<br>• 基于批判信号的多轮函数调用强化学习：将Guided Iterative Chain中的误差反馈转化为可学习信号，开展端到端的RL优化<br>• 高级工具自动抽象与可迁移组合：学习跨任务的工具复合与抽象机制，实现难查询自动合成与跨领域迁移</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24591" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24591" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Frontier AI agents show increasing promise as scientific research assistants, and may eventually be useful for extended, open-ended research workflows. However, in order to use agents for novel research, we must first assess the underlying faithfulness and correctness of their work. To evaluate agents as research assistants, we introduce ReplicationBench, an evaluation framework that tests whether agents can replicate entire research papers drawn from the astrophysics literature. Astrophysics, where research relies heavily on archival data and computational study while requiring little real-world experimentation, is a particularly useful testbed for AI agents in scientific research. We split each paper into tasks which require agents to replicate the paper's core contributions, including the experimental setup, derivations, data analysis, and codebase. Each task is co-developed with the original paper authors and targets a key scientific result, enabling objective evaluation of both faithfulness (adherence to original methods) and correctness (technical accuracy of results). ReplicationBench is extremely challenging for current frontier language models: even the best-performing language models score under 20%. We analyze ReplicationBench trajectories in collaboration with domain experts and find a rich, diverse set of failure modes for agents in scientific research. ReplicationBench establishes the first benchmark of paper-scale, expert-validated astrophysics research tasks, reveals insights about agent performance generalizable to other domains of data-driven science, and provides a scalable framework for measuring AI agents' reliability in scientific research.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 缺少“论文级、端到端”的科研代理评测：现有MMLU/GPQA/HLE等仅测静态知识，SWE-Bench/PlanBench等不要求领域知识，难以衡量真实科研可靠性与长时程执行。<br>• 需要同时衡量“忠实度与正确性”：在真实论文语境中检验代理是否遵循原方法并得到数值一致结果，以降低“看起来会做、实际不会做”的风险。<br>• 现有评测难以规模化且易受作弊/污染：依赖人工rubric或原代码仓库，且论文泄漏答案导致抄录与记忆污染；缺乏系统性的手稿掩蔽与无计算基线对照。<br>• 天体物理是理想试验场却缺基准：该领域高度可复现、全流程可计算，但缺覆盖多论文、多任务的标准化复现评测；实证显示当前前沿模型在该基准上平均得分低（表2，最佳≈19%）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出ReplicationBench：选取19篇经同行评审且可复现的天体物理论文，作者共建107个客观可打分任务（扩展集再增11篇/58任务），提供掩蔽手稿、数据与执行元数据，在受控容器中让代理从零复现并提交数值结果；以作者容差进行自动化终值评分，并辅以专家评审与LLM裁判标注失败模式和控制记忆污染。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 从复现到发现：面向开放式天体物理研究的代理评测框架拓展：将固定答案的复现任务扩展到假设生成、实验设计与迭代分析，评测代理在原创研究中的有效性。<br>• 工具与记忆增强对论文级复现的因果影响研究：系统消融浏览、多模态输入、长期记忆、规划器与人类监督等能力，对复现成功率与失败模式的影响。<br>• 跨领域ReplicationBench：面向数据驱动科学的可扩展任务生成与自动评分：将框架迁移至生物、气候、材料等领域，改进人机混合任务生成与防作弊、可验证评分机制。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Rethinking Visual Intelligence: Insights from Video Pretraining</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24448" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24448" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problem-solving. We investigate Video Diffusion Models (VDMs) as a promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design a controlled evaluation in which both a pretrained LLM and a pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：视觉领域尚未像语言领域那样通过大规模预训练获得强的组合性理解、样本效率与通用问题求解能力；现有模型在抽象推理与结构化视觉理解上明显不足（见第1页摘要与第1节）。<br>• 局限性：多数工作将视觉任务转为文本或仅关注高保真生成，弱化空间与时序先验；缺少在相同适配策略下、跨模态公平对照来隔离“预训练先验”的贡献（第1–3页）。<br>• 重要性：要迈向视觉基础模型，需要能快速从极少示例习得新任务的框架；视频预训练蕴含的空间结构与时间动态先验有望提升数据效率与泛化能力，本文提供受控验证（第2页图1展示在ConceptARC多概念上的优势轮廓）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>将图像到图像任务重构为“输入→输出”的短视频过渡序列，用预训练视频扩散模型（VDM）在仅冻结骨干的前提下插入LoRA进行轻量适配；并与同样仅插入LoRA、以JSON到JSON形式训练的LLM进行对照，统一评估少样本的技能获取效率。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 基于视频预训练的视觉上下文学习：让VDM在时序提示中实现类LLM的in-context学习，提升组合性与少样本泛化<br>• 跨模态共享适配器的联合训练：构建VDM+LLM共享LoRA/提示槽位的协同框架，强化文本-视频间的可迁移推理<br>• 从过渡视频到世界模型：将VDM用于规划/控制（如路径规划、元胞自动机），研究时空先验对决策与预测的一体化建模</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Batch Speculative Decoding Done Right</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22876" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22876" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Speculative decoding speeds up LLM inference by using a small draft model to propose multiple tokens that a target model verifies in parallel. Extending this idea to batches is essential for production serving, but it introduces the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, breaking right-alignment and corrupting position IDs, attention masks, and KV-cache state. We show that several existing batch implementations violate output equivalence-the fundamental requirement that speculative decoding must produce identical token sequences to standard autoregressive generation. These violations occur precisely due to improper handling of the ragged tensor problem. In response, we (1) characterize the synchronization requirements that guarantee correctness, (2) present a correctness-first batch speculative decoding EQSPEC that exposes realignment as consuming 40% of overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences and dynamically forms same-length groups, to reduce the realignment overhead while preserving per-sequence speculative speedups. On the SpecBench dataset, across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our approach achieves up to 3times throughput improvement at batch size 8 compared to batch size 1, with efficient scaling through batch size 8, while maintaining 95% output equivalence. Our method requires no custom kernels and integrates cleanly with existing inference stacks. Our code is available at https://github.com/eBay/spec_dec.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 批量推测解码的“参差张量”问题：同一批次内各序列接收的草稿token数不同，破坏右对齐并导致位置ID、注意力mask与KV-cache不同步，使GPU无法以规则矩阵批处理（图2，第3页）。<br>• 正确性要求的缺失：推测加速必须满足与标准自回归生成的“输出等价性”，但现有批量实现（如BSP、DSD）在批量>1时出现重复词或<unk>等错误，输出被破坏（图1，第2页；表1，第7页）。<br>• 既要“对”又要“快”的矛盾：维持正确性的对齐与同步开销在批量下显著上升（位置ID、mask重算与KV-cache重排），成为吞吐瓶颈（图5b显示对齐开销最高接近40%，第7页；第4–5页算法描述）。<br>• 现有三类方案的局限：掩码/非连续位置ID方案需定制CUDA且易失序（BSP）；回退到批内最小接受长度会丢弃已验证token导致吞吐塌缩；动态填充若实现不当会出现bonus token取样错误（从草稿而非目标）、KV-cache膨胀与失对齐（DSD）（第3–4页）。<br>• 生产系统落地困难：连续批处理+分页注意力在推测下形成“嵌套参差”，使vLLM与SGLang在大并发下出现负加速（图6，第8页），亟需兼顾正确性与可扩展性的系统设计。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>论文提出EQSPEC与EXSPEC两阶段路线：EQSPEC以正确性优先，通过“Unpad–Append–Repad”最小化重对齐，严格同步位置ID、注意力mask与KV-cache，并在首个失配处由目标模型采样bonus token；EXSPEC以跨批调度减少对齐开销，维护滑动窗口并动态组成同长分组，能直接拼接免重对齐，失败时再回退EQSPEC。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• SpecServe：跨批同长分组与连续批处理的统一调度框架——将EXSPEC融入生产级continuous batching与paged attention，化解“嵌套参差”，恢复prefill–decode分离并提升大并发可扩展性。<br>• KV-Pooler：面向批推测的KV-cache亲和布局与零拷贝重对齐——设计新的KV分页、索引与搬移原语，降低ckv(B)与重排成本，缓解EXSPEC引入的局部性损失（呼应表2的时延权衡）。<br>• LenBucket+：基于长度与接受率统计的动态分桶与在线排序——从图5(c)的分组率证据出发，建立分组率-吞吐模型，在线选择窗口W、批大小B与草稿长度k，最大化同长分组率并最小化coverhead(B)。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-11">

    <div class="paper">
        <h2 class="paper-title">SAO-Instruct: Free-form Audio Editing using Natural Language Instructions</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22795" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22795" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Generative models have made significant progress in synthesizing high-fidelity audio from short textual descriptions. However, editing existing audio using natural language has remained largely underexplored. Current approaches either require the complete description of the edited audio or are constrained to predefined edit instructions that lack flexibility. In this work, we introduce SAO-Instruct, a model based on Stable Audio Open capable of editing audio clips using any free-form natural language instruction. To train our model, we create a dataset of audio editing triplets (input audio, edit instruction, output audio) using Prompt-to-Prompt, DDPM inversion, and a manual editing pipeline. Although partially trained on synthetic data, our model generalizes well to real in-the-wild audio clips and unseen edit instructions. We demonstrate that SAO-Instruct achieves competitive performance on objective metrics and outperforms other audio editing approaches in a subjective listening study. To encourage future research, we release our code and model weights.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 缺少支持“自由文本指令”的高保真音频编辑能力：现有生成模型对简短/含糊提示易偏离意图，难以做定向微调且保留整体语境（见文中对选择性编辑与背景保持的讨论）。<br>• 现有方法局限：零样本反演需完整目标描述、对措辞敏感且不直观；指令式方法多受限于固定任务集合（如ADD/REMOVE等），缺乏灵活自由的编辑表达。<br>• 技术挑战：音频具有长时序依赖与高维度特性，编辑需在局部修改的同时维持全球一致性与自然性；用户指令跨度大且常不完全指定，难以与固定操作一一映射。<br>• 数据匮乏：缺少“输入音频—编辑指令—目标音频”三元组数据，阻碍监督式学习自由指令编辑能力。<br>• 实务需求强：真实录音常含噪声/失真/构图问题，人工逐步编辑成本高，期望以自然语言快速迭代精修。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出SAO-Instruct：在Stable Audio Open上进行指令跟随微调，利用LLM生成的编辑指令和三元组数据（Prompt-to-Prompt全合成、DDPM反演半合成、确定性手工编辑）进行训练；推理时将输入音频编码并加噪作为初始状态，结合指令与时长条件去噪生成以实现选择性编辑并保留上下文。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Multilingual SAO-Instruct: Toward Cross-Lingual Free-Form Audio Editing：扩展多语言指令理解与编辑一致性，覆盖非英语场景。<br>• Chain-of-Edits for Complex Audio Workflows：支持多步复合编辑与可回溯历史，提升复杂场景编辑的可编排性。<br>• Human-in-the-Loop Bayesian Control for Edit Strength：在推理中自适应优化CFG/噪声/注意力注入以细粒度控制编辑幅度。<br>• Scene-Aware Seamless Mixing for Added Elements：学习式能量/空间/混响匹配，使新增声音与背景自然融合、减少“贴片感”。<br>• Scaling Triplet Data via Weak Supervision and Self-Training：利用弱标注与自训练扩充三元组数据规模与多样性，提升泛化与鲁棒性。<br>• Beyond CLAP: Comprehensive Metrics for Instructional Audio Editing：构建超越CLAP的编辑相关性与保真度客观指标与主观协议。<br>• Music-Specific Instruct Editing with Harmonic Constraints：面向音乐域引入和声/节奏一致性约束，支持编曲级自由指令编辑。<br>• Real-time Instruct Audio Diffusion：优化推理加速与蒸馏，实现低时延实时指令编辑与交互式预览。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22590" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22590" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In today's rapidly expanding data landscape, knowledge extraction from unstructured text is vital for real-time analytics, temporal inference, and dynamic memory frameworks. However, traditional static knowledge graph (KG) construction often overlooks the dynamic and time-sensitive nature of real-world data, limiting adaptability to continuous changes. Moreover, recent zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance on prebuilt ontologies often suffer from instability across multiple runs, as well as incomplete coverage of key facts. To address these challenges, we introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that builds and continuously updates Temporal Knowledge Graphs (TKGs) from unstructured texts. ATOM splits input documents into minimal, self-contained "atomic" facts, improving extraction exhaustivity and stability. Then, it constructs atomic TKGs from these facts while employing a dual-time modeling that distinguishes when information is observed from when it is valid. The resulting atomic TKGs are subsequently merged in parallel. Empirical evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17% better stability, and over 90% latency reduction compared to baseline methods, demonstrating a strong scalability potential for dynamic TKG construction.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 现实知识具有动态与时间敏感特性，静态KG难以支持变化追踪与时序推理，亟需能持续更新的时间知识图（TKG）。<br>• 现有零/小样本LLM建图方法在长上下文下存在“遗忘效应”，导致事实覆盖不全与信息遗漏，且跨次运行结果不稳定；论文在图2（第7页）显示上下文变长时事实与时间抽取的RMATCH显著下降。<br>• 多数方法忽略或错误处理时间维，常将“观察时间”和“事实有效期”混淆，造成时间错配；附录图F.2–F.4（第14–16页）对比显示基线将观察时间当作tstart引发误归因。<br>• 依赖LLM进行增量式实体/关系/时间消歧与合并会引发高成本、上下文爆炸与可扩展性差的问题；图3（第8页）表明ATOM相对基线方法降低了>90%构图延迟。<br>• 现有流程在大规模流式文本场景下缺乏稳定、可并行的整体架构，难以同时保证抽取的完备性、稳定性与低时延。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>ATOM先将文档分解为最小自含的“原子事实”，并行抽取时间五元组(es, rp, eo, tstart, tend)，采用双时间建模显式区分观察时间与事实有效期。随后以嵌入+阈值的无LLM并行合并完成实体/关系/时间对齐，并将“终止动作”规范化为仅更新tend的肯定式，实现可扩展的DTKG持续更新。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• FineFact：面向原子事实分解的指令微调与对齐：降低幻觉与时间遗漏，提升分解的稳定性与一致性。<br>• Learn2Resolve：监督式实体/关系消歧替代阈值合并：引入类型感知与对比学习，减少错合并并提升扩展性。<br>• TimeBench-Dual：双时间建模的时间解析基准与指标：量化评测tobs与(tstart,tend)解析的准确性与一致性。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Generalization or Memorization: Dynamic Decoding for Mode Steering</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22099" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22099" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) exhibit a troubling duality, capable of both remarkable generalization and brittle, verbatim memorization of their training data. This unpredictability undermines their reliability in high-stakes applications. In this work, we propose a unified framework to understand, identify, and control these distinct reasoning modes. First, we introduce a theoretical model based on the Information Bottleneck (IB) principle, formalizing generalization as the learning of a compressed, task-relevant representation and memorization as a failure to compress. Building on this theory, we develop Dynamic Mode Steering (DMS), a novel inference-time algorithm which comprises two components: (1) a lightweight, causally-grounded linear probe that identifies the model's instantaneous reliance on memorization, and (2) a dynamic activation steering mechanism that nudges the model's computation towards pre-identified generalization circuits. We frame DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning and faithfulness tasks demonstrate that DMS significantly improves logical consistency and factual accuracy, thereby offering a principled approach to enhancing LLM reliability.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题与重要性：LLM 在“泛化”与“记忆化”两种模式间不可预测切换，易产生逻辑不一致、机械背书式谬误与训练数据复述，削弱高风险场景下的可靠性与安全性。<br>• 理论与度量缺口：缺少统一、可操作的理论框架来刻画并区分两种模式；需要一种能在推理时刻动态识别模型所处模式的信号。<br>• 现有方法局限：标准解码仅作用于末端概率分布，无法选择内部计算路径；既有激活牵引多为静态、缺乏因果验证与自适应性；常规探针多为相关而非因果；训练期正则/微调成本高、难以覆盖已部署模型。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出动态模式牵引（DMS）：在因果定位的关键层上，用轻量线性探针实时估计“记忆化”程度，并按该分数自适应地向残差流注入由“泛化—记忆”类均值差构成的引导向量，从而把推理轨迹从记忆回路牵引至泛化回路。整体可视为一种内生的自对比式解码，训练免调整、纯推理期生效。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 无监督模式向量发现：基于稀疏字典/因子分析在激活空间自动挖掘“记忆—泛化”轴，摆脱标注与启发式标记依赖。<br>• 多属性自适应联合牵引：多探针+多向量的动态协同（如诚实、无害、推理三维），含冲突调度与稳定性控制策略。<br>• 信息瓶颈与几何统一理论：将IB与“神经塌缩”几何统一建模，给出动态牵引下的泛化界与分布外稳定性保证。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-12">

    <div class="paper">
        <h2 class="paper-title">S-Chain: Structured Visual Chain-of-Thought For Medicine</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22728" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22728" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Faithful reasoning in medical vision-language models (VLMs) requires not only accurate predictions but also transparent alignment between textual rationales and visual evidence. While Chain-of-Thought (CoT) prompting has shown promise in medical visual question answering (VQA), no large-scale expert-level dataset has captured stepwise reasoning with precise visual grounding. We introduce S-Chain, the first large-scale dataset of 12,000 expert-annotated medical images with bounding boxes and structured visual CoT (SV-CoT), explicitly linking visual regions to reasoning steps. The dataset further supports 16 languages, totaling over 700k VQA pairs for broad multilingual applicability. Using S-Chain, we benchmark state-of-the-art medical VLMs (ExGra-Med, LLaVA-Med) and general-purpose VLMs (Qwen2.5-VL, InternVL2.5), showing that SV-CoT supervision significantly improves interpretability, grounding fidelity, and robustness. Beyond benchmarking, we study its synergy with retrieval-augmented generation, revealing how domain knowledge and visual grounding interact during autoregressive reasoning. Finally, we propose a new mechanism that strengthens the alignment between visual evidence and reasoning, improving both reliability and efficiency. S-Chain establishes a new benchmark for grounded medical reasoning and paves the way toward more trustworthy and explainable medical VLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：医疗VLM在高风险场景需要“可解释且可追溯”的推理，但现有模型多为黑盒，只给最终答案而缺少将文本理由与图像证据（ROI）精确对齐的过程（见图1，第3页）。<br>• 重要性：临床决策要求从定位→描述→标准化分级（如MTA/GCA/Koedam）→诊断的可验证链路，否则难以建立信任与可审核性（第3–5页）。<br>• 现有方法局限：多数CoT数据为LLM自动生成、仅文本或弱视觉对齐，易出现幻觉/事实错误与不可靠的框选，缺乏专家核验与多语覆盖（表1，第4页；图10，第21页）。<br>• 数据瓶颈：专家级多模态、可视化落地（bounding box）的逐步推理标注昂贵稀缺，限制了可训练与可评测的数据规模与质量（第2–5页）。<br>• 训练错位：自回归训练中，模型的文字推理与真实ROI可能脱耦，导致“看似合理”的理由却未依据对应图像证据（第8–10页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出S-Chain：一个含12k图像、专家核验的结构化视觉CoT（SV-CoT）数据集，四阶段链路（Q1定位ROI框→Q2病灶描述→Q3标准化分级→Q4诊断），并以自回归SFT强制模型按步骤生成；进一步结合医学RAG与轻量对齐正则（ROI锚定InfoNCE+监督对比）强化“理由—证据”一致性。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 从2D到3D的结构化视觉链：多序列/体数据上的SV-CoT学习与评估：将四阶段SV-CoT扩展到3D体素与多序列MRI，研究对分级与诊断的系统性收益。<br>• 忠实解码器：约束ROI一致性的多模态推理与解码策略：设计强制引用已定位ROI的注意力/解码约束与对齐损失，降低幻觉并提升理由—证据一致性。<br>• 检索对齐的临床VLM：SV-CoT与动态医学RAG的联合优化：构建任务自适应检索—推理闭环，学习何时检索、检索何物与如何在链式推理中融合。<br>• 多语跨域蒸馏：用多语言SV-CoT提升低资源医学场景：研究跨语言一致性正则与教师—学生蒸馏，增强低资源语言与新模态的迁移泛化。<br>• 人机共标注的主动SV-CoT构建：降低专家负担的轻量标注框架：通过主动学习/不确定性驱动选择关键样本与关键步骤（如仅校正ROI或分级），降本增效。<br>• 可信评测基准：面向视觉—推理一致性的指标与压力测试：建立衡量CoT-ROI对齐、临床可核验性与鲁棒性的标准化指标与基准套件。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23667" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23667" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Structural topology optimization (TO) is central to engineering design but remains computationally intensive due to complex physics and hard constraints. Existing deep-learning methods are limited to fixed square grids, a few hand-coded boundary conditions, and post-hoc optimization, preventing general deployment. We introduce Optimize Any Topology (OAT), a foundation-model framework that directly predicts minimum-compliance layouts for arbitrary aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines a resolution- and shape-agnostic autoencoder with an implicit neural-field decoder and a conditional latent-diffusion model trained on OpenTO, a new corpus of 2.2 million optimized structures covering 2 million unique boundary-condition configurations. On four public benchmarks and two challenging unseen tests, OAT lowers mean compliance up to 90% relative to the best prior models and delivers sub-1 second inference on a single GPU across resolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These results establish OAT as a general, fast, and resolution-free framework for physics-aware topology optimization and provide a large-scale dataset to spur further research in generative modeling for inverse design. Code & data can be found at https://github.com/ahnobari/OptimizeAnyTopology.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 传统最小柔度拓扑优化需反复FEA与梯度迭代，计算开销大、难以支持交互式或大规模设计探索。<br>• 现有深度学习直接预测方法泛化差：局限固定方形网格与分辨率、少量手工边界条件、单载荷配置，遇到任意形状/分辨率/内载荷即失效。<br>• 表示方式受限：多依赖像素/FEA场作为输入，分辨率与形状绑定、推理慢，且常需事后迭代优化修补，难以工程部署。<br>• 数据集狭窄：公开数据规模小、配置单一，无法支撑具备“基础模型”能力的训练与评测。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出OAT：以分辨率与形状无关的自编码器（隐式神经场渲染）构造固定维潜空间，并在其中训练条件潜空间扩散模型；用BPOM将边界条件与载荷点云编码，联同体积分数、像素尺寸、长宽比嵌入实现端到端最小柔度拓扑生成，必要时辅以少步SIMP精修。训练于涵盖任意域形状/分辨率与随机内外载荷/约束的OpenTO（220万样本），实现64–256分辨率下近常数时延、亚秒级推理与显著更低柔度误差。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 优化器引导的拓扑生成扩散对齐：结合SIMP/FEA反馈与偏好优化/强化学习，降低失效率并提升物理合规性<br>• 跨物理多目标的OAT：将最小柔度扩展至应力约束、屈曲与热耦合，构建统一的多物理/多目标生成框架<br>• 少样本高效迁移的拓扑基础模型：基于LoRA/Adapter/元学习，实现对新材料、制造约束与三维场景的快速适配</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22373" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22373" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Visualization, a domain-specific yet widely used form of imagery, is an effective way to turn complex datasets into intuitive insights, and its value depends on whether data are faithfully represented, clearly communicated, and aesthetically designed. However, evaluating visualization quality is challenging: unlike natural images, it requires simultaneous judgment across data encoding accuracy, information expressiveness, and visual aesthetics. Although multimodal large language models (MLLMs) have shown promising performance in aesthetic assessment of natural images, no systematic benchmark exists for measuring their capabilities in evaluating visualizations. To address this, we propose VisJudge-Bench, the first comprehensive benchmark for evaluating MLLMs' performance in assessing visualization aesthetics and quality. It contains 3,090 expert-annotated samples from real-world scenarios, covering single visualizations, multiple visualizations, and dashboards across 32 chart types. Systematic testing on this benchmark reveals that even the most advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a correlation with human ratings of only 0.429. To address this issue, we propose VisJudge, a model specifically designed for visualization aesthetics and quality assessment. Experimental results demonstrate that VisJudge significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a 19.8% reduction) and increasing the consistency with human experts to 0.681 (a 58.7% improvement) compared to GPT-5. The benchmark is available at https://github.com/HKUSTDial/VisJudgeBench.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：现有MLLM在自然图像审美评估上表现不错，但难以同时判别数据可视化的“数据忠实度（Fidelity）—信息表达力（Expressiveness）—视觉美感（Aesthetics）”三维质量，易被“好看但误导”的图表迷惑（见图2，第2页）。<br>• 重要性：可视化广泛用于将复杂数据转化为认知与决策，质量失真会带来误导（轴缩放、配色暗示、信息过载等），影响洞察与决策可靠性（见图1，第2页）。<br>• 现有基准局限：ChartQA/ChartInsights等主要考查“读懂图表”而非“设计质量”；AVA/ArtiMuse等只评美感不评信息有效性；VisEval/VIS-Shepherd聚焦NL2VIS是否符合文本意图，忽略图表内在设计质量的全面评价（见表1，第3页）。<br>• 方法缺口：缺乏覆盖单图、多视图与仪表板、并在三维度上系统打分的真实场景基准；通用MLLM与人类专家在判断上存在显著偏差（论文报告最强基线MAE=0.551，相关系数=0.429，第1-2页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>构建VisJudge-Bench基准：围绕“忠实度-表达力-美学”的多维框架，针对单图/多图/仪表板共3090个真实样本与32类图表，采用自适应问题生成与专家标注，形成六个可量化子维度的评分体系；并提出专门的VisJudge模型用于可视化质量判别。实验证明其明显优于通用MLLM，与人类专家更一致（MAE降至0.442，相关提升至0.681，第1-2、4-5页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 从“评估”到“修复”：基于VisJudge的可视化自动改写与纠偏框架：将评审信号作为奖励/约束，闭环优化轴域、标注、布局与配色。<br>• 任务与用户情境感知的仪表板质量评估：引入任务难度、交互与受众画像，构建情境化的多维评分与解释。<br>• 跨文化与多语言的可视化审美对齐学习：扩展基准到多文化语境，研究审美差异对设计建议与评分一致性的影响。<br>• 生成-评审联合训练的可视化智能体：将NL2VIS生成器与VisJudge联合优化，实现“生成-评审-改写”的协同提升。<br>• 面向可解释判据的因果可视化评估：以因果判据拆解失真来源（轴缩放、编码冲突等），输出结构化解释与可操作修复建议。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-13">

    <div class="paper">
        <h2 class="paper-title">GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22319" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22319" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recently, GRPO-based reinforcement learning has shown remarkable progress in optimizing flow-matching models, effectively improving their alignment with task-specific rewards. Within these frameworks, the policy update relies on importance-ratio clipping to constrain overconfident positive and negative gradients. However, in practice, we observe a systematic shift in the importance-ratio distribution-its mean falls below 1 and its variance differs substantially across timesteps. This left-shifted and inconsistent distribution prevents positive-advantage samples from entering the clipped region, causing the mechanism to fail in constraining overconfident positive updates. As a result, the policy model inevitably enters an implicit over-optimization stage-while the proxy reward continues to increase, essential metrics such as image quality and text-prompt alignment deteriorate sharply, ultimately making the learned policy impractical for real-world use. To address this issue, we introduce GRPO-Guard, a simple yet effective enhancement to existing GRPO frameworks. Our method incorporates ratio normalization, which restores a balanced and step-consistent importance ratio, ensuring that PPO clipping properly constrains harmful updates across denoising timesteps. In addition, a gradient reweighting strategy equalizes policy gradients over noise conditions, preventing excessive updates from particular timestep regions. Together, these designs act as a regulated clipping mechanism, stabilizing optimization and substantially mitigating implicit over-optimization without relying on heavy KL regularization. Extensive experiments on multiple diffusion backbones (e.g., SD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard significantly reduces over-optimization while maintaining or even improving generation quality.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：GRPO在流匹配/扩散模型中的重要性比率分布先天“左移”（均值<1）且跨步长方差不一致，导致PPO裁剪对正优势样本几乎失效，训练迅速进入“隐式过优化”，代理分数上升而金指标（图像质量、文图一致性）显著下降（见第1页图1左、右；第5页图2）。<br>• 重要性：过优化会造成多样性、细节与质量崩坏，使策略在真实应用中不可用（第1页图1右可视化对比直观展示退化）。训练过程中代理-金指标背离严重，难以在线稳健监控（第7页表1与第8页图4）。<br>• 现有方法局限：依赖重KL正则虽能抑制过优化但显著拖慢学习；既有工作多关注效率或采样一致性（如TempFlowGRPO、MixGRPO、Flow-CPS），未触及裁剪失效的根因（扩散采用高斯转移概率而非LLM离散token概率，导致比率分布异常）；步间噪声与调度造成梯度不平衡，高噪声步几乎不被裁剪、低噪声步被过裁剪（第5页图2c、第12页图12）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出GRPO-Guard：在每个去噪步对log重要性比率进行标准化（RatioNorm）以将均值拉回≈0并对齐方差，使PPO裁剪能对正/负优势样本对称生效；同时加入跨步长的梯度再加权（δ=1/dt，DanceGRPO为β/dt）均衡各步梯度，抑制单一步条件主导训练，从而在无需重KL的前提下显著缓解过优化（第5-6页式(8)，第7页图3、表1）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Step-Adaptive Regulated Clipping for Flow RL：学习式/自适应的逐步裁剪区间与温度标定，进一步增强在不同采样调度与任务下的稳定性。<br>• Unified Multi-Proxy Reward for KL-free Flow Alignment：将HPSv2/ImageReward/UnifiedReward等多金指标蒸馏为高效统一奖励模型，缩小代理-金指标鸿沟，源头降低过优化。<br>• GRPO-Guard-V: Temporally Granular Clipping for Video Rectified Flows：将受控裁剪与梯度均衡扩展至时序/视频生成，结合时间粒度奖励分配（如G2RPO/TempFlowGRPO）提升时序一致性与稳健性。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21323" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21323" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The alignment of vision-language representations endows current Vision-Language Models (VLMs) with strong multi-modal reasoning capabilities. However, the interpretability of the alignment component remains uninvestigated due to the difficulty in mapping the semantics of multi-modal representations into a unified concept set. To address this problem, we propose VL-SAE, a sparse autoencoder that encodes vision-language representations into its hidden activations. Each neuron in its hidden layer correlates to a concept represented by semantically similar images and texts, thereby interpreting these representations with a unified concept set. To establish the neuron-concept correlation, we encourage semantically similar representations to exhibit consistent neuron activations during self-supervised training. First, to measure the semantic similarity of multi-modal representations, we perform their alignment in an explicit form based on cosine similarity. Second, we construct the VL-SAE with a distance-based encoder and two modality-specific decoders to ensure the activation consistency of semantically similar representations. Experiments across multiple VLMs (e.g., CLIP, LLaVA) demonstrate the superior capability of VL-SAE in interpreting and enhancing the vision-language alignment. For interpretation, the alignment between vision and language representations can be understood by comparing their semantics with concepts. For enhancement, the alignment can be strengthened by aligning vision-language representations at the concept level, contributing to performance improvements in downstream tasks, including zero-shot image classification and hallucination elimination. Codes are available at https://github.com/ssfgunner/VL-SAE.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：难以将视觉与语言表示的语义映射到“统一概念集”，导致无法直接比较两模态语义并解释VLM的对齐机制（页1–2）<br>• 重要性：视觉—语言对齐是VLM推理的核心，误对齐会引发对象幻觉等失误，限制零样本分类与多模态问答的可靠性与安全性（页1、8–9）<br>• 现有方法局限：预定义概念集覆盖不足、标注成本高、可扩展性差；可学习SAE端到端获取概念但不可控，用两套SAE各自学习会产生“概念不匹配”；共享单一SAE又受两模态分布差异与LVLM隐式对齐下“相似度难度量”的双重影响；单解码器会把模态分布信息挤入激活，降低跨模态一致性（页2、5–6）</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出VL-SAE：在同一稀疏自编码器中使用基于归一化欧氏距离的编码器与两套模态特定解码器，结合Top-K稀疏，使语义相近的图文表示获得一致激活并形成统一概念集；对LVLM先以辅助自编码器+InfoNCE将隐式对齐显式化（余弦相似可度量），再用VL-SAE学习概念并用于解释与对齐增强。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 均匀稀疏VL-SAE：抑制死神经元与高频概念的结构正则——通过均匀激活与竞争机制提升概念覆盖与稳健性<br>• 概念图对齐：基于概念关系图的跨模态推理与对齐增强——在概念层建图建模组合/因果关系以加强对齐与可解释推理<br>• 持续学习的统一概念集：面向增量任务与域迁移的自适应VL-SAE——支持概念增量与遗忘/反遗忘，稳定更新并维持跨模态一致</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20155" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20155" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Understanding objects at the level of their constituent parts is fundamental to advancing computer vision, graphics, and robotics. While datasets like PartNet have driven progress in 3D part understanding, their reliance on untextured geometries and expert-dependent annotation limits scalability and usability. We introduce PartNeXt, a next-generation dataset addressing these gaps with over 23,000 high-quality, textured 3D models annotated with fine-grained, hierarchical part labels across 50 categories. We benchmark PartNeXt on two tasks: (1) class-agnostic part segmentation, where state-of-the-art methods (e.g., PartField, SAMPart3D) struggle with fine-grained and leaf-level parts, and (2) 3D part-centric question answering, a new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary part grounding. Additionally, training Point-SAM on PartNeXt yields substantial gains over PartNet, underscoring the dataset's superior quality and diversity. By combining scalable annotation, texture-aware labels, and multi-task evaluation, PartNeXt opens new avenues for research in structured 3D understanding.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 部件级3D理解是视觉、图形与机器人任务的关键中间表示，支撑分割、生成与操控等能力（见第2页引言）。<br>• 现有主流数据集（如PartNet）依赖无纹理重网格，导致纹理缺失与几何形变，颜色/材质线索不可用，且标注需专业3D操作（画切割线、检视截面），难以众包与扩展（第2–3页）。<br>• 3D部件标注本身困难：内部/遮挡部件难可视化、界面不直观、细粒度一致性和跨类层级体系难设计与维护（第2–3页）。<br>• 现有方法在细粒度与叶节点部件上表现不佳（PartField、SAMesh、SAMPart3D），难以在弱纹理与连接区域取得兼顾语义与粒度的分割（第7–8页）。<br>• 3D多模态/LLM在开放词汇部件定位与问答上能力不足，计数与定位误差明显，难以遵循格式化输出（第8–9页，附录E）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出PartNeXt：包含23,519个高质量纹理网格、覆盖50类、35万部件的细粒度层级数据集；采用可众包的Web标注系统（层级式流程+双面板视图+连通/框选/逐面选择），直接在带纹理网格上逐面标注，并以CLIP筛选资产、GPT-4o辅助构建跨类层级与示例。基于此建立两个基准（类别无关部件实例分割与部件中心3D问答：计数/分类/定位），并验证训练Point-SAM可显著优于用PartNet训练的版本（第1、3–6、8–9页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• OpenPart-LLM: 开放词汇的跨类3D部件层级与标注框架——融合VLM自动扩展层级与同义词规范，减少人工模板依赖并提升开放词汇覆盖。<br>• TexGeo-SAM3D: 融合纹理-几何-多视图的可提示3D分割基础模型——在PartNeXt上联合点云/网格/贴图训练，攻克弱纹理与叶节点过/欠分割问题。<br>• PartReasonerQA: 面向层级结构与功能的3D多步部件推理基准——在计数/分类/定位之上引入层级路径推理、可供性与功能问答，并配套可解释评测。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-14">

    <div class="paper">
        <h2 class="paper-title">Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23828" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23828" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present a comprehensive evaluation of the ability of large language models (LLMs) to process culturally grounded language, specifically to understand and pragmatically use figurative expressions that encode local knowledge and cultural nuance. Using figurative language as a proxy for cultural nuance and local knowledge, we design evaluation tasks for contextual understanding, pragmatic use, and connotation interpretation in Arabic and English. We evaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms, multidialectal Arabic proverbs, and English proverbs. Our results show a consistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower than for English proverbs, and performance for Egyptian idioms is 10.28% lower than for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07% relative to understanding, though providing contextual idiomatic sentences improves accuracy by 10.66%. Models also struggle with connotative meaning, reaching at most 85.58% agreement with human annotators on idioms with 100% inter-annotator agreement. These findings demonstrate that figurative language serves as an effective diagnostic for cultural reasoning: while LLMs can often interpret figurative meaning, they face challenges in using it appropriately. To support future research, we release Kinayat, the first dataset of Egyptian Arabic idioms designed for both figurative understanding and pragmatic use evaluation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 缺少对LLM“语用能力”的系统评测：既有工作多停留在能否解释成语/谚语的“理解”层面，未评估在具体语境中的恰当使用与社会适配；本文量化出语用使用准确率较理解平均下降14.07%，加入上下文可提升10.66%（表3，第8页；图22，第20页）。<br>• 跨语种/方言文化推理差距未被刻画：模型在英语谚语 > 阿拉伯语谚语 > 埃及阿拉伯语成语上呈稳定层级，阿语谚语较英语低4.29%，埃及成语较阿语谚语低10.28%（摘要，第1页；图2，第6页）。<br>• 现有基准忽视“内涵/感情色彩”维度与其主观性：即便在100%人类一致样本上，模型内涵判定最高仅85.58%（成语）与74.04%（谚语），显示对隐含情感与社会规范把握不足（第7页；表11，第21页）。<br>• 多选题易受弱干扰项与否定式设定影响：传统错误选项不够迷惑，且在“选错答案”的否定设定下显著降级（成语从76.29%→70.97%，谚语从86.57%→82.71%；表6，第18页），难以真实区分理解深度。<br>• 记忆偏置与语料不均衡：谚语补全英文显著高于阿文（75.43% vs 10.65%，表7，第18页），提示模型可能依赖英语记忆而非跨文化推理，需区分记忆与推理能力。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>以成语/谚语为文化语义代理，构建跨语言统一评测框架（理解MCQ/上下文MCQ/否定式MCQ、语用使用、内涵判定、谚语补全），并在22个LLM上系统测评；提出“语用使用”新任务、采用通用+SRL两类高质干扰项生成与人工复核，发布埃及阿拉伯语成语数据集Kinayat支撑理解与语用评测。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• PragmaticsBench-Gen：多方言成语的自由生成语用评测与偏好对齐——从选择题扩展至开放式生成，构建人类偏好数据与DPO/RLHF对齐，评测恰当性、礼貌与受众适配。<br>• 检索增强的文化记忆：跨方言语用适配的RAG与工具化训练——引入地方知识库/典故检索与可解释证据，提高语用使用与内涵判定在低资源方言上的表现与可控性。<br>• 多视角内涵建模与稳健对抗评测：基于语义角色与因果图的强干扰生成——采用“透视式”多群体标签分布，结合SRL/因果扰动自动化构造强干扰，系统衡量跨文化语用鲁棒性。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22264" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22264" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Patent text embeddings enable prior art search, technology landscaping, and patent analysis, yet existing benchmarks inadequately capture patent-specific challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15 tasks across retrieval, classification, paraphrase, and clustering, with 2.06 million examples. PatenTEB employs domain-stratified splits, domain specific hard negative mining, and systematic coverage of asymmetric fragment-to-document matching scenarios absent from general embedding benchmarks. We develop the patembed model family through multi-task training, spanning 67M to 344M parameters with context lengths up to 4096 tokens. External validation shows strong generalization: patembed-base achieves state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445 previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM. Systematic ablations reveal that multi-task training improves external generalization despite minor benchmark costs, and that domain-pretrained initialization provides consistent advantages across task families. All resources will be made available at https://github.com/iliass-y/patenteb. Keywords: patent retrieval, sentence embeddings, multi-task learning, asymmetric retrieval, benchmark evaluation, contrastive learning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 缺少覆盖专利真实需求的嵌入评测：通用基准（如MTEB）无专利任务，现有专利数据集多为单任务或缺系统协议，无法评估长文、非对称片段→文档匹配与跨领域检索等关键场景（见论文§1–§2.3）。<br>• 专利文本特殊性带来建模难题：文档超长且强结构化、术语密集、语义角色（Problem/Effect/Solution）明确，跨技术领域语义迁移困难（§1）。<br>• 现有方法局限：PatentSBERTa侧重分类、PAECTER依赖引文学到文档相似、BERT-for-Patents仅域预训练；缺统一多任务框架、非对称检索覆盖和长上下文支持（§2.2）。<br>• 评测与训练易泄漏与不贴近实务：缺域分层切分与域感知硬负例，导致评测高估与不稳定（§3，Fig.1）。<br>• 基准优化与泛化脱节：单一任务最优并不等于外部泛化最优，多任务虽略降基准分但显著提升外部表现（§6.2，§7.1）。<br>• 跨领域检索显著受挫：同域(IN)到异域(OUT)性能下降达3–6倍，词汇/语义鸿沟仍是主要瓶颈（§6.2，Fig.5）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出PatenTEB基准：涵盖15个任务（检索、分类、释义、聚类），约206万样本，按IPC3域分层切分、防泄漏与域感知硬负例，并系统覆盖非对称片段→文档匹配。提出patembed模型族（67M–344M）：以专利域预训练初始化，结合多任务对比/分类损失与指令化提示统一训练，并通过知识蒸馏与长上下文变体适配不同资源；在BigPatent聚类与DAPFAM检索上取得SOTA。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 知识增强的跨领域专利检索：结合IPC本体与引文/同族图的对比学习以缩小IN/OUT域性能鸿沟<br>• 多语种与跨法域的PatenTEB-XL：构建多语专利基准与模型，评测跨语检索与家族对齐能力<br>• 角色条件化的非对称片段检索：围绕Problem/Effect/Solution设计细粒度提示与损失以提升片段→文档/片段→片段匹配<br>• 自适应多任务权重与持续学习的专利嵌入：引入动态损失权重/元学习与时间增量训练，缓解基准-泛化偏差与时间漂移<br>• 图文多模态专利嵌入：融合说明书图纸与文本，增强技术实体与结构化知识对齐以改善检索与聚类</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 14</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>← 上一页</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button><button class="page-btn" onclick="goToPage(11)">11</button><button class="page-btn" onclick="goToPage(12)">12</button><button class="page-btn" onclick="goToPage(13)">13</button><button class="page-btn" onclick="goToPage(14)">14</button></span>
            <button id="next-btn" onclick="changePage(1)">下一页 →</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-10-29 23:22:13 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 14;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>