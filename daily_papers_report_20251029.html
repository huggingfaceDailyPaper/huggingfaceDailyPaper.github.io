<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - October 29, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">October 29, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">InteractComp: Evaluating Search Agents With Ambiguous Queries</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24668" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24668" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Language agents have demonstrated remarkable potential in web search and information retrieval. However, these search agents assume user queries are complete and unambiguous, an assumption that diverges from reality where users begin with incomplete queries requiring clarification through interaction. Yet most agents lack interactive mechanisms during the search process, and existing benchmarks cannot assess this capability. To address this gap, we introduce InteractComp, a benchmark designed to evaluate whether search agents can recognize query ambiguity and actively interact to resolve it during search. Following the principle of easy to verify, interact to disambiguate, we construct 210 expert-curated questions across 9 domains through a target-distractor methodology that creates genuine ambiguity resolvable only through interaction. Evaluation of 17 models reveals striking failure: the best model achieves only 13.73% accuracy despite 71.50% with complete context, exposing systematic overconfidence rather than reasoning deficits. Forced interaction produces dramatic gains, demonstrating latent capability current strategies fail to engage. Longitudinal analysis shows interaction capabilities stagnated over 15 months while search performance improved seven-fold, revealing a critical blind spot. This stagnation, coupled with the immediate feedback inherent to search tasks, makes InteractComp a valuable resource for both evaluating and training interaction capabilities in search agents. The code is available at https://github.com/FoundationAgents/InteractComp.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Real-world queries are often incomplete and ambiguous, yet most search agents assume fully specified intent and proceed without clarification.<br>• Current agents rarely interact after search begins (typically at most one upfront clarification), leading to premature commitment, wrong answers, and wasted compute.<br>• Existing benchmarks are misaligned: search benchmarks (e.g., BrowseComp, GAIA) provide complete context upfront, while interaction benchmarks (e.g., IN3, Tau-Bench, UserBench) are not grounded in verifiable search outcomes—so they cannot assess disambiguation during search.<br>• There is no standardized way to evaluate whether agents can detect ambiguity and actively ask the right clarifying questions before retrieving and answering.<br>• Empirically, models are systematically overconfident: with full disambiguating context they reach ~68–72% accuracy, but only ~6–14% when interaction is available yet optional—indicating strategy failure rather than reasoning limits.<br>• Longitudinally, interaction capability has stagnated while web-search performance soared (seven-fold on BrowseComp); Figure 1 on page 2 shows this widening gap, revealing a critical blind spot.<br>• Search tasks provide clean, verifiable rewards, making them ideal for training uncertainty-aware, interactive agents—yet this opportunity is underutilized.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>INTERACTCOMP is a bilingual (EN/ZH) benchmark built via a target–distractor design: questions are composed from attributes shared by a lesser-known target and a popular distractor, while distinguishing attributes are hidden in a context only the simulated user can access. Agents must detect ambiguity, ask yes/no questions to elicit hidden context from a constrained responder, and then search and answer; a two-stage verification pipeline ensures questions are unanswerable without interaction but easy to verify once clarified.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Ask Before You Answer: Reinforcement Learning with Verifiable Rewards for Proactive Clarification in Web Agents: Train agents with search-outcome rewards to learn when and what to ask before committing to answers.<br>• Calibrate-to-Ask: Uncertainty Estimation for Triggering Clarifying Questions in Search Agents: Improve confidence calibration and uncertainty signals that reliably trigger interaction instead of premature answering.<br>• Query Ambiguity Detectors: Learning to Recognize When Search Alone Is Insufficient: Build detectors that flag underspecified queries and route the agent into an ask-first workflow.<br>• Minimal-Question Planning: Information-Gain–Optimized Yes/No Question Synthesis: Plan the smallest set of attribute queries that maximally disambiguate target vs. distractor under cost constraints.<br>• InteractComp++: A Multimodal, Cross-Lingual Benchmark for Ambiguity Resolution in Web Agents: Extend the benchmark to images, tables, and mixed-language contexts to stress-test interactive disambiguation.<br>• Cost-Aware Interactive Search: Adaptive Budgets and Stopping Rules for Efficient Clarification: Learn policies that balance ask/search/answer to optimize accuracy–cost trade-offs with principled stopping criteria.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Tongyi DeepResearch Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24701" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24701" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Closed-source deep-research systems and lack of open, reproducible methodology/models; intermediate research processes remain inaccessible to the community (pp. 2–3).<br>• General LLMs lack agentic inductive bias; post-training-only approaches force simultaneous alignment and agency learning, causing optimization conflicts and sub‑optimal performance (pp. 3–5).<br>• Scarcity and high cost of high-quality agentic data; research-level questions and trajectories are hard to obtain and verify; human annotation is impractical at scale (pp. 3–4, 7–8).<br>• Real-world environment instability (non‑stationarity, API latency/failures, cost) undermines scalable training and reliable diagnosis (pp. 4–5, 9–10).<br>• Long-horizon tasks exceed context windows; need principled context management to prevent overflow and maintain coherent long-term reasoning (pp. 5–6).<br>• Importance: Deep Research can autonomously perform multi-step information seeking in minutes (vs. hours for humans), promising large productivity gains and democratized research capability (p. 2).<br>• Existing frameworks often rely on brittle prompt engineering or rigid workflows that don’t scale with model capability growth (p. 5).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>An end-to-end agentic pipeline unifies Agentic Continual Pre-training (32k→128k) with supervised fine-tuning and strict on-policy GRPO-style RL, trained on a fully automated synthetic data engine and run in stage-specific environments (prior/simulated/real) with a robust tool sandbox and Markovian context management. This is illustrated by the training pipeline on page 6 (Figure 2) and the RL framework on page 9 (Figure 5); Heavy Mode (page 12, Figure 6) scales test-time via parallel research and integrative synthesis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Beyond 128K: Hierarchical Context Management for Million-Token Deep-Research Agents: Design hierarchical memory, retrieval, and compression to sustain coherent reasoning far beyond 128K tokens.<br>• Partial-Rollout and Off-Policy RL for Web Agents: Enable efficient credit assignment with truncated rollouts and stabilized off-policy updates to reduce training cost while preserving performance.<br>• Preference-Aligned Research Reporting via RLAIF/RLHF: Optimize report fidelity, usefulness, and user preference alignment with scalable preference data and outcome-grounded rewards.<br>• Sim-to-Real Transfer via Environment Scaling and Domain Randomization: Close the sim-to-real gap for browsing agents by systematically varying simulated web environments and calibrating to real APIs.<br>• General Agentic Tool Use Beyond Deep Research: Learn to autonomously discover, select, and orchestrate diverse tools (beyond search/code/scholar) for broader task families and domains.<br>• Test-Time Parallelism with Adaptive Synthesis: Improve Heavy Mode by learning adaptive selection, deduplication, and uncertainty-aware synthesis of parallel research trajectories.<br>• Self-Evolving Synthetic Data Engines for Superhuman Research: Build closed-loop data flywheels that iteratively harden tasks, verify answers, and elevate agent capabilities without human labels.<br>• Interpolative Model Merging for Capability Composition in Agents: Theorize and optimize parameter-space interpolation to compose specialized agent variants into stronger generalists.<br>• Memory-Augmented Agent Foundation Models: Integrate long-term episodic/semantic memory modules to enable persistent goals, revisitation, and cross-task learning.<br>• Safety and Robustness in Open-World Web Agents: Develop adversarial evaluation and defenses for hallucination, prompt injection, and unreliable sources in live browsing.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AgentFold: Long-Horizon Web Agents with Proactive Context Management</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24699" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24699" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>LLM-based web agents show immense promise for information seeking, yet their effectiveness on long-horizon tasks is hindered by a fundamental trade-off in context management. Prevailing ReAct-based agents suffer from context saturation as they accumulate noisy, raw histories, while methods that fixedly summarize the full history at each step risk the irreversible loss of critical details. Addressing these, we introduce AgentFold, a novel agent paradigm centered on proactive context management, inspired by the human cognitive process of retrospective consolidation. AgentFold treats its context as a dynamic cognitive workspace to be actively sculpted, rather than a passive log to be filled. At each step, it learns to execute a `folding' operation, which manages its historical trajectory at multiple scales: it can perform granular condensations to preserve vital, fine-grained details, or deep consolidations to abstract away entire multi-step sub-tasks. The results on prominent benchmarks are striking: with simple supervised fine-tuning (without continual pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or matches open-source models of a dramatically larger scale, such as the DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like OpenAI's o4-mini.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Long-horizon web agents face a core trade-off between retaining comprehensive histories and keeping context concise; append-only ReAct-style logs cause context saturation, burying key signals in noise and inflating compute/memory.<br>• Fixed, full-history summarization at every step risks premature and irreversible loss of critical details, with compounding error over many turns.<br>• Current agents lack a proactive, learnable mechanism to curate their own context (e.g., selective look-back and retrospective consolidation) akin to human cognition.<br>• There is no suitable training data showing trajectories that interleave acting with strategic context folding, hindering learning of such behaviors.<br>• Existing methods are often evaluated on simpler, shorter-horizon retrieval tasks and struggle to scale coherently to 100–500+ turns while maintaining accuracy and efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>AgentFold treats context as a dynamic workspace composed of Multi-Scale State Summaries and a high-fidelity Latest Interaction, and at each step outputs a folding directive (granular condensation or deep consolidation), an explanation, and a tool call to proactively sculpt history. It is trained via a Fold-Generator data pipeline with rejection sampling and supervised fine-tuning to internalize the folding behavior and achieve sub-linear context growth with strong benchmark performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Reinforcement Learning for Adaptive Folding Policies: Learn state- and task-dependent folding strategies that directly optimize success, retention, and compute efficiency over long horizons.<br>• Hybrid Memory AgentFold: Combine proactive folding with external long-term memory/RAG to persist knowledge across sessions and scale beyond context windows.<br>• FoldBench: A Benchmark and Metrics Suite for Proactive Context Management: Standardize evaluation of retention, abstraction quality, and efficiency for 100–500+ turn agent trajectories.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">RoboOmni: Proactive Robot Manipulation in Omni-modal Context</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23763" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23763" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision-Language-Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively. In this work, we introduce cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands. To address this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction. To address the absence of training data for proactive intention recognition in robotic manipulation, we build OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Robots in real homes rarely receive explicit commands; they must infer latent user intent from speech, environmental sounds, and visual cues.<br>• Existing VLAs depend on text or ASR-transcribed speech, losing paralinguistic information (tone, overlap, speaker identity) and typically ignoring environmental audio.<br>• Modular planner–controller pipelines are fragmented, suffer semantic drift at interfaces, and add latency; end-to-end multimodal LLMs often produce only language, not actions.<br>• There is a lack of training/evaluation data for proactive intention recognition that jointly covers speech, non-verbal sound events, and vision under realistic acoustic conditions.<br>• Current models struggle with context-dependent, ambiguous multi-object/goal tasks and cannot proactively clarify intent through natural speech interaction.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>RoboOmni is an end-to-end Perceiver–Thinker–Talker–Executor omni-modal LLM that fuses speech, environmental audio, and vision and autoregressively generates both clarification speech and robot actions using a unified token space with FAST+ discrete action tokens. Trained with a single next-token objective on the 140k-episode OmniAction dataset (six contextual instruction types), it unifies intent recognition, spoken confirmation, and execution.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• RoboOmni-RLHF: Aligning Proactive Robots with Human Preferences via Speech-Based Feedback: Incorporate online human feedback and safety constraints to refine intent inference and dialog-driven action policies.<br>• Streaming RoboOmni: Real-Time Streaming Speech–Vision–Action with Low-Latency Control: Extend to incremental, streaming multimodal processing for immediate intent detection and continuous control.<br>• RoboOmni-Personalize: Speaker- and Household-Aware Paralinguistic Grounding for Manipulation: Personalize to user timbre, prosody, and household conventions with privacy-preserving adaptation to improve intent attribution and robustness.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23691" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23691" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Game-TARS, a generalist game agent trained with a unified, scalable action space anchored to human-aligned native keyboard-mouse inputs. Unlike API- or GUI-based approaches, this paradigm enables large-scale continual pre-training across heterogeneous domains, including OS, web, and simulation games. Game-TARS is pre-trained on over 500B tokens with diverse trajectories and multimodal data. Key techniques include a decaying continual loss to reduce causal confusion and an efficient Sparse-Thinking strategy that balances reasoning depth and inference cost. Experiments show that Game-TARS achieves about 2 times the success rate over the previous sota model on open-world Minecraft tasks, is close to the generality of fresh humans in unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet in FPS benchmarks. Scaling results on training-time and test-time confirm that the unified action space sustains improvements when scaled to cross-game and multimodal data. Our results demonstrate that simple, scalable action representations combined with large-scale pre-training provide a promising path toward generalist agents with broad computer-use abilities.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Fragmented, environment- and API-specific action spaces limit generalization; a universal, human-native interface is needed to scale across OS, web, and 2D/3D games (pp. 5–6).<br>• GUI/API agents are tied to OS/app ecosystems and require heavy engineering, capping cross-domain transfer and data scalability (p. 6, Table 1 on p. 5).<br>• Retrofitting reasoning onto action logs is low-fidelity; timestamp drift breaks causality and turns models into inverse-dynamics predictors rather than decision-makers (pp. 6–7).<br>• Long, non-Markovian trajectories are dominated by repeated actions; standard cross-entropy overfits to easy repetitions, causing causal confusion in imitation learning (p. 8).<br>• Pretraining induces action-prior bias (action-space violations, behavioral inertia); agents ignore task-specific bindings without robust instruction following (p. 9).<br>• Continuous action scales (e.g., mouse sensitivity) vary widely across environments and hardware, requiring on-the-fly calibration (p. 10).<br>• Always-on chain-of-thought increases latency and hallucination risk; agents need selective, sparse reasoning at key decision points (pp. 11–12, 18).<br>• Long-horizon tasks exceed typical context windows; scalable episodic memory is required to retain task state over thousands of steps (pp. 11–12).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Define a unified, human-native device-level action space (mouseMove, mouseClick, keyPress) and train a VLM-based agent with large-scale continual pretraining on native sparse ReAct think-aloud trajectories using causal alignment and a decaying loss that down-weights repeated actions. Post-train for instruction-following and efficiency via automatic action-space augmentation, inverse-dynamics prediction, multimodal ICL and continuous-action calibration, sparse-thinking rejection fine-tuning, and a two-tier long-term memory; incorporate cross-domain agentic data to enhance generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Beyond Keyboard and Mouse: A Unified Human-Native Action Space for Touch, VR, and Gesture Interfaces: Extend the device-level paradigm to mobile multitouch, pen, haptics, and VR controllers while preserving causal, time-aligned control.<br>• Compute-Aware Deliberation: Online RL for Adaptive Sparse Thinking in Generalist Agents: Learn a policy that decides when to think versus act, optimizing task reward under token/latency budgets.<br>• Causal Trajectory Alignment at Scale: Multi-Modal Anchors for High-Frequency Agent Data: Generalize visual-anchor alignment to multi-sensor streams (cursor, eye gaze, IMU) for precise sub-frame causal syncing.<br>• Retrieval-Augmented Sparse Thoughts for Million-Step Continual Tasks: Build external, queryable memory over compressed thought summaries to sustain reasoning across ultra-long horizons.<br>• Hybrid Control Selection: Learning to Mix Device-Level Actions with Tool/MCP APIs for Efficient Computer Use: Train agents to choose between low-level inputs and high-level tools to balance reliability, speed, and flexibility.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Uniform Discrete Diffusion with Metric Path for Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24717" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24717" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency. In this work, we revisit discrete generative modeling and present Uniform discRete diffuSion with metric pAth (URSA), a simple yet powerful framework that bridges the gap with continuous approaches for the scalable video generation. At its core, URSA formulates the video generation task as an iterative global refinement of discrete spatiotemporal tokens. It integrates two key designs: a Linearized Metric Path and a Resolution-dependent Timestep Shifting mechanism. These designs enable URSA to scale efficiently to high-resolution image synthesis and long-duration video generation, while requiring significantly fewer inference steps. Additionally, we introduce an asynchronous temporal fine-tuning strategy that unifies versatile tasks within a single model, including interpolation and image-to-video generation. Extensive experiments on challenging video and image generation benchmarks demonstrate that URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods. Code and models are available at https://github.com/baaivision/URSA</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Discrete visual generators lag behind continuous diffusion due to error accumulation and long-context inconsistency, especially in video.<br>• AR and masked diffusion rely on non-refinable local generation—once tokens are produced, errors cannot be corrected—leading to low quality and unnatural motion.<br>• Poor scalability of discrete methods to high resolution and long-duration videos; lack of precise perturbation control causes many-step inference and temporal incoherence.<br>• Synchronous per-frame noise schedules underuse temporal redundancy and hinder unified multitask video generation (e.g., text-to-video, image-to-video, interpolation/extrapolation).<br>• Need a simple, efficient discrete framework that enables iterative global refinement to bridge the performance gap with continuous models while reducing inference steps.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>URSA formulates image/video generation as uniform discrete diffusion that iteratively refines global spatiotemporal tokens along a metric-induced probability path defined by codebook-embedding distances. It introduces a linearized metric path, resolution-dependent timestep shifting, and asynchronous frame-wise timestep scheduling, trained via discrete flow matching and sampled with an efficient Euler solver.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Learning Tokenizer-Aware Metric Paths for Discrete Video Diffusion: Jointly learn the distance metric and codebook so the probability path adapts to tokenizer geometry, boosting fidelity and compression.<br>• Content-Adaptive Asynchronous Scheduling for Minute-Scale Video: Predict per-frame timesteps from motion/scene saliency and use hierarchical windows to sustain coherence over very long videos.<br>• Hybrid Continuous–Discrete Generators via Unified Flow Matching: Fuse continuous latents with discrete tokens under a shared metric path to further close the quality gap with fewer inference steps.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Repurposing Synthetic Data for Fine-grained Search Agent Supervision</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24694" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24694" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. This critical limitation renders them unable to distinguish informative "near-miss" samples-those with substantially correct reasoning but a flawed final answer-from complete failures, thus discarding valuable learning signals. We address this by leveraging the very entities discarded during training. Our empirical analysis reveals a strong positive correlation between the number of ground-truth entities identified during an agent's reasoning process and final answer accuracy. Building on this insight, we introduce Entity-aware Group Relative Policy Optimization (E-GRPO), a novel framework that formulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to effectively learn from these "near-misses". Experiments on diverse question-answering (QA) and deep research benchmarks show that E-GRPO consistently and significantly outperforms the GRPO baseline. Furthermore, our analysis reveals that E-GRPO not only achieves superior accuracy but also induces more efficient reasoning policies that require fewer tool calls, demonstrating a more effective and sample-efficient approach to aligning search agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Outcome-only GRPO rewards are sparse and discard rich entity signals embedded during synthetic data generation, weakening supervision for complex web search tasks.<br>• GRPO treats all failed rollouts equally, failing to distinguish informative near-misses (many correct entities, wrong final answer) from complete failures.<br>• Sparse rewards are especially harmful for long-horizon search agents with many tool calls, leading to poor sample efficiency and suboptimal reasoning policies.<br>• Process-level supervision alternatives (PRMs, tree/branching search) are costly or impractical for open-ended, dynamic web environments.<br>• A mismatch exists between entity-centric data synthesis (which generates ground-truth entities) and RL training (which discards them), wasting fine-grained learning signals.<br>• Need a simple, robust, and computationally cheap way to densify rewards that also reduces unnecessary tool calls and encourages efficient information synthesis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>E-GRPO augments GRPO with a dense, entity-aware reward: incorrect rollouts receive partial credit α·γ̂ proportional to the normalized fraction of ground-truth entities explicitly mentioned in the agent’s thoughts, while correct rollouts get 1. This repurposes synthetic-data entities into fine-grained supervision with negligible overhead, enabling learning from near-misses and yielding more accurate, tool-efficient policies.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Semantic E-GRPO: Robust Entity-Aware Rewards with Canonicalization and Soft Matching: Replace exact string match with alias/canonicalization and lightweight semantic matching (with anti-reward-hacking safeguards) to credit entity coverage more reliably across paraphrases.<br>• Relation- and Step-aware Reward Shaping for Web Agents: Extend from entity coverage to relation/path correctness and per-step credit assignment, aligning rewards with knowledge-graph edges and timing of entity acquisition to further reduce tool calls and errors.<br>• Online E-GRPO: Learning without Synthetic Ground-Truth Entities: Mine and validate entities on-the-fly from the live web via weak/self-supervision, enabling continual adaptation and entity-aware rewards in real-world settings without pre-synthesized labels.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24563" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24563" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>With advances in decision-making and reasoning capabilities, multimodal agents show strong potential in computer application scenarios. Past evaluations have mainly assessed GUI interaction skills, while tool invocation abilities, such as those enabled by the Model Context Protocol (MCP), have been largely overlooked. Comparing agents with integrated tool invocation to those evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP, the first comprehensive and fair benchmark for assessing computer-use agents' tool invocation, GUI operation, and decision-making abilities in a real-world environment. We design a novel automated code-generation pipeline to create tools and combine them with a curated selection from existing tools. Rigorous manual validation yields 158 high-quality tools (covering 7 common applications), each verified for correct functionality, practical applicability, and versatility. Extensive evaluations of state-of-the-art multimodal agents on OSWorld-MCP show that MCP tools generally improve task success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1% to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of assessing tool invocation capabilities. However, even the strongest models have relatively low tool invocation rates, Only 36.3%, indicating room for improvement and highlighting the benchmark's challenge. By explicitly measuring MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents and sets a new standard for evaluating performance in complex, tool-assisted environments. Our code, environment, and data are publicly available at https://osworld-mcp.github.io.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Existing GUI-agent benchmarks ignore MCP tool invocation, making comparisons unfair for agents that can use external tools<br>• Lack of an integrated, real-world evaluation that jointly measures GUI actions, tool invocation, and hybrid decision-making<br>• Scarcity of high-quality, diverse, and validated MCP tools; existing servers are simple, overlapping, or task-specific<br>• Current MCP-focused benchmarks cover few tools, rely on LLM-based grading, or define tasks by available tools—creating a gap from realistic computer-use<br>• No standardized metrics to capture when tools should be invoked and how they affect efficiency and success</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>They introduce OSWorld-MCP, a dynamic benchmark where agents can interleave GUI actions with MCP tool calls, backed by 158 validated tools produced via an automated code-generation, filtering, and MCP-wrapping pipeline. The benchmark adds Tool Invocation Rate and Average Completion Steps, and uses RAG to present app-relevant tools for fair, efficient evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Learning to Compose MCP Tools for Complex Computer-Use Tasks: Train and evaluate models on multi-step, multi-tool planning and execution to improve performance on high-complexity tasks<br>• When to Click vs. When to Call: Policy Learning for Hybrid GUI+Tool Decision-Making: Develop decision policies (e.g., RL or offline RL) that optimize the choice between GUI actions and tool calls to maximize accuracy and minimize steps<br>• Tool Retrieval Matters: Optimizing RAG and Prompt Ordering for Scalable Tool Invocation: Study retrieval, summarization, and description-order effects to improve tool selection, reduce context length, and raise TIR without hurting ACS</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24698" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24698" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel thinking faces two key challenges in this setting: inefficiency from repeatedly rolling out from scratch, and difficulty in integrating long-horizon reasoning trajectories during answer generation, as limited context capacity prevents full consideration of the reasoning process. To address these issues, we propose ParallelMuse, a two-stage paradigm designed for deep IS agents. The first stage, Functionality-Specified Partial Rollout, partitions generated sequences into functional regions and performs uncertainty-guided path reuse and branching to enhance exploration efficiency. The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information relevant to answer derivation and synthesize a coherent final answer. Experiments across multiple open-source agents and benchmarks demonstrate up to 62% performance improvement with a 10--30% reduction in exploratory token consumption.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Conventional parallel thinking is inefficient in deep information-seeking (IS) agents because full from-scratch rollouts repeatedly re-generate low-uncertainty prefixes, wasting tokens and compute.<br>• Existing partial-rollout methods assume token-level functional homogeneity; they ignore the distinct behaviors and uncertainty patterns of reasoning vs. tool-use (exploration), leading to suboptimal branching.<br>• Long-horizon trajectories exceed context limits, making it hard to aggregate rich intermediate reasoning; naive truncation loses crucial planning and decomposition information.<br>• Answer selection via majority vote or self-estimated confidence fails in agentic settings due to distribution shifts from external tool responses and miscalibrated confidence; correct answers often don’t dominate numerically.<br>• Despite high redundancy in agent trajectories (many explored entities don’t contribute to the final answer), current methods don’t exploit lossless compression opportunities to integrate maximal useful reasoning within context budgets.<br>• These problems matter because deep IS agents must scale search and reasoning at test time efficiently, achieving higher accuracy with lower token cost in real-world web environments.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>ParallelMuse is a two-stage paradigm: (1) Functionality-Specified Partial Rollout segments generations into reasoning vs. exploration, measures step-level uncertainty, and launches partial branches with KV-cache reuse and asynchronous decoding to expand only high-potential steps; (2) Compressed Reasoning Aggregation losslessly condenses each trajectory into a structured report (plan, tools/params, subanswers, final reasoning) and aggregates these reports to derive the answer without majority-vote bias or extra tool calls.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• ParallelMuse-X: Scaling Agentic Parallel Thinking to Rich Tool Ecologies: Extend functionality-specified branching and aggregation to dozens of heterogeneous tools/APIs with tool-aware uncertainty, scheduling, and provenance constraints.<br>• Learning to Branch: Reinforcement Learning for Functionality-Aware Partial Rollouts: Learn a branching policy and uncertainty estimator that jointly optimize exploration gain and token budget, unifying partial rollouts with MCTS-style credit assignment.<br>• Graph-Compressed Aggregation for Long-Horizon Agents: Automatically extract and fuse information-state graphs from trajectories, using neural graph compression and graph-based aggregation to preserve planning and attribution under tight context budgets.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24697" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24697" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Model (LLM)-based agents have emerged as a transformative approach for open-ended problem solving, with information seeking (IS) being a core capability that enables autonomous reasoning and decision-making. While prior research has largely focused on improving retrieval depth, we observe that current IS agents often suffer from low search efficiency, which in turn constrains overall performance. A key factor underlying this inefficiency is the sparsity of target entities in training tasks, which limits opportunities for agents to learn and generalize efficient search behaviors. To address these challenges, we propose WebLeaper, a framework for constructing high-coverage IS tasks and generating efficient solution trajectories. We formulate IS as a tree-structured reasoning problem, enabling a substantially larger set of target entities to be embedded within a constrained context. Leveraging curated Wikipedia tables, we propose three variants for synthesizing IS tasks, Basic, Union, and Reverse-Union, to systematically increase both IS efficiency and efficacy. Finally, we curate training trajectories by retaining only those that are simultaneously accurate and efficient, ensuring that the model is optimized for both correctness and search performance. Extensive experiments on both basic and comprehensive settings, conducted on five IS benchmarks, BrowserComp, GAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method consistently achieves improvements in both effectiveness and efficiency over strong baselines.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Current IS agents search inefficiently, with a very low valid-action rate (~0.04), leading to redundant queries, long chains, higher cost, and capped performance (Fig. 2, p.2)<br>• Training tasks are entity-sparse, giving few informative cues and encouraging shortcut behaviors; small target sets (n) make ISE unstable and biased, hurting learning (Proposition 1, Sec. 2.3)<br>• Prior work emphasizes retrieval depth over efficiency and often allows keyword-based shortcuts instead of multi-source reasoning (Sec. 1, Sec. 3.1.3)<br>• Reward signals are misaligned: binary success is too sparse for entity-rich answers; EM/F1 are brittle to semantic variants; LLM-as-judge is costly and inconsistent for long entity lists (Sec. 3.3)<br>• Improving efficiency matters for both capability and cost: the paper shows consistent gains across benchmarks, indicating efficiency improvements translate to higher overall efficacy (Figure 1, p.1)</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>WebLeaper synthesizes entity-dense, tree-structured IS tasks from curated Wikipedia tables—via Basic, Union (maximal biclique-based subtree union), and Reverse-Union (fuzzed-anchor, union-based reasoning)—then trains on trajectories filtered by Information-Seeking Rate (ISR) and Information-Seeking Efficiency (ISE). It further applies RL with a hybrid soft F-score reward over entity sets (and GRPO) to jointly optimize correctness and search efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Curriculum and Active Task Synthesis for Entity-Dense Web Agents: Progressively increase entity density and union complexity with adaptive ISR/ISE thresholds and online difficulty calibration to target model weaknesses.<br>• Multimodal WebLeaper: Entity-Intensive Information Seeking across Text, Tables, and Vision: Extend tree-structured synthesis to images/PDFs/visual tables with cross-modal soft matching and anti-shortcut Reverse-Union for multimodal sources.<br>• Shortcut-Resistant Reward Shaping and Evaluation for Efficient Web Search: Develop adversarial reverse-union/counterfactual tasks and judge-light soft metrics with offline ISE estimators for scalable, stable, and robust efficiency training.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24695" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24695" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Training large language model agents on tasks at the frontier of their capabilities is key to unlocking advanced reasoning. We introduce a data synthesis approach inspired by the educational theory of the Zone of Proximal Development (ZPD), which defines this frontier as tasks an LLM cannot solve alone but can master with guidance. To operationalize this, we present the AgentFrontier Engine, an automated pipeline that synthesizes high-quality, multidisciplinary data situated precisely within the LLM's ZPD. This engine supports both continued pre-training with knowledge-intensive data and targeted post-training on complex reasoning tasks. From the same framework, we derive the ZPD Exam, a dynamic and automated benchmark designed to evaluate agent capabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on our synthesized data, which achieves state-of-the-art results on demanding benchmarks like Humanity's Last Exam, even surpassing some leading proprietary agents. Our work demonstrates that a ZPD-guided approach to data synthesis offers a scalable and effective path toward building more capable LLM agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• LLMs still struggle with in-depth, cross-domain knowledge fusion and long-horizon reasoning; existing corpora lack systematic support for agentic skills such as tool use, planning, and self-reflection.<br>• Current data-synthesis paradigms (query-centric or document-centric) mostly test localized comprehension; they do not create tasks requiring multi-document integration, so RAG pipelines degrade on heterogeneous, multi-source problems.<br>• There is no principled way to calibrate task difficulty at the capability frontier—self-generated data often remains within the model’s ceiling, and heuristic difficulty labels are coarse, hindering scalable learning at the frontier.<br>• Expert-curated frontier benchmarks (e.g., HLE) are costly and static, causing saturation and limiting continuous, scalable evaluation of advancing agents.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>AgentFrontier Engine: a three-stage, ZPD-calibrated data synthesis pipeline that builds multi-source seed QAs, iteratively escalates them via a tool-augmented agent (search, scholar, browser, code), and filters by LKP–MKO adversarial calibration with Best-of-N to keep items unsolvable by the base model but solvable with tools. It also derives ZPD Exam, a self-evolving benchmark targeting the same frontier, and trains an open model (AgentFrontier-30B-A3B) via CPT on knowledge-intensive data and post-training on verified ZPD trajectories.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Graduated Scaffolding for ZPD-Guided Agent Training: Design tiered assistance (strategic hints → subgoals → partial solutions) to replace all-or-nothing trajectories and study how scaffold granularity impacts autonomy and sample efficiency.<br>• From RFT to RL: Reinforcement Learning with ZPD-Aligned Rewards: Convert ZPD verification signals and pass@N gaps into reward functions, initializing from the RFT policy to learn exploratory, high-reward research behaviors beyond demonstrations.<br>• Agents as Tool Creators: Hierarchical Composition and Program Synthesis: Enable agents to compose meta-tools and synthesize new functions on demand, expanding beyond fixed toolsets to dynamically extend capabilities for novel problem classes.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Group Relative Attention Guidance for Image Editing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24657" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24657" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recently, image editing based on Diffusion-in-Transformer models has undergone rapid development. However, existing editing methods often lack effective control over the degree of editing, limiting their ability to achieve more customized results. To address this limitation, we investigate the MM-Attention mechanism within the DiT model and observe that the Query and Key tokens share a bias vector that is only layer-dependent. We interpret this bias as representing the model's inherent editing behavior, while the delta between each token and its corresponding bias encodes the content-specific editing signals. Based on this insight, we propose Group Relative Attention Guidance, a simple yet effective method that reweights the delta values of different tokens to modulate the focus of the model on the input image relative to the editing instruction, enabling continuous and fine-grained control over editing intensity without any tuning. Extensive experiments conducted on existing image editing frameworks demonstrate that GRAG can be integrated with as few as four lines of code, consistently enhancing editing quality. Moreover, compared to the commonly used Classifier-Free Guidance, GRAG achieves smoother and more precise control over the degree of editing. Our code will be released at https://github.com/little-misfit/GRAG-Image-Editing.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Existing DiT-based editors lack effective, continuous, fine-grained control over edit strength, forcing prompt engineering or multiple inference runs to reach desired results.<br>• It is hard to balance instruction-following with preserving the source image’s content and structure, leading to either under-editing or loss of fidelity.<br>• Common guidance (e.g., Classifier-Free Guidance) offers coarse, unsmooth control; additionally, a strong layer-dependent bias in MM-Attention Q/K embeddings dilutes token-specific signals, limiting precise controllability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Group Relative Attention Guidance (GRAG) decomposes attention keys into a group-level bias and per-token deviations, then reweights them with tunable coefficients (λ, δ) inside MM-DiT attention to modulate the balance between instruction and source. This training-free, few-line plug-in provides smooth, continuous control over editing intensity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Adaptive GRAG: Learn layer- and token-wise schedules for λ and δ across denoising timesteps to automatically trade off fidelity and edit strength.<br>• Bias-Aware Multimodal Alignment for Editing: Introduce training objectives/architectures that reduce harmful Q/K bias and better align text–image subspaces to stabilize controllable edits.<br>• GRAG for Video and 3D Editing: Extend group-relative guidance to temporal and 3D token groups for coherent, fine-grained control in video and 3D scene editing.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">VisCoder2: Building Multi-Language Visualization Coding Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23642" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23642" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) have recently enabled coding agents capable of generating, executing, and revising visualization code. However, existing models often fail in practical workflows due to limited language coverage, unreliable execution, and lack of iterative correction mechanisms. Progress has been constrained by narrow datasets and benchmarks that emphasize single-round generation and single-language tasks. To address these challenges, we introduce three complementary resources for advancing visualization coding agents. VisCode-Multi-679K is a large-scale, supervised dataset containing 679K validated and executable visualization samples with multi-turn correction dialogues across 12 programming languages. VisPlotBench is a benchmark for systematic evaluation, featuring executable tasks, rendered outputs, and protocols for both initial generation and multi-round self-debug. Finally, we present VisCoder2, a family of multi-language visualization models trained on VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4.1, with further gains from iterative self-debug, reaching 82.4% overall execution pass rate at the 32B scale, particularly in symbolic or compiler-dependent languages.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Existing visualization coding agents have limited language coverage, unreliable execution, and lack iterative self-correction, leading to frequent failures in real workflows (pp. 1–2).<br>• Current datasets/benchmarks are narrow: single language, single-round generation, many non-executable snippets, and no runtime validation or multi-turn correction signals (pp. 2–3).<br>• Real visualization practice is iterative; agents must leverage execution feedback and rendered outputs across languages, especially for brittle symbolic/compiler-dependent languages like LilyPond/LaTeX/Asymptote (pp. 2, 8–9; Table 3 on p. 9 shows large gaps on these languages).<br>• There is no standardized, multi-language execute–render–score protocol with rendered references to reliably evaluate both one-shot generation and multi-round self-debug (pp. 6–8).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Introduce a three-part framework: (1) VisCode-Multi-679K, a 679K-sample, execution-validated, multi-language dataset with multi-turn correction dialogues; (2) VisPlotBench, an 8-language, 888-task execute–render–score benchmark with multi-round self-debug; and (3) VisCoder2 models (3B–32B) that iteratively generate, execute, render, and self-debug visualization code, achieving up to 82.4% execution pass rate at 32B with self-debug (Table 3, p. 9).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Grammar-Aware Self-Debugging for Symbolic Visualization Languages: Integrate formal grammars and compiler feedback for LilyPond, LaTeX, and Asymptote to reduce structural and semantic errors highlighted in the error analysis (Table 5, p. 10).<br>• Balanced Multi-Source Data Synthesis for Low-Resource Vis Languages: Generate and curate balanced synthetic, domain-specific, and natural code to fix dataset imbalance and amplify gains observed from CoSyn and Code-Feedback in ablations (Table 6, p. 11).<br>• Beyond Execute–Render–Score: Multimodal Judges and Robust Tooling for Agents: Develop stronger visual–semantic judges and tighter runtime integration to resolve persistent renderer and environment failures that self-debugging alone could not fix (pp. 10–11).</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24711" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24711" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model capacity while preserving computational efficiency. Despite its notable success in large language models (LLMs), existing attempts to apply MoE to Diffusion Transformers (DiTs) have yielded limited gains. We attribute this gap to fundamental differences between language and visual tokens. Language tokens are semantically dense with pronounced inter-token variation, while visual tokens exhibit spatial redundancy and functional heterogeneity, hindering expert specialization in vision MoE. To this end, we present ProMoE, an MoE framework featuring a two-step router with explicit routing guidance that promotes expert specialization. Specifically, this guidance encourages the router to partition image tokens into conditional and unconditional sets via conditional routing according to their functional roles, and refine the assignments of conditional image tokens through prototypical routing with learnable prototypes based on semantic content. Moreover, the similarity-based expert allocation in latent space enabled by prototypical routing offers a natural mechanism for incorporating explicit semantic guidance, and we validate that such guidance is crucial for vision MoE. Building on this, we propose a routing contrastive loss that explicitly enhances the prototypical routing process, promoting intra-expert coherence and inter-expert diversity. Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives. Code and models will be made publicly available.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• MoE underperforms in Diffusion Transformers compared to LLMs: token-choice (DiT-MoE) and expert-choice (EC-DiT) routing yield limited or marginal gains, and recent global token-selection schemes (DiffMoE) still struggle.<br>• Visual tokens are semantically diffuse and highly redundant, unlike language tokens that are dense and separable, leading to weak expert specialization and homogeneous experts.<br>• Classifier-free guidance introduces functional heterogeneity (conditional vs. unconditional tokens), but naive routers treat them uniformly, hindering specialization.<br>• Lack of explicit semantic routing guidance in vision MoE prevents intra-expert coherence and inter-expert diversity.<br>• Need to scale DiTs’ capacity efficiently (fixed activated parameters) while improving quality under modern objectives (Rectified Flow and DDPM).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>ProMoE introduces a two-step router with explicit guidance: (1) conditional routing that hard-splits tokens into unconditional tokens processed by dedicated unconditional experts and conditional tokens routed to standard experts; (2) prototypical routing that assigns conditional tokens via cosine similarity to learnable prototypes tied to experts, using identity activation with top-K selection. A routing contrastive loss pulls each expert’s prototype toward the centroid of its assigned tokens and pushes it from others, enforcing intra-expert coherence and inter-expert diversity; shared experts capture common knowledge while keeping activated parameters constant.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Multi-Modal ProMoE: Explicitly Guided Routing for Text–Image–Video Diffusion: Extend conditional/prototypical routing to modality-aware experts with cross-modal prototypes and masks.<br>• Hierarchical Prototypical Routing for MoE Diffusion Transformers: Introduce multi-level prototypes (global/local, layer- and timestep-aware) to capture fine-to-coarse semantics.<br>• Curriculum CFG-Aware Routing for Diffusion MoE: Dynamically schedule conditional vs. unconditional routing strength across timesteps/CFG scales during training and inference.<br>• Self-Supervised Semantic Routing without Fixed Prototypes: Learn token clusters and routing via contrastive/cluster discovery across timesteps, removing explicit prototype parameters.<br>• Cross-Layer and Cross-Timestep Routing Consistency in MoE DiTs: Regularize assignment stability across layers/timesteps to preserve expert roles and reduce reassignment noise.<br>• Token Sparsification Meets Expert Routing: Joint Patch Pruning and MoE for DiTs: Co-design token pruning with routing to mitigate visual redundancy and cut compute.<br>• Systems-Efficient ProMoE: Expert Parallelism and Low-Latency Inference for Two-Step Routers: Optimize distributed training/inference and memory layouts for large-scale experts.<br>• ProMoE at Ultra-High Resolution and Long-Range Generation: Adapt prototypes and routing to spatial/temporal long-context settings (e.g., 1024^2 images, video) with locality-aware experts.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24693" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24693" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning. We formalize audio 4D intelligence that is defined as reasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to measure it. STAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories. Our data curation pipeline uses two methods to ensure high-quality samples. For foundational tasks, we use procedurally synthesized and physics-simulated audio. For holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance. Unlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5\% temporal, -35.2\% spatial), evidencing its focus on linguistically hard-to-describe cues. Evaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning. Our STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Existing audio benchmarks largely test text-recoverable semantics; models lose only ~6–9% when answering from captions on prior suites, masking deficits in fine-grained perception and 4D (time+3D) reasoning.<br>• Real-world tasks require "audio 4D intelligence"—reasoning about sound dynamics over time and in 3D space—critical for embodied AI safety, navigation, and interaction.<br>• Current settings are mostly single-clip, coarse-grained QA; they lack multi-audio comparison/integration and quantitative profiling of basic acoustic attributes.<br>• Common model preprocessing collapses multi-channel audio to mono, destroying spatial cues (ITD/ILD) and crippling spatial reasoning.<br>• Evaluation protocols often miss robustness (e.g., instability across option permutations) and lack expert-validated, high-quality, linguistically hard-to-describe audio cues.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>STAR-Bench is a hierarchical benchmark that (1) quantifies foundational acoustic perception (pitch, loudness, duration, azimuth, elevation, distance) via procedurally synthesized and physics-simulated binaural audio under absolute range and relative discrimination tasks, and (2) evaluates holistic spatio-temporal reasoning through audio segment reordering (continuous processes, discrete scripts) and spatial tasks (static localization, multi-source relations, dynamic trajectories). A four-stage curation pipeline (AI-assisted filtering plus human annotation/validation) and robust evaluation (multi-run option rotation with AA/ACR, native vs channel-wise stereo inputs) ensure reliability and reveal model bottlenecks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Natively Spatial LALMs: End-to-end multi-channel audio encoders that preserve ITD/ILD cues with spatial pretraining on large-scale spatialized corpora for genuine 3D reasoning.<br>• Dense Audio Captioners for 4D Reasoning: Training pipelines that produce fine-grained, temporally grounded and spatially aware captions to unlock knowledge-grounded audio reasoning.<br>• Multi-Audio Grounding Networks: Architectures and objectives for comparing, aligning, and integrating multiple audio clips (e.g., cross-clip attention and contrastive grounding) to improve ordering and relational inference.<br>• JND-Aware Self-Supervised Audio Pretraining: Pretext tasks that explicitly model human just-noticeable differences for pitch, loudness, duration, and spatial attributes to boost fine-grained perception.<br>• Physics-Guided Audio Reasoners: Incorporating differentiable priors (Doppler, inverse-square law, resonance/decay) or tool-use modules to improve causal spatio-temporal inference.<br>• Consistency-Tuned Audio LMs: Training for output stability under option permutations and channel presentations (AA→ACR gap minimization) via adversarial/circular evaluations and consistency losses.<br>• Audio-Visual 4D Embodied Bench: Extending STAR-Bench to synchronized AV streams and interactive tasks for robots/agents that act on spatial audio cues in closed-loop settings.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24514" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24514" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. Inspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad. The internal visual representations of MLLMs have traditionally been confined to perceptual understanding. We repurpose them to support generative visual thought without compromising reasoning ability. Building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process. It allows the model to interleave textual reasoning with the generation of visual latents. These latents guide the internal thought process and can be translated into sketch images for interpretability. To realize this, we introduce two components: a Context-Aware Vision Head autoregressively produces visual representations, and a pretrained Sketch Decoder renders these into human-interpretable images. We evaluate the framework on our new dataset MazePlanning. Experiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance to their backbone. It further generalizes across distinct frontier MLLMs, including Gemma3 and Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our framework opens new opportunities for richer human-computer interaction and broader applications. More details and resources are available on our project page: https://latent-sketchpad.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• MLLMs excel at perception but struggle with complex multimodal reasoning that demands precise spatial planning and dynamic visual grounding; text-only Chain-of-Thought lacks spatial expressiveness.<br>• Existing approaches often depend on external tools (detectors, executors) with limited coverage and integration overhead, or on unified generators optimized for pixel realism instead of abstract, reasoning-friendly visual representations.<br>• Frontier MLLMs (e.g., Gemma3, Qwen2.5-VL) cannot natively generate visual content during reasoning; repurposing their pretrained visual features for generative visual thoughts is largely unexplored.<br>• Lack of interpretable internal visual states makes it hard to inspect, guide, and debug reasoning; a plug-and-play solution that preserves pretrained reasoning capacity is needed.<br>• Human cognition naturally interleaves language with mental sketches; enabling visual imagination in MLLMs promises better accuracy, robustness, and richer human–computer interaction.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Latent Sketchpad equips pretrained MLLMs with a Context-Aware Vision Head that autoregressively generates visual latents interleaved with text by attending to both global (previous images) and local (current image) context, trained via latent regression while freezing the backbone. A separately pretrained Sketch Decoder (AlignerNet → frozen VAE decoder) translates these latents into human-interpretable sketch images for transparent inspection.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• End-to-End Visual-Thought Fine-Tuning for MLLMs: Jointly optimize the backbone and Vision Head so visual thoughts improve task rewards directly, not just latent matching.<br>• Adaptive Sketching: When and What to Draw During Reasoning: Learn policies that decide insertion points and granularity of visual latents under compute/token budgets.<br>• 3D Latent Sketchpad for Embodied and Robotics Planning: Generalize visual latents to 3D (e.g., occupancy grids/scene graphs) for navigation and manipulation tasks.<br>• Learning Visual-Thought Priors via Self-Play and RL: Use reinforcement learning to reward successful plans and consistent visual traces, improving spatial reasoning.<br>• Cross-Model Visual Thought Distillation: Distill visual latent trajectories from stronger teachers into compact MLLMs to transfer visual reasoning skills efficiently.<br>• High-Fidelity, Domain-Aware Sketch Decoders: Replace generic VAE decoding with domain-specific decoders (e.g., diagrams, maps, medical) to boost structural fidelity.<br>• Memory-Augmented Latent Sketchpad for Long-Horizon Tasks: Add persistent visual memory to maintain spatial coherence across long, multi-turn workflows.<br>• Benchmarking Interleaved Visual Reasoning Beyond Mazes: Create datasets and metrics (e.g., layout consistency, visual success) for charts, circuits, CAD, and program synthesis.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24320" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24320" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Training critiquing language models to assess and provide feedback on model outputs is a promising way to improve LLMs for complex reasoning tasks. However, existing approaches typically rely on stronger supervisors for annotating critique data. To address this, we propose Critique-RL, an online RL approach for developing critiquing language models without stronger supervision. Our approach operates on a two-player paradigm: the actor generates a response, the critic provides feedback, and the actor refines the response accordingly. We first reveal that relying solely on indirect reward signals from the actor's outputs for RL optimization often leads to unsatisfactory critics: while their helpfulness (i.e., providing constructive feedback) improves, the discriminability (i.e., determining whether a response is high-quality or not) remains poor, resulting in marginal performance gains. To overcome this, Critique-RL adopts a two-stage optimization strategy. In stage I, it reinforces the discriminability of the critic with direct rule-based reward signals; in stage II, it introduces indirect rewards based on actor refinement to improve the critic's helpfulness, while maintaining its discriminability via appropriate regularization. Extensive experiments across various tasks and models show that Critique-RL delivers substantial performance improvements. For example, it achieves a 9.02% gain on in-domain tasks and a 5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Scalable oversight gap: Training critique models usually depends on stronger supervisors or expensive annotations that don’t match the learner’s output distribution, limiting scale and reliability.<br>• Oracle dependency: Prompted critiquing often assumes an oracle verifier at test time; without it, models struggle with discrimination and hit performance bottlenecks.<br>• Indirect-RL limitation: RL with only outcome-based indirect rewards (from actor refinements) improves helpfulness but fails to optimize discriminability, yielding conservative/aggressive failure modes and unstable training.<br>• Need for joint objectives: Effective critics must both discriminate correctness and provide actionable feedback; existing methods rarely optimize these capabilities jointly and stably.<br>• Compute efficiency: There is a need to lift the performance ceiling more compute-efficiently than pure response sampling at inference, exploiting critique-refinement loops.<br>• Generalization demand: Critique training should transfer across tasks, model sizes, and domains (in-domain and out-of-domain).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Critique-RL is a two-stage online RL framework that trains a separate critique model in a two-player loop with a fixed actor: Stage I maximizes a direct discrimination reward aligning the critic’s correctness judgment with an oracle on the actor’s original response; Stage II adds an indirect refinement reward based on the actor’s post-critique correctness while regularizing toward the Stage I policy and retaining the discrimination reward to preserve discriminability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Oracle-Free Critique-RL via Self-Verified Rewards: Train critics without external verifiers using self-consistency, debate-style cross-checks, and agreement-based pseudo-rewards for both discrimination and helpfulness.<br>• Joint Actor–Critic Co-Learning for End-to-End Reasoning: Co-train the actor and critic with curriculum and stability constraints to jointly improve reasoning, discrimination, and feedback quality.<br>• Committee-of-Critics: Ensembling Specialized Critics under Compute Budgets: Build domain/error-type specialized critics and design routing/aggregation policies to maximize accuracy per FLOP.<br>• Extending Critique-RL to Open-Ended Tasks with Learned Reward Models: Use reward models or AI feedback to provide soft discrimination and refinement rewards for summarization, dialogue, and safety critiques.<br>• Adaptive Test-Time Compute Allocation for Critique Loops: Develop policies for dynamic sampling, early stopping, and iterative refinement depth to optimize accuracy–compute trade-offs.<br>• Weak-to-Strong Oversight with Critique-RL: Study how smaller critics can reliably supervise stronger actors and analyze robustness, transfer, and failure modes.<br>• Theory of Two-Stage Critiquing RL: Provide convergence and stability analyses, and characterize trade-offs between discriminability and helpfulness under different reward weightings and KL constraints.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24702" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24702" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an "interlingua" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Fragmented, heterogeneous agent datasets (formats, action/observation spaces like HTML vs. accessibility trees) make aggregation and reuse hard, leaving valuable data underutilized (see Table 1, p. 3).<br>• Many-to-many conversion burden: each dataset–agent harness needs a bespoke converter, creating O(D×A) engineering effort that slows research and harms reproducibility (Figure 2, p. 6).<br>• Lack of a simple yet expressive common schema prevents standardized supervision signals, consistent tool/action logging, and plug-and-play integration across agents.<br>• Difficult cross-dataset analysis and quality control due to inconsistent representations and reasoning annotations, hindering data-driven selection and improvement (Table 2, p. 6).<br>• Existing unification attempts are task- or agent-specific rather than community-wide standards, still requiring per-dataset engineering.<br>• High cost and complexity of curating multi-step agent trajectories (manual, synthetic, rollouts), which amplifies the need to maximize reuse via standardization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Introduce the Agent Data Protocol (ADP): a lightweight Pydantic schema that standardizes agent trajectories as alternating Actions (API/Code/Message) and Observations (Text/Web) with metadata and automated validators, plus a two-hop pipeline (Raw→ADP, ADP→SFT) and QA to collapse O(D×A) converters to O(D+A). Converting 13 datasets into ADP and then into multiple agent harnesses (OpenHands, SWE-Agent, AgentLab) enables scalable SFT, yielding ~20% average gains across SWE-Bench, WebArena, AgentBench OS, and GAIA (Tables 3–5, pp. 8–9).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• ADP-V: A Multimodal Agent Data Protocol for Vision, GUI, and Screen Interactions: Extend ADP to images, screenshots, viewport videos, and UI states to capture richer perceptions for desktop/web agents.<br>• Eval-ADP: A Standardized Evaluation and Environment Protocol for Reproducible Agent Benchmarks: Define interoperable evaluation logs, environment snapshots, and replayable artifacts so datasets, agents, and benchmarks compose cleanly.<br>• Auto-ADP: Automated Extraction and Validation of Agent Trajectories from Heterogeneous Logs: Build tooling to automatically ingest raw agent/system logs into ADP with strong schema validation, reasoning checks, and data quality scoring.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large Reasoning Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21978" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21978" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement learning with verifiable rewards (RLVR) has delivered impressive gains in mathematical and multimodal reasoning and has become a standard post-training paradigm for contemporary language and vision-language models. However, the RLVR recipe introduces a significant risk of capability regression, where models forget foundational skills after prolonged training without employing regularization strategies. We empirically confirm this concern, observing that open-source reasoning models suffer performance degradation on core capabilities such as perception and faithfulness. While imposing regularization terms like KL divergence can help prevent deviation from the base model, these terms are calculated on the current task, thus they do not guarantee broader knowledge. Meanwhile, commonly used experience replay across heterogeneous domains makes it nontrivial to decide how much training focus each objective should receive. To address this, we propose RECAP-a replay strategy with dynamic objective reweighting for general knowledge preservation. Our reweighting mechanism adapts in an online manner using short-horizon signals of convergence and instability, shifting the post-training focus away from saturated objectives and toward underperforming or volatile ones. Our method is end-to-end and readily applicable to existing RLVR pipelines without training additional models or heavy tuning. Extensive experiments on benchmarks based on Qwen2.5-VL-3B and Qwen2.5-VL-7B demonstrate the effectiveness of our method, which not only preserves general capabilities but also improves reasoning by enabling more flexible trade-offs among in-task rewards.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Reasoning-focused RL with verifiable rewards (RLVR) boosts math/reasoning but induces catastrophic forgetting of general capabilities (perception, OCR, robustness, faithfulness/safety), with reasoning-tuned VLMs often underperforming their base models on non-reasoning benchmarks.<br>• Standard KL regularization is imposed on the target task and cannot guarantee preservation of non-target skills; format rewards are easy and saturate, risking over-optimization and diversity/exploration collapse.<br>• Mixed-domain replay is nontrivial: heterogeneous rewards/losses have different magnitudes, convergence speeds, and variances, making static/manual reweighting and mixtures suboptimal.<br>• Prolonged RL without principled regularization increases hallucinations and jailbreak susceptibility; existing pipelines lack an online mechanism to balance progress across objectives while preventing capability drift.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>RECAP: Replay general-capability data during RLVR and dynamically reweight objectives using short-horizon signals—per-objective convergence rate and instability—computed over sliding windows; convert s = (convergence + instability) into weights via a temperature-softmax to down-weight saturated signals (e.g., format) and emphasize slow/volatile ones. This end-to-end, magnitude-agnostic scheduler drops into GRPO/RLVR+SFT without auxiliary models, preserving general skills while improving reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Adaptive Replay for Preference- and Process-Supervised Reasoning: Extend RECAP beyond RLVR to DPO/ORPO and process reward models, unifying dynamic reweighting across RL and non-RL alignment losses.<br>• Bandit-Guided Objective Scheduling for Continual Multimodal RL: Learn data sampling and loss weights with contextual bandits or meta-learning, using short-horizon rewards to optimize retention–reasoning trade-offs online.<br>• Safety- and Factuality-Preserving Reasoning RL: Integrate verifiable safety/factuality rewards into RECAP’s scheduler to prevent jailbreaks and hallucinations while scaling reasoning performance.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22037" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22037" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Scaling laws research has focused overwhelmingly on English -- yet the most prominent AI models explicitly serve billions of international users. In this work, we undertake the largest multilingual scaling laws study to date, totaling 774 multilingual training experiments, spanning 10M-8B model parameters, 400+ training languages and 48 evaluation languages. We introduce the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual pretraining, which outperforms existing scaling laws' out-of-sample generalization often by more than 0.3 R^2. Our analyses of the experiments shed light on multilingual learning dynamics, transfer properties between languages, and the curse of multilinguality. First, we derive a cross-lingual transfer matrix, empirically measuring mutual benefit scores between 38 x 38=1444 language pairs. Second, we derive a language-agnostic scaling law that reveals how to optimally scale model size and data when adding languages without sacrificing performance. Third, we identify the computational crossover points for when to pretrain from scratch versus finetune from multilingual checkpoints. We hope these findings provide the scientific foundation for democratizing scaling laws across languages, and enable practitioners to efficiently scale models -- beyond English-first AI.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• English-centric scaling laws fail to explain or optimize massively multilingual pretraining where users span hundreds of languages.<br>• Existing laws ignore multi-epoch data repetition and break in data-constrained regimes, making them unreliable for low/mid-resource languages.<br>• Prior multilingual laws do not separate target-language data, other-language data, and specific transfer languages, limiting out-of-sample generalization across model size, tokens, compute, and new mixtures.<br>• Practitioners lack an empirical, language-to-language transfer map to choose co-training languages that help rather than interfere.<br>• There is no principled recipe for handling the curse of multilinguality—how to scale model size and data as the number of languages grows.<br>• No clear guidance exists on when to pretrain from scratch versus finetune a multilingual checkpoint for best compute efficiency.<br>• The compute-efficiency tax of multilingual vocabularies/mixtures is undocumented and needs quantification.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes ATLAS, a repetition-aware scaling law that decomposes effective data into target-language tokens, a small set of top transfer languages with learned weights, and the remainder, all passed through a saturation function to model multi-epoch repetition, within a Chinchilla-style loss. Validated on 774 runs across 10M–8B models and 400+ training languages, it delivers strong out-of-sample fits and is complemented by a 38×38 empirical transfer matrix, a capacity law for the curse of multilinguality with compute-optimal scaling rules, and a pretrain-versus-finetune compute crossover heuristic.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Learning Transfer Weights from Typology: A Principled ATLAS Prior: Map linguistic family/script/domain features to τ priors to improve zero-shot mixture generalization and reduce data needed to fit transfer terms.<br>• ATLAS at Frontier Scale: Validating Multilingual Scaling Laws Beyond 8B Parameters: Test ATLAS on 10B–100B+ models and alternative corpora to assess stability of exponents, transfer weights, and compute-optimal frontiers.<br>• Adaptive Language Mixture Scheduling via ATLAS-Guided Control: Use real-time ATLAS fits to dynamically schedule language sampling to maintain iso-loss while optimizing fairness and compute.<br>• Joint Vocabulary–Model–Data Scaling for Multilingual LMs: Integrate ATLAS with vocabulary scaling laws to co-optimize tokenizer size, model size, and language mix under a fixed compute budget.<br>• Causal Analysis of Asymmetric Cross‑Lingual Transfer: Identify mechanisms behind transfer asymmetry via interventions on script sharing, tokenization, and domain alignment; derive causal adjustments to τ.<br>• ATLAS for Multimodal and Code‑Switching Pretraining: Extend the effective data and transfer terms to multimodal inputs and code-switched text, modeling cross-modal and cross-script transfer explicitly.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20661" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20661" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable progress. However, two key challenges remain : 1) the absence of a large-scale high-quality UHR T2I dataset, and (2) the neglect of tailored training strategies for fine-grained detail synthesis in UHR scenarios. To tackle the first challenge, we introduce UltraHR-100K, a high-quality dataset of 100K UHR images with rich captions, offering diverse content and strong visual fidelity. Each image exceeds 3K resolution and is rigorously curated based on detail richness, content complexity, and aesthetic quality. To tackle the second challenge, we propose a frequency-aware post-training method that enhances fine-detail generation in T2I diffusion models. Specifically, we design (i) Detail-Oriented Timestep Sampling (DOTS) to focus learning on detail-critical denoising steps, and (ii) Soft-Weighting Frequency Regularization (SWFR), which leverages Discrete Fourier Transform (DFT) to softly constrain frequency components, encouraging high-frequency detail preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks demonstrate that our approach significantly improves the fine-grained detail quality and overall fidelity of UHR image generation. The code is available at https://github.com/NJU-PCALab/UltraHR-100k{here}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Lack of an open-source, large-scale, high-quality ultra-high-resolution (UHR) text-to-image dataset; existing sets (e.g., Aesthetic-4K ≈10K images) are too small and lack rigorous curation, limiting generalization and fidelity at 4K+ (Table 2; Figures 2–4)<br>• Pretrained T2I models are typically trained at ≤1024×1024 and degrade when directly scaled to UHR, causing artifacts and loss of fine details—problematic for real-world applications needing fine-grained fidelity (digital art, virtual content, commercial design)<br>• Training-free UHR methods (architecture/inference tweaks) rely on base models not exposed to UHR data and tend to over-smooth, hallucinate details, and incur long inference time, limiting practicality<br>• Existing training-based UHR approaches prioritize efficiency (e.g., token compression) but neglect fine-grained detail synthesis; lack detail-oriented training strategies tailored to high-frequency content<br>• Insufficient standardized evaluation for 4K detail fidelity; a comprehensive benchmark is needed to assess fine-grained quality and overall realism (UltraHR-eval4K introduced)</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces UltraHR-100K, a 100K-image, >3K-resolution dataset curated via a multi-stage pipeline (sharpness/edge prefiltering, then intersection of detail richness via GLCM, content complexity via Shannon entropy, and aesthetic quality via LAION predictor; captions generated by Gemini 2.0) to supply diverse, high-fidelity UHR training data (Table 1; Figures 2–4). It further proposes frequency-aware post-training (FAPT) comprising Detail-Oriented Timestep Sampling (DOTS, Beta-distributed emphasis on late denoising steps; Figure 5) and Soft-Weighting Frequency Regularization (SWFR, DFT-based soft constraints on frequency bands) to enhance high-frequency detail without sacrificing structure.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Frequency-Aware Curriculum for 8K+ Diffusion: Extend DOTS/SWFR with progressive resolution scaling and adaptive Beta scheduling to stabilize and accelerate training at 8K and beyond.<br>• Open UltraHR Captioning Without Proprietary VLMs: Develop an open-source, self-refining captioning/QA pipeline (distillation + consistency checks) to match or surpass Gemini-level captions for UHR images.<br>• Perceptual Metrics for Fine-Grained UHR Detail: Create frequency- and detail-sensitive evaluation metrics and a standardized 4K/8K benchmark to better correlate with human judgments of micro-texture and structural fidelity.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.17439" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.17439" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• 2D VLM-based VLAs lack reliable 3D spatial reasoning, creating a gap between 2D perception and 3D action that harms generalization and adaptability in the real world.<br>• Explicit 3D integration (e.g., point clouds, depth) depends on specialized sensors, suffers low modality transferability, and is hard to scale due to limited 3D annotations in large datasets.<br>• Weak 3D cues (pseudo-depth, learnable spatial embeddings) provide insufficient geometry, cannot capitalize on higher-fidelity 3D inputs when available, and disrupt vision–language alignment, degrading zero-shot and spatial-prompt performance.<br>• Policies struggle with geometry-heavy tasks (depth, height, scale, spatial relations) and robustness under clutter and scene variations—now a bottleneck for reliable generalist robotics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>FALCON injects rich 3D spatial tokens from spatial foundation models into a Spatial-Enhanced Action Head, keeping the VLM focused on semantics while conditioning action generation directly on geometry. An Embodied Spatial Model produces these tokens from RGB and can optionally fuse depth and pose without retraining or architectural changes, enabling strong modality transferability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Temporal FALCON: Leveraging Video-Consistent Spatial Tokens for Closed-Loop Manipulation: Extend the spatial model with temporal aggregation to stabilize geometry and handle dynamic scenes.<br>• FALCON-SLAM: Scaling Spatial Tokens to Mobile Manipulation via Multi-View Scene Graphs: Fuse multi-view/global mapping with spatial tokens to unify navigation and manipulation.<br>• Uncertainty-Aware FALCON: Risk-Sensitive Control from Probabilistic Spatial Priors: Model uncertainty over spatial tokens and propagate it to action selection for safer execution.<br>• Tactile-Visual FALCON: Fusing Touch and Spatial Foundation Priors in the Action Head: Integrate tactile signals as tokens to improve grasping and contact-rich manipulation.<br>• Data-Efficient Spatial–Language Alignment via Self-Training and Synthetic 3D Supervision: Use synthetic depth/pose and self-training to better align spatial tokens with language using minimal labeled robot data.<br>• On-Device FALCON: Token Compression and Distillation for Real-Time Embedded Control: Compress spatial tokens and distill the policy for low-latency deployment on edge hardware.<br>• Interactive Spatial Prompting: Language-Driven Constraints over 3D Tokens for Safety and Precision: Design language interfaces that constrain or reweight spatial tokens to enforce task-specific and safety constraints.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SPICE: Self-Play In Corpus Environments Improves Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24684" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24684" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Self-improving systems require environmental interaction for continuous adaptation. We introduce SPICE (Self-Play In Corpus Environments), a reinforcement learning framework where a single model acts in two roles: a Challenger that mines documents from a large corpus to generate diverse reasoning tasks, and a Reasoner that solves them. Through adversarial dynamics, the Challenger creates an automatic curriculum at the frontier of the Reasoner's capability, while corpus grounding provides the rich, near-inexhaustible external signal necessary for sustained improvement. Unlike existing ungrounded self-play methods that offer more limited benefits, SPICE achieves consistent gains across mathematical (+8.9%) and general reasoning (+9.8%) benchmarks on multiple model families. Our analysis reveals how document grounding is a key ingredient in SPICE to continuously generate its own increasingly challenging goals and achieve them, enabling sustained self-improvement.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Ungrounded self‑play for LLMs quickly plateaus or collapses due to hallucination amplification and information symmetry between generator and solver; sustained improvement needs external, verifiable signals (pp. 2–3; Figure 4b on page 8 shows corpus grounding markedly boosts learning).<br>• Existing RL with verifiable rewards (RLVR) is powerful but narrow and labor‑intensive—often confined to math/code with specialized executors and human‑curated problems—creating a verification bottleneck and poor scalability (p. 2).<br>• Static synthetic data or variational synthesis remains bounded by initial coverage/memorized pretraining, limiting novelty and challenge generation (p. 2).<br>• Lack of an automatic, adaptive curriculum keeps tasks either too easy or too hard; adversarial co‑evolution at the frontier of capability is needed (Figure 3 on page 8 shows Challenger makes questions harder while Reasoner improves).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>SPICE is a corpus‑grounded self‑play RL framework where a single LLM alternates between a Challenger that mines documents to generate MCQ or free‑form, document‑grounded tasks and a Reasoner that solves them without document access, creating information asymmetry. The model is co‑trained with a Gaussian variance‑based curriculum reward (peaking at ~50% Reasoner pass rate) for the Challenger and binary correctness verification for the Reasoner, yielding sustained, domain‑general gains (Figure 1 on page 1).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Beyond Text: Multimodal SPICE with Visual, Tabular, and Code Corpora: Extend corpus grounding to images, charts, tables, and executable code with modality‑specific verifiers to broaden reasoning and verification coverage.<br>• Learning to Retrieve: Joint Optimization of Document Mining and Self‑Play Curricula: Train a retrieval/controller policy that actively selects documents to maximize variance‑based learning progress, enabling adaptive, data‑efficient curricula.<br>• Provable Curricula for Self‑Play Reasoning: A Theory of Variance‑Centered Rewards and Information Asymmetry: Develop formal guarantees on stability, convergence, and optimality of variance‑based curriculum under asymmetric information.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24081" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24081" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>To date, there exist almost no culturally-specific evaluation benchmarks for large language models (LLMs) that cover a large number of languages and cultures. In this paper, we present Global PIQA, a participatory commonsense reasoning benchmark for over 100 languages, constructed by hand by 335 researchers from 65 countries around the world. The 116 language varieties in Global PIQA cover five continents, 14 language families, and 23 writing systems. In the non-parallel split of Global PIQA, over 50% of examples reference local foods, customs, traditions, or other culturally-specific elements. We find that state-of-the-art LLMs perform well on Global PIQA in aggregate, but they exhibit weaker performance in lower-resource languages (up to a 37% accuracy gap, despite random chance at 50%). Open models generally perform worse than proprietary models. Global PIQA highlights that in many languages and cultures, everyday knowledge remains an area for improvement, alongside more widely-discussed capabilities such as complex reasoning and expert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA provides a glimpse into the wide diversity of cultures in which human language is embedded.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• There is a scarcity of culturally-specific, multilingual evaluation benchmarks; most existing ones are translations from English and miss local foods, customs, laws, and everyday practices.<br>• Machine-translated or English-sourced benchmarks often propagate Anglocentric bias and artifacts, yielding unnatural items and poor cultural relevance—especially in low-resource languages.<br>• Commonsense is culture-bound; current evaluations overemphasize abstract/math/logical reasoning and under-test everyday physical commonsense across diverse communities.<br>• Existing localized datasets either cover few languages or focus on expert knowledge rather than everyday physical commonsense known by community members.<br>• Annotation pipelines that rely on paid crowdwork can be extractive and yield variable quality; a participatory, community-led approach is needed.<br>• There is limited visibility into multilingual disparities: performance gaps between high- and low-resource languages and between open-weight and proprietary models remain under-measured.<br>• Current evaluation setups are inconsistent across base vs. instruction-tuned models, complicating fair comparisons.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Global PIQA is a participatory, non-parallel physical commonsense benchmark: 335 native-speaker researchers authored PIQA-style (prompt + two options) items for 116 language varieties, followed by standardized cleaning, English MT for inspection, cultural-specificity tagging, and subsampling to 100 balanced items per language. Models are evaluated with a unified harness in completion (base) or prompted (instruction-tuned) formats, enabling large-scale cross-language comparisons and analysis of cultural effects.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Disentangling Language Proficiency and Cultural Knowledge with a Parallel Global PIQA: Build and validate a parallel (translated-and-corrected) split to isolate linguistic competence from culture-specific knowledge and quantify their separate contributions to accuracy gaps.<br>• Closing the Commonsense Gap in Low-Resource Languages via Targeted Post-Training: Use Global PIQA to drive PEFT/RLHF/RAG finetuning and community-curated data augmentation that specifically uplifts underperforming languages and scripts.<br>• Culturally-Aware Instruction Tuning for Multilingual LLMs: Systematically study prompt design and instruction-tuning choices (and thinking budgets) to reduce auxiliary-task burdens for small models and improve robustness on culturally-specific items.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">Latent Chain-of-Thought for Visual Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23925" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23925" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Chain-of-thought (CoT) reasoning is critical for improving the interpretability and reliability of Large Vision-Language Models (LVLMs). However, existing training algorithms such as SFT, PPO, and GRPO may not generalize well across unseen reasoning tasks and heavily rely on a biased reward model. To address this challenge, we reformulate reasoning in LVLMs as posterior inference and propose a scalable training algorithm based on amortized variational inference. By leveraging diversity-seeking reinforcement learning algorithms, we introduce a novel sparse reward function for token-level learning signals that encourage diverse, high-likelihood latent CoT, overcoming deterministic sampling limitations and avoiding reward hacking. Additionally, we implement a Bayesian inference-scaling strategy that replaces costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank optimal rationales and answers. We empirically demonstrate that the proposed method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in terms of effectiveness, generalization, and interpretability.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Existing SFT/PPO/GRPO pipelines underperform on unseen visual reasoning tasks, as they optimize next-token or scalar rewards without modeling dependencies across full reasoning trajectories, leading to poor generalization and mode collapse.<br>• KL-constrained RL (e.g., PPO/GRPO) limits exploration around SFT baselines, hindering discovery of novel rationales and inviting reward hacking due to biased or brittle reward models.<br>• Deterministic CoT generation fails to capture uncertainty/diversity; true posterior sampling over latent rationales is intractable, and common approximations (MCMC/RL) under-cover the distribution.<br>• Visual CoT chains are long (~1k tokens), making token-level reward signals computationally prohibitive and exploration unstable, often causing catastrophic forgetting when unconstrained.<br>• Inference-time search (Best-of-N, beam search) is costly, relies on external critics/verifiers, and lacks a probabilistically principled way to select optimal rationale–answer pairs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>LaCoT reframes visual chain-of-thought as latent posterior inference and trains a rationale sampler via GFlowNets-based amortized variational inference with a token-level marginal reward approximation and a reference-guided exploration filter (RGFN) to preserve diversity without KL constraints. At inference, a Bayesian scheme (BiN) samples multiple rationales, computes length-normalized joint likelihoods, and selects answers by estimated marginal likelihood, replacing heuristic BoN/beam and avoiding external critics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Distilling Latent Chains-of-Thought for Efficient Visual Reasoning: Use the learned sampler to generate high-quality rationales and distill them into smaller LVLMs for faster, robust reasoning.<br>• Scaling Reference-Guided GFlowNets to Long Multimodal Chains: Develop memory-efficient training (adaptive λ, curriculum exploration) to stably learn posterior-aligned rationales for billion-token contexts and larger (≥70B) LVLMs.<br>• Uncertainty-Calibrated BiN for Reliable Reasoning: Enhance Bayesian inference-time scaling with posterior weighting, variance reduction, and uncertainty calibration to further reduce hallucinations and improve answer selection across domains.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22768" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22768" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As Large Vision-Language Models (LVLMs) are increasingly deployed in domains such as shopping, health, and news, they are exposed to pervasive persuasive content. A critical question is how these models function as persuadees-how and why they can be influenced by persuasive multimodal inputs. Understanding both their susceptibility to persuasion and the effectiveness of different persuasive strategies is crucial, as overly persuadable models may adopt misleading beliefs, override user preferences, or generate unethical or unsafe outputs when exposed to manipulative messages. We introduce MMPersuade, a unified framework for systematically studying multimodal persuasion dynamics in LVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs images and videos with established persuasion principles across commercial, subjective and behavioral, and adversarial contexts, and (ii) an evaluation framework that quantifies both persuasion effectiveness and model susceptibility via third-party agreement scoring and self-estimated token probabilities on conversation histories. Our study of six leading LVLMs as persuadees yields three key insights: (i) multimodal inputs substantially increase persuasion effectiveness-and model susceptibility-compared to text alone, especially in misinformation scenarios; (ii) stated prior preferences decrease susceptibility, yet multimodal information maintains its persuasive advantage; and (iii) different strategies vary in effectiveness across contexts, with reciprocity being most potent in commercial and subjective contexts, and credibility and logic prevailing in adversarial contexts. By jointly analyzing persuasion effectiveness and susceptibility, MMPersuade provides a principled foundation for developing models that are robust, preference-consistent, and ethically aligned when engaging with persuasive multimodal content.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• No benchmark to systematically study multimodal persuasion on LVLMs; prior work is largely text-only, single-turn, and lacks vision-grounding (Sec. 1–2; Figure 1 p.1, Figure 2 p.3).<br>• Safety and alignment risk: LVLMs can be over-persuaded by manipulative multimodal inputs, leading to misinformation uptake or overriding user preferences (Abstract; bottom panel of Figure 3 p.7).<br>• Existing evaluations mostly measure explicit agreement only and use coarse, binary metrics; they fail to capture implicit belief shifts and persuasion timing/strength (Sec. 3.2–3.3; Related Work p.10).<br>• Lack of controlled modality ablations (text vs. text+caption vs. full multimodal) to isolate visual contributions to persuasion (Sec. 3.1; Figure 2 p.3).<br>• No framework to study how prior preferences/stubbornness modulate susceptibility, despite being critical in real decisions (Sec. 4.2; Figure 4 p.8).<br>• Limited understanding of which persuasion strategies are most effective across contexts in multimodal settings (Tables 2–3 p.8).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>MMPersuade provides a large-scale dataset (450 scenarios; 62,160 images; 4,756 videos) mapped to Cialdini’s and Aristotle’s persuasion principles via a six-step generation pipeline (Figure 6 p.15), and a controlled multi-turn evaluation where a static persuader interacts with LVLM persuadees under text-only, text+caption, and multimodal conditions (Figure 2 p.3). Stance is measured by third-party agreement scoring and self-estimated token probabilities, unified by a new Persuasion Discounted Cumulative Gain (PDCG) metric that rewards earlier and stronger persuasion (Sec. 3.2–3.3; Figure 3 p.7).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Adaptive Multimodal Persuaders vs. Robust LVLMs: Closing the Gap to Realistic Attacks—Replace the static persuader with adaptive, strategy-aware agents and evaluate defenses under dynamic multi-turn pressure.<br>• Preference-Consistent Alignment for Multimodal Robustness—Train LVLMs to honor stated user preferences under visual persuasion using RLHF/RLAIF with PDCG-aware objectives.<br>• Causal Attribution of Visual Cues in Persuasion—Use causal interventions to isolate which visual attributes (e.g., warmth, authority cues) most shift implicit beliefs across contexts.<br>• Real-World, Human-Created Multimodal Persuasion Benchmark—Augment synthetic media with curated human-produced images/videos (and audio) to test external validity and longer-form content.<br>• Cross-Cultural and Multilingual Multimodal Persuasion Robustness—Evaluate how cultural norms and language shape susceptibility and strategy effectiveness across regions.<br>• Prompt Policy Optimization to Reduce Multimodal Susceptibility—Systematically learn system prompts/policies that minimize PDCG under adversarial visuals (cf. Figure 5 p.9; Figure 21 p.31).<br>• Early-Warning Detectors for Persuasive Intent in Vision-Language Inputs—Build classifiers that flag persuasive strategies and trigger counter-arguing or calibrated refusals before belief shifts.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24645" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24645" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Function calling (FC) empowers large language models (LLMs) and autonomous agents to interface with external tools, a critical capability for solving complex, real-world problems. As this ability becomes increasingly central to advanced AI systems, the need for high-quality, multi-turn training data to develop and refine it cannot be overstated. Existing data synthesis methods, such as random environment sampling or multi-agent role-playing, are not powerful enough to generate high-quality data in real-world environments. Practical challenges come in three folds: targeted model training, isolation of tool architecture, and multi-turn logical dependency. To address these structural deficiencies, we present FunReason-MT, a novel data synthesis framework for real-world multi-turn tool use. FunReason-MT resolves the complexity barrier in multi-turn FC data by employing 1) Environment-API Graph Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query Synthesis to simplify hard query construction, and 3) Guided Iterative Chain for sophisticated CoT generation. Evaluations on Berkeley Function-Calling Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built upon FunReason-MT generated data achieves state-of-the-art performance among comparable-sized models, outperforming most close-source models. Further performance improvements on BFCLv4 confirm that FunReason-MT provides a reliable and robust source for agentic learning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• High-quality multi-turn function-calling (FC) data is scarce, creating a performance ceiling for tool-using LLMs in real-world, agentic settings.<br>• Random environment sampling misses rare, complex, long-tail tool-use events and offers low controllability and reliability.<br>• Multi-agent role-playing tends to produce simplistic “happy-path” dialogs with limited diversity and weak realism.<br>• Targeted model training is hard: existing methods cannot reliably center trajectories on a specific complex target tool and its collaboration with others.<br>• Isolation of tool architecture: modular sub-tools hinder synthesizing hard, logical-jump queries that require implicit prerequisite execution.<br>• Multi-turn logical dependency: models struggle to produce complete, accurate, and consistent CoT across turns in unfamiliar environments, leading to incomplete/incorrect trajectories.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>A top-down data synthesis pipeline that (1) samples legal, goal-directed multi-step traces via an API relation graph with target-biased tool selection, (2) reverse-synthesizes an “advanced tool” and a hard query from the trace, and (3) uses a guided iterative reasoning–critique loop to generate validated CoT and function calls; repeated to build robust multi-turn trajectories.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Automatic API-Graph Induction from Real Systems: Learn tool dependency graphs and parameters from production logs to reduce manual graph design and improve realism.<br>• Curriculum-Guided Target Tool Mastery via Graph Difficulty Scheduling: Progressively increase graph distance, dependency depth, and tool interactions to scaffold multi-turn competence.<br>• Adversarial Hard-Query Generation for Robust Tool Use: Train a query generator to create worst-case logical-jump tasks that stress compositional reasoning and error recovery.<br>• Zero/Low-Shot Unseen-Tool Generalization with Compositional Advanced Tools: Evaluate and train models to compose newly abstracted tools from primitives and transfer to unseen APIs.<br>• Agentic RL on Top of FunReason-MT Initialization: Combine the synthesized dataset with online environment feedback and RL to further improve planning, exploration, and calibration.<br>• Safety- and Constraint-Aware Multi-Turn Tool Use: Integrate policy, permission, and budget constraints into the API graph and guided chain to ensure safe, auditable execution.<br>• Faithful CoT and Function-Call Verification at Scale: Develop automatic verifiers that check CoT-logical consistency and FC-ground-truth alignment across long trajectories.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24591" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24591" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Frontier AI agents show increasing promise as scientific research assistants, and may eventually be useful for extended, open-ended research workflows. However, in order to use agents for novel research, we must first assess the underlying faithfulness and correctness of their work. To evaluate agents as research assistants, we introduce ReplicationBench, an evaluation framework that tests whether agents can replicate entire research papers drawn from the astrophysics literature. Astrophysics, where research relies heavily on archival data and computational study while requiring little real-world experimentation, is a particularly useful testbed for AI agents in scientific research. We split each paper into tasks which require agents to replicate the paper's core contributions, including the experimental setup, derivations, data analysis, and codebase. Each task is co-developed with the original paper authors and targets a key scientific result, enabling objective evaluation of both faithfulness (adherence to original methods) and correctness (technical accuracy of results). ReplicationBench is extremely challenging for current frontier language models: even the best-performing language models score under 20%. We analyze ReplicationBench trajectories in collaboration with domain experts and find a rich, diverse set of failure modes for agents in scientific research. ReplicationBench establishes the first benchmark of paper-scale, expert-validated astrophysics research tasks, reveals insights about agent performance generalizable to other domains of data-driven science, and provides a scalable framework for measuring AI agents' reliability in scientific research.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• There is no rigorous benchmark to test whether AI agents can faithfully and correctly replicate entire scientific papers end-to-end, a prerequisite for trustworthy AI-assisted research.<br>• Existing evaluations emphasize static knowledge (e.g., MMLU, GPQA) or generic agency (planning, tool use, coding) and fail to capture domain expertise and long-horizon, paper-scale workflows.<br>• Scientific evaluation requires realistic, sandboxed, and computationally reproducible settings; astrophysics offers strong data-sharing and reproducibility standards but lacks such an agent benchmark.<br>• Current assessments rely heavily on human rubrics and are hard to scale; objective, automated grading of final results with tolerance for numerical variability is needed.<br>• Models can cheat or memorize published values; masking manuscripts and contamination checks are required to measure genuine capability.<br>• Understanding agent failure modes (persistence, procedural errors, technical execution) in realistic research pipelines is underexplored.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>ReplicationBench is a paper-scale benchmark that decomposes peer-reviewed astrophysics papers into objectively gradable, author-validated tasks with masked manuscripts and provided datasets, then evaluates LLM agents in a sandboxed code-execution environment. It measures faithfulness and correctness via automated end-result grading with tolerances and complements this with expert qualitative trace reviews and memorization/cheating audits.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• ReplicationBench Across Sciences: A Generalized, Paper-Scale Benchmark for Computational Biology, Earth Science, and Materials Research — Extend the framework beyond astrophysics using domain datasets and objective end-result grading.<br>• Tool-Augmented Co-Scientists: Evaluating Web Access, Long-Term Memory, and Multimodal Inputs on Paper-Scale Replication — Systematically study how richer agent affordances affect replication success and failure modes.<br>• Persistence and Process: Training Scaffolds and Process Supervision to Reduce Early Termination and Procedural Errors — Develop agent scaffolding and learning signals that improve long-horizon execution fidelity.<br>• Cheating-Resistant Scientific Benchmarks: Manuscript Masking, Contamination Audits, and LLM Judges for Faithfulness — Design stronger anti-cheating protocols and automated judges to ensure genuine computation.<br>• Scalable Task Synthesis for ReplicationBench: Hybrid LLM–Human Pipelines to Generate Thousands of Expert-gradable Tasks — Automate high-quality task generation using reproducibility frameworks (e.g., ShowYourWork) with expert curation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Rethinking Visual Intelligence: Insights from Video Pretraining</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24448" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24448" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problem-solving. We investigate Video Diffusion Models (VDMs) as a promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design a controlled evaluation in which both a pretrained LLM and a pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Vision models still lag LLMs in compositional understanding, sample efficiency, and general-purpose problem solving, limiting progress toward truly general visual foundation models.<br>• Existing image-pretrained models lack strong spatiotemporal inductive biases needed to capture structure and dynamics crucial for visual reasoning.<br>• Video models are often treated purely as generators rather than as problem solvers; their temporal priors are underutilized for structured visual tasks.<br>• There is a lack of controlled, apples-to-apples comparisons between VDMs and LLMs under matched adaptation schemes and natural modalities.<br>• Efficient adaptation with minimal supervision is underexplored in vision; current methods often require extensive task-specific training or large labeled datasets.<br>• Benchmarks like ARC-AGI and ConceptARC expose weaknesses in compositional reasoning that current visual approaches fail to address.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Reframe image-to-image tasks as short “transition videos” and fine-tune a pretrained Video Diffusion Model with LoRA (freezing the backbone), conditioning on the input frame and a neutral text embedding; at inference, generate a video and read the final frame as the prediction. For fair comparison, adapt LLMs with symmetric LoRA on JSON-to-JSON translations, using identical splits and minimal supervision.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Temporal In-Context Learning for Video Diffusion Reasoners: Enable few-shot, training-free adaptation by prompting VDMs with temporal demonstration clips that condition the model to perform new tasks on the fly.<br>• Symbolic–Video Hybrid Models for Compositional Visual Reasoning: Combine VDM spatiotemporal priors with differentiable program induction or neuro-symbolic modules to improve rule discovery on ARC-style tasks.<br>• Scaling Laws and Data Recipes for Visual Intelligence in VDMs: Systematically study how model size, pretraining data, and transition-video construction affect data efficiency and generalization across ARC, planning, and cellular automata.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Batch Speculative Decoding Done Right</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22876" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22876" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Speculative decoding speeds up LLM inference by using a small draft model to propose multiple tokens that a target model verifies in parallel. Extending this idea to batches is essential for production serving, but it introduces the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, breaking right-alignment and corrupting position IDs, attention masks, and KV-cache state. We show that several existing batch implementations violate output equivalence-the fundamental requirement that speculative decoding must produce identical token sequences to standard autoregressive generation. These violations occur precisely due to improper handling of the ragged tensor problem. In response, we (1) characterize the synchronization requirements that guarantee correctness, (2) present a correctness-first batch speculative decoding EQSPEC that exposes realignment as consuming 40% of overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences and dynamically forms same-length groups, to reduce the realignment overhead while preserving per-sequence speculative speedups. On the SpecBench dataset, across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our approach achieves up to 3times throughput improvement at batch size 8 compared to batch size 1, with efficient scaling through batch size 8, while maintaining 95% output equivalence. Our method requires no custom kernels and integrates cleanly with existing inference stacks. Our code is available at https://github.com/eBay/spec_dec.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Ragged-tensor problem in batch speculative decoding: sequences accept different numbers of draft tokens, breaking rectangular layouts and desynchronizing position IDs, attention masks, and KV-cache (Figure 2, p.3).<br>• Existing batch methods violate output equivalence, producing corrupted outputs (e.g., repetition in BSP, <unk> in DSD) due to broken synchronization, incorrect bonus-token handling, and KV-cache misuse (Figure 1, p.2; Section 2, p.3).<br>• Prior design choices are flawed: masking/position-ID schemes need custom kernels (hurting portability), while rollback wastes verified tokens and collapses throughput to the worst sequence as batch grows (Section 2, p.3).<br>• Correct dynamic padding requires tight, iteration-by-iteration synchronization of padding, position IDs, attention, and KV-cache—an often-missed implementation detail (Algorithm 2, p.4–5).<br>• Realignment overhead is large (≈40%) and grows superlinearly with batch size, limiting scalability even when single-sequence speculation helps (Figure 5b, p.7; Section 3.1, p.4–5).<br>• Production systems struggle: vLLM speculative decoding underperforms at higher concurrency, SGLang+EAGLE shows negative speedups—indicating deeper scheduler redesign is needed (Figure 6, p.8).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>EQSPEC enforces minimal-correctness synchronization via an unpad–append–repad pipeline that recomputes position IDs and masks, handles the bonus token from the target model, and realigns rank-4 KV-cache with precise index offsets each round. EXSPEC reduces this overhead by keeping a sliding pool of sequences and dynamically batching same-length groups (no realignment needed), lazily falling back to EQSPEC only when grouping fails.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Scheduling SpecDec for Continuous Batching with Paged Attention: A unified scheduler that handles nested raggedness and integrates EXSPEC with production engines (e.g., vLLM/SGLang) for stable positive speedups.<br>• Memory-Locality-Aware KV-Cache for Cross-Batch Speculation: New cache layouts/compaction to cut per-verification latency while preserving EXSPEC’s reduced operation count.<br>• Theoretical Bounds on Alignment Overhead in Batched Speculation: Modeling c_overhead(B)=c_pad+c_kv under acceptance-variance to derive optimal k, window size W, and batch policies.<br>• Learned Grouping and Workload Shaping for Batched SpecDec: Predictive bucketing by length/acceptance to raise same-length grouping rates, combining dynamic sorting with EXSPEC.<br>• Portable Kernel Primitives for Dynamic Padding and KV Realignment: Lightweight, model-agnostic CUDA ops that accelerate unpad–append–repad without bespoke per-model kernels.<br>• Deterministic, 100% Output-Equivalent Speculative Decoding: Techniques to eliminate floating-point nondeterminism and tie-breaking drift, closing the ~5% gap to exact equivalence.<br>• Hybrid Tree-and-Sequence Speculation with Cross-Batch Scheduling: Marrying EAGLE/Medusa-style trees with EXSPEC’s batching to boost acceptance while retaining lossless verification.<br>• Multi-Draft and Lookahead-Enhanced EXSPEC: Co-optimizing draft size/number and lookahead decoding to shrink c_draft and lift acceptance, synergizing with grouping.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-11">

    <div class="paper">
        <h2 class="paper-title">SAO-Instruct: Free-form Audio Editing using Natural Language Instructions</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22795" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22795" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Generative models have made significant progress in synthesizing high-fidelity audio from short textual descriptions. However, editing existing audio using natural language has remained largely underexplored. Current approaches either require the complete description of the edited audio or are constrained to predefined edit instructions that lack flexibility. In this work, we introduce SAO-Instruct, a model based on Stable Audio Open capable of editing audio clips using any free-form natural language instruction. To train our model, we create a dataset of audio editing triplets (input audio, edit instruction, output audio) using Prompt-to-Prompt, DDPM inversion, and a manual editing pipeline. Although partially trained on synthetic data, our model generalizes well to real in-the-wild audio clips and unseen edit instructions. We demonstrate that SAO-Instruct achieves competitive performance on objective metrics and outperforms other audio editing approaches in a subjective listening study. To encourage future research, we release our code and model weights.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Free-form, natural-language audio editing is underexplored; users need to selectively modify specific aspects of an audio clip while preserving the overall context and ambience (pp.1–2; Fig. 1 on p.2)<br>• Zero-shot diffusion inversion methods require full, precise input/target descriptions that are hard to author and brittle to phrasing, making them unintuitive for real workflows (pp.2–3, Sec. 2.2)<br>• Prior instruction-based editors (e.g., AUDIT) support only predefined operations and cannot handle diverse, underspecified free-form instructions with varying scope (p.2)<br>• No available triplet datasets (input audio, edit instruction, edited audio) hinder supervised training for instruction-following audio editors (p.3)<br>• Current text-to-audio models show inconsistent prompt adherence on complex captions and often alter background context during edits; editing must preserve context while applying targeted changes (pp.4–5; Table 7 on p.15)</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>SAO-Instruct fine-tunes Stable Audio Open for instruction-based editing by conditioning on a free-form instruction, timing, and the input audio latent, and denoising from a noised encoding of the input to preserve context (Fig. 2 on p.3). It is trained on 150k triplets synthesized via three complementary pipelines—Prompt-to-Prompt with cross-attention injection and Bayesian optimization plus Gemini/CLAP filtering (Fig. 3 on p.5), DDPM inversion (ZETA) with BO, and deterministic manual edits with LLM-generated instructions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Composable SAO-Instruct: Multi-Step and Long-Horizon Audio Editing from Natural Language: Enable sequences of dependent edits and edit graphs, with controllable edit strength and order.<br>• Multilingual SAO-Instruct: Cross-Lingual and Code-Switching Free-Form Audio Editing: Extend instruction following beyond English, including cross-lingual alignment and culturally specific audio concepts.<br>• SAO-Instruct-Music: Structure-Aware Free-Form Instruction-Based Music Editing: Adapt the method to music with harmony/rhythm constraints and source-aware controls for musically consistent edits.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22590" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22590" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In today's rapidly expanding data landscape, knowledge extraction from unstructured text is vital for real-time analytics, temporal inference, and dynamic memory frameworks. However, traditional static knowledge graph (KG) construction often overlooks the dynamic and time-sensitive nature of real-world data, limiting adaptability to continuous changes. Moreover, recent zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance on prebuilt ontologies often suffer from instability across multiple runs, as well as incomplete coverage of key facts. To address these challenges, we introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that builds and continuously updates Temporal Knowledge Graphs (TKGs) from unstructured texts. ATOM splits input documents into minimal, self-contained "atomic" facts, improving extraction exhaustivity and stability. Then, it constructs atomic TKGs from these facts while employing a dual-time modeling that distinguishes when information is observed from when it is valid. The resulting atomic TKGs are subsequently merged in parallel. Empirical evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17% better stability, and over 90% latency reduction compared to baseline methods, demonstrating a strong scalability potential for dynamic TKG construction.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Static KG construction ignores temporal dynamics, limiting temporal reasoning and real-time updates<br>• Zero-/few-shot LLM-based methods are non-exhaustive on long contexts due to the “forgetting effect,” missing key facts and temporal details<br>• Instability across runs: identical prompts on the same text yield different KGs because of LLM stochasticity<br>• Lack of dual-time modeling (observation vs. validity time) causes temporal misattribution (e.g., treating publication date as event start)<br>• Poor scalability and high latency: heavy reliance on LLM calls for entity/temporal resolution and incremental pipelines prevent parallelism<br>• Entity/relation resolution errors and semantic drift without predefined ontologies (e.g., ambiguous entity names)<br>• Incomplete temporal resolution: difficulty aligning start/end actions and resolving relative time expressions consistently<br>• Need for a system that is exhaustive, stable across runs, and supports continuous, dynamic TKG updates at scale</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>ATOM decomposes documents into minimal, self-contained atomic facts with observation timestamps, extracts temporal 5-tuples in parallel with few-shot prompting (normalizing end-actions), and merges atomic TKGs via an LLM-independent, embedding-based, parallel binary merge that resolves entities/relations and aligns validity periods under dual-time modeling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Fine-Tuned Atomic Fact Decomposer for Temporal KGs: Train a task-specific LLM to reduce hallucinations and improve attachment of validity periods during atomic decomposition<br>• Supervised Entity and Relation Resolver for Atomic Merging: Replace thresholded cosine similarity with a learned classifier to cut false merges while preserving scalability<br>• Benchmarks and Metrics for Temporal Resolution in KGs: Create datasets and standardized measures for start/end alignment, relative-time normalization, and dual-time correctness<br>• Adaptive Similarity Learning for Scalable KG Merging: Learn dynamic, domain-aware thresholds and hybrid similarity functions for entities/relations over time<br>• Uncertainty-Aware Dual-Time Temporal KGs: Model and propagate uncertainty for observation and validity times (intervals, distributions) to improve downstream reasoning<br>• Multimodal ATOM: Temporal KG Construction from Text, Tables, and Images: Extend atomic fact extraction and dual-time modeling beyond text to multimodal sources<br>• Streaming ATOM: Hardware-Aware Parallelism for Low-Latency DTKG Updates: Optimize batching, threading, and memory to minimize end-to-end latency under real-time loads<br>• Human-in-the-Loop Verification for Atomic TKGs: Active learning and selective review to correct decomposed facts and time assignments with minimal annotation cost<br>• Agentic Reasoning over Dual-Time DTKGs: Integrate ATOM with agent memory and planning to enable temporal inference, forecasting, and decision support<br>• Continual and Cross-Domain Adaptation of Temporal KGs: Methods for domain shift, multilinguality, and continual learning while preserving temporal consistency</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Generalization or Memorization: Dynamic Decoding for Mode Steering</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22099" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22099" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) exhibit a troubling duality, capable of both remarkable generalization and brittle, verbatim memorization of their training data. This unpredictability undermines their reliability in high-stakes applications. In this work, we propose a unified framework to understand, identify, and control these distinct reasoning modes. First, we introduce a theoretical model based on the Information Bottleneck (IB) principle, formalizing generalization as the learning of a compressed, task-relevant representation and memorization as a failure to compress. Building on this theory, we develop Dynamic Mode Steering (DMS), a novel inference-time algorithm which comprises two components: (1) a lightweight, causally-grounded linear probe that identifies the model's instantaneous reliance on memorization, and (2) a dynamic activation steering mechanism that nudges the model's computation towards pre-identified generalization circuits. We frame DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning and faithfulness tasks demonstrate that DMS significantly improves logical consistency and factual accuracy, thereby offering a principled approach to enhancing LLM reliability.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• LLMs unpredictably switch between robust generalization and brittle verbatim memorization, undermining reliability via fluent falsehoods, bias propagation, and failures under distribution shift.<br>• Lack of a unified, principled theory to characterize and distinguish these modes; phenomena like grokking indicate distinct solutions in the loss landscape that current accounts do not formalize.<br>• Standard decoding (greedy, nucleus, beam) manipulates output logits but cannot control the model’s internal reasoning mode, so it fails to prevent memorized falsehoods or improve multi-step reasoning.<br>• Prior probes are largely correlational and may not target causally responsible components; existing activation-steering approaches are typically static, non-adaptive, and not triggered by real-time mode detection.<br>• Training-time fixes (e.g., weight decay, prolonged training, fine-tuning) are costly and cannot provide per-input, inference-time control over the model’s mode of operation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Dynamic Mode Steering (DMS) is a training-free, inference-time algorithm that uses a causally validated linear probe at a specific layer to detect momentary reliance on memorization, then applies a dynamically scaled activation-steering vector (mean generalization minus mean memorization activations) to nudge the model toward generalization circuits. This forms an adaptive, self-contrastive decoding scheme grounded in Information Bottleneck theory and causal activation patching.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Unsupervised Discovery of Behavior Vectors for Mode Control: Learn steering directions without labeled prompts using sparse dictionary learning, PCA, or autoencoders over activations.<br>• Multi-Attribute Dynamic Steering in LLMs: Jointly control generalization, honesty, and harmlessness via multiple probes/vectors with conflict-resolution and stability guarantees.<br>• From Information Bottleneck to Activation Geometry: A Theory of Mode Steering: Derive formal links between IB objectives, neural collapse geometry, and provable effects of activation perturbations.<br>• Causally Guided Multi-Layer and Time-Varying Steering for Robust Reasoning: Extend causal tracing to select sets of layers and schedule steering over the forward pass for harder reasoning tasks.<br>• Cross-Model Transfer and Robustness of Mode Probes: Study transferability of probes and steering vectors across model sizes/architectures and calibrate under dataset and domain shift.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-12">

    <div class="paper">
        <h2 class="paper-title">S-Chain: Structured Visual Chain-of-Thought For Medicine</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22728" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22728" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Faithful reasoning in medical vision-language models (VLMs) requires not only accurate predictions but also transparent alignment between textual rationales and visual evidence. While Chain-of-Thought (CoT) prompting has shown promise in medical visual question answering (VQA), no large-scale expert-level dataset has captured stepwise reasoning with precise visual grounding. We introduce S-Chain, the first large-scale dataset of 12,000 expert-annotated medical images with bounding boxes and structured visual CoT (SV-CoT), explicitly linking visual regions to reasoning steps. The dataset further supports 16 languages, totaling over 700k VQA pairs for broad multilingual applicability. Using S-Chain, we benchmark state-of-the-art medical VLMs (ExGra-Med, LLaVA-Med) and general-purpose VLMs (Qwen2.5-VL, InternVL2.5), showing that SV-CoT supervision significantly improves interpretability, grounding fidelity, and robustness. Beyond benchmarking, we study its synergy with retrieval-augmented generation, revealing how domain knowledge and visual grounding interact during autoregressive reasoning. Finally, we propose a new mechanism that strengthens the alignment between visual evidence and reasoning, improving both reliability and efficiency. S-Chain establishes a new benchmark for grounded medical reasoning and paves the way toward more trustworthy and explainable medical VLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Lack of faithful, interpretable reasoning in medical VLMs: models often output answers without transparent links between textual rationales and visual evidence (black-box behavior)<br>• No large-scale expert-verified datasets that couple stepwise Chain-of-Thought with explicit visual grounding (bounding boxes) for clinical imaging<br>• Synthetic CoTs (e.g., GPT-generated) are unstructured, weakly grounded, and prone to hallucinations and factual errors; bounding boxes are often missing/misaligned<br>• High cost and complexity of expert multimodal annotation hinder scalable, clinically valid supervision—especially for multilingual use<br>• Standard end-to-end training discards intermediate reasoning, enabling shortcut learning and reducing faithfulness during autoregressive generation<br>• Weak alignment between CoT text and ROI evidence (text-only box supervision underuses spatial cues), limiting reliability and robustness</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Introduce S-Chain: a 12k-image, 16-language dataset with expert-verified Structured Visual CoT linking four stages—ROI localization, lesion description, lesion grading (MTA/GCA/Koedam), and final diagnosis—via bounding boxes and structured prompts, then train VLMs with autoregressive supervised fine-tuning to generate grounded steps before the answer. Additionally, study RAG synergy and propose a lightweight ROI-anchoring and supervised-contrastive regularization to better align CoT embeddings with ROI vision tokens and separate disease-specific reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Faithful Vision-Language Reasoning via ROI-Aware Contrastive Pretraining: Scale ROI-anchoring and cross-modal contrastive objectives into pretraining to tightly couple visual tokens with stepwise CoT across diseases and modalities<br>• Temporal SV-CoT for Longitudinal Neuroimaging: Extend SV-CoT to time-series MRI/PET with non-linear, multi-expert clinical workflows to model disease progression and treatment response<br>• From Boxes to Masks: Fine-Grained Grounding for Medical Chain-of-Thought: Replace boxes with segmentation/keypoints and enforce mask-conditioned decoding to improve grounding fidelity and reduce hallucinations</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23667" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23667" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Structural topology optimization (TO) is central to engineering design but remains computationally intensive due to complex physics and hard constraints. Existing deep-learning methods are limited to fixed square grids, a few hand-coded boundary conditions, and post-hoc optimization, preventing general deployment. We introduce Optimize Any Topology (OAT), a foundation-model framework that directly predicts minimum-compliance layouts for arbitrary aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines a resolution- and shape-agnostic autoencoder with an implicit neural-field decoder and a conditional latent-diffusion model trained on OpenTO, a new corpus of 2.2 million optimized structures covering 2 million unique boundary-condition configurations. On four public benchmarks and two challenging unseen tests, OAT lowers mean compliance up to 90% relative to the best prior models and delivers sub-1 second inference on a single GPU across resolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These results establish OAT as a general, fast, and resolution-free framework for physics-aware topology optimization and provide a large-scale dataset to spur further research in generative modeling for inverse design. Code & data can be found at https://github.com/ahnobari/OptimizeAnyTopology.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Conventional TO is computationally expensive (iterative FEA + optimization) and must be re-solved for every change in loads/fixtures/geometry, blocking interactive or large-scale design.<br>• Prior DL methods are tied to fixed square grids and image-based representations, lacking shape/aspect-ratio/resolution invariance and thus failing on arbitrary domains.<br>• Generalization is poor because models are trained on tiny, hand-crafted boundary-condition sets; they often need post-hoc optimization and slow dramatically at higher resolutions; large, diverse datasets covering interior/distributed loads and fixtures have been absent.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>OAT couples a resolution-/shape-agnostic autoencoder with a convolutional neural-field renderer to encode arbitrary topologies into fixed-size latents, and a conditional latent diffusion model (with classifier-free guidance) conditioned on BPOM-encoded loads/fixtures plus volume fraction, aspect ratio, and cell size to generate minimum-compliance designs. Trained on the 2.2M-sample OpenTO dataset, it enables sub-second, resolution-free inference with optional few-step SIMP refinement.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Optimizer-Guided RL for Failure-Resilient Topology Diffusion: Use FEA/SIMP-derived rewards and preference optimization to reduce failure rate and improve compliance without post-hoc optimization.<br>• Multi-Physics OAT: A Foundation Model for Stress, Buckling, and Thermal-Constrained TO: Extend conditioning, data, and training to handle coupled objectives and constraints across multiple physics.<br>• Few-Shot Adaptation of Resolution-Free TO Foundation Models: Develop lightweight fine-tuning methods to adapt OAT to new physics, materials, and manufacturing constraints with minimal labeled data.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22373" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22373" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Visualization, a domain-specific yet widely used form of imagery, is an effective way to turn complex datasets into intuitive insights, and its value depends on whether data are faithfully represented, clearly communicated, and aesthetically designed. However, evaluating visualization quality is challenging: unlike natural images, it requires simultaneous judgment across data encoding accuracy, information expressiveness, and visual aesthetics. Although multimodal large language models (MLLMs) have shown promising performance in aesthetic assessment of natural images, no systematic benchmark exists for measuring their capabilities in evaluating visualizations. To address this, we propose VisJudge-Bench, the first comprehensive benchmark for evaluating MLLMs' performance in assessing visualization aesthetics and quality. It contains 3,090 expert-annotated samples from real-world scenarios, covering single visualizations, multiple visualizations, and dashboards across 32 chart types. Systematic testing on this benchmark reveals that even the most advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a correlation with human ratings of only 0.429. To address this issue, we propose VisJudge, a model specifically designed for visualization aesthetics and quality assessment. Experimental results demonstrate that VisJudge significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a 19.8% reduction) and increasing the consistency with human experts to 0.681 (a 58.7% improvement) compared to GPT-5. The benchmark is available at https://github.com/HKUSTDial/VisJudgeBench.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Lack of a comprehensive, domain-specific benchmark to assess visualization quality jointly across Fidelity, Expressiveness, and Aesthetics.<br>• General MLLMs focus on surface aesthetics and underperform on visualization-specific judgment, showing large gaps vs. human experts (e.g., MAE 0.551, correlation 0.429).<br>• Existing benchmarks are misaligned: chart QA tests information extraction (not design quality), natural-image aesthetics ignores communicative effectiveness, and NL2VIS evaluation checks query-match rather than intrinsic visualization quality.<br>• Prior evaluations seldom cover the full spectrum of visualization forms (single charts, multi-view, dashboards) and are not adaptively tailored to chart types.<br>• High-stakes importance: misleading encodings and poor readability can distort insights and decisions, making multi-dimensional, expert-aligned assessment essential.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>The paper builds VisJudge-Bench, a 3,090-sample, expert-annotated benchmark spanning 32 chart types and three visualization forms, with an adaptive, chart-aware evaluation framework across six sub-dimensions (Data Fidelity; Readability and Insight Discovery; Design Style, Visual Composition, and Color Harmony) using metadata-driven question generation and rigorous quality control. It further proposes VisJudge, a visualization-specific evaluator that substantially narrows the human–model gap (MAE 0.442; correlation 0.681).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• VisJudge-RM: Reward Modeling for RLHF in Visualization Generation: Train a reward model on VisJudge-Bench to guide NL2VIS/ChartGPT systems toward fidelity-, expressiveness-, and aesthetics-aware outputs.<br>• ChartEval-X: Cross-Domain and Cross-Language Generalization of Visualization Quality Judges: Evaluate and adapt judges to domains (medical, finance) and multilingual chart text.<br>• Explainable VisJudging: Rationale and Counterfactual Explanations for Chart Quality Assessment: Generate fine-grained justifications and show minimal edits that fix fidelity/expressiveness/aesthetics errors.<br>• Robust VisJudging: Adversarial and Misleading Chart Defense: Stress-test and harden judges against deceptive encodings (e.g., axis truncation, color scale misuse, 3D distortions).<br>• Human–AI Co-Judging: Modeling Expert Disagreement and Uncertainty Calibration in Visualization Assessment: Learn annotator preference distributions and calibrate confidence to align with expert variance.<br>• Accessible VisJudging: Integrating Accessibility and Color-Vision-Deficiency Metrics into Visualization Quality Assessment: Extend the framework with accessibility-aware scoring and remediation suggestions.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-13">

    <div class="paper">
        <h2 class="paper-title">GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22319" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22319" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recently, GRPO-based reinforcement learning has shown remarkable progress in optimizing flow-matching models, effectively improving their alignment with task-specific rewards. Within these frameworks, the policy update relies on importance-ratio clipping to constrain overconfident positive and negative gradients. However, in practice, we observe a systematic shift in the importance-ratio distribution-its mean falls below 1 and its variance differs substantially across timesteps. This left-shifted and inconsistent distribution prevents positive-advantage samples from entering the clipped region, causing the mechanism to fail in constraining overconfident positive updates. As a result, the policy model inevitably enters an implicit over-optimization stage-while the proxy reward continues to increase, essential metrics such as image quality and text-prompt alignment deteriorate sharply, ultimately making the learned policy impractical for real-world use. To address this issue, we introduce GRPO-Guard, a simple yet effective enhancement to existing GRPO frameworks. Our method incorporates ratio normalization, which restores a balanced and step-consistent importance ratio, ensuring that PPO clipping properly constrains harmful updates across denoising timesteps. In addition, a gradient reweighting strategy equalizes policy gradients over noise conditions, preventing excessive updates from particular timestep regions. Together, these designs act as a regulated clipping mechanism, stabilizing optimization and substantially mitigating implicit over-optimization without relying on heavy KL regularization. Extensive experiments on multiple diffusion backbones (e.g., SD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard significantly reduces over-optimization while maintaining or even improving generation quality.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• In GRPO for flow-matching models (e.g., FlowGRPO/DanceGRPO), the importance-ratio distribution is systematically left-shifted (mean < 1) and its variance varies by timestep, so PPO clipping fails to constrain overconfident positive-advantage updates (Figure 2 on page 5).<br>• This causes implicit over-optimization/reward hacking: proxy scores rise while gold metrics (image quality, text–prompt alignment, diversity) collapse, yielding unusable policies (Figure 1 on page 1; training curves in Figure 4 on page 8).<br>• Root cause: a mismatch between Gaussian transition probabilities in diffusion/flow models and the discrete-token assumptions behind GRPO in LLMs leads to timestep-dependent bias and variance in ratios (Section 3.2.1, pages 4–5).<br>• Existing fixes (heavy KL penalties) slow learning and still leave high-noise steps under-regularized; recent efficiency-focused variants (TempFlowGRPO, MixGRPO, Flow-CPS) largely overlook over-optimization (Section 2, pages 2–3).<br>• Gradient magnitudes are highly imbalanced across timesteps, causing single-step domination and accelerating over-optimization (Figure 3 on page 7).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>GRPO-Guard introduces RatioNorm to normalize per-timestep log-importance ratios, recentering the mean and stabilizing variance so PPO clipping symmetrically truncates harmful updates across all denoising steps, and applies timestep-aware gradient reweighting (δ = 1/dt for FlowGRPO; δ = β/dt for DanceGRPO) to equalize gradient magnitudes across noise levels. Together, these form a regulated clipping mechanism that mitigates implicit over-optimization without heavy KL regularization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Adaptive Per-Timestep Clipping for Diffusion RL: Learn noise-aware, step-conditioned clip ranges on top of RatioNorm to further balance positive/negative truncation under varying dynamics.<br>• Gold-Score Faithful Reward Ensembles for Flow-Matching RL: Build scalable, efficient multi-metric reward models that better approximate human-preference gold scores, reducing proxy–gold gaps and residual reward hacking.<br>• Integrating Ratio-Normalized GRPO with Noise-Consistent and Mixed Sampling: Combine GRPO-Guard with Flow-CPS and MixGRPO/TempFlowGRPO to jointly achieve stability, accuracy, and efficiency in policy optimization.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21323" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21323" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The alignment of vision-language representations endows current Vision-Language Models (VLMs) with strong multi-modal reasoning capabilities. However, the interpretability of the alignment component remains uninvestigated due to the difficulty in mapping the semantics of multi-modal representations into a unified concept set. To address this problem, we propose VL-SAE, a sparse autoencoder that encodes vision-language representations into its hidden activations. Each neuron in its hidden layer correlates to a concept represented by semantically similar images and texts, thereby interpreting these representations with a unified concept set. To establish the neuron-concept correlation, we encourage semantically similar representations to exhibit consistent neuron activations during self-supervised training. First, to measure the semantic similarity of multi-modal representations, we perform their alignment in an explicit form based on cosine similarity. Second, we construct the VL-SAE with a distance-based encoder and two modality-specific decoders to ensure the activation consistency of semantically similar representations. Experiments across multiple VLMs (e.g., CLIP, LLaVA) demonstrate the superior capability of VL-SAE in interpreting and enhancing the vision-language alignment. For interpretation, the alignment between vision and language representations can be understood by comparing their semantics with concepts. For enhancement, the alignment can be strengthened by aligning vision-language representations at the concept level, contributing to performance improvements in downstream tasks, including zero-shot image classification and hallucination elimination. Codes are available at https://github.com/ssfgunner/VL-SAE.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Lack of interpretability for vision-language alignment: existing VLMs align image/text features but we cannot explain the alignment because semantics from both modalities aren’t mapped to a unified concept set (Abstract; p.1-2)<br>• Single-modality interpretations don’t transfer: applying concept methods separately to image and text yields concept mismatch across modalities, preventing fair semantic comparison (Fig. 1a-b, p.2)<br>• Pre-defined concept sets are limited and costly: they miss many semantics and require labeled examples per concept; learnable SAEs are uncontrollable and yield mismatched concept spaces across modalities (p.2-3)<br>• Measuring cross-modal semantic similarity is hard in LVLMs: alignment is implicit, so similarity isn’t directly tied to cosine similarity as in contrastive models (p.4)<br>• Modality-specific distributions break activation consistency: a single decoder forces activations to absorb distributional info, degrading cross-modal concept consistency (p.6)<br>• Misalignment leads to practical failures (e.g., hallucinations): irrelevant single-modality concepts get activated and aren’t filtered without a unified concept space (Fig. 5, p.8; Fig. 6, p.9)</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>VL-SAE introduces an auxiliary autoencoder to make LVLM features explicitly aligned (via InfoNCE on intermediate representations) and a sparse autoencoder with a distance-based encoder and two modality-specific decoders to map both image and text representations into a unified, consistent concept set. Neuron activations (Top-K) reflect shared concepts across modalities, enabling interpretation and improving downstream tasks (zero-shot classification, hallucination mitigation).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Uniformly Activated Multimodal SAEs: Reducing Dead Neurons and Balancing Concept Coverage: Design training/regularization to avoid dead neurons and encourage uniform activation for richer, more stable concept sets.<br>• Relational Concept Graphs for Vision-Language Alignment: Model inter-concept relations (e.g., with graph neural networks) to capture compositional and contextual semantics beyond independent concepts.<br>• Concept-Guided Decoding for Hallucination-Free LVLMs: Integrate concept activations more deeply into generation (training-time and decoding-time) to constrain text to vision-grounded concepts.<br>• Benchmarking Multimodal Concept Sets: Metrics, Protocols, and Datasets: Develop standardized, robust metrics beyond CLIP similarity and curated datasets to evaluate concept consistency, diversity, and usability.<br>• Continual and Unlearning with Unified Concept Sets in VLMs: Use concept-level control for selective unlearning and continual learning while preserving cross-modal alignment.<br>• Scaling VL-SAE to Web-Scale and New Modalities (Audio/Video/3D): Study data scaling laws and extend the unified concept set to additional modalities and temporal/spatial structure.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20155" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20155" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Understanding objects at the level of their constituent parts is fundamental to advancing computer vision, graphics, and robotics. While datasets like PartNet have driven progress in 3D part understanding, their reliance on untextured geometries and expert-dependent annotation limits scalability and usability. We introduce PartNeXt, a next-generation dataset addressing these gaps with over 23,000 high-quality, textured 3D models annotated with fine-grained, hierarchical part labels across 50 categories. We benchmark PartNeXt on two tasks: (1) class-agnostic part segmentation, where state-of-the-art methods (e.g., PartField, SAMPart3D) struggle with fine-grained and leaf-level parts, and (2) 3D part-centric question answering, a new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary part grounding. Additionally, training Point-SAM on PartNeXt yields substantial gains over PartNet, underscoring the dataset's superior quality and diversity. By combining scalable annotation, texture-aware labels, and multi-task evaluation, PartNeXt opens new avenues for research in structured 3D understanding.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Existing 3D part datasets (e.g., PartNet) lose texture due to remeshing and often deform geometry, removing vital color/material cues and creating alignment burdens for textured use cases (pp. 2–3, Table 7 p.16).<br>• Annotation workflows demand expert 3D skills (mesh cutting, interior inspection), limiting crowdsourcing, throughput, and scalability for fine-grained, hierarchical labels (pp. 2–5).<br>• Annotating interior/occluded parts and maintaining consistent, extensible hierarchies across categories is hard; taxonomy design and variant coverage are costly (pp. 3–5).<br>• SOTA class-agnostic part segmentation struggles on fine-grained and leaf-level parts, with over-segmentation, weak-texture discontinuity, and failure to separate adjacent regions (Sec. 4.1, Fig. 4, Table 2/8).<br>• 3D LLMs lack robust part-centric understanding (counting, classification, grounding), with low accuracy/IoU and formatting issues; no established benchmark existed (Sec. 4.2, Table 3, Figs. 8–9).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Build PartNeXt: 23,519 high-quality textured meshes across 50 categories with fine-grained, hierarchical part labels, created via a scalable web interface that operates directly on textured meshes (dual-panel views) and offers connected-component, 2D box, and per-face selection, plus AI assistance (CLIP-based asset filtering and GPT-4o–bootstrapped hierarchies and visual examples). Release two benchmarks—class-agnostic part instance segmentation and 3D part-centric QA—and show training Point-SAM on PartNeXt yields stronger generalization than training on PartNet (Sec. 4.3, Table 4).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Open-Vocabulary 3D Part Annotation via VLM-Guided Hierarchy Induction: Leverage VLMs with active learning to auto-expand taxonomies and annotate parts in the wild, addressing limited predefined hierarchies and scalability.<br>• Texture-Aware, Hierarchy-Consistent Part Segmentation Networks: Design models that jointly exploit geometry and texture while controlling granularity to reduce over-segmentation and preserve continuity on leaf-level parts.<br>• Part-Centric 3D LLMs for Counting, Classification, and Grounding: Develop architectures and training curricula tailored to structured 3D outputs, improving robustness on the proposed QA benchmark with precise 3D grounding and count reasoning.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-14">

    <div class="paper">
        <h2 class="paper-title">Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23828" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23828" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present a comprehensive evaluation of the ability of large language models (LLMs) to process culturally grounded language, specifically to understand and pragmatically use figurative expressions that encode local knowledge and cultural nuance. Using figurative language as a proxy for cultural nuance and local knowledge, we design evaluation tasks for contextual understanding, pragmatic use, and connotation interpretation in Arabic and English. We evaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms, multidialectal Arabic proverbs, and English proverbs. Our results show a consistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower than for English proverbs, and performance for Egyptian idioms is 10.28% lower than for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07% relative to understanding, though providing contextual idiomatic sentences improves accuracy by 10.66%. Models also struggle with connotative meaning, reaching at most 85.58% agreement with human annotators on idioms with 100% inter-annotator agreement. These findings demonstrate that figurative language serves as an effective diagnostic for cultural reasoning: while LLMs can often interpret figurative meaning, they face challenges in using it appropriately. To support future research, we release Kinayat, the first dataset of Egyptian Arabic idioms designed for both figurative understanding and pragmatic use evaluation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• LLMs’ cultural reasoning remains underexplored: models can often paraphrase figurative meaning but struggle to use it appropriately in context, affecting real-world communication tasks.<br>• Prior work emphasizes figurative comprehension over pragmatic use; little evaluation of context sensitivity, social appropriateness, and affect/connotation.<br>• Arabic is under-served: major Arabic cultural benchmarks largely omit figurative language or include it sparsely; no dedicated Egyptian Arabic idiom resource for pragmatic evaluation before this work.<br>• Existing MCQ setups are vulnerable to shortcuts (implausible distractors, option-order bias) and rarely test negation or context-conditioned interpretation.<br>• The balance between memorization and reasoning across languages/dialects (English vs Arabic; MSA vs Egyptian dialect) is poorly understood.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces a unified evaluation suite and the Kinayat dataset of Egyptian Arabic idioms to test figurative interpretation, pragmatic use, connotation, negation, completion, and explanation generation across 22 LLMs. It constructs hard, plausible distractors via two strategies (general and SRL-based), adds contextual sentences, and uses both automatic (accuracy, BERTScore) and LLM-as-a-judge evaluations to quantify a consistent ‘Pragmatics Gap’ and cross-lingual/cross-dialect gaps.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Beyond MCQ: Generative Pragmatic Competence for Arabic Idioms: Evaluate free-form generation of idioms in context with human and rubric-based scoring for appropriateness, register, and persona.<br>• Perspectivist Connotation Modeling for Figurative Language in Arabic: Collect multi-perspective labels and model connotation as distributions conditioned on context, demographics, and stance.<br>• Adversarial Cultural Stress Tests via Structured Semantic Perturbations: Use SRL/AMR to craft minimally altered, culturally plausible distractors and probe robustness to negation and role swaps.<br>• Cross-Dialect Transfer Learning for Figurative Language in the Arab World: Build multi-dialect idiom/proverb corpora and study transfer, few-shot adaptation, and dialect-aware instruction tuning.<br>• Multimodal Cultural Grounding of Proverbs and Idioms: Incorporate historical, musical, and visual artifacts to ground figurative meaning and improve pragmatic use.<br>• Debiasing MCQ Cultural Benchmarks: Quantify and mitigate option-order and format biases with calibrated decoding, randomized layouts, and logit-normalized scoring.<br>• Reasoning vs. Memorization in Figurative Knowledge Across Languages: Controlled ablations and synthetic variants to disentangle recall of seen proverbs from genuine cultural reasoning.<br>• Pragmatics-RLHF for Idiom Appropriateness: Curate Kinayat++ with human feedback on usage quality and optimize models with preference learning focused on social appropriateness.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22264" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22264" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Patent text embeddings enable prior art search, technology landscaping, and patent analysis, yet existing benchmarks inadequately capture patent-specific challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15 tasks across retrieval, classification, paraphrase, and clustering, with 2.06 million examples. PatenTEB employs domain-stratified splits, domain specific hard negative mining, and systematic coverage of asymmetric fragment-to-document matching scenarios absent from general embedding benchmarks. We develop the patembed model family through multi-task training, spanning 67M to 344M parameters with context lengths up to 4096 tokens. External validation shows strong generalization: patembed-base achieves state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445 previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM. Systematic ablations reveal that multi-task training improves external generalization despite minor benchmark costs, and that domain-pretrained initialization provides consistent advantages across task families. All resources will be made available at https://github.com/iliass-y/patenteb. Keywords: patent retrieval, sentence embeddings, multi-task learning, asymmetric retrieval, benchmark evaluation, contrastive learning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Lack of patent-specific evaluation: General benchmarks (e.g., MTEB) omit patent tasks, and existing patent datasets are single-task and miss asymmetric fragment-to-document matching, domain stratification, and leakage prevention (see Sec. 2.3; construction workflow in Fig. 1, p.6).<br>• Real-world retrieval is cross-domain and asymmetric: Performance drops 3–6× when matching across disjoint IPC domains, a scenario not well captured by prior work (see Fig. 5, p.20; DAPFAM results in Table 16, p.35).<br>• Long, structured, technical texts: Patents require embeddings that handle long-range dependencies, specialized rhetoric (problem/effect/solution), and role-asymmetric queries (titles/problems/effects) beyond standard web/news text.<br>• Fragmented modeling landscape: Prior patent encoders focus on subsets (classification or document similarity) and lack a unified, multi-task encoder with prompt conditioning and strong external generalization (Sec. 2.2).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Introduce PatenTEB, a 15-task benchmark with domain-stratified splits, domain-aware hard-negative mining, and comprehensive coverage of symmetric/asymmetric retrieval, classification, paraphrase, and clustering. Train patembed, an instruction-prompted multi-task dual-encoder family (67M–344M) initialized from BERT-for-Patents with contrastive/triplet losses, prompt-based task conditioning, knowledge distillation across sizes, and long-context variants (validated on BigPatent and DAPFAM; Table 16, p.35).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Knowledge-Augmented Cross-Domain Patent Retrieval: Integrate IPC ontologies, citation/inventor graphs, and concept linking into hybrid encoders to close the 3–6× IN→OUT domain gap observed in retrieval (Fig. 5; Table 16).<br>• Multilingual PatenTEB and patembed-X: Build a cross-lingual benchmark and instruction-tuned encoders for global patents, aligning multilingual titles/abstracts/claims into a shared embedding space for zero-shot prior art search.<br>• AutoWeighted Multi-Task Instruction Tuning for Patent Embeddings: Learn task weights/curricula (e.g., uncertainty- or gradient-based) to optimize external generalization without sacrificing in-benchmark balance across retrieval, paraphrase, classification, and clustering.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 14</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>← 上一页</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button><button class="page-btn" onclick="goToPage(11)">11</button><button class="page-btn" onclick="goToPage(12)">12</button><button class="page-btn" onclick="goToPage(13)">13</button><button class="page-btn" onclick="goToPage(14)">14</button></span>
            <button id="next-btn" onclick="changePage(1)">下一页 →</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-10-30 02:01:30 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 14;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>