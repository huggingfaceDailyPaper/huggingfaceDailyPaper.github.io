<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - October 29, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "SF Pro Display", -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Helvetica Neue", "Arial", "Noto Sans", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", sans-serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">October 29, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">InteractComp: Evaluating Search Agents With Ambiguous Queries</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24668" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24668" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Language agents have demonstrated remarkable potential in web search and information retrieval. However, these search agents assume user queries are complete and unambiguous, an assumption that diverges from reality where users begin with incomplete queries requiring clarification through interaction. Yet most agents lack interactive mechanisms during the search process, and existing benchmarks cannot assess this capability. To address this gap, we introduce InteractComp, a benchmark designed to evaluate whether search agents can recognize query ambiguity and actively interact to resolve it during search. Following the principle of easy to verify, interact to disambiguate, we construct 210 expert-curated questions across 9 domains through a target-distractor methodology that creates genuine ambiguity resolvable only through interaction. Evaluation of 17 models reveals striking failure: the best model achieves only 13.73% accuracy despite 71.50% with complete context, exposing systematic overconfidence rather than reasoning deficits. Forced interaction produces dramatic gains, demonstrating latent capability current strategies fail to engage. Longitudinal analysis shows interaction capabilities stagnated over 15 months while search performance improved seven-fold, revealing a critical blind spot. This stagnation, coupled with the immediate feedback inherent to search tasks, makes InteractComp a valuable resource for both evaluating and training interaction capabilities in search agents. The code is available at https://github.com/FoundationAgents/InteractComp.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°å®æŸ¥è¯¢å­˜åœ¨æ­§ä¹‰ä¸”ä¿¡æ¯ä¸å®Œæ•´ï¼Œä½†ç°æœ‰æœç´¢ä»£ç†æ™®éå‡è®¾æŸ¥è¯¢å®Œæ•´ï¼Œå¯¼è‡´åœ¨æœªæ¾„æ¸…çš„æƒ…å†µä¸‹æ­¦æ–­ä½œç­”ã€äº§ç”Ÿé”™è¯¯ä¸èµ„æºæµªè´¹ï¼ˆå›¾1ï¼Œç¬¬2é¡µæ˜¾ç¤ºï¼šå®Œæ•´æŸ¥è¯¢åŸºå‡†æˆç»©å¿«é€Ÿæå‡ï¼Œè€Œäº¤äº’æ¶ˆæ­§ä»»åŠ¡é•¿æœŸåœæ»ï¼‰ã€‚<br>â€¢ è¯„æµ‹ç¼ºå£çªå‡ºï¼šæœç´¢ç±»åŸºå‡†ï¼ˆå¦‚BrowseCompã€GAIAï¼‰é»˜è®¤ä¿¡æ¯å……åˆ†ï¼Œæ— æ³•è¡¡é‡â€œè¯†åˆ«æ­§ä¹‰å¹¶ä¸»åŠ¨æ¾„æ¸…â€çš„èƒ½åŠ›ï¼›äº¤äº’ç±»åŸºå‡†ï¼ˆå¦‚IN3ã€Tau-Benchï¼‰åˆç¼ºå°‘å¯éªŒè¯çš„æ£€ç´¢è½åœ°ä»»åŠ¡ï¼Œéš¾ä»¥é‡åŒ–äº¤äº’å¯¹ç­”æ¡ˆæ­£ç¡®æ€§çš„è´¡çŒ®ï¼ˆç¬¬3é¡µç›¸å…³å·¥ä½œï¼‰ã€‚<br>â€¢ å…³é”®ç—‡ç»“æ˜¯ç­–ç•¥è€Œéèƒ½åŠ›ï¼šåœ¨å¯ç”¨äº¤äº’æ¡ä»¶ä¸‹ï¼Œæ¨¡å‹ä»ç³»ç»Ÿæ€§è¿‡åº¦è‡ªä¿¡ã€å¾ˆå°‘å‘é—®ï¼Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä½äºâ€œæä¾›å®Œæ•´ä¸Šä¸‹æ–‡â€æ—¶çš„ä¸Šé™ï¼›å¼ºåˆ¶äº¤äº’å¯å¤§å¹…æå‡å‡†ç¡®ç‡ï¼Œè¡¨æ˜èƒ½åŠ›æ˜¯æ½œåœ¨çš„ä½†æœªè¢«æœ‰æ•ˆè§¦å‘ï¼ˆè¡¨2ç¬¬7é¡µæœ€ä½³ä»…13.73%ï¼›è¡¨3ç¬¬8é¡µç»™è¶³ä¸Šä¸‹æ–‡å¯è¾¾71.50%ï¼›å¼ºåˆ¶äº¤äº’å®éªŒç¬¬8-9é¡µå¯ç¿»å€ï¼‰ã€‚<br>â€¢ è®­ç»ƒä¿¡å·æœºä¼šï¼šæœç´¢ä»»åŠ¡ç­”æ¡ˆæ˜“éªŒè¯ä¸”åé¦ˆæ¸…æ™°ï¼Œé€‚åˆç”¨ä½œå¼ºåŒ–å­¦ä¹ ç­‰è®­ç»ƒäº¤äº’ç­–ç•¥ï¼ˆç¬¬9é¡µç»“è®ºï¼‰ï¼Œå¡«è¡¥â€œä½•æ—¶é—®ã€é—®ä»€ä¹ˆâ€çš„ç­–ç•¥å­¦ä¹ ç©ºç™½ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºINTERACTCOMPåŸºå‡†ï¼šé‡‡ç”¨ç›®æ ‡-å¹²æ‰°é¡¹æ„é€ ï¼Œç”¨å…±äº«å±æ€§ç”Ÿæˆä¸å¯ç›´æ¥åˆ¤å®šçš„æ­§ä¹‰é—®å¥ï¼Œå°†åŒºåˆ†æ€§å±æ€§ä½œä¸ºéšè—ä¸Šä¸‹æ–‡ï¼Œéœ€é€šè¿‡ä¸æ¨¡æ‹Ÿç”¨æˆ·çš„å°é—­å¼é—®ç­”ï¼ˆyes/no/I don't knowï¼‰æ¥æ¶ˆæ­§ã€‚é…åˆReActå¼ä»£ç†ä¸ä¸¤é˜¶æ®µæ•°æ®æ ¡éªŒä¸è‡ªåŠ¨è¯„åˆ†åè®®ï¼Œç³»ç»Ÿè¯„ä¼°å¹¶è¯Šæ–­â€œè¯†åˆ«æ­§ä¹‰â€”ä¸»åŠ¨æé—®â€”æ£€ç´¢â€”ä½œç­”â€çš„å…¨æµç¨‹èƒ½åŠ›ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Ask-or-Answer: åŸºäºä¸ç¡®å®šæ€§æ ¡å‡†çš„æ¾„æ¸…è§¦å‘å­¦ä¹ ï¼šä»¥æ ¡å‡†æŸå¤±ä¸ç½®ä¿¡ä¼°è®¡è®­ç»ƒâ€œä½•æ—¶æé—®â€çš„é—¨æ§ç­–ç•¥ï¼Œé™ä½è¿‡åº¦è‡ªä¿¡ã€‚<br>â€¢ Interact-RL: ç”¨å¯éªŒè¯æœç´¢å›æŠ¥ä¼˜åŒ–ä¸»åŠ¨äº¤äº’ä»£ç†ï¼šå°†ç­”å¯¹ç‡ä¸æé—®æˆæœ¬çº³å…¥å¥–åŠ±ï¼Œå¼ºåŒ–å­¦ä¹ æœ€ä¼˜æé—®æ•°é‡ä¸é¡ºåºã€‚<br>â€¢ Ask-Search Co-Planning: æé—®ä¸æ£€ç´¢çš„è”åˆè§„åˆ’æ¡†æ¶ï¼šæŠŠå‘é—®ä¸æœç´¢ç»Ÿä¸€æœ¬ä½“åŒ–ä¸ºè¡ŒåŠ¨ç©ºé—´ï¼Œå­¦ä¹ ååŒç­–ç•¥ä»¥æœ€å°åŒ–æ— æ•ˆæ£€ç´¢ã€‚<br>â€¢ Robust Clarification under Noisy Users: é¢å‘å™ªå£°ä¸ä¸ç¡®å®šåé¦ˆçš„ç¨³å¥äº¤äº’ï¼šæ‰©å±•æ¨¡æ‹Ÿå™¨åˆ°å«å™ª/å»¶è¿Ÿ/éäºŒå…ƒå›ç­”ï¼Œå¹¶å­¦ä¹ å¯¹æŠ—æ€§ç¨³å¥ç­–ç•¥ã€‚<br>â€¢ Multilingual & Multimodal InteractComp: è·¨è¯­è¨€ä¸å¤šæ¨¡æ€çš„æ­§ä¹‰æ¶ˆè§£åŸºå‡†ï¼šå°†ç›®æ ‡-å¹²æ‰°é¡¹æ‰©å±•åˆ°å›¾åƒ/è§†é¢‘/è¡¨æ ¼ä¸å¤šè¯­åœºæ™¯ï¼Œç ”ç©¶è·¨æ¨¡æ€æ¾„æ¸…æé—®ã€‚<br>â€¢ From Forced to Free Interaction: å¼ºåˆ¶äº¤äº’åˆ°è‡ªå‘äº¤äº’çš„ç­–ç•¥è’¸é¦ï¼šç”¨å¼ºåˆ¶äº¤äº’äº§ç”Ÿçš„é«˜æ€§èƒ½è½¨è¿¹åšç¦»çº¿RLä¸ç­–ç•¥è’¸é¦ï¼Œæå‡è‡ªå‘äº¤äº’è´¨é‡ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Tongyi DeepResearch Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24701" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24701" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šæ·±åº¦ç ”ç©¶ä»»åŠ¡éœ€è¦å¤šæ­¥æ¨ç†ã€è·¨æºæ£€ç´¢ä¸é•¿æ—¶ç¨‹è§„åˆ’ï¼Œç°æœ‰ç³»ç»Ÿå¤§å¤šé—­æºã€è¿‡ç¨‹ä¸å¯è§ï¼Œç¼ºä¹å¼€æ”¾ã€å¯å¤ç°çš„æ–¹æ³•ä¸æ¨¡å‹<br>â€¢ èƒ½åŠ›ç¼ºå£ï¼šé€šç”¨åŸºç¡€æ¨¡å‹ç¼ºå°‘â€œä»£ç†æ€§â€å½’çº³åç½®ï¼Œä»…åšåè®­ç»ƒåŒæ—¶å­¦ä¹ å¯¹é½ä¸ä»£ç†è¡Œä¸ºæ˜“äº§ç”Ÿä¼˜åŒ–å†²çªã€æ•ˆæœæ¬¡ä¼˜<br>â€¢ æ•°æ®ç“¶é¢ˆï¼šç ”ç©¶çº§é—®é¢˜ä¸ä»£ç†è½¨è¿¹å¤©ç„¶ç¨€ç¼ºï¼Œäººå·¥æ ‡æ³¨æˆæœ¬é«˜ä¸”éš¾éªŒè¯ï¼Œéš¾ä»¥ä¾èµ–è‡ªç„¶æ•°æ®å®ç°å¯æ‰©å±•è®­ç»ƒ<br>â€¢ ç¯å¢ƒæŒ‘æˆ˜ï¼šçœŸå®Webç¯å¢ƒéå¹³ç¨³ã€äº¤äº’æˆæœ¬é«˜ä¸”æ˜“å¤±è´¥ï¼Œå¯¼è‡´è®­ç»ƒ/è¯„æµ‹éš¾ä»¥ç¨³å®šå¤ç°ã€éš¾ä»¥å½’å› <br>â€¢ ä¸Šä¸‹æ–‡é™åˆ¶ï¼šè¶…é•¿æ¨ç†ä¸å¤šè½®å·¥å…·è°ƒç”¨æ˜“å¯¼è‡´ä¸Šä¸‹æ–‡æ‹¥å¡ä¸æ³¨æ„åŠ›æ¶£æ•£ï¼Œä¼ ç»ŸReActéš¾åœ¨è¶…é•¿åºåˆ—ä¸­ä¿æŒèšç„¦å’Œè®°å¿†</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºç»Ÿä¸€çš„ç«¯åˆ°ç«¯ä»£ç†åŒ–è®­ç»ƒèŒƒå¼ï¼šä¸­è®­ç»ƒç”¨Agentic CPTï¼ˆ32Kâ†’128Kï¼‰ä¸å¤§è§„æ¨¡è‡ªåŠ¨åˆæˆâ€œè§„åˆ’â€”æ¨ç†â€”å†³ç­–â€”å‡½æ•°è°ƒç”¨â€æ•°æ®æ³¨å…¥ä»£ç†æ€§åç½®ï¼›åè®­ç»ƒä»¥é«˜è´¨é‡å¯éªŒè¯åˆæˆQAåšSFTå†·å¯åŠ¨ï¼Œå¹¶åœ¨æ²™ç®±åŒ–å·¥å…·ä¸å¼‚æ­¥rolloutæ¡†æ¶ä¸‹è¿›è¡Œä¸¥æ ¼on-policyçš„RLVRå¼ºåŒ–å­¦ä¹ ï¼Œé…å¥—â€œå…ˆéªŒä¸–ç•Œâ€”æ¨¡æ‹Ÿâ€”çœŸå®â€åˆ†é˜¶æ®µç¯å¢ƒä¸æ¨¡å‹å‚æ•°åŠ æƒèåˆã€‚æ ¸å¿ƒæ‰§è¡Œé‡‡ç”¨ReActç»“åˆé©¬å°”å¯å¤«å¼ä¸Šä¸‹æ–‡ç®¡ç†ï¼Œå…¼é¡¾é•¿ç¨‹ä»»åŠ¡çš„è®°å¿†å‹ç¼©ä¸ç¨³æ€æ¨ç†ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Beyond 128Kï¼šè‡ªé€‚åº”ä¸Šä¸‹æ–‡ç®¡ç†ä¸å¯æ‰©å±•è®°å¿†ç”¨äºé•¿ç¨‹æ·±åº¦ç ”ç©¶ï¼šç ”ç©¶æ›´å¼ºçš„å‹ç¼©ä¸æ£€ç´¢è®°å¿†ã€å±‚çº§æ‘˜è¦ä¸å¯å­¦ä¹ çŠ¶æ€é‡æ„ï¼Œçªç ´è¶…é•¿ä¸Šä¸‹æ–‡ç“¶é¢ˆ<br>â€¢ Partial-Rollout Off-Policy RLï¼šæå‡ä»£ç†åŒ–å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡ä¸ç¨³å®šæ€§ï¼šæ¢ç´¢éƒ¨åˆ†è½¨è¿¹å­¦ä¹ ã€é‡è¦æ€§é‡‡æ ·ä¸æ–¹å·®é™ä½æŠ€æœ¯ï¼Œç¼“è§£åˆ†å¸ƒåç§»å¹¶é™ä½äº¤äº’æˆæœ¬<br>â€¢ é€šç”¨ä»£ç†åŸºç¡€æ¨¡å‹ï¼šä»æ·±åº¦ç ”ç©¶åˆ°è·¨åŸŸå·¥å…·ä½¿ç”¨çš„ç»Ÿä¸€æ¡†æ¶ï¼šæ‰©å±•ç¯å¢ƒä¸å·¥å…·æ—ã€åˆ¶å®šç¯å¢ƒè°ƒåº¦ä¸sim-to-realè¿ç§»ç­–ç•¥ï¼Œå¹¶å¼•å…¥åå¥½å¯¹é½æå‡æŠ¥å‘Šå¿ å®åº¦ä¸å¯ç”¨æ€§</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AgentFold: Long-Horizon Web Agents with Proactive Context Management</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24699" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24699" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>LLM-based web agents show immense promise for information seeking, yet their effectiveness on long-horizon tasks is hindered by a fundamental trade-off in context management. Prevailing ReAct-based agents suffer from context saturation as they accumulate noisy, raw histories, while methods that fixedly summarize the full history at each step risk the irreversible loss of critical details. Addressing these, we introduce AgentFold, a novel agent paradigm centered on proactive context management, inspired by the human cognitive process of retrospective consolidation. AgentFold treats its context as a dynamic cognitive workspace to be actively sculpted, rather than a passive log to be filled. At each step, it learns to execute a `folding' operation, which manages its historical trajectory at multiple scales: it can perform granular condensations to preserve vital, fine-grained details, or deep consolidations to abstract away entire multi-step sub-tasks. The results on prominent benchmarks are striking: with simple supervised fine-tuning (without continual pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or matches open-source models of a dramatically larger scale, such as the DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like OpenAI's o4-mini.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šé•¿ç¨‹ç½‘é¡µä»£ç†åœ¨â€œä¿¡æ¯å®Œæ•´æ€§ vs. ä¸Šä¸‹æ–‡ç®€æ´æ€§â€ä¹‹é—´ä¸¤éš¾ï¼›è¿½åŠ å¼æ—¥å¿—ä¼šç´¯ç§¯å¤§é‡å™ªå£°ä¸å†—ä½™ï¼Œå›ºå®šé¢‘ç‡çš„å…¨å²æ‘˜è¦åˆæ˜“ä¸¢å¤±å…³é”®ç»†èŠ‚ä¸”ä¸å¯é€†ã€‚<br>â€¢ é‡è¦æ€§ï¼šä¸Šä¸‹æ–‡è†¨èƒ€ä¸å™ªå£°ç›´æ¥å¯¼è‡´æ¨ç†åå·®ã€æˆæœ¬é£™å‡å’Œå¯äº¤äº’æ­¥æ•°å—é™ï¼Œä¸¥é‡åˆ¶çº¦åœ¨ BrowseCompã€WideSearch ç­‰é•¿å‘¨æœŸä¿¡æ¯æ£€ç´¢/ç»¼åˆä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸å¯æ‰©å±•æ€§ã€‚<br>â€¢ ç°æœ‰å±€é™ï¼šReAct çš„ append-only ç­–ç•¥é€ æˆâ€œä¸Šä¸‹æ–‡é¥±å’Œâ€ï¼›MEM/å›ºå®šæ‘˜è¦ç±»æ–¹æ³•æ¯æ­¥å‹ç¼©å…¨å†å²ï¼Œå…³é”®ç»†èŠ‚éšå‹ç¼©æ¬¡æ•°ç´¯ç§¯æ€§æµå¤±ï¼›ç¼ºå°‘æŠŠâ€œè®°å¿†ç®¡ç†â€çº³å…¥ç­–ç•¥ç©ºé—´ã€å¯åœ¨æ¨ç†ä¸­ä¸»åŠ¨æ“ä½œä¸Šä¸‹æ–‡çš„ä»£ç†ä¸ç›¸åº”é«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡º AgentFoldï¼šå°†ä¸Šä¸‹æ–‡è®¾è®¡ä¸ºâ€œå¤šå°ºåº¦çŠ¶æ€æ‘˜è¦ + æœ€è¿‘äº¤äº’â€çš„åŠ¨æ€å·¥ä½œå°ï¼Œå¹¶åœ¨æ¯æ­¥åŒæ—¶è¾“å‡ºåŠ¨ä½œä¸æŠ˜å æŒ‡ä»¤ï¼Œé€šè¿‡ç»†ç²’åº¦å‡ç»ƒï¼ˆå•æ­¥ï¼‰ä¸æ·±åº¦åˆå¹¶ï¼ˆå¤šæ­¥ï¼‰ä¸»åŠ¨é›•ç¢å†å²ï¼Œæ—¢ä¿ç»†èŠ‚åˆæ§ä½“é‡ï¼›é…å¥— Fold-Generator ç”Ÿæˆç»“æ„åŒ–è½¨è¿¹å¹¶è¿›è¡Œ SFTï¼ˆåŸºäº Qwen3-30B-A3Bï¼‰ï¼Œåœ¨é•¿ç¨‹ä»»åŠ¡ä¸­å®ç°é«˜æ•ˆã€ç¨³å¥çš„ä¸Šä¸‹æ–‡ç®¡ç†ä¸SOTAæ€§èƒ½ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ FoldRL: é¢å‘é•¿ç¨‹Webä»£ç†çš„å¼ºåŒ–å­¦ä¹ é©±åŠ¨å¤šå°ºåº¦è®°å¿†æŠ˜å ï¼šä»¥ä»»åŠ¡æˆåŠŸç‡ä¸ä¸Šä¸‹æ–‡æˆæœ¬ä¸ºå¥–åŠ±ï¼Œå­¦ä¹ ä½•æ—¶/å¤šå¤§èŒƒå›´æŠ˜å ä¸ä½•ç§æ‘˜è¦ç²’åº¦ã€‚<br>â€¢ CertFold: å…·æœ‰ä¿¡æ¯ä¿çœŸåº¦ä¿è¯çš„å¯éªŒè¯ä¸Šä¸‹æ–‡æŠ˜å ï¼šå¼•å…¥ä¸ç¡®å®šæ€§è¯„ä¼°ä¸è¯¯å·®ä¸Šç•Œï¼Œæä¾›å…³é”®è¯æ®ä¸ä¸¢å¤±çš„æ¦‚ç‡æ€§æˆ–å½¢å¼åŒ–ä¿è¯ã€‚<br>â€¢ CrossTask-Fold: è·¨ä»»åŠ¡è¿ç§»ä¸å¤–éƒ¨é•¿æœŸè®°å¿†ååŒçš„ä¸»åŠ¨æŠ˜å ä»£ç†ï¼šå°†æŠ˜å ç­–ç•¥ä¸å‘é‡æ£€ç´¢/å¤–éƒ¨è®°å¿†ç»“åˆï¼Œæå‡è·¨é¢†åŸŸã€è·¨è¯­è¨€ä¸GUIç¯å¢ƒçš„å¯è¿ç§»æ€§ä¸å¯æ‰©å±•æ€§ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">RoboOmni: Proactive Robot Manipulation in Omni-modal Context</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23763" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23763" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision-Language-Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively. In this work, we introduce cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands. To address this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction. To address the absence of training data for proactive intention recognition in robotic manipulation, we build OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ æ—¥å¸¸äººæœºäº’åŠ¨ä¸­çš„æŒ‡ä»¤å¤šä¸ºéšå¼ï¼Œæœºå™¨äººéœ€ä»è¯­éŸ³è¯­è°ƒã€ç¯å¢ƒå£°ä¸è§†è§‰çº¿ç´¢ä¸­ä¸»åŠ¨æ¨æ–­å¹¶åœ¨æ‰§è¡Œå‰ç¡®è®¤æ„å›¾ï¼Œè€Œéè¢«åŠ¨ç­‰å¾…æ˜¾å¼å‘½ä»¤ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å¤šä¾èµ–æ–‡æœ¬æˆ–ASRè½¬å†™è¯­éŸ³ï¼Œä¸¢å¤±å‰¯è¯­è¨€ä¿¡æ¯ï¼ˆè¯­è°ƒã€æƒ…æ„Ÿã€è¯´è¯äººèº«ä»½ï¼‰ä¸”å¿½ç•¥ç¯å¢ƒå£°éŸ³ï¼Œå¯¼è‡´ç†è§£æ­§ä¹‰ã€æ—¶åºå¯¹é½å›°éš¾ä¸é¢å¤–å»¶è¿Ÿã€‚<br>â€¢ æ¶æ„ä¸æ•°æ®åŒé‡ç¼ºå£ï¼šæ¨¡å—åŒ–è§„åˆ’-æ§åˆ¶å­˜åœ¨æ¥å£å‰²è£‚ä¸è¯­ä¹‰æ¼‚ç§»ï¼Œç«¯åˆ°ç«¯VLAä»å›´ç»•æ˜¾å¼æŒ‡ä»¤ç¼ºä¹è·¨æ¨¡æ€æ—¶ç©ºèåˆçš„æ„å›¾è¯†åˆ«ï¼›åŒæ—¶ç¼ºå°‘åŒ…å«è¯­éŸ³/ç¯å¢ƒå£°/è§†è§‰å¹¶æ”¯æŒâ€œä¸Šä¸‹æ–‡æŒ‡ä»¤â€æ¨æ–­çš„å¤§è§„æ¨¡è®­ç»ƒä¸è¯„æµ‹æ•°æ®ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºRoboOmniï¼šåŸºäºPerceiverâ€“Thinkerâ€“Talkerâ€“Executorçš„ä¸€ä½“åŒ–ç«¯åˆ°ç«¯å…¨æ¨¡æ€LLMï¼Œå°†è¯­éŸ³ï¼ˆå«ç¯å¢ƒå£°ï¼‰ã€è§†è§‰ä¸æ–‡æœ¬ç»Ÿä¸€ç¼–ç å¹¶è‡ªå›å½’åœ°äº§ç”Ÿå¯¹è¯ä¸ç¦»æ•£åŒ–åŠ¨ä½œï¼ˆFAST+ tokenï¼‰ï¼Œåœ¨åŒä¸€åºåˆ—ä¸­å®Œæˆæ„å›¾è¯†åˆ«ã€è¯­éŸ³äº¤äº’ç¡®è®¤ä¸åŠ¨ä½œæ‰§è¡Œï¼›å¹¶æ„å»ºå«14ä¸‡æ¡æ ·æœ¬çš„OmniActionæ•°æ®é›†ä»¥è®­ç»ƒä¸è¯„æµ‹è·¨æ¨¡æ€ä¸Šä¸‹æ–‡æŒ‡ä»¤ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ RoboOmni-Stream: å®æ—¶æµå¼è·¨æ¨¡æ€æ„å›¾è·Ÿè¸ªä¸ä½å»¶è¿Ÿæ‰§è¡Œâ€”â€”é¢å‘è¿ç»­è¯­éŸ³ä¸è§†é¢‘çš„å¢é‡å¯¹é½ä¸åœ¨çº¿åŠ¨ä½œè§£ç ã€‚<br>â€¢ OmniAction++: è·¨è¯­è¨€ã€å¤šè®¾å¤‡ä¸çœŸå®å™ªå£°ç¯å¢ƒçš„ä¸Šä¸‹æ–‡æŒ‡ä»¤åŸºå‡†â€”â€”æ‰©å±•å¤šè¯­ç§ã€çœŸå®å½•éŸ³ä¸å£°å­¦åœºæ™¯ä»¥è¯„æµ‹é²æ£’æ³›åŒ–ã€‚<br>â€¢ SafeRoboOmni: èåˆå½¢å¼åŒ–çº¦æŸä¸ç¬¦å·è§„åˆ’çš„å¯éªŒè¯ç«¯åˆ°ç«¯æ“æ§â€”â€”åœ¨åŠ¨ä½œtokenè§£ç ä¸­å¼•å…¥å®‰å…¨çº¦æŸä¸å¯è¯æ˜çš„æ‰§è¡Œä¿éšœã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23691" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23691" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Game-TARS, a generalist game agent trained with a unified, scalable action space anchored to human-aligned native keyboard-mouse inputs. Unlike API- or GUI-based approaches, this paradigm enables large-scale continual pre-training across heterogeneous domains, including OS, web, and simulation games. Game-TARS is pre-trained on over 500B tokens with diverse trajectories and multimodal data. Key techniques include a decaying continual loss to reduce causal confusion and an efficient Sparse-Thinking strategy that balances reasoning depth and inference cost. Experiments show that Game-TARS achieves about 2 times the success rate over the previous sota model on open-world Minecraft tasks, is close to the generality of fresh humans in unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet in FPS benchmarks. Scaling results on training-time and test-time confirm that the unified action space sustains improvements when scaled to cross-game and multimodal data. Our results demonstrate that simple, scalable action representations combined with large-scale pre-training provide a promising path toward generalist agents with broad computer-use abilities.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ åŠ¨ä½œç©ºé—´ç¢ç‰‡åŒ–ä¸ç¯å¢ƒè€¦åˆï¼šç°æœ‰API/GUI/æ¸¸æˆå†…é«˜å±‚æŒ‡ä»¤éœ€ä¸ºæ¯ä¸ªç¯å¢ƒå•ç‹¬é€‚é…ï¼Œéš¾ä»¥è·¨æ¸¸æˆå¤ç”¨ä¸è§„æ¨¡åŒ–é¢„è®­ç»ƒï¼Œé™åˆ¶å¼€æ”¾ä¸–ç•Œæ³›åŒ–èƒ½åŠ›ã€‚<br>â€¢ å› æœå¯¹é½ä¸è®­ç»ƒä¿¡å·å¤±çœŸï¼šé«˜é¢‘é‡å¤åŠ¨ä½œå¯¼è‡´æ ‡å‡†äº¤å‰ç†µè¢«æ˜“æ ·æœ¬ä¸»å¯¼å¹¶äº§ç”Ÿå› æœæ··æ·†ï¼›å¤šæ¨¡æ€å½•åˆ¶çš„æ—¶é—´æˆ³åç§»ç ´åè§‚æµ‹-åŠ¨ä½œå› æœé“¾ï¼Œå‰Šå¼±æ”¿ç­–å­¦ä¹ æœ‰æ•ˆæ€§ã€‚<br>â€¢ æ¨ç†ä¸è®°å¿†ä½æ•ˆï¼šç¦»çº¿â€œè¡¥æ€è€ƒâ€éš¾ä»¥è¿˜åŸçœŸå®è®¤çŸ¥ä¸”æˆæœ¬é«˜ã€æ˜“å¹»è§‰ï¼›ç¼ºå°‘å¯æ§çš„ç¨€ç–æ€è€ƒä¸é•¿ç¨‹è®°å¿†æœºåˆ¶ï¼›æŒ‡ä»¤éµå¾ªæ˜“å—é¢„è®­ç»ƒåŠ¨ä½œå…ˆéªŒå’Œç»‘å®šä¹ æƒ¯ï¼ˆé”®ä½ï¼‰å¹²æ‰°ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºGame-TARSï¼šä»¥é”®é¼ ä¸ºäºº-åŸç”Ÿç»Ÿä¸€åŠ¨ä½œç©ºé—´ï¼Œè¿›è¡Œå¤§è§„æ¨¡è¿ç»­é¢„è®­ç»ƒï¼Œç»“åˆè§†è§‰é”šç‚¹å› æœå¯¹é½ä¸è¡°å‡æŸå¤±ç¼“è§£é‡å¤åŠ¨ä½œä¸»å¯¼ï¼Œå¹¶ç”¨åœ¨çº¿think-aloudé‡‡é›†çš„ç¨€ç–ReActè½¨è¿¹è®­ç»ƒã€‚éšåé€šè¿‡åŠ¨ä½œç©ºé—´è‡ªåŠ¨å¢å¹¿+é€†åŠ¨åŠ›å­¦è¾…åŠ©çš„æŒ‡ä»¤éµå¾ªã€RFTå¼ºåŒ–çš„ç¨€ç–æ€è€ƒã€åŒå±‚ï¼ˆä¸Šä¸‹æ–‡+æ‘˜è¦ï¼‰è®°å¿†ä¸è·¨åŸŸAgentè½¨è¿¹åè®­ç»ƒï¼Œå®ç°è·¨æ¸¸æˆ/ç½‘é¡µ/OSçš„é€šç”¨ä»£ç†ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è·¨è®¾å¤‡ç»Ÿä¸€äºº-åŸç”ŸåŠ¨ä½œç©ºé—´ï¼šå°†é”®é¼ ç»Ÿä¸€åŠ¨ä½œæ‰©å±•åˆ°è§¦æ§/æ‰‹æŸ„/è¯­éŸ³è¾“å…¥ï¼Œç³»ç»Ÿè¯„ä¼°è·¨å¹³å°æ³›åŒ–ä¸æ•°æ®é‡‡é›†å¯æ‹“å±•æ€§<br>â€¢ è‡ªé€‚åº”ç¨€ç–æ€è€ƒè°ƒåº¦ï¼šåŸºäºä¸ç¡®å®šæ€§ä¸æ¨ç†ä»£ä»·çš„æ€è€ƒè§¦å‘ä¸æ·±åº¦æ§åˆ¶ï¼Œè”åˆæ³•å¥–åŠ±çš„åœ¨çº¿RFTä»¥é™ä½å¹»è§‰ä¸å»¶è¿Ÿ<br>â€¢ å› æœä¸€è‡´çš„è¿ç»­é¢„è®­ç»ƒç†è®ºï¼šé¢å‘éé©¬å°”å¯å¤«è½¨è¿¹è®¾è®¡å¸¦æ”¶æ•›ä¿è¯çš„è¡°å‡æŸå¤±ä¸é‡‡æ ·ç­–ç•¥ï¼Œæå‡å…³é”®è½¬ç§»ä¸å¤šåŸŸæ³›åŒ–</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Uniform Discrete Diffusion with Metric Path for Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24717" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24717" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency. In this work, we revisit discrete generative modeling and present Uniform discRete diffuSion with metric pAth (URSA), a simple yet powerful framework that bridges the gap with continuous approaches for the scalable video generation. At its core, URSA formulates the video generation task as an iterative global refinement of discrete spatiotemporal tokens. It integrates two key designs: a Linearized Metric Path and a Resolution-dependent Timestep Shifting mechanism. These designs enable URSA to scale efficiently to high-resolution image synthesis and long-duration video generation, while requiring significantly fewer inference steps. Additionally, we introduce an asynchronous temporal fine-tuning strategy that unifies versatile tasks within a single model, including interpolation and image-to-video generation. Extensive experiments on challenging video and image generation benchmarks demonstrate that URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods. Code and models are available at https://github.com/baaivision/URSA</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç¦»æ•£è§†è§‰ç”Ÿæˆï¼ˆè‡ªå›å½’/æ©ç æ‰©æ•£ï¼‰ç›¸è¾ƒè¿ç»­æ‰©æ•£å­˜åœ¨æ˜æ˜¾æ€§èƒ½å·®è·ï¼šå±äºâ€œä¸å¯å†ç»†åŒ–çš„å±€éƒ¨ç”Ÿæˆâ€ï¼Œæ˜“äº§ç”Ÿè¯¯å·®ç´¯ç§¯ï¼Œé•¿æ—¶ç©ºä¸€è‡´æ€§å·®ï¼Œå°¤å…¶åœ¨è§†é¢‘ä»»åŠ¡ä¸Šè¡¨ç°ä¸è¶³ã€‚<br>â€¢ é•¿åºåˆ—/é«˜åˆ†è¾¨ç‡ç”Ÿæˆéœ€è¦å¯¹æ‰°åŠ¨å¼ºåº¦ä¸æ—¶é—´è·¯å¾„è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼›ç°æœ‰ç¦»æ•£æ¦‚ç‡è·¯å¾„å¯¹ä¸åŒé•¿åº¦ä¸åˆ†è¾¨ç‡ç¼ºä¹è‡ªé€‚åº”ï¼Œè®­ç»ƒä¸é‡‡æ ·ä¸ç¨³å®šã€æ­¥æ•°å¤šã€æ•ˆç‡ä½ã€‚<br>â€¢ ç¦»æ•£æ‰©æ•£çš„é‡‡æ ·è¯¯å·®è¾ƒå¤§ï¼Œç¼ºå°‘ç±»ä¼¼è¿ç»­æ‰©æ•£çš„å…¨å±€è¿­ä»£ç»†åŒ–æœºåˆ¶ï¼Œå¯¼è‡´ç”»é¢è´¨é‡ä¸è¿åŠ¨è‡ªç„¶æ€§ä¸è¶³ã€‚<br>â€¢ å¤šä»»åŠ¡ç»Ÿä¸€éš¾ï¼šåŒæ­¥æ—¶æ­¥ä¸ºæ‰€æœ‰å¸§æ–½åŠ ç›¸åŒå™ªå£°ï¼Œéš¾ä»¥å…¼é¡¾å±€éƒ¨é‡å»ºä¸å…¨å±€æ—¶åºï¼Œé™åˆ¶äº†å›¾ç”Ÿè§†é¢‘ã€æ’å¸§ã€å¤–æ¨ç­‰ä»»åŠ¡åœ¨å•ä¸€æ¨¡å‹ä¸­çš„ç»Ÿä¸€ã€‚<br>â€¢ ç°æœ‰è§†é¢‘tokenizeræ—¶ç©ºå‹ç¼©ä¸é‡å»ºè´¨é‡æœ‰é™ï¼Œè¿›ä¸€æ­¥æ”¾å¤§ç¦»æ•£æ–¹æ³•åœ¨é•¿ä¸Šä¸‹æ–‡ä¸‹çš„è„†å¼±æ€§ï¼ŒäºŸéœ€ç®—æ³•ä¾§å¼¥åˆä¸è¿ç»­æ–¹æ³•çš„æ€§èƒ½å·®è·ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>URSAæå‡ºâ€œå¸¦åº¦é‡è·¯å¾„çš„ç»Ÿä¸€ç¦»æ•£æ‰©æ•£â€ï¼Œä»ç±»åˆ«å™ªå£°å‡ºå‘å¯¹ç¦»æ•£æ—¶ç©ºtokenè¿›è¡Œå…¨å±€è¿­ä»£ç»†åŒ–ï¼šä»¥åµŒå…¥è·ç¦»é©±åŠ¨çš„çº¿æ€§åŒ–åº¦é‡æ¦‚ç‡è·¯å¾„ä¸ºæ ¸å¿ƒï¼Œå¹¶ç»“åˆåˆ†è¾¨ç‡ç›¸å…³çš„æ—¶æ­¥åç§»ä¸å¸§çº§å¼‚æ­¥æ—¶åˆ»è°ƒåº¦ï¼Œå®ç°é«˜åˆ†è¾¨ç‡ä¸é•¿è§†é¢‘çš„å°‘æ­¥æ•°é«˜æ•ˆç”ŸæˆåŠå¤šä»»åŠ¡ç»Ÿä¸€ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Learnable Metric Path for Discrete Diffusionï¼šå°†Î²tä¸è·ç¦»åº¦é‡è®¾ä¸ºå¯å­¦ä¹ /è‡ªé€‚åº”ï¼Œä¾æ®å†…å®¹ä¸åºåˆ—é•¿åº¦åŠ¨æ€è°ƒæ•´æ‰°åŠ¨è½¨è¿¹ï¼Œè¿›ä¸€æ­¥é™ä½é‡‡æ ·æ­¥æ•°ä¸è¯¯å·®ã€‚<br>â€¢ End-to-End Co-Training of Tokenizer and URSAï¼šè”åˆä¼˜åŒ–ç¦»æ•£è§†é¢‘tokenizerä¸åº¦é‡è·¯å¾„/ç”Ÿæˆå™¨ï¼Œä½¿é‡å»ºä¿çœŸåº¦ä¸æ—¶ç©ºå‹ç¼©ç‡ä¸ç”Ÿæˆè´¨é‡ååŒæå‡ã€‚<br>â€¢ Hierarchical Asynchronous Scheduling for Long-Form Videoï¼šå¼•å…¥é•œå¤´/ç‰‡æ®µ/å¸§çš„åˆ†å±‚å¼‚æ­¥æ—¶æ­¥ä¸è·¨æ®µä¸€è‡´æ€§çº¦æŸï¼Œç¨³å®šæ‰©å±•è‡³åˆ†é’Ÿçº§é•¿è§†é¢‘å¹¶æ”¯æŒå¯æ§é•œå¤´è¯­æ³•ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Repurposing Synthetic Data for Fine-grained Search Agent Supervision</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24694" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24694" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. This critical limitation renders them unable to distinguish informative "near-miss" samples-those with substantially correct reasoning but a flawed final answer-from complete failures, thus discarding valuable learning signals. We address this by leveraging the very entities discarded during training. Our empirical analysis reveals a strong positive correlation between the number of ground-truth entities identified during an agent's reasoning process and final answer accuracy. Building on this insight, we introduce Entity-aware Group Relative Policy Optimization (E-GRPO), a novel framework that formulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to effectively learn from these "near-misses". Experiments on diverse question-answering (QA) and deep research benchmarks show that E-GRPO consistently and significantly outperforms the GRPO baseline. Furthermore, our analysis reveals that E-GRPO not only achieves superior accuracy but also induces more efficient reasoning policies that require fewer tool calls, demonstrating a more effective and sample-efficient approach to aligning search agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šåŸºäºGRPOçš„æœç´¢ä»£ç†é‡‡ç”¨ç¨€ç–çš„ç»“æœå¼å¥–åŠ±ï¼Œæ— æ³•åŒºåˆ†â€œè¿‘ä¼¼æ­£ç¡®â€çš„æ¨ç†ä¸â€œå®Œå…¨å¤±è´¥â€ï¼Œå¯¼è‡´å¤§é‡æœ‰ä»·å€¼çš„ä¸­é—´å­¦ä¹ ä¿¡å·è¢«ä¸¢å¼ƒï¼ˆè§å›¾2ï¼Œç¬¬5é¡µï¼‰ã€‚<br>â€¢ åŸå› é‡è¦æ€§ï¼šåˆæˆæ•°æ®æ˜¯å®ä½“ä¸­å¿ƒç”Ÿæˆçš„ï¼Œä½†è®­ç»ƒé˜¶æ®µæŠŠè¿™äº›å®ä½“ä¿¡æ¯ä¸¢å¼ƒï¼Œé”™å¤±å¯å¤ç”¨çš„ç»†ç²’åº¦ã€ä½æˆæœ¬ç›‘ç£æ¥æºï¼›è€Œåœ¨å¼€æ”¾ç½‘ç»œæœç´¢ä¸­ï¼Œè½¨è¿¹é•¿ã€å·¥å…·è°ƒç”¨å¤šï¼Œç¨€ç–å¥–åŠ±å°¤ä¸ºè‡´å‘½ï¼Œå½±å“æ ·æœ¬ä¸è®¡ç®—æ•ˆç‡ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ä¸æ ‘æœç´¢è™½å¯ç»†ç²’åº¦ç›‘ç£ï¼Œä½†åœ¨å¼€æ”¾åŸŸç½‘é¡µç¯å¢ƒä¸­æ ‡æ³¨æˆæœ¬é«˜ã€é‡‡æ ·ä¸è®¡ç®—ä¸å¯æ‰©å±•ï¼Œéš¾ä»¥å®é™…è½åœ°ã€‚<br>â€¢ å…³é”®è¯æ®ï¼šå®ä½“åŒ¹é…ç‡ä¸æ­£ç¡®ç‡æ˜¾è‘—æ­£ç›¸å…³ï¼Œæ­£ç¡®è½¨è¿¹çš„å®ä½“åŒ¹é…ç‡æ˜¾è‘—é«˜äºé”™è¯¯è½¨è¿¹ï¼ˆçº¦4:1é—®é¢˜å æ¯”ä¼˜åŠ¿ï¼›è§å›¾1å³ä¸Šä¸å³ä¸‹ï¼Œç¬¬3é¡µï¼‰ï¼›E-GRPOåœ¨å¤šåŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºGRPOï¼Œä¸”å­¦åˆ°æ›´å°‘å·¥å…·è°ƒç”¨çš„é«˜æ•ˆç­–ç•¥ï¼ˆè¡¨1ï¼Œç¬¬8é¡µï¼›è¡¨2ï¼Œç¬¬9é¡µï¼›å›¾3ä¸­é—´ï¼Œç¬¬10é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºE-GRPOï¼šå°†åˆæˆæ•°æ®ä¸­çš„çœŸå€¼å®ä½“é‡ç”¨ä¸ºç»†ç²’åº¦ç›‘ç£ï¼Œåœ¨GRPOä¸­ä¸ºé”™è¯¯æ ·æœ¬æŒ‰å½’ä¸€åŒ–å®ä½“åŒ¹é…ç‡ç»™äºˆéƒ¨åˆ†å¥–åŠ±ï¼ˆæ­£ç¡®ä¸º1ï¼Œé”™è¯¯ä¸ºÎ±Â·Î³Ì‚ï¼Œå¼‚å¸¸ä¸º0ï¼‰ï¼Œä»¥å¯†é›†çš„å®ä½“æ„ŸçŸ¥å¥–åŠ±è®¡ç®—ç»„ç›¸å¯¹ä¼˜åŠ¿å¹¶è¿›è¡Œè£å‰ªä¼˜åŒ–ã€‚å®ä½“åŒ¹é…åœ¨æ€è€ƒå—å†…åšç²¾ç¡®å­—ç¬¦ä¸²åŒ¹é…å¹¶ç»„å†…å½’ä¸€åŒ–ï¼Œé…åˆKL-freeä¸clip-higherç­–ç•¥ï¼Œç¨³å®šä¸”ä½å¼€é”€åœ°åŒºåˆ†â€œè¿‘ä¼¼æ­£ç¡®â€å’Œâ€œå®Œå…¨å¤±è´¥â€ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ä»å®ä½“åŒ¹é…åˆ°å…³ç³»ä¸€è‡´æ€§çš„E-GRPOï¼šå°†å®ä½“å…³ç³»å›¾ä¸æ¨ç†é“¾çš„ä¸€è‡´æ€§çº³å…¥å¥–åŠ±ï¼Œå¼ºåŒ–å¤šè·³æ¨ç†çš„å…³ç³»é“¾å¯¹é½ä¸ç»„åˆæ³›åŒ–ã€‚<br>â€¢ è‡ªé€‚åº”Î±è°ƒåº¦çš„E-GRPOï¼šåŸºäºæ ·æœ¬éš¾åº¦ä¸ä¸ç¡®å®šæ€§ã€è®­ç»ƒè¿›åº¦ä¸ç»„å†…åˆ†å¸ƒåŠ¨æ€è°ƒèŠ‚å®ä½“å¥–åŠ±æƒé‡ï¼Œå¹³è¡¡æ¢ç´¢æ•ˆç‡ä¸æœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§ã€‚<br>â€¢ è¿‡ç¨‹çº§è½»é‡PRMä¸E-GRPOååŒä¼˜åŒ–ï¼šä»¥æ­¥éª¤çº§å‘½ä¸­å®ä½“ä¸è¯æ®å¯¹é½ä¸ºä¼ªæ ‡ç­¾è®­ç»ƒè½»é‡PRMï¼Œä¸å®ä½“æ„ŸçŸ¥ä¼˜åŠ¿è”åˆé©±åŠ¨é«˜æ•ˆã€ç¨³å®šçš„è¿‡ç¨‹ç›‘ç£ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24563" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24563" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>With advances in decision-making and reasoning capabilities, multimodal agents show strong potential in computer application scenarios. Past evaluations have mainly assessed GUI interaction skills, while tool invocation abilities, such as those enabled by the Model Context Protocol (MCP), have been largely overlooked. Comparing agents with integrated tool invocation to those evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP, the first comprehensive and fair benchmark for assessing computer-use agents' tool invocation, GUI operation, and decision-making abilities in a real-world environment. We design a novel automated code-generation pipeline to create tools and combine them with a curated selection from existing tools. Rigorous manual validation yields 158 high-quality tools (covering 7 common applications), each verified for correct functionality, practical applicability, and versatility. Extensive evaluations of state-of-the-art multimodal agents on OSWorld-MCP show that MCP tools generally improve task success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1% to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of assessing tool invocation capabilities. However, even the strongest models have relatively low tool invocation rates, Only 36.3%, indicating room for improvement and highlighting the benchmark's challenge. By explicitly measuring MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents and sets a new standard for evaluating performance in complex, tool-assisted environments. Our code, environment, and data are publicly available at https://osworld-mcp.github.io.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°æœ‰GUIåŸºå‡†å¿½ç•¥MCPç­‰å¤–éƒ¨å·¥å…·è°ƒç”¨ï¼Œæ— æ³•å…¬å¹³ã€å…¨é¢è¯„ä¼°è®¡ç®—æœºä½¿ç”¨ä»£ç†åœ¨â€œå·¥å…·+GUIâ€æ··åˆåœºæ™¯ä¸‹çš„çœŸå®èƒ½åŠ›ã€‚<br>â€¢ ç°å®ä»»åŠ¡å¸¸å¯é€šè¿‡å·¥å…·æ›´é«˜æ•ˆç¨³å¥åœ°å®Œæˆï¼ˆå¦‚ä¸€é”®å®‰è£…æ‰©å±•ï¼‰ï¼Œä»…é GUIåŠ¨ä½œæ—¢ä½æ•ˆåˆè„†å¼±ï¼Œå¯¼è‡´è¯„æµ‹ä¸åº”ç”¨è„±èŠ‚ã€‚<br>â€¢ å·²æœ‰MCPè¯„æµ‹è¦†ç›–çš„å·¥å…·æ•°é‡å°‘ã€åŠŸèƒ½é‡å å¤šã€ä»»åŠ¡è®¾è®¡å—å¯ç”¨å·¥å…·æŸç¼šï¼Œç¼ºä¹å¯¹å¼€æ”¾å¤æ‚ä»»åŠ¡ä¸è§†è§‰GUIç†è§£çš„ç»¼åˆè€ƒæŸ¥ã€‚<br>â€¢ ç¼ºå°‘ç»Ÿä¸€çš„é«˜è´¨é‡ã€è·¨åº”ç”¨ï¼ˆ7ç±»è½¯ä»¶ï¼‰ä¸”å¯å¤ç”¨çš„å·¥å…·é›†ä¸è‡ªåŠ¨åŒ–æ„å»ºæµç¨‹ï¼Œéš¾ä»¥å½¢æˆå¯æ¯”æ€§ä¸å¯æ‰©å±•çš„è¯„æµ‹åŸºåº§ã€‚<br>â€¢ ç¼ºä¹è¡¡é‡â€œä½•æ—¶ç”¨å·¥å…·/ä½•æ—¶ç”¨GUIâ€çš„å†³ç­–èƒ½åŠ›ä¸å·¥å…·åˆ©ç”¨åº¦çš„æ ‡å‡†åŒ–æŒ‡æ ‡ï¼Œç°æœ‰è¯„æµ‹æ— æ³•åˆ†è§£åˆ†æå·¥å…·ä½¿ç”¨ç­–ç•¥ä¸æ•ˆç‡ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºOSWorld-MCPåŸºå‡†ï¼šåœ¨OSWorldçœŸå®ç¯å¢ƒä¸­å°†158ä¸ªé«˜è´¨é‡MCPå·¥å…·ä¸GUIæ“ä½œèåˆï¼Œä»£ç†æ¯æ­¥å¯è‡ªé€‚åº”é€‰æ‹©è°ƒç”¨å·¥å…·æˆ–æ‰§è¡ŒGUIåŠ¨ä½œï¼Œå¹¶ä»¥å‡†ç¡®ç‡ã€å·¥å…·è°ƒç”¨ç‡TIRä¸å¹³å‡å®Œæˆæ­¥æ•°ACSè¿›è¡Œè¯„ä¼°ã€‚å·¥å…·é›†é€šè¿‡â€œä»£ç ç”Ÿæˆ-å¯ç”¨æ€§è¿‡æ»¤-å·¥å…·å°è£…â€çš„è‡ªåŠ¨æµæ°´çº¿ä¸äººå·¥ç”„é€‰è·å¾—ï¼Œå¹¶ä»¥RAGæŒ‰åº”ç”¨æ£€ç´¢ç›¸å…³å·¥å…·ä»¥æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ä¸å¹²æ‰°ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ é¢å‘GUIâ€“MCPæ··åˆä»£ç†çš„é²æ£’å·¥å…·é“¾è§„åˆ’ï¼šå±‚æ¬¡åŒ–/è¯¾ç¨‹å­¦ä¹ ä¸æ‰§è¡Œç›‘ç£ï¼Œæå‡å¤šè½®ã€å¤šå·¥å…·ç»„åˆä¸‹çš„è§„åˆ’ä¸æ•…éšœæ¢å¤èƒ½åŠ›ã€‚<br>â€¢ é¡ºåºä¸æ•æ„Ÿçš„MCPå·¥å…·æç¤ºä¸ä¸Šä¸‹æ–‡å‹ç¼©ï¼šè®¾è®¡é¡ºåºä¸æ•æ„Ÿæç¤ºã€ç»“æ„åŒ–æ£€ç´¢ä¸å‹ç¼©ç­–ç•¥ï¼Œç¼“è§£å·¥å…·æè¿°é¡ºåºä¸é•¿ä¸Šä¸‹æ–‡å¯¹è°ƒç”¨ç‡å’Œæ€§èƒ½çš„å½±å“ã€‚<br>â€¢ é¢å‘ç²¾ç¡®å·¥å…·è°ƒç”¨çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼šä»¥TIR/ACS/æˆåŠŸç‡ä¸ºå¤šç›®æ ‡å¥–åŠ±ï¼Œç»“åˆç¦»çº¿-åœ¨çº¿RLä¸åäº‹å®å›æ”¾ï¼Œæå‡â€œä½•æ—¶ç”¨/ç”¨å“ªä¸ª/å¦‚ä½•ä¸²è”â€çš„å†³ç­–è´¨é‡ã€‚<br>â€¢ å¤§è§„æ¨¡è‡ªåŠ¨åŒ–MCPå·¥å…·åˆæˆä¸éªŒè¯ï¼šæ‰©å±•åˆ°æ›´å¤šåº”ç”¨ä¸OSï¼Œç»“åˆå•æµ‹/å½¢å¼åŒ–éªŒè¯ä¸äººå®¡é—­ç¯ï¼ŒæŒç»­ç”Ÿæˆé«˜è´¨é€šç”¨å·¥å…·é›†ã€‚<br>â€¢ é¢å‘çœŸå®OSçš„å®‰å…¨æ„ŸçŸ¥MCPä»£ç†ï¼šå¼•å…¥æƒé™çº¦æŸä¸å®‰å…¨æƒ©ç½šï¼Œè”åŠ¨OS-Harmç­‰åŸºå‡†è¯„æµ‹ï¼Œé™ä½è¯¯æ“ä½œä¸è¶Šæƒé£é™©å¹¶æå‡ç¨³å¥æ€§ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24698" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24698" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel thinking faces two key challenges in this setting: inefficiency from repeatedly rolling out from scratch, and difficulty in integrating long-horizon reasoning trajectories during answer generation, as limited context capacity prevents full consideration of the reasoning process. To address these issues, we propose ParallelMuse, a two-stage paradigm designed for deep IS agents. The first stage, Functionality-Specified Partial Rollout, partitions generated sequences into functional regions and performs uncertainty-guided path reuse and branching to enhance exploration efficiency. The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information relevant to answer derivation and synthesize a coherent final answer. Experiments across multiple open-source agents and benchmarks demonstrate up to 62% performance improvement with a 10--30% reduction in exploratory token consumption.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ä¼ ç»Ÿå¹¶è¡Œæ€è€ƒåœ¨æ·±åº¦ä¿¡æ¯æ£€ç´¢ä»£ç†ä¸­ä½æ•ˆï¼šå¤šæ¬¡ä»å¤´å±•å¼€é‡å¤æ¢ç´¢ï¼Œæ— æ³•å¤ç”¨å·²ç¡®å®šçš„å‰ç¼€ï¼Œå¯¼è‡´é‡‡æ ·ä¸è®¡ç®—é¢„ç®—æµªè´¹ã€‚<br>â€¢ é•¿ç¨‹äº¤äº’â€“æ¨ç†è½¨è¿¹å†—é•¿ä¸”é«˜åº¦å†—ä½™ï¼Œå—ä¸Šä¸‹æ–‡çª—å£é™åˆ¶éš¾ä»¥åœ¨ç”Ÿæˆé˜¶æ®µæ•´åˆå…¨éƒ¨æœ‰æ•ˆä¸­é—´æ¨ç†ï¼›å¤šæ•°æŠ•ç¥¨ä¸åŸºäºç½®ä¿¡åº¦çš„é€‰æ‹©åœ¨å¼•å…¥å¤–éƒ¨ä¿¡æ¯åæ˜“å¤±å‡†ã€æ˜“åç½®ã€‚<br>â€¢ ç°æœ‰éƒ¨åˆ†å±•å¼€é»˜è®¤â€œä»¤ç‰ŒåŠŸèƒ½åŒè´¨â€ï¼ŒæœªåŒºåˆ†æ¨ç†ä¸å·¥å…·è°ƒç”¨ï¼ˆæ¢ç´¢ï¼‰ç­‰åŠŸèƒ½åŒºçš„ä¸åŒä¸ç¡®å®šæ€§æ¨¡å¼ï¼ˆå¦‚æ—©æœŸæ¢ç´¢ä¸ç¡®å®šæ€§æ›´é«˜ã€æ¨ç†å³°å€¼ç¨åå‡ºç°ï¼‰ï¼Œæ— æ³•è¿›è¡ŒæŒ‰åŠŸèƒ½åŒºçš„å®šå‘åˆ†æ”¯ä¸é«˜æ•ˆæ‰©å±•ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºPARALLELMUSEï¼Œä¸¤é˜¶æ®µï¼š1ï¼‰åŠŸèƒ½åŒºæŒ‡å®šçš„éƒ¨åˆ†å±•å¼€ï¼Œä»¥æ­¥éª¤çº§PPLåˆ†åˆ«åº¦é‡æ¨ç†/æ¢ç´¢åŒºä¸ç¡®å®šæ€§ï¼Œé€‰æ‹©é«˜ä¸ç¡®å®šæ€§æ­¥éª¤åšå¼‚æ­¥åˆ†æ”¯å¹¶å¤ç”¨KVç¼“å­˜ä»¥å®šå‘æ‰©å±•ï¼›2ï¼‰å‹ç¼©å¼æ¨ç†èšåˆï¼Œå°†æ¯æ¡è½¨è¿¹å‹ç¼©ä¸ºåŒ…å«è®¡åˆ’ã€æ–¹æ³•ä¸æœ€ç»ˆåˆæˆçš„ç»“æ„åŒ–æŠ¥å‘Šï¼Œåœ¨ä¸è¿½åŠ å·¥å…·è°ƒç”¨çš„å‰æä¸‹è·¨è½¨è¿¹èšåˆç”Ÿæˆç­”æ¡ˆã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Func-Branch++: é¢å‘å¤šå·¥å…·/å¤šè¡Œä¸ºä»£ç†çš„åŠŸèƒ½åŒºä¸ç¡®å®šæ€§é©±åŠ¨å¹¶è¡Œå±•å¼€â€”â€”å°†åŠŸèƒ½åŒºä»â€œæ¨ç†/æ¢ç´¢â€æ‰©å±•åˆ°æ›´å¤šå·¥å…·è¡Œä¸ºï¼Œå­¦ä¹ å¼é€‰æ‹©åˆ†æ”¯ä¸é¢„ç®—åˆ†é…ã€‚<br>â€¢ GraphFuse: åŸºäºå®ä½“â€“å…³ç³»å›¾çš„æ— æŸè½¨è¿¹å‹ç¼©ä¸è·¨æ¨¡å‹èšåˆâ€”â€”æ˜¾å¼é‡å»ºä¿¡æ¯çŠ¶æ€å›¾ï¼Œç»“åˆå¼ºèšåˆå™¨è¿›è¡Œç»“æ„åŒ–è¯æ®èåˆä¸ä¸€è‡´æ€§æ£€éªŒã€‚<br>â€¢ Calib-Seek: æ·±åº¦ä¿¡æ¯ä»£ç†çš„ç½®ä¿¡åº¦æ ¡å‡†ä¸ç¨³å¥ç­”æ¡ˆé€‰æ‹©â€”â€”åœ¨å¤–éƒ¨æ£€ç´¢å¹²æ‰°ä¸‹è¿›è¡Œç½®ä¿¡åº¦æ ¡å‡†ã€åå¤šæ•°åç½®çš„èšåˆå‡†åˆ™ä¸ä¸ç¡®å®šæ€§ä¼°è®¡æ”¹è¿›ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24697" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24697" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Model (LLM)-based agents have emerged as a transformative approach for open-ended problem solving, with information seeking (IS) being a core capability that enables autonomous reasoning and decision-making. While prior research has largely focused on improving retrieval depth, we observe that current IS agents often suffer from low search efficiency, which in turn constrains overall performance. A key factor underlying this inefficiency is the sparsity of target entities in training tasks, which limits opportunities for agents to learn and generalize efficient search behaviors. To address these challenges, we propose WebLeaper, a framework for constructing high-coverage IS tasks and generating efficient solution trajectories. We formulate IS as a tree-structured reasoning problem, enabling a substantially larger set of target entities to be embedded within a constrained context. Leveraging curated Wikipedia tables, we propose three variants for synthesizing IS tasks, Basic, Union, and Reverse-Union, to systematically increase both IS efficiency and efficacy. Finally, we curate training trajectories by retaining only those that are simultaneously accurate and efficient, ensuring that the model is optimized for both correctness and search performance. Extensive experiments on both basic and comprehensive settings, conducted on five IS benchmarks, BrowserComp, GAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method consistently achieves improvements in both effectiveness and efficiency over strong baselines.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ LLM ä¿¡æ¯æ£€ç´¢ä»£ç†æ£€ç´¢ä½æ•ˆï¼šå¦‚å›¾2ï¼ˆç¬¬2é¡µï¼‰æ‰€ç¤ºï¼Œæœ‰æ•ˆåŠ¨ä½œæ¯”ä¾‹å³°å€¼ä»…çº¦0.04ï¼Œæ™®éå­˜åœ¨å†—ä½™æŸ¥è¯¢ã€æ— å…³æ£€ç´¢ä¸è¿‡é•¿é“¾æ¡ã€‚<br>â€¢ è®­ç»ƒæ•°æ®ç›®æ ‡å®ä½“ç¨€ç–ï¼šå¯¼è‡´åœ¨æœ‰é™ä¸Šä¸‹æ–‡ä¸‹éš¾ä»¥å­¦åˆ°é«˜æ•ˆæœç´¢ç­–ç•¥ï¼Œå¹¶å¼•å…¥æ•ˆç‡åº¦é‡åå·®ï¼›å‘½é¢˜1è¯æ˜ ISE æ–¹å·®éšç›®æ ‡å®ä½“æ•° n å¢é•¿è€Œé™è‡³ O(1/n)ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•åé‡â€œæ£€ç´¢æ·±åº¦/å¤šæ­¥ç®¡çº¿â€ï¼Œå¿½è§†æœç´¢æ•ˆç‡ä¸å®ä½“è¦†ç›–åº¦ï¼Œä¸”æ˜“è¢«å…³é”®è¯æ·å¾„ç»•è¿‡çœŸå®åˆæˆæ¨ç†ã€‚<br>â€¢ å¥–åŠ±ä¸è¯„æµ‹ä¸åŒ¹é…ï¼šäºŒå…ƒæˆåŠŸå¥–åŠ±åœ¨å®ä½“å¯†é›†ä»»åŠ¡ä¸­è¿‡äºç¨€ç–ï¼›EM/F1 è„†å¼±ä¸ç¨³ï¼ŒLLM è£åˆ¤æˆæœ¬é«˜ä¸”ä¸€è‡´æ€§å·®ï¼Œéš¾ä»¥æ”¯æ’‘RLä¼˜åŒ–ã€‚<br>â€¢ ç»“æœæ˜¯è®¡ç®—/æ—¶é—´æˆæœ¬é«˜ã€è®­ç»ƒä¿¡å·å¼±ï¼Œé™åˆ¶äº†å¼€ç¯ä¿¡æ¯å¯»å€ä¸æ•´ä½“è¡¨ç°ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºWebLeaperï¼šå°†ä¿¡æ¯å¯»å€å»ºæ¨¡ä¸ºæ ‘çŠ¶æ¨ç†ï¼Œä»ç»´åŸºè¡¨æ ¼åˆæˆå®ä½“å¯†é›†ä»»åŠ¡ï¼ˆBasic/Union/Reverse-Unionï¼‰ï¼Œå¹¶ä»¥ISRä¸ISEç­›é€‰â€œæ—¢æ­£ç¡®åˆé«˜æ•ˆâ€çš„è½¨è¿¹è¿›è¡ŒSFTä¸RLè®­ç»ƒã€‚RLé˜¶æ®µä½¿ç”¨åŸºäºè½¯ç²¾ç¡®ç‡/å¬å›ç‡çš„åŠ æƒF-scoreæ··åˆå¥–åŠ±ï¼Œç¨³å®šæå‡ä¿¡æ¯è¦†ç›–ä¸æœç´¢æ•ˆç‡ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Auto-Scaling Entity-Rich Curricula for Web Agentsï¼šåŸºäºå­¦ä¹ è¿›åº¦è‡ªé€‚åº”è°ƒèŠ‚å®ä½“æ•°é‡ä¸è·¨æºéš¾åº¦ï¼Œæå‡ç¨³å®šæ€§ä¸æ³›åŒ–<br>â€¢ Online ISR/ISE-Aware Planning and Early-Stoppingï¼šåœ¨æ¨ç†æ—¶åŠ¨æ€ä¼°è®¡ISR/ISEä»¥å¼•å¯¼æŸ¥è¯¢ã€è®¿é—®ä¸åœæ­¢ç­–ç•¥ï¼Œå‡å°‘å†—ä½™åŠ¨ä½œ<br>â€¢ Cross-Modal Reverse-Union beyond Wikipediaï¼šå°†Union/Reverse-Unionæ‰©å±•åˆ°å¤šæ¨¡æ€ä¸éç»“æ„åŒ–ç½‘ç«™ï¼ŒéªŒè¯é²æ£’æ€§ä¸å¯è¿ç§»æ€§</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24695" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24695" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Training large language model agents on tasks at the frontier of their capabilities is key to unlocking advanced reasoning. We introduce a data synthesis approach inspired by the educational theory of the Zone of Proximal Development (ZPD), which defines this frontier as tasks an LLM cannot solve alone but can master with guidance. To operationalize this, we present the AgentFrontier Engine, an automated pipeline that synthesizes high-quality, multidisciplinary data situated precisely within the LLM's ZPD. This engine supports both continued pre-training with knowledge-intensive data and targeted post-training on complex reasoning tasks. From the same framework, we derive the ZPD Exam, a dynamic and automated benchmark designed to evaluate agent capabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on our synthesized data, which achieves state-of-the-art results on demanding benchmarks like Humanity's Last Exam, even surpassing some leading proprietary agents. Our work demonstrates that a ZPD-guided approach to data synthesis offers a scalable and effective path toward building more capable LLM agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ LLMåœ¨è·¨æ–‡æ¡£/è·¨é¢†åŸŸçš„çŸ¥è¯†èåˆä¸æ·±åº¦ç ”ç©¶ä»»åŠ¡ä¸Šèƒ½åŠ›ä¸è¶³ï¼ŒRAGåœ¨å¤šæºå¼‚æ„ä¿¡æ¯ç»¼åˆæ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œéš¾ä»¥æ”¯æ’‘å¤æ‚æ¨ç†<br>â€¢ è®­ç»ƒè¯­æ–™ç¼ºä¹ç³»ç»Ÿæ€§æ”¯æŒä»£ç†å¼èƒ½åŠ›ï¼ˆå·¥å…·ä½¿ç”¨ã€è‡ªåã€è§„åˆ’ã€å¤šæ­¥æ¨ç†ï¼‰çš„é«˜è´¨é‡æ•°æ®ï¼›ç°æœ‰ä¸“å®¶åŸºå‡†ï¼ˆå¦‚HLEï¼‰æ˜‚è´µä¸”ä¸å¯æ‰©å±•ï¼Œè¯„æµ‹é€æ¸é¥±å’Œ<br>â€¢ ç°æœ‰æ•°æ®åˆæˆèŒƒå¼ï¼ˆquery-centricä¸document-centricï¼‰å¤šè€ƒå¯Ÿå±€éƒ¨ç†è§£ï¼Œéš¾ä»¥ç”Ÿæˆéœ€è¦å¤šæ–‡çŒ®ç»¼åˆã€æŠ½è±¡å½’çº³ä¸å¯è®¡ç®—éªŒè¯çš„å¤æ‚ä»»åŠ¡<br>â€¢ ç¼ºå°‘ç²¾ç¡®çš„éš¾åº¦æ ¡å‡†æœºåˆ¶ï¼Œéš¾ä»¥å°†ä»»åŠ¡å¯¹é½åˆ°æ¨¡å‹çš„â€œæœ€è¿‘å‘å±•åŒºï¼ˆZPDï¼‰â€ï¼›å¯å‘å¼å çº¦æŸä¸è‡ªç”Ÿæˆå¸¸åœç•™åœ¨æ¨¡å‹è¡¨è¾¾ä¸Šé™å†…ï¼Œå‡çº§å™ªå£°å¤§ã€ä¸å¯æŒç»­</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºAgentFrontier Engineï¼šä¸€ä¸ªåŸºäºZPDçš„ä¸‰é˜¶æ®µæ•°æ®åˆæˆä¸æ ¡å‡†æ¡†æ¶â€”â€”(1) ä»å¤šæ–‡æ¡£å¤åˆå•å…ƒç”Ÿæˆç§å­é—®ç­”ï¼›(2) ä»¥å…·å¤‡æœç´¢/å­¦æœ¯/æµè§ˆ/ä»£ç å·¥å…·çš„ä»£ç†è¿­ä»£å‡çº§é—®é¢˜çš„çŸ¥è¯†å¹¿åº¦ã€æŠ½è±¡å±‚æ¬¡ã€äº‹å®æ ¡éªŒä¸å¯è®¡ç®—æ€§ï¼›(3) é€šè¿‡â€œLKPâ€“MKOâ€å¯¹æŠ—æ ¡å‡†ç­›å‡ºä½äºZPDçš„æ•°æ®ï¼ˆç®€å•æ ·æœ¬ç”¨äºCPTï¼Œå‰æ²¿æ ·æœ¬ç”¨äºåè®­ç»ƒï¼‰ã€‚åŒæ—¶åŸºäºåŒä¸€æµç¨‹æ„å»ºè‡ªè¿›åŒ–çš„ZPD Examï¼Œå¹¶å¯¹Qwen3-30Bè¿›è¡ŒCPT+RFTä»¥æ˜¾è‘—æå‡è·¨å­¦ç§‘æ·±åº¦ç ”ç©¶èƒ½åŠ›ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ZPD-åˆ†çº§æ”¯æ¶å­¦ä¹ ï¼šä»å…¨è½¨è¿¹ç¤ºèŒƒåˆ°å±‚çº§æç¤ºçš„è‡ªé€‚åº”è¾…åŠ©æ¡†æ¶â€”â€”å°†MKOç”±â€œå…¨è§£â€æ‰©å±•ä¸ºä»ç­–ç•¥æç¤ºåˆ°å­ç›®æ ‡åˆ†è§£çš„æ¸è¿›å¼æ”¯æ¶<br>â€¢ ä»æ¨¡ä»¿åˆ°æ¢ç´¢ï¼šåŸºäºZPDä¿¡å·çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç ”ç©¶ä»£ç†â€”â€”ä»¥RFTä¸ºå…ˆéªŒã€ç”¨BoN/å¯éªŒè¯åé¦ˆåšRLï¼Œå¼¥åˆpass@1ä¸pass@Næ€§èƒ½é¸¿æ²Ÿ<br>â€¢ ä»£ç†ä»å·¥å…·ä½¿ç”¨åˆ°å·¥å…·åˆ›é€ ï¼šé¢å‘å¤æ‚ç ”ç©¶ä»»åŠ¡çš„å±‚çº§å·¥å…·ç»„åˆä¸ç¨‹åºåˆæˆâ€”â€”åŠ¨æ€ç”Ÿæˆâ€œå…ƒå·¥å…·â€ä¸æŒ‰éœ€ç¨‹åºä»¥æ‰©å±•é—®é¢˜å¯è§£ç©ºé—´</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Group Relative Attention Guidance for Image Editing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24657" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24657" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recently, image editing based on Diffusion-in-Transformer models has undergone rapid development. However, existing editing methods often lack effective control over the degree of editing, limiting their ability to achieve more customized results. To address this limitation, we investigate the MM-Attention mechanism within the DiT model and observe that the Query and Key tokens share a bias vector that is only layer-dependent. We interpret this bias as representing the model's inherent editing behavior, while the delta between each token and its corresponding bias encodes the content-specific editing signals. Based on this insight, we propose Group Relative Attention Guidance, a simple yet effective method that reweights the delta values of different tokens to modulate the focus of the model on the input image relative to the editing instruction, enabling continuous and fine-grained control over editing intensity without any tuning. Extensive experiments conducted on existing image editing frameworks demonstrate that GRAG can be integrated with as few as four lines of code, consistently enhancing editing quality. Moreover, compared to the commonly used Classifier-Free Guidance, GRAG achieves smoother and more precise control over the degree of editing. Our code will be released at https://github.com/little-misfit/GRAG-Image-Editing.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°æœ‰DiT/å¤šæ¨¡æ€æ³¨æ„åŠ›ç¼–è¾‘æ–¹æ³•ç¼ºä¹å¯¹ç¼–è¾‘å¼ºåº¦çš„è¿ç»­ã€ç»†ç²’åº¦æ§åˆ¶ï¼Œéš¾ä»¥åœ¨â€œéµå¾ªç¼–è¾‘æŒ‡ä»¤â€ä¸â€œä¿æŒåŸå›¾ä¸€è‡´æ€§â€ä¹‹é—´å–å¾—å¯è°ƒèŠ‚çš„å¹³è¡¡ã€‚<br>â€¢ æ¨¡å‹å†…éƒ¨çš„MM-Attentionä¸­Query/KeyåµŒå…¥å­˜åœ¨å±‚ä¾èµ–çš„å…±äº«åç½®å‘é‡ï¼Œå¯¼è‡´tokené—´è¯­ä¹‰å·®å¼‚è¢«ç¨€é‡Šï¼Œé™ä½äº†å¯¹å…·ä½“ç¼–è¾‘ä¿¡å·çš„çµæ•åº¦ä¸å¯æ§æ€§ã€‚<br>â€¢ ç”¨æˆ·å¸¸éœ€ä¾èµ–æç¤ºå·¥ç¨‹æˆ–å¤šæ¬¡æ¨ç†åå¤è¯•é”™ï¼›å¸¸ç”¨çš„Classifier-Free Guidanceè°ƒèŠ‚ä¸å¤Ÿå¹³æ»‘ã€ç²¾ç¡®ï¼Œéš¾ä»¥å¾—åˆ°ç”¨æˆ·æœŸæœ›çš„æ¸è¿›å¼ç¼–è¾‘æ•ˆæœã€‚<br>â€¢ æ–‡æœ¬ä¸å›¾åƒåœ¨ç»Ÿä¸€åµŒå…¥ç©ºé—´ä¸­å‘ˆç°ä¸åŒçš„RoPEé¢‘ç‡åˆ†å¸ƒï¼Œè·¨æ¨¡æ€å¯¹é½ä¸è¶³ä¹Ÿé™åˆ¶äº†ç²¾ç¡®å¯æ§çš„ç¼–è¾‘è¡¨ç°ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºGroup Relative Attention Guidanceï¼ˆGRAGï¼‰ï¼šåœ¨MM-Attentionä¸­ä»¥tokenç»„ä¸ºå•ä½è®¡ç®—ç»„å‡å€¼Kbiasä½œä¸ºå…±äº«åç½®ï¼Œå¹¶ç”¨K' = Î»Â·Kbias + Î´Â·(Kâˆ’Kbias)é‡åŠ æƒåç½®ä¸åå·®ï¼Œè¿ç»­åœ°è°ƒèŠ‚æ¨¡å‹å¯¹åŸå›¾ä¸æŒ‡ä»¤çš„ä¾§é‡ä¸èšç„¦å¼ºåº¦ã€‚è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒã€ä»…éœ€æå°‘ä»£ç å³å¯é›†æˆåˆ°ç°æœ‰DiTç¼–è¾‘å™¨ä¸­ï¼Œç›¸æ¯”CFGæä¾›æ›´å¹³æ»‘ã€ç²¾ç»†çš„ç¼–è¾‘å¼ºåº¦æ§åˆ¶ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ åŸºäºåç½®æ„ŸçŸ¥çš„é€å±‚è°ƒåº¦ç”¨äºDiTå›¾åƒç¼–è¾‘ï¼šå­¦ä¹ /æœç´¢é€å±‚Î»ã€Î´æ›²çº¿ï¼Œè‡ªé€‚åº”å¹³è¡¡å„å±‚çš„å…±äº«åç½®ä¸tokenå·®åˆ†ä»¥æå‡å¯æ§æ€§ä¸ä¿çœŸåº¦ã€‚<br>â€¢ é¢å‘ç›¸å¯¹æ³¨æ„åŠ›å¼•å¯¼çš„è¯­ä¹‰åˆ†ç»„æœºåˆ¶ï¼šä»¥è¯­ä¹‰åˆ†å‰²/æ³¨æ„åŠ›èšç±»æ›¿ä»£è¿ç»­tokenåˆ‡ç‰‡ï¼Œå®ç°å¯¹å±€éƒ¨åŒºåŸŸä¸å¯¹è±¡çº§ç¼–è¾‘çš„æ›´ç²¾ç¡®è°ƒæ§ã€‚<br>â€¢ æ··åˆCFGä¸GRAGçš„é²æ£’å›¾åƒç¼–è¾‘å¼•å¯¼ï¼šç»“åˆå…¨å±€å¼ºåº¦æ§åˆ¶ï¼ˆCFGï¼‰ä¸ç›¸å¯¹æ³¨æ„åŠ›ç»†åŒ–ï¼ˆGRAGï¼‰ï¼Œåœ¨å¤šåœºæ™¯ä¸‹è·å¾—æ›´ç¨³å®šã€å¯é¢„æµ‹çš„ç¼–è¾‘è½¨è¿¹ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">VisCoder2: Building Multi-Language Visualization Coding Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23642" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23642" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) have recently enabled coding agents capable of generating, executing, and revising visualization code. However, existing models often fail in practical workflows due to limited language coverage, unreliable execution, and lack of iterative correction mechanisms. Progress has been constrained by narrow datasets and benchmarks that emphasize single-round generation and single-language tasks. To address these challenges, we introduce three complementary resources for advancing visualization coding agents. VisCode-Multi-679K is a large-scale, supervised dataset containing 679K validated and executable visualization samples with multi-turn correction dialogues across 12 programming languages. VisPlotBench is a benchmark for systematic evaluation, featuring executable tasks, rendered outputs, and protocols for both initial generation and multi-round self-debug. Finally, we present VisCoder2, a family of multi-language visualization models trained on VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4.1, with further gains from iterative self-debug, reaching 82.4% overall execution pass rate at the 32B scale, particularly in symbolic or compiler-dependent languages.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å¯é æ€§ä¸è¿­ä»£ä¿®å¤ç¼ºå¤±ï¼šç°æœ‰å¯è§†åŒ–ä»£ç ä»£ç†å¸¸åœ¨å•è½®ç”Ÿæˆä¸­å‡ºç°å´©æºƒã€æ¸²æŸ“é”™è¯¯æˆ–è¯­ä¹‰åå·®ï¼Œç¼ºå°‘åŸºäºæ‰§è¡Œæ—¥å¿—ä¸æ¸²æŸ“ç»“æœçš„è‡ªæˆ‘çº é”™æœºåˆ¶ï¼Œéš¾ä»¥ç¬¦åˆçœŸå®â€œç”Ÿæˆâ€”è¿è¡Œâ€”ä¿®æ­£â€çš„å·¥ä½œæµã€‚<br>â€¢ å¤šè¯­è¨€è¦†ç›–ä¸è¶³ä¸”ç¬¦å·/ç¼–è¯‘å‹è¯­è¨€æœ€è„†å¼±ï¼šå¤šæ•°æ–¹æ³•é›†ä¸­äº Python/Vegaâ€‘Liteï¼Œéš¾ä»¥æ³›åŒ–åˆ° LaTeXã€LilyPondã€Asymptote ç­‰è¯­æ³•ä¸ç¼–è¯‘ä¸¥æ ¼çš„è¯­è¨€ï¼Œè·¨ç”Ÿæ€ä¸€è‡´æ€§å·®ã€å®¹é”™ä½ã€‚<br>â€¢ æ•°æ®ä¸è¯„æµ‹èµ„æºåŒ®ä¹ï¼šå…¬å¼€æ•°æ®å¤šä¸å¯æ‰§è¡Œæˆ–ç¼ºå°‘æ¸²æŸ“è¾“å‡ºä¸å¤šè½®äº¤äº’ï¼Œæ—¢æœ‰åŸºå‡†åé‡å•è¯­è¨€ã€å•å›åˆï¼Œæ— æ³•ç³»ç»Ÿè¡¡é‡è·¨è¯­è¨€ä¸å¤šè½®ä¿®å¤èƒ½åŠ›ï¼Œåˆ¶çº¦è®­ç»ƒä¸å®¢è§‚è¯„ä¼°ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºä¸‰ä½ä¸€ä½“æŠ€æœ¯è·¯çº¿ï¼šæ„å»ºå« 12 è¯­è¨€ã€679K æ¡å¯æ‰§è¡Œæ ·æœ¬ä¸ 66K å¤šè½®çº é”™å¯¹è¯çš„ VisCodeâ€‘Multiâ€‘679Kï¼›å‘å¸ƒè¦†ç›– 8 è¯­è¨€ã€888 ä»»åŠ¡å¹¶æ”¯æŒâ€œæ‰§è¡Œâ€”æ¸²æŸ“â€”è¯„åˆ†â€”å¤šè½®è‡ªè°ƒè¯•â€çš„ VisPlotBenchï¼›è®­ç»ƒå…·å¤‡â€œç”Ÿæˆâ€”æ‰§è¡Œâ€”æ¸²æŸ“â€”è‡ªçº é”™â€é—­ç¯çš„ VisCoder2 æ¨¡å‹å®¶æ—ï¼ˆåŸºäº Qwen2.5â€‘Coder å¾®è°ƒï¼‰ï¼Œåœ¨ 32B è§„æ¨¡è‡ªè°ƒè¯•ä¸‹æ•´ä½“æ‰§è¡Œé€šè¿‡ç‡è¾¾ 82.4%ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Grammar-Aware Self-Debugging for Visualization DSLsï¼šä»¥è¯­æ³•æ ‘/ç¼–è¯‘å™¨åé¦ˆçº¦æŸè§£ç ä¸è®­ç»ƒï¼Œé’ˆå¯¹ LaTeX/LilyPond/Asymptote é™ä½ç»“æ„ä¸æ¥å£é”™è¯¯ï¼Œç¨³å›ºç¬¦å·è¯­è¨€æ‰§è¡Œç‡ã€‚<br>â€¢ Execution-Grounded Reward Learning for Chart Agentsï¼šå°†æ‰§è¡Œæ—¥å¿—ã€ä»»åŠ¡è¯„åˆ†ä¸è§†è§‰ç›¸ä¼¼åº¦ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œè¿›è¡ŒRL/DPOä»¥ä¼˜åŒ–å¯æ‰§è¡Œæ€§ä¸è¯­ä¹‰-æ„ŸçŸ¥ä¸€è‡´æ€§ã€‚<br>â€¢ VisPlotBench 2.0: Multimodal and Interactive Evaluationï¼šæ‰©å±•åˆ°äº¤äº’å¼å›¾è¡¨ã€å›¾åˆ°ç /ç åˆ°å›¾ã€å¤šå‰ç«¯æ¸²æŸ“å·®å¼‚ä¸æ›´ç»†ç²’åº¦åˆ¤åˆ†å™¨ï¼Œè¦†ç›–æ›´å¤šè¯­è¨€ä¸åº“ä»¥æå‡å¤–æ¨ä¸å…¬å¹³è¯„æµ‹ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24711" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24711" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model capacity while preserving computational efficiency. Despite its notable success in large language models (LLMs), existing attempts to apply MoE to Diffusion Transformers (DiTs) have yielded limited gains. We attribute this gap to fundamental differences between language and visual tokens. Language tokens are semantically dense with pronounced inter-token variation, while visual tokens exhibit spatial redundancy and functional heterogeneity, hindering expert specialization in vision MoE. To this end, we present ProMoE, an MoE framework featuring a two-step router with explicit routing guidance that promotes expert specialization. Specifically, this guidance encourages the router to partition image tokens into conditional and unconditional sets via conditional routing according to their functional roles, and refine the assignments of conditional image tokens through prototypical routing with learnable prototypes based on semantic content. Moreover, the similarity-based expert allocation in latent space enabled by prototypical routing offers a natural mechanism for incorporating explicit semantic guidance, and we validate that such guidance is crucial for vision MoE. Building on this, we propose a routing contrastive loss that explicitly enhances the prototypical routing process, promoting intra-expert coherence and inter-expert diversity. Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives. Code and models will be made publicly available.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°æœ‰DiTä¸­çš„MoEå¢ç›Šæœ‰é™ï¼šè§†è§‰tokenå…·æœ‰é«˜ç©ºé—´å†—ä½™ä¸åŠŸèƒ½å¼‚è´¨æ€§ï¼ˆCFGå¼•å…¥æœ‰/æ— æ¡ä»¶ä¸¤ç±»tokenï¼‰ï¼Œå¯¼è‡´ä¸“å®¶éš¾ä»¥ä¸“ç²¾ã€è·¯ç”±å¤±è¡¡ï¼ŒDiT-MoE/EC-DiT/DiffMoEæå‡æœ‰é™æˆ–ä¸ç¨³å®šã€‚<br>â€¢ ç¨ å¯†DiTæ¿€æ´»å…¨éƒ¨å‚æ•°ã€è®¡ç®—å¼€é”€å¤§ï¼ŒäºŸéœ€åœ¨ä¸å¢åŠ æ¿€æ´»å‚æ•°çš„å‰æä¸‹æå‡æ¨¡å‹å®¹é‡ä¸ç”Ÿæˆè´¨é‡çš„ç¨€ç–åŒ–æ–¹æ¡ˆã€‚<br>â€¢ ç¼ºä¹æ˜¾å¼è¯­ä¹‰/åŠŸèƒ½è·¯ç”±æŒ‡å¯¼ï¼Œéš¾ä»¥åŒæ—¶å®ç°â€œä¸“å®¶å†…ä¸€è‡´æ€§ï¼ˆç›¸ä¼¼tokenå½’ä¸€ç±»ï¼‰â€ä¸â€œä¸“å®¶é—´å¤šæ ·æ€§ï¼ˆä¸“å®¶åŠŸèƒ½å·®å¼‚åŒ–ï¼‰â€ï¼Œé™åˆ¶å¯æ‰©å±•æ€§ä¸ä¸Šé™ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºProMoEï¼šé‡‡ç”¨å¸¦æ˜¾å¼è·¯ç”±æŒ‡å¯¼çš„ä¸¤æ­¥è·¯ç”±ã€‚å…ˆè¿›è¡Œæ¡ä»¶è·¯ç”±ï¼Œç¡¬åˆ’åˆ†æ— æ¡ä»¶tokenåˆ°æ— æ¡ä»¶ä¸“å®¶ã€æœ‰æ¡ä»¶tokenè¿›å…¥æ ‡å‡†ä¸“å®¶æ± ï¼›å†ä»¥å¯å­¦ä¹ åŸå‹çš„ä½™å¼¦ç›¸ä¼¼åº¦æ‰§è¡ŒåŸå‹è·¯ç”±ï¼ˆTop-Kï¼‰ï¼Œå¹¶å¼•å…¥è·¯ç”±å¯¹æ¯”æŸå¤±å¼ºåŒ–è¯­ä¹‰èšåˆä¸ä¸“å®¶åˆ†ç¦»ï¼ŒåŒæ—¶é…åˆå…±äº«ä¸“å®¶å­¦ä¹ å…±æ€§ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ æ—¶ç©ºè‡ªé€‚åº”åŸå‹è·¯ç”±ï¼šåœ¨ä¸åŒæ‰©æ•£æ—¶åˆ»/ç½‘ç»œå±‚åŠ¨æ€è°ƒæ•´åŸå‹ä¸æ¸©åº¦ï¼Œæå‡ç»†ç²’åº¦ä¸“å®¶åˆ†å·¥ä¸ç¨³å®šæ€§ã€‚<br>â€¢ è·¨æ¨¡æ€ä¸å¤šä»»åŠ¡ProMoEï¼šå°†æ¡ä»¶+åŸå‹è·¯ç”±æ¨å¹¿åˆ°æ–‡æœ¬-å›¾åƒã€è§†é¢‘ç”Ÿæˆä¸ç¼–è¾‘ä»»åŠ¡ï¼Œæ„å»ºç»Ÿä¸€ç¨€ç–è·¯ç”±æ¡†æ¶ã€‚<br>â€¢ å¯è§£é‡Šä¸ç¨³å¥çš„è·¯ç”±å¯¹æ¯”å­¦ä¹ ï¼šåŠ å…¥åŸå‹å¯è§†åŒ–ä¸é²æ£’æ­£åˆ™ï¼Œæå‡å¯¹åˆ†å¸ƒåç§»ä¸å¯¹æŠ—å™ªå£°çš„è·¯ç”±ç¨³å®šæ€§ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24693" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24693" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning. We formalize audio 4D intelligence that is defined as reasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to measure it. STAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories. Our data curation pipeline uses two methods to ensure high-quality samples. For foundational tasks, we use procedurally synthesized and physics-simulated audio. For holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance. Unlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5\% temporal, -35.2\% spatial), evidencing its focus on linguistically hard-to-describe cues. Evaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning. Our STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°æœ‰éŸ³é¢‘åŸºå‡†ä¸»è¦è€ƒæŸ¥å¯ç”±æ–‡æœ¬å­—å¹•æ¢å¤çš„ç²—ç²’åº¦è¯­ä¹‰ï¼Œéš¾ä»¥è¡¡é‡ç»†ç²’åº¦å£°å­¦æ„ŸçŸ¥ä¸æ·±åº¦æ—¶ç©ºæ¨ç†ï¼›ç”¨â€œéŸ³é¢‘å­—å¹•â€ä½œç­”çš„æ€§èƒ½å‡ ä¹ä¸ä¸‹é™ï¼Œè€Œåœ¨STAR-Benchä¸Šé™å¹…æ˜¾è‘—ï¼ˆå›¾1å·¦ï¼Œç¬¬2é¡µï¼‰ï¼Œæš´éœ²å‡ºåŸºå‡†ä¸æ¨¡å‹å¯¹éè¯­è¨€åŒ–å£°å­¦çº¿ç´¢çš„ç¼ºå£ã€‚<br>â€¢ å®é™…åº”ç”¨ï¼ˆembodied/æœºå™¨äººã€å®‰å…¨é¢„è­¦ã€åœºæ™¯ç†è§£ï¼‰éœ€è¦â€œ4DéŸ³é¢‘æ™ºèƒ½â€ï¼ˆæ—¶é—´+ä¸‰ç»´ç©ºé—´ï¼‰æ¥ç†è§£å£°æºéšæ—¶é—´ä¸ç©ºé—´çš„åŠ¨æ€å˜åŒ–ï¼Œäººç±»å¯å‡­éš¾ä»¥è¨€è¡¨çš„å£°å­¦çº¿ç´¢å®Œæˆæ­¤ç±»åˆ¤æ–­ï¼ˆå¦‚æ°´ä½ã€æ¥è½¦è½¨è¿¹ï¼‰ï¼ˆç¬¬2é¡µå¼•è¨€ï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•ä¸è¯„æµ‹çš„å±€é™ï¼šå¤šä¸ºå•éŸ³é¢‘ã€é™æ€ç©ºé—´ä»»åŠ¡ï¼Œç¼ºå¤±å¤šéŸ³é¢‘å¯¹æ¯”/æ•´åˆä¸åŠ¨æ€è½¨è¿¹æ¨ç†ï¼›ç¼ºå°‘å¯¹å…­å¤§åŸºç¡€å±æ€§ï¼ˆéŸ³é«˜ã€å“åº¦ã€æ—¶é•¿ã€æ–¹ä½ã€ä¿¯ä»°ã€è·ç¦»ï¼‰çš„å®šé‡æµ‹è¯„ï¼›å¸¸è§é¢„å¤„ç†å°†å¤šé€šé“å¹³å‡ä¸ºå•å£°é“ï¼Œå¯¼è‡´ç©ºé—´çº¿ç´¢ä¸¢å¤±ï¼ˆå›¾3ï¼Œç¬¬6é¡µï¼‰ï¼›è¯„æµ‹é²æ£’æ€§ä¸äººå·¥æ ¸éªŒä¸è¶³ï¼ˆè¡¨1ï¼Œç¬¬3é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºSTAR-Benchåˆ†å±‚åŸºå‡†ï¼šä»¥ç¨‹åºåŒ–/ç‰©ç†ä»¿çœŸæ„å»ºâ€œåŸºç¡€å£°å­¦æ„ŸçŸ¥â€å…­å±æ€§çš„ç»å¯¹èŒƒå›´ä¸ç›¸å¯¹æ•æ„Ÿåº¦æµ‹è¯„ï¼Œç»“åˆçœŸå®ä¸–ç•ŒéŸ³é¢‘ä¸Šçš„â€œæ•´ä½“æ—¶ç©ºæ¨ç†â€ï¼ˆç‰‡æ®µé‡æ’çš„æ—¶é—´æ¨ç†ä¸é™æ€/å¤šæºå…³ç³»/åŠ¨æ€è½¨è¿¹çš„ç©ºé—´æ¨ç†ï¼‰ï¼Œå¹¶é…å¥—AI+äººå·¥å››é˜¶æ®µæ•°æ®ç­›é€‰ä¸AA/ACRé²æ£’è¯„æµ‹ã€åŸç”Ÿ/åˆ†é€šé“ä¸¤ç§ç«‹ä½“å£°è¾“å…¥ç­–ç•¥ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Beyond-Mono: åŸç”Ÿå¤šé€šé“éŸ³é¢‘ç¼–ç å™¨ç”¨äº4Dç©ºé—´æ¨ç†ï¼šè®¾è®¡ç«¯åˆ°ç«¯ä¿çœŸå¤„ç†ITD/ILD/HRTFçº¿ç´¢çš„å¤šé€šé“LALMï¼Œé¿å…é€šé“å¹³å‡é€ æˆçš„ç©ºé—´ä¿¡æ¯ä¸¢å¤±ï¼Œæ”¯æŒåŠ¨æ€è½¨è¿¹å»ºæ¨¡ã€‚<br>â€¢ Dense-AAC Pretraining for Reasoning: é¢å‘ç»†ç²’åº¦å£°å­¦çº¿ç´¢çš„ç¨ å¯†éŸ³é¢‘å­—å¹•é¢„è®­ç»ƒï¼šæ„å»ºç¨ å¯†éŸ³é¢‘æè¿°æ•°æ®ä¸è®­ç»ƒç›®æ ‡ï¼Œæ¡¥æ¥æ„ŸçŸ¥ä¸çŸ¥è¯†ï¼Œæå‡æ—¶é—´ç‰‡æ®µé‡æ’ä¸å¤šæºç©ºé—´å…³ç³»æ¨ç†ã€‚<br>â€¢ Multi-Audio Grounding Transformers: é¢å‘å¤šéŸ³é¢‘å¯¹æ¯”ä¸å¯¹é½çš„è·¨ç‰‡æ®µæ¨ç†æ¨¡å‹ï¼šå¼•å…¥è·¨éŸ³é¢‘å¯¹é½ã€æ£€ç´¢ä¸ä¸€è‡´æ€§çº¦æŸï¼Œæå‡å¯¹å¤šæ®µéŸ³é¢‘çš„æ¯”è¾ƒã€æ•´åˆä¸æ—¶ç©ºå®šä½èƒ½åŠ›ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24514" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24514" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. Inspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad. The internal visual representations of MLLMs have traditionally been confined to perceptual understanding. We repurpose them to support generative visual thought without compromising reasoning ability. Building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process. It allows the model to interleave textual reasoning with the generation of visual latents. These latents guide the internal thought process and can be translated into sketch images for interpretability. To realize this, we introduce two components: a Context-Aware Vision Head autoregressively produces visual representations, and a pretrained Sketch Decoder renders these into human-interpretable images. We evaluate the framework on our new dataset MazePlanning. Experiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance to their backbone. It further generalizes across distinct frontier MLLMs, including Gemma3 and Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our framework opens new opportunities for richer human-computer interaction and broader applications. More details and resources are available on our project page: https://latent-sketchpad.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°æœ‰MLLMåœ¨éœ€è¦ç²¾ç¡®ç©ºé—´æ¨ç†ä¸åŠ¨æ€è§†è§‰å®šä½çš„å¤šæ­¥åœºæ™¯ï¼ˆå¦‚è·¯å¾„è§„åˆ’ã€æŒç»­çŠ¶æ€è·Ÿè¸ªï¼‰ä¸­è¡¨ç°æ¬ ä½³ï¼Œå•çº¯æ–‡æœ¬CoTéš¾ä»¥æ‰¿è½½ç©ºé—´ç»“æ„ä¸æ—¶åºçº¦æŸã€‚<br>â€¢ äººç±»æ¨ç†å¸¸å€ŸåŠ©â€œå¿ƒæ™ºè‰å›¾â€è¿›è¡Œæƒ³è±¡ä¸è§„åˆ’ï¼›å°†è§†è§‰æ€ç»´å†…åŒ–åˆ°æ¨¡å‹ä¸­æœ‰æœ›æ˜¾è‘—æå‡å¤æ‚å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ä¸å¯è§£é‡Šæ€§ã€‚<br>â€¢ ä¾èµ–å¤–éƒ¨å·¥å…·ï¼ˆæ£€æµ‹å™¨ã€å¯æ‰§è¡Œç¨‹åºï¼‰çš„æ–¹æ¡ˆå—å·¥å…·èƒ½åŠ›ä¸å¤–éƒ¨ç¯å¢ƒçº¦æŸï¼Œé—­ç¯éš¾ã€é²æ£’æ€§å—é™ï¼Œä¸”ä¸æ˜“ç«¯åˆ°ç«¯ä¸€ä½“åŒ–ã€‚<br>â€¢ ç»Ÿä¸€æ–‡ç”Ÿå›¾ç”Ÿæˆæ¨¡å‹æ›´ä¾§é‡åƒç´ é€¼çœŸåº¦è€ŒéæŠ½è±¡ç»“æ„è¡¨è¾¾ï¼Œè®­ç»ƒä»£ä»·é«˜ã€ä¸å¼ºæ„ŸçŸ¥å‹MLLMè¡”æ¥æˆæœ¬å¤§ï¼Œä¸åˆ©äºæ¨ç†ä¸­çš„å³æ—¶è§†è§‰æƒ³è±¡ã€‚<br>â€¢ ç°æœ‰å¼ºé¢„è®­ç»ƒMLLMï¼ˆå¦‚ Gemma3ã€Qwen2.5-VLï¼‰è™½å…·å¼ºè§†è§‰ç†è§£åŠ›ï¼Œå´ç¼ºä¹åŸç”Ÿè§†è§‰å†…å®¹ç”Ÿæˆï¼›å…¶å†…éƒ¨è§†è§‰è¡¨å¾æœªè¢«â€œå†åˆ©ç”¨â€ä¸ºç”Ÿæˆå¼è§†è§‰æ€è€ƒé€šé“ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºLatent Sketchpadï¼šåœ¨é¢„è®­ç»ƒMLLMä¸­åŠ å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„Vision Headï¼Œä½¿æ¨¡å‹åœ¨è‡ªå›å½’æ¨ç†ä¸­å¯äº¤æ›¿ç”Ÿæˆæ–‡æœ¬ä¸è§†è§‰æ½œå˜é‡ï¼ˆé€šè¿‡å› æœäº¤/è‡ªæ³¨æ„åŠ›èåˆå…¨å±€å†å²å›¾åƒä¸å½“å‰å±€éƒ¨ä¸Šä¸‹æ–‡ï¼‰ï¼Œå¹¶ç”¨ç‹¬ç«‹é¢„è®­ç»ƒçš„Sketch Decoderï¼ˆAlignerNetæ˜ å°„è‡³VAEæ½œç©ºé—´ã€æ¥å…¥å†»ç»“çš„SDXL-VAEè§£ç å™¨ï¼‰å°†æ½œå˜é‡å¯è§†åŒ–ä¸ºè‰å›¾ä»¥æå‡å¯è§£é‡Šæ€§ã€‚è®­ç»ƒä»…å›å½’æ½œå˜é‡ä¸”å†»ç»“éª¨å¹²ï¼Œå®ç°å³æ’å³ç”¨åœ°ä¸ºGemma3ã€Qwen2.5-VLç­‰èµ‹äºˆâ€œè§†è§‰æ€è€ƒâ€èƒ½åŠ›ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Scaling Visual Thought to Real-World Tasksï¼šå°†æ½œåœ¨è‰å›¾ä»è¿·å®«è§„åˆ’æ‰©å±•åˆ°å®¤å†…å¯¼èˆªã€æœºå™¨äººæŠ“å–ä¸å¤šç›®æ ‡äº¤äº’ç­‰å¤æ‚åœºæ™¯ï¼ŒéªŒè¯è·¨ä»»åŠ¡æ³›åŒ–ä¸æ•°æ®åˆæˆç­–ç•¥ã€‚<br>â€¢ When and Where to Sketch: Learning Triggers for Visual Thoughtï¼šå­¦ä¹ å¼ç­–ç•¥å†³å®šä½•æ—¶ç”Ÿæˆ/æ›´æ–°è§†è§‰æ½œå˜é‡ä¸å¯è§†åŒ–æ­¥æ•°ï¼Œåœ¨æ¨ç†é¢„ç®—ä¸‹å®ç°ä»£ä»·â€“æ”¶ç›Šæœ€ä¼˜ã€‚<br>â€¢ Structure-Preserving Latent Training for Visual Reasoningï¼šå¼•å…¥æ‹“æ‰‘/å‡ ä½•ä¸€è‡´æ€§ä¸ç‰©ç†å¯è¾¾æ€§çº¦æŸï¼Œæå‡æ½œåœ¨è‰å›¾å¯¹ç¯å¢ƒç»“æ„ä¸åŠ¨åŠ›å­¦çš„éµå¾ªåº¦ä¸OODé²æ£’æ€§ã€‚<br>â€¢ Joint Tuning for Languageâ€“Vision Latent Alignmentï¼šä»åªè®­ç»ƒå¤´éƒ¨æ‰©å±•ä¸ºå¯¹è¿æ¥å™¨/å°‘é‡éª¨å¹²çš„è½»é‡è”åˆå¾®è°ƒï¼Œå¼ºåŒ–è¯­è¨€â€”è§†è§‰æ½œç©ºé—´çš„è¯­ä¹‰ä¸å‡ ä½•å¯¹é½ã€‚<br>â€¢ Uncertainty-Aware and Branching Visual Thoughtsï¼šåœ¨æ½œå˜é‡ä¸è‰å›¾ä¸Šå»ºæ¨¡ä¸ç¡®å®šæ€§ä¸å¤šå‡è®¾åˆ†æ”¯ï¼Œæ”¯æŒæ­§ä¹‰åœºæ™¯ä¸­çš„åˆ†å¸ƒå¼è§„åˆ’ä¸äººæœºåä½œå†³ç­–ã€‚<br>â€¢ Tool-Augmented Visual Sketching Loopï¼šå°†å†…ç”Ÿè‰å›¾ä¸å¤–éƒ¨æ£€ç´¢/ä»¿çœŸå·¥å…·ååŒï¼Œä½¿â€œå†…åœ¨å¯è§†åŒ–â€”å¤–éƒ¨éªŒè¯â€”å†è§„åˆ’â€å½¢æˆé—­ç¯ï¼Œæé«˜ç¨³å®šæ€§ä¸å¯æ‰©å±•æ€§ã€‚<br>â€¢ Lightweight and High-Fidelity Sketch Decodingï¼šæ¢ç´¢æ‰©æ•£/Rectified Flowè§£ç å™¨ã€è’¸é¦ä¸é‡åŒ–ä»¥å…¼é¡¾å¯è§†åŒ–ä¿çœŸåº¦ä¸åœ¨çº¿æ¨ç†æ—¶å»¶ï¼Œç”¨äºå®æ—¶åº”ç”¨ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24320" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24320" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Training critiquing language models to assess and provide feedback on model outputs is a promising way to improve LLMs for complex reasoning tasks. However, existing approaches typically rely on stronger supervisors for annotating critique data. To address this, we propose Critique-RL, an online RL approach for developing critiquing language models without stronger supervision. Our approach operates on a two-player paradigm: the actor generates a response, the critic provides feedback, and the actor refines the response accordingly. We first reveal that relying solely on indirect reward signals from the actor's outputs for RL optimization often leads to unsatisfactory critics: while their helpfulness (i.e., providing constructive feedback) improves, the discriminability (i.e., determining whether a response is high-quality or not) remains poor, resulting in marginal performance gains. To overcome this, Critique-RL adopts a two-stage optimization strategy. In stage I, it reinforces the discriminability of the critic with direct rule-based reward signals; in stage II, it introduces indirect rewards based on actor refinement to improve the critic's helpfulness, while maintaining its discriminability via appropriate regularization. Extensive experiments across various tasks and models show that Critique-RL delivers substantial performance improvements. For example, it achieves a 9.02% gain on in-domain tasks and a 5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šåœ¨æ²¡æœ‰æ›´å¼ºç›‘ç£ä¸æµ‹è¯•æ—¶æ— oracleéªŒè¯çš„ç°å®æ¡ä»¶ä¸‹ï¼Œå¦‚ä½•è®­ç»ƒæ—¢èƒ½â€œåˆ¤æ–­å¥½åâ€ï¼ˆåˆ¤åˆ«æ€§ï¼‰åˆèƒ½â€œç»™å‡ºå¯æ“ä½œåé¦ˆâ€ï¼ˆå¸®åŠ©æ€§ï¼‰çš„æ‰¹åˆ¤æ¨¡å‹ï¼Œç”¨äºå¯æ‰©å±•ç›‘ç£å¤æ‚æ¨ç†ä¸å†³ç­–ä»»åŠ¡ã€‚<br>â€¢ é‡è¦æ€§ï¼šLLMåœ¨æ•°å­¦æ¨ç†ã€ä»£ç ä¸å†³ç­–ç­‰é«˜éš¾ä»»åŠ¡ä¸Šæ€¥éœ€å¯é ç›‘ç£ï¼›æ‰¹åˆ¤æ¨¡å‹å¯æ˜¾è‘—æå‡æ¨ç†å‡†ç¡®ç‡ã€æŠ¬é«˜æ€§èƒ½ä¸Šé™å¹¶æ›´é«˜æ•ˆåˆ©ç”¨æ¨ç†è®¡ç®—ï¼ˆè§æ–‡ä¸­å›¾1ã€è¡¨1ä¸OODç»“æœï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šâ‘  ä¾èµ–æ›´å¼ºç›‘ç£çš„SFT/æ•°æ®æ„é€ æˆæœ¬é«˜ã€éš¾æ‰©å±•ï¼Œä¸”æ ‡æ³¨åˆ†å¸ƒä¸æ¨¡å‹è¾“å‡ºåˆ†å¸ƒä¸åŒ¹é…ï¼›â‘¡ ä»…é æç¤ºå·¥ç¨‹å¸¸å‡è®¾æµ‹è¯•æ—¶å­˜åœ¨oracleåˆ¤åˆ«å™¨ï¼Œç»•è¿‡â€œåˆ¤åˆ«â€èƒ½åŠ›ï¼Œè„±ç¦»oracleå³é‡æ€§èƒ½ç“¶é¢ˆï¼›â‘¢ ä»…ç”¨ç»“æœå‹é—´æ¥å¥–åŠ±ï¼ˆå¦‚åŸºäºæ”¹å†™æ˜¯å¦æ­£ç¡®ï¼‰åšRLä¼šå¯¼è‡´å¸®åŠ©æ€§æå‡è€Œåˆ¤åˆ«æ€§ä¸è¶³ï¼Œè¿›è€Œå‡ºç°è¿‡åº¦ä¿å®ˆ/è¿‡åº¦æ¿€è¿›çš„å¤±è´¥æ¨¡å¼ï¼Œè®­ç»ƒæ˜“é™·å…¥ç“¶é¢ˆç”šè‡³å´©å¡Œï¼ˆæ–‡ä¸­å›¾3åˆ†æï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºCritique-RLï¼šåœ¨ä¸¤è§’è‰²ï¼ˆActorç”Ÿæˆâ€”Criticæ‰¹åˆ¤â€”Actoræ”¹å†™ï¼‰çš„åœ¨çº¿RLæ¡†æ¶ä¸‹ï¼Œå¯¹â€œæ‰¹åˆ¤æ¨¡å‹â€è¿›è¡Œä¸¤é˜¶æ®µå¼ºåŒ–ã€‚Stage Iç”¨ç›´æ¥ã€åŸºäºè§„åˆ™çš„åˆ¤åˆ«å¥–åŠ±å¼ºåŒ–Criticçš„åˆ¤åˆ«æ€§ï¼›Stage IIå¼•å…¥åŸºäºActoræ”¹å†™æ­£ç¡®æ€§çš„é—´æ¥å¥–åŠ±æå‡å¸®åŠ©æ€§ï¼Œå¹¶ä»¥åˆ¤åˆ«å¥–åŠ±ä¸KLæ­£åˆ™ä¿æŒåˆ¤åˆ«èƒ½åŠ›ç¨³å®šã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ä»åˆ¤åˆ«åˆ°å…±è®­ï¼šè”åˆä¼˜åŒ–Actorâ€“Criticä»¥æå‡ååŒæ¼”åŒ–ï¼šè®©Actorä¸å†å†»ç»“ï¼Œä¸Criticè¿›è¡ŒåŒå‘RLå…±è®­ï¼Œç ”ç©¶ç¨³å®šæ€§ã€æ”¶æ•›æ€§ä¸æ•´ä½“æ”¶ç›Šã€‚<br>â€¢ å¼€æ”¾å¼ä»»åŠ¡çš„Critique-RL-RMï¼šå¼•å…¥å­¦ä¹ å‹å¥–åŠ±æ¨¡å‹çš„ä¸¤é˜¶æ®µæ‰¹åˆ¤RLï¼šä»¥å¥–åŠ±æ¨¡å‹/AIåé¦ˆæ›¿ä»£è§„åˆ™åˆ¤åˆ«æ‰©å±•åˆ°æ‘˜è¦ã€å¯¹è¯ç­‰å¼€æ”¾å¼ä»»åŠ¡ã€‚<br>â€¢ ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„æ ¡å‡†æ‰¹åˆ¤æ¨¡å‹ï¼šåœ¨å¥–åŠ±ä¸æŸå¤±ä¸­å¼•å…¥ç½®ä¿¡åº¦ä¸æ ¡å‡†çº¦æŸï¼Œç¼“è§£ä¿å®ˆ/æ¿€è¿›å¤±è¡¡å¹¶æå‡Acc@Disä¸æ€»ä½“æ•ˆç›Šã€‚<br>â€¢ å¤šæ‰¹åˆ¤è€…è¾©è®ºä¸é›†æˆçš„ä¸¤é˜¶æ®µRLï¼šè®­ç»ƒå¤šæ ·Criticå¹¶åœ¨æ¨ç†æ—¶åšè¾©è®º/é›†æˆï¼Œç ”ç©¶è®¡ç®—-æ€§èƒ½æƒè¡¡ä¸é²æ£’æ€§æå‡ã€‚<br>â€¢ è·¨è§„æ¨¡å¼±åˆ°å¼ºçš„å¯æ‰©å±•ç›‘ç£ï¼šç”¨è¾ƒå°Criticç›‘ç£æ›´å¼ºActorï¼ˆweak-to-strongï¼‰ï¼Œç³»ç»Ÿåˆ†æè¿ç§»æ•ˆæœä¸æ³›åŒ–è¾¹ç•Œã€‚<br>â€¢ ä¸¤é˜¶æ®µCritique-RLçš„ç†è®ºä¿è¯ï¼šå¯¹åˆ¤åˆ«-å¸®åŠ©æ€§çš„ç›®æ ‡æƒè¡¡ç»™å‡ºæ”¶æ•›ä¸ç¨³å®šæ€§åˆ†æï¼Œé˜æ˜è®­ç»ƒå´©å¡Œçš„é¿å…æ¡ä»¶ã€‚<br>â€¢ è‡ªé€‚åº”æ¨ç†è®¡ç®—åˆ†é…çš„Critique-RLï¼šå­¦ä¹ å¼æ§åˆ¶responseâ€“critiqueâ€“refineçš„é¢„ç®—åˆ†é…ä¸é‡‡æ ·ç­–ç•¥ï¼Œæå‡è®¡ç®—æ•ˆç‡ä¸æ€§èƒ½ä¸Šé™ã€‚<br>â€¢ é¢å‘å®‰å…¨ä¸å¯¹æŠ—åœºæ™¯çš„é²æ£’æ‰¹åˆ¤ç›‘ç£ï¼šè®¾è®¡æŠ—åˆ†å¸ƒå¤–/å¯¹æŠ—æ”»å‡»çš„åˆ¤åˆ«å¥–åŠ±ä¸è®­ç»ƒæµç¨‹ï¼Œæå‡å®é™…éƒ¨ç½²å¯é æ€§ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large Reasoning Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21978" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21978" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement learning with verifiable rewards (RLVR) has delivered impressive gains in mathematical and multimodal reasoning and has become a standard post-training paradigm for contemporary language and vision-language models. However, the RLVR recipe introduces a significant risk of capability regression, where models forget foundational skills after prolonged training without employing regularization strategies. We empirically confirm this concern, observing that open-source reasoning models suffer performance degradation on core capabilities such as perception and faithfulness. While imposing regularization terms like KL divergence can help prevent deviation from the base model, these terms are calculated on the current task, thus they do not guarantee broader knowledge. Meanwhile, commonly used experience replay across heterogeneous domains makes it nontrivial to decide how much training focus each objective should receive. To address this, we propose RECAP-a replay strategy with dynamic objective reweighting for general knowledge preservation. Our reweighting mechanism adapts in an online manner using short-horizon signals of convergence and instability, shifting the post-training focus away from saturated objectives and toward underperforming or volatile ones. Our method is end-to-end and readily applicable to existing RLVR pipelines without training additional models or heavy tuning. Extensive experiments on benchmarks based on Qwen2.5-VL-3B and Qwen2.5-VL-7B demonstrate the effectiveness of our method, which not only preserves general capabilities but also improves reasoning by enabling more flexible trade-offs among in-task rewards.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šæ¨ç†å¯¼å‘çš„RLVR/GRPOåè®­ç»ƒä¼šå¯¼è‡´é€šç”¨èƒ½åŠ›é—å¿˜ï¼ˆæ„ŸçŸ¥ã€OCRã€é²æ£’æ€§ã€äº‹å®æ€§/å®‰å…¨ï¼‰ï¼Œå¼€æºæ¨ç†æ¨¡å‹åœ¨éæ¨ç†åŸºå‡†ä¸Šæ™®éé€€æ­¥ï¼›å¦‚ç¬¬2é¡µå›¾1å±•ç¤ºäº†Qwen2.5-VLç³»åˆ—åœ¨A-OKVQAã€VisOnlyã€OCRBenchç­‰ä¸Šçš„ä¸‹é™ï¼Œå›¾2æ˜¾ç¤ºä»…æ•°å­¦å¼ºåŒ–è®­ç»ƒä½¿LISAæ„ŸçŸ¥ä»»åŠ¡è¿…é€ŸåŠ£åŒ–ã€‚<br>â€¢ é‡è¦æ€§ï¼šå®é™…ç³»ç»Ÿéœ€è¦åœ¨ä¸ç‰ºç‰²é¢„è®­ç»ƒè·å¾—çš„åŸºç¡€é€šç”¨æŠ€èƒ½å‰æä¸‹æå‡æ¨ç†ï¼Œå¦åˆ™ä¼šå¸¦æ¥å¹»è§‰å¢å¤šä¸è¶Šç‹±è„†å¼±æ€§ä¸Šå‡ï¼Œå½±å“å¯é æ€§ä¸å®‰å…¨æ€§ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šKLæ­£åˆ™é€šå¸¸åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šè®¡ç®—ï¼Œæ— æ³•ä¿è¯è·¨åŸŸçŸ¥è¯†ä¿ç•™ï¼›ä¸å°‘æ¨ç†RLç®¡çº¿ä¸ºå¢å¡‘æ€§è€Œå‡å¼±/ç§»é™¤KLï¼Œè¿›ä¸€æ­¥åŠ å‰§é—å¿˜ï¼›ç»éªŒå›æ”¾/å¤šåŸŸé™æ€æ··åˆéš¾ä»¥è®¾å®šå„ç›®æ ‡æƒé‡ï¼Œä¸åŒå¥–åŠ±æ”¶æ•›é€Ÿåº¦å·®å¼‚å¤§ï¼ˆç¬¬7é¡µå›¾4æ˜¾ç¤ºæ ¼å¼å¥–åŠ±æ˜“å¿«æ”¶æ•›å¹¶é¥±å’Œï¼‰ï¼Œæ‰‹å·¥è°ƒæƒæ—¢æ˜‚è´µåˆä¸å¯æ³›åŒ–ï¼ˆå¦‚MoDoMoDoéœ€è®­ç»ƒä»£ç†æ¨¡å‹å¹¶äº‹å…ˆè·çŸ¥ç›®æ ‡æ€§èƒ½ï¼Œä»ä¾èµ–æ‰‹è°ƒæƒé‡ï¼‰ã€‚<br>â€¢ è¯„æµ‹åå·®ï¼šä¸¥æ ¼æ ¼å¼å¥–åŠ±ä¸æ ¼å¼æ•æ„Ÿè¯„æµ‹æ˜“æŠŠâ€œä¼šæŒ‰æ¨¡æ¿ä½œç­”â€è¯¯åˆ¤ä¸ºâ€œä¼šæ¨ç†â€ï¼Œæ©ç›–çœŸå®èƒ½åŠ›é€€åŒ–ï¼›ä»…ç»“æœå‹å¥–åŠ±ç¨€ç–ä¸”ä¸ç¨³å®šï¼Œæ˜“å‡ºç°æ¢ç´¢/å¤šæ ·æ€§å¡Œç¼©ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºRECAPï¼ˆReplay-Enhanced Capability Preservationï¼‰ï¼šåœ¨RLVRè®­ç»ƒä¸­å›æ”¾é€šç”¨èƒ½åŠ›æ•°æ®ï¼Œå¹¶ä¾æ®æ¯ä¸ªç›®æ ‡ï¼ˆå¥–åŠ±/æŸå¤±ï¼‰çš„çŸ­æ—¶æ”¶æ•›ç‡ä¸ä¸ç¨³å®šåº¦ï¼Œåœ¨çº¿è®¡ç®—ä¼˜å…ˆçº§å¹¶é€šè¿‡æ¸©åº¦è½¯æœ€å¤§åŒ–å¾—åˆ°åŠ¨æ€æƒé‡ï¼Œå¯¹å¤šç›®æ ‡è”åˆæŸå¤±è¿›è¡Œè‡ªé€‚åº”é‡åŠ æƒã€‚è¯¥æ–¹æ³•ç«¯åˆ°ç«¯ã€é‡çº²æ— å…³ã€æ— éœ€é¢å¤–æ¨¡å‹ï¼Œèƒ½åœ¨æ ¼å¼ä¿¡å·é¥±å’Œåè‡ªåŠ¨æŠŠè®­ç»ƒé‡å¿ƒè½¬å‘æ¬ å­¦æˆ–é«˜æ–¹å·®ç›®æ ‡ï¼Œä»è€ŒåŒæ—¶ä¿ç•™é€šç”¨èƒ½åŠ›å¹¶æå‡æ¨ç†æ­£ç¡®æ€§ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ RECAP++ï¼šå°†åŠ¨æ€é‡åŠ æƒæ‰©å±•åˆ°åå¥½ä¸è¿‡ç¨‹ç›‘ç£çš„ç»Ÿä¸€åè®­ç»ƒæ¡†æ¶â€”â€”æŠŠDPO/ORPOç­‰åå¥½ä¼˜åŒ–ä¸PRMè¿‡ç¨‹å¥–åŠ±çº³å…¥åŒä¸€åœ¨çº¿æ”¶æ•›-ä¸ç¨³å®šåº¦è°ƒåº¦å™¨ï¼Œç³»ç»Ÿæ¯”è¾ƒä¸åŒç›‘ç£ç²’åº¦ä¸‹çš„èƒ½åŠ›ä¿ç•™ä¸æ¨ç†å¢ç›Šã€‚<br>â€¢ AutoReplayï¼šåŸºäºä¸ç¡®å®šæ€§ä¸è¡¨ç°é€€åŒ–ä¿¡å·çš„è·¨åŸŸè‡ªé€‚åº”å›æ”¾é‡‡æ ·â€”â€”è”åˆæ ·æœ¬éš¾åº¦/ä¸ç¡®å®šæ€§ä¸çŸ­çª—æ”¶æ•›è¯Šæ–­ï¼ŒåŠ¨æ€å†³å®šå„åŸŸé‡‡æ ·ç‡ä¸å¥–åŠ±é€€ç«ç­–ç•¥ï¼Œå‡å°‘äººå·¥é…æ¯”ä¸ä»£ç†æ¨¡å‹å¼€é”€ã€‚<br>â€¢ ThinkLessRLï¼šé¢å‘é«˜æ•ˆæ¨ç†çš„æ€ç»´é•¿åº¦è‡ªé€‚åº”æ§åˆ¶ä¸å¥–åŠ±æ•´å½¢â€”â€”æ£€æµ‹æ ¼å¼/æ€ç»´å¥–åŠ±é¥±å’Œåè‡ªé€‚åº”å‡å¼±é•¿é“¾å¥–åŠ±ï¼Œçº¦æŸå†—é•¿CoTä»¥é™è€—æé€Ÿï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡æœ€ç»ˆæ­£ç¡®ç‡ï¼ˆå‘¼åº”ç¬¬10é¡µæ€ç»´é•¿åº¦ç¼©çŸ­çš„å®è¯ï¼‰ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22037" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22037" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Scaling laws research has focused overwhelmingly on English -- yet the most prominent AI models explicitly serve billions of international users. In this work, we undertake the largest multilingual scaling laws study to date, totaling 774 multilingual training experiments, spanning 10M-8B model parameters, 400+ training languages and 48 evaluation languages. We introduce the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual pretraining, which outperforms existing scaling laws' out-of-sample generalization often by more than 0.3 R^2. Our analyses of the experiments shed light on multilingual learning dynamics, transfer properties between languages, and the curse of multilinguality. First, we derive a cross-lingual transfer matrix, empirically measuring mutual benefit scores between 38 x 38=1444 language pairs. Second, we derive a language-agnostic scaling law that reveals how to optimally scale model size and data when adding languages without sacrificing performance. Third, we identify the computational crossover points for when to pretrain from scratch versus finetune from multilingual checkpoints. We hope these findings provide the scientific foundation for democratizing scaling laws across languages, and enable practitioners to efficiently scale models -- beyond English-first AI.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ è‹±è¯­ä¸­å¿ƒçš„ç¼©æ”¾å¾‹åè§ï¼šç°æœ‰ç ”ç©¶å‡ ä¹åªåœ¨è‹±è¯­ä¸Šæ‹Ÿåˆä¸éªŒè¯ï¼Œä½†ä¸»æµæ¨¡å‹è¦æœåŠ¡æ•°ç™¾è¯­è¨€ï¼Œç¼ºä¹å¯é çš„å¤šè¯­ç§ç¼©æ”¾è§„å¾‹<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šChinchillaä¸è€ƒè™‘å¤šè½®æ•°æ®é‡å¤ï¼ŒDCSLéœ€è·¨ä¸€è½®å‰åå¤§é‡è§‚æµ‹ä¸”éš¾ä»¥ç»Ÿä¸€æ‹Ÿåˆï¼ŒMSLåªæŒ‰è¯­ç³»èšåˆæ— æ³•å»ºæ¨¡å…·ä½“è¯­è¨€é—´è½¬ç§»ä¸å¹²æ‰°<br>â€¢ å®è·µç—›ç‚¹æœªè§£ï¼šå¦‚ä½•é‡åŒ–è¯­è¨€ä¸¤ä¸¤ä¹‹é—´çš„æ­£è¿ç§»/å¹²æ‰°ã€å¦‚ä½•è®¾è®¡æœ€ä¼˜å¤šè¯­æ··åˆæ¯”ä¾‹ã€å¦‚ä½•åœ¨æ‰©å¤§è¦†ç›–è¯­è¨€æ—¶ä¿æŒå•è¯­æ€§èƒ½<br>â€¢ èƒ½åŠ›ä¸ç®—åŠ›æƒè¡¡ä¸æ¸…ï¼šå¤šè¯­è¯è¡¨/å¤šè¯­è®­ç»ƒå¸¦æ¥çš„â€œæ•ˆç‡ç¨â€ã€æ¨¡å‹å®¹é‡å¯¼è‡´çš„â€œå¤šè¯­è¯…å’’â€å¦‚ä½•éšNã€Dã€Kå˜åŒ–å¹¶ç»™å‡ºå¯æ“ä½œçš„æ‰©å±•æ³•åˆ™<br>â€¢ è®­ç»ƒè·¯å¾„é€‰æ‹©å›°éš¾ï¼šé¢å¯¹æ—¢æœ‰å¤šè¯­åŸºåº§ï¼Œä½•æ—¶åº”ä»é›¶é¢„è®­ç»ƒï¼Œä½•æ—¶åº”ä»å¤šè¯­æ£€æŸ¥ç‚¹ç»§ç»­å•è¯­é¢„è®­ç»ƒï¼ˆfinetuneï¼‰ç¼ºå°‘å¯æ³›åŒ–çš„å†³ç­–å‡†åˆ™</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºATLASç¼©æ”¾å¾‹ï¼šåœ¨æ ‡å‡†L(N,D)æ¡†æ¶ä¸­å¼•å…¥â€œé‡å¤æ„ŸçŸ¥â€çš„æœ‰æ•ˆæ•°æ®Deffï¼Œå°†ç›®æ ‡è¯­è¨€ã€è‹¥å¹²å¯è¿ç§»è¯­è¨€åŠå…¶å®ƒè¯­è¨€åˆ†é¡¹å»ºæ¨¡ï¼Œå¹¶ä»¥é¥±å’Œå‡½æ•°SÎ»å¤„ç†å¤šè½®æ•°æ®é‡å¤ï¼Œå­¦ä¹ è¯­è¨€é—´è½¬ç§»æƒé‡Ï„ä»¥è‡ªé€‚åº”è·¨è¯­ç§è¿ç§»ã€‚åŸºäºå¤§è§„æ¨¡å®è¯ï¼Œæ„å»ºè·¨è¯­è¿ç§»çŸ©é˜µï¼Œè¿›ä¸€æ­¥ç»™å‡ºåˆ»ç”»å¤šè¯­è¯…å’’çš„Kâ€“Nâ€“Dç¼©æ”¾å¾‹ä¸Kâ†’rKæ‰©è¯­çš„ç­‰æŸå¤±ä¸ç®—åŠ›æœ€ä¼˜é—­å¼è§£ï¼ŒåŒæ—¶æä¾›é¢„è®­ç»ƒvså¾®è°ƒçš„è®¡ç®—äº¤å‰ç‚¹å¯å‘å¼ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ATLAS-Plusï¼šé¢å‘å­è¯ä¸è¯è¡¨è‡ªé€‚åº”çš„ä¸€ä½“åŒ–å¤šè¯­ç¼©æ”¾å¾‹â€”â€”å°†è¯è¡¨å¤§å°/è„šæœ¬å…±äº«çº³å…¥Deffä¸Ï„å­¦ä¹ ï¼Œç»Ÿä¸€å»ºæ¨¡è¯è¡¨é€‰æ‹©å¯¹è·¨è¯­è¿ç§»çš„å½±å“<br>â€¢ AutoMix-ATLASï¼šåŸºäºè·¨è¯­è½¬ç§»çŸ©é˜µçš„è‡ªé€‚åº”æ•°æ®æ··åˆä¼˜åŒ–â€”â€”ç”¨BTS/FASå…ˆéªŒä¸ATLASæ‹Ÿåˆï¼Œåœ¨çº¿å†³ç­–å¤šè¯­é‡‡æ ·ç‡<br>â€¢ Capacity-Aware MoE for Multilingualityï¼šé¢å‘å¤šè¯­è¯…å’’çš„å®¹é‡åˆ†é…ä¸ä¸“å®¶è·¯ç”±â€”â€”åœ¨ATLASç­‰æŸçº¦æŸä¸‹å­¦ä¹ ä¸“å®¶æ•°ä¸è·¯ç”±ç­–ç•¥<br>â€¢ ATLAS for Low-Resource Onboardingï¼šä½èµ„æºè¯­è¨€å¢é‡æ¥å…¥çš„æœ€å°ç®—åŠ›æ‰©å±•æ³•åˆ™â€”â€”Kâ†’rKé—­å¼è§£æŒ‡å¯¼ä¸»åŠ¨æ•°æ®æ”¶é›†ä¸è®­ç»ƒæ—¥ç¨‹<br>â€¢ From Language to Multimodalityï¼šå°†ATLASæ‰©å±•åˆ°å¤šæ¨¡æ€é¢„è®­ç»ƒçš„è¿ç§»ä¸å®¹é‡ç¼©æ”¾å¾‹â€”â€”ç»Ÿä¸€æ–‡æœ¬ä¸è§†è§‰/è¯­éŸ³æºçš„Deffä¸Ï„å‚æ•°åŒ–<br>â€¢ Pretrain-or-Finetune 2.0ï¼šè·¨åŸºåº§ä¸æ··åˆå·®å¼‚çš„è®­ç»ƒè·¯å¾„é€‰æ‹©å®šå¾‹â€”â€”æ¨å¹¿äº¤å‰ç‚¹æ¨¡å‹ï¼Œçº³å…¥ä¸åŒåŸºåº§è®­ç»ƒæ—¶é•¿ä¸æ··åˆåˆ†å¸ƒçš„å½±å“</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24702" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24702" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an "interlingua" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç¢ç‰‡åŒ–ä¸å¼‚æ„æ ¼å¼é˜»ç¢å¤§è§„æ¨¡SFTï¼šå¤šæºä»£ç†æ•°æ®åˆ†æ•£åœ¨ä¸åŒæ ¼å¼ã€å·¥å…·ä¸æ¥å£ä¸­ï¼Œéš¾ä»¥ç›´æ¥åˆå¹¶ä¸å¤ç”¨ï¼Œä¼ ç»Ÿåšæ³•éœ€ä¸ºæ¯ä¸ªæ•°æ®é›†Ã—æ¯ä¸ªä»£ç†æ¡†æ¶å®šåˆ¶è½¬æ¢ï¼Œå·¥ç¨‹æˆæœ¬å‘ˆO(DÃ—A)ï¼ˆè§å›¾2ï¼Œç¬¬6é¡µï¼‰ã€‚<br>â€¢ æ•°æ®æ•´ç†å¤æ‚ä¸”æˆæœ¬é«˜ï¼šäººå·¥ã€åˆæˆä¸rolloutä¸‰ç±»é‡‡é›†æ–¹å¼å„æœ‰ä¸è¶³ï¼Œè´¨é‡éªŒè¯å›°éš¾ï¼Œéš¾ä»¥åœ¨è§„æ¨¡ã€è´¨é‡ä¸å¤šæ ·æ€§é—´å–å¾—å¹³è¡¡ï¼ˆÂ§2.2ï¼‰ã€‚<br>â€¢ è·¨æ•°æ®é›†åˆ†æä¸å¯¹æ¯”å›°éš¾ï¼šç»“æ„ä¸è¡¨ç¤ºå·®å¼‚å¤§ï¼Œéš¾ä»¥ç³»ç»Ÿè¯„ä¼°è¦†ç›–åº¦ä¸è´¨é‡ï¼Œé™åˆ¶äº†æ•°æ®é©±åŠ¨çš„é€‰æ‹©ä¸æ”¹è¿›ï¼ˆÂ§2.2ï¼‰ã€‚<br>â€¢ ç¼ºä¹é€šç”¨æ ‡å‡†å¯¼è‡´å…¬å¼€å¤§è§„æ¨¡ä»£ç†SFTç¨€ç¼ºï¼šç“¶é¢ˆä¸åœ¨äºæ•°æ®ç¼ºä¹è€Œåœ¨äºç¼ºæ ‡å‡†ï¼Œç°æœ‰ç»Ÿä¸€å°è¯•å¤šä¸ºä»»åŠ¡/ç³»ç»Ÿç‰¹å®šï¼Œéš¾ä»¥ç¤¾åŒºçº§å¤ç”¨ï¼ˆæ‘˜è¦ã€å¼•è¨€ï¼‰ã€‚<br>â€¢ éœ€è¦ä¿ƒè¿›è·¨ä»»åŠ¡è¿ç§»ä¸æ³›åŒ–ï¼šå•åŸŸå¾®è°ƒæ˜“äº§ç”Ÿè´Ÿè¿ç§»ï¼›ç»Ÿä¸€è¯­æ–™å¯æå‡è·¨åŸŸæ³›åŒ–ï¼ˆè§è¡¨6ï¼Œç¬¬10é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºAgent Data Protocolï¼ˆADPï¼‰ï¼Œä»¥ç®€æ´è€Œå…·è¡¨è¾¾åŠ›çš„Pydanticæ¨¡å¼å°†ä»£ç†è½¨è¿¹ç»Ÿä¸€ä¸ºActionâ€“Observationåºåˆ—ï¼šActionæ¶µç›–API/Code/Messageï¼ŒObservationæ¶µç›–Text/Webï¼Œå¹¶é…å¥—è‡ªåŠ¨éªŒè¯ä¸è´¨æ£€ã€‚é€šè¿‡ä¸¤æ®µå¼Rawâ†’ADPâ†’SFTæ¢çº½è¾å°„å¼ç®¡çº¿ï¼Œå°†å¤šå¯¹å¤šè½¬æ¢é™ä¸ºO(D+A)ï¼ˆè§å›¾1ç¬¬2é¡µã€å›¾2ç¬¬6é¡µï¼‰ï¼Œæ”¯æŒ13ä¸ªæ•°æ®é›†ä¸å¤šä»£ç†æ¡†æ¶æ— ç¼è®­ç»ƒè½¬æ¢ï¼ˆè§è¡¨2ä¸è¡¨3â€“5ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ADP-Mï¼šé¢å‘å¤šæ¨¡æ€çš„ä»£ç†æ•°æ®åè®®æ‰©å±•â€”â€”å°†ADPæ‰©å±•è‡³å›¾åƒ/è§†é¢‘/å±å¹•å½•åˆ¶ä¸GUIè§†è§‰è¯­ä¹‰ï¼Œç»Ÿä¸€å¤šæ¨¡æ€Actionâ€“Observationå¹¶éªŒè¯åœ¨Web/UI/æ¡Œé¢ä»»åŠ¡ä¸­çš„æ”¶ç›Šã€‚<br>â€¢ ADP-Evalï¼šè¯„æµ‹ä¸ç¯å¢ƒå·¥ä»¶çš„åè®®åŒ–æ ‡å‡†â€”â€”å°†ç¯å¢ƒçŠ¶æ€ã€é‡æ”¾è„šæœ¬ã€å¿«ç…§ä¸æ ‡æ³¨ç»Ÿä¸€ä¸ºå¯ç»„åˆçš„è¯„æµ‹åè®®ï¼Œæ‰“é€šâ€œæ•°æ®â€”è®­ç»ƒâ€”è¯„æµ‹â€çš„å¯å¤ç°æµæ°´çº¿ã€‚<br>â€¢ AutoADPï¼šå¼‚æ„æ•°æ®åˆ°ADPçš„è‡ªåŠ¨è½¬æ¢ä¸è´¨é‡æ§åˆ¶â€”â€”åˆ©ç”¨LLMä¸ç¨‹åºåˆæˆè‡ªåŠ¨å®ŒæˆRawâ†’ADPæ˜ å°„ï¼Œå†…ç½®ä¸€è‡´æ€§æ£€æŸ¥ã€æ¨ç†è¦†ç›–ç‡ä¸é”™è¯¯æ£€æµ‹ï¼Œé™ä½è½¬æ¢æˆæœ¬å¹¶æå‡æ•°æ®è´¨é‡ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20661" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20661" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable progress. However, two key challenges remain : 1) the absence of a large-scale high-quality UHR T2I dataset, and (2) the neglect of tailored training strategies for fine-grained detail synthesis in UHR scenarios. To tackle the first challenge, we introduce UltraHR-100K, a high-quality dataset of 100K UHR images with rich captions, offering diverse content and strong visual fidelity. Each image exceeds 3K resolution and is rigorously curated based on detail richness, content complexity, and aesthetic quality. To tackle the second challenge, we propose a frequency-aware post-training method that enhances fine-detail generation in T2I diffusion models. Specifically, we design (i) Detail-Oriented Timestep Sampling (DOTS) to focus learning on detail-critical denoising steps, and (ii) Soft-Weighting Frequency Regularization (SWFR), which leverages Discrete Fourier Transform (DFT) to softly constrain frequency components, encouraging high-frequency detail preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks demonstrate that our approach significantly improves the fine-grained detail quality and overall fidelity of UHR image generation. The code is available at https://github.com/NJU-PCALab/UltraHR-100k{here}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç¼ºä¹å¼€æºã€è§„æ¨¡è¶³å¤Ÿä¸”è´¨é‡å¯æ§çš„UHRæ–‡æœ¬-å›¾åƒæ•°æ®é›†ï¼Œç°æœ‰æ¨¡å‹å¤šåœ¨â‰¤1024è®­ç»ƒï¼Œç›´æ¥æ‰©å±•åˆ°4Kä¼šå‡ºç°ç»†èŠ‚ç¼ºå¤±ä¸ç»“æ„ä¼ªå½±ï¼Œéš¾ä»¥æ»¡è¶³çœŸå®åº”ç”¨å¯¹é«˜é¢‘ç»†èŠ‚ä¸é«˜ä¿çœŸéœ€æ±‚ï¼ˆè§ç¬¬1é¡µæ‘˜è¦ä¸ç¬¬2é¡µå¼•è¨€ï¼›è¡¨2ç¬¬4é¡µå¯¹æ¯”ï¼‰ã€‚<br>â€¢ è®­ç»ƒè‡ªç”±æ–¹æ³•é€šè¿‡ç½‘ç»œ/æ¨ç†æ”¹åŠ¨å®ç°UHRï¼Œä½†æ™®éè¿‡åº¦å¹³æ»‘ã€ç»†èŠ‚ä¸å¯ä¿¡ä¸”æ¨ç†è€—æ—¶é•¿ï¼Œä¸”ä¾èµ–æœªè§è¿‡UHRæ•°æ®çš„åŸºåº§æ¨¡å‹ï¼Œå…ˆå¤©èƒ½åŠ›ä¸è¶³ï¼ˆç¬¬2é¡µå¼•è¨€ï¼›ç›¸å…³å·¥ä½œ2.2ï¼Œç¬¬4é¡µï¼‰ã€‚<br>â€¢ è®­ç»ƒå‹æ–¹æ³•ä¾§é‡æ•ˆç‡è€Œå¿½è§†ç»†ç²’åº¦ç»†èŠ‚ï¼Œå…¬å¼€æ•°æ®å¦‚Aesthetic-4Kè§„æ¨¡å°ä¸”ç­›é€‰æ ‡å‡†ä¸ä¸¥ï¼Œé™åˆ¶æ³›åŒ–ä¸é«˜è´¨é‡åˆæˆï¼ˆç¬¬2é¡µå¯¹Aesthetic-4Kçš„è®¨è®ºï¼›å›¾3ç¬¬3é¡µã€å›¾4ç¬¬5é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºUltraHR-100Kï¼ˆ10ä¸‡+å¼ ã€æœ€å°3Kã€åŸºäºç»†èŠ‚ä¸°å¯Œåº¦/å†…å®¹å¤æ‚åº¦/ç¾å­¦ä¸‰ç»´åº¦ä¸¥é€‰å¹¶é…Gemini 2.0é•¿æè¿°ï¼›å‚è§è¡¨1ç¬¬3é¡µã€å›¾3-4ï¼‰ä¸é¢‘ç‡æ„ŸçŸ¥åè®­ç»ƒFAPTï¼šä»¥DOTSç”¨Betaåˆ†å¸ƒåå‘åæœŸå»å™ªæ—¶é—´æ­¥å¼ºåŒ–ç»†èŠ‚ï¼ˆå›¾5ç¬¬5é¡µï¼‰ï¼Œå¹¶ç”¨åŸºäºDFTçš„SWFRå¯¹é¢‘è°±æ–½åŠ è½¯æƒé‡çº¦æŸä»¥ä¿ç•™é«˜é¢‘ç»†èŠ‚ï¼ˆç¬¬6é¡µï¼‰ï¼Œä»è€Œæ˜¾è‘—æå‡4Kç”Ÿæˆçš„ç»†èŠ‚ä¸ä¿çœŸã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è‡ªé€‚åº”é¢‘ç‡è¯¾ç¨‹å­¦ä¹ çš„8Kæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼šå°†DOTSä¸SWFRæ‰©å±•ä¸ºåŠ¨æ€é¢‘å¸¦/æ—¶é—´æ­¥æƒé‡è°ƒåº¦ï¼Œå®ç°ç«¯åˆ°ç«¯8Kç”Ÿæˆå¹¶é™ä½è®­ç»ƒä¸æ¨ç†æˆæœ¬ã€‚<br>â€¢ äººæœºååŒçš„UHRè¯­ä¹‰-ç¾å­¦å¯¹é½ä¸åå¥½ä¼˜åŒ–ï¼šåœ¨UltraHR-100Kä¸Šå¼•å…¥äººç±»åå¥½ä¸VLMæ ¡å¯¹å½¢æˆé«˜è´¨é‡å¤šç²’åº¦æ ‡æ³¨ï¼Œç»“åˆRLHF/DPOä¼˜åŒ–ç»†èŠ‚ä¸å®¡ç¾ã€‚<br>â€¢ è·¨åŸŸé¢‘ç‡è‡ªé€‚åº”UHRç”Ÿæˆä¸è¯„æµ‹ï¼šé¢å‘åŒ»å­¦ã€é¥æ„Ÿã€å·¥ä¸šè®¾è®¡ç­‰é¢†åŸŸï¼Œæå‡ºé¢‘ç‡è‡ªé€‚åº”å¾®è°ƒä¸UltraHR-eval4K++åŸºå‡†ï¼Œç³»ç»Ÿè¯„ä¼°è·¨åŸŸç»†èŠ‚è¿ç§»ä¸é²æ£’æ€§ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.17439" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.17439" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ VLAsåœ¨3Dç‰©ç†ä¸–ç•Œä¸­æ‰§è¡Œå´å¤šåŸºäº2Dç¼–ç å™¨ï¼Œå­˜åœ¨æ˜¾è‘—çš„ç©ºé—´æ¨ç†ç¼ºå£ï¼Œå¯¼è‡´å¯¹æ–°åœºæ™¯/èƒŒæ™¯/å°ºåº¦/é«˜åº¦å˜åŒ–çš„æ³›åŒ–ä¸é€‚åº”æ€§ä¸è¶³ï¼ˆç¬¬1â€“3é¡µï¼‰<br>â€¢ æ˜¾å¼3Dé›†æˆéœ€ä¸“ç”¨ä¼ æ„Ÿå™¨ï¼ˆç‚¹äº‘/æ·±åº¦ï¼‰ï¼Œè·å–éš¾ã€æˆæœ¬é«˜ï¼Œä¸ç¼ºä¹3Dæ ‡æ³¨çš„å¤§è§„æ¨¡æ•°æ®ä¸åŒ¹é…ï¼Œä¸”åœ¨RGB-onlyã€RGB-Dã€ä½å§¿ç­‰ä¸åŒæ¨¡æ€é—´å¯è¿ç§»æ€§å·®ï¼ˆç¬¬2é¡µå›¾1ä¸ç›¸å…³å·¥ä½œ2.1ï¼‰<br>â€¢ éšå¼3Dçº¿ç´¢ï¼ˆä¼ªæ·±åº¦ã€å¯å­¦ä¹ ç©ºé—´åµŒå…¥ï¼‰ç©ºé—´è¡¨å¾å¼±ï¼Œéš¾ä»¥æ•è·å¯é å‡ ä½•å…ˆéªŒï¼Œä¸”å°†ç©ºé—´åµŒå…¥æ‹¼æ¥è¿›VLMæ˜“ç ´åè§†è§‰-è¯­è¨€å¯¹é½ï¼Œå¯¼è‡´åµŒå…¥æ¼‚ç§»ä¸é›¶æ ·æœ¬æ¨ç†é€€åŒ–ï¼ˆç¬¬2â€“3é¡µï¼‰<br>â€¢ è¿«åˆ‡éœ€è¦ä¸€ç§æ—¢èƒ½ä»RGBè·å¾—å¼ºå‡ ä½•å…ˆéªŒã€åœ¨å¯ç”¨æ—¶æ— ç¼å¸æ”¶æ·±åº¦/ä½å§¿ï¼Œåˆä¸ç ´åè¯­è¨€æ¨ç†ä¸å¯¹é½çš„ç©ºé—´ä¿¡æ¯æ³¨å…¥èŒƒå¼ï¼ˆæ‘˜è¦ä¸ç¬¬3é¡µï¼‰</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºFALCONï¼šä»¥ç©ºé—´åŸºç¡€æ¨¡å‹ä»RGBæå–ç¨ å¯†ç©ºé—´tokenï¼Œå¹¶é€šè¿‡Embodied Spatial Modelå¯é€‰èåˆæ·±åº¦ä¸ä½å§¿ï¼›å°†ç©ºé—´tokenåœ¨Spatial-Enhanced Action Headå†…ä¸VLMäº§ç”Ÿçš„è¯­ä¹‰åŠ¨ä½œtokenè¿›è¡Œè½»é‡èåˆï¼Œä»åŠ¨ä½œå¤´è€ŒéVLMä¸»å¹²æ³¨å…¥ç©ºé—´å…ˆéªŒï¼Œå…¼é¡¾3Dæ¨ç†ã€è·¨æ¨¡æ€å¯è¿ç§»æ€§ä¸è¯­ä¹‰å¯¹é½ï¼ˆç¬¬1â€“5é¡µï¼Œå›¾2ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ç©ºé—´-è¯­ä¹‰è‡ªé€‚åº”èåˆçš„åŠ¨ä½œå¤´ï¼šå¼•å…¥é—¨æ§/åŠ¨æ€è·¯ç”±ä¸å¯¹æ¯”-é‡å»ºè”åˆçº¦æŸï¼Œè‡ªåŠ¨å¹³è¡¡ç©ºé—´å…ˆéªŒä¸è¯­è¨€è¯­ä¹‰ï¼Œæå‡è·¨ä»»åŠ¡ä¸é•¿æ—¶åºæ³›åŒ–<br>â€¢ å¤§è§„æ¨¡æ— 3Dæ ‡æ³¨çš„ç©ºé—´å…ˆéªŒè’¸é¦ï¼šä»ç©ºé—´åŸºç¡€æ¨¡å‹ä¸åˆæˆå¤šè§†è§’æ•°æ®è’¸é¦å‡ ä½•å…ˆéªŒåˆ°VLAï¼Œè¦†ç›–å¤šæœºå™¨äºº/å¤šç›¸æœºï¼Œå¼ºåŒ–é›¶æ ·æœ¬è·¨åŸŸèƒ½åŠ›<br>â€¢ ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„ESMä¸ä¸»åŠ¨è§†è§’é€‰æ‹©ï¼šåœ¨ESMä¸­å»ºæ¨¡æ·±åº¦/ä½å§¿ä¸ç¡®å®šæ€§ä¸é®æŒ¡ï¼Œå¹¶è”åŠ¨ç›¸æœºä½å§¿/è§†è§’è§„åˆ’ï¼Œå®ç°é£é™©æ•æ„Ÿçš„ç¨³å¥æ“çºµ</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24081" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24081" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>To date, there exist almost no culturally-specific evaluation benchmarks for large language models (LLMs) that cover a large number of languages and cultures. In this paper, we present Global PIQA, a participatory commonsense reasoning benchmark for over 100 languages, constructed by hand by 335 researchers from 65 countries around the world. The 116 language varieties in Global PIQA cover five continents, 14 language families, and 23 writing systems. In the non-parallel split of Global PIQA, over 50% of examples reference local foods, customs, traditions, or other culturally-specific elements. We find that state-of-the-art LLMs perform well on Global PIQA in aggregate, but they exhibit weaker performance in lower-resource languages (up to a 37% accuracy gap, despite random chance at 50%). Open models generally perform worse than proprietary models. Global PIQA highlights that in many languages and cultures, everyday knowledge remains an area for improvement, alongside more widely-discussed capabilities such as complex reasoning and expert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA provides a glimpse into the wide diversity of cultures in which human language is embedded.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šç°æœ‰å¤šè¯­è¨€è¯„æµ‹å¤šç”±è‹±æ–‡æ•°æ®ç¿»è¯‘è€Œæ¥ï¼Œç¼ºä¹è¦†ç›–æœ¬åœ°é£Ÿç‰©ã€ä¹ ä¿—ã€æ—¥å¸¸ç‰©ä»¶ä¸è§„èŒƒçš„æ–‡åŒ–ç‰¹å®šåŸºå‡†ï¼Œå¯¼è‡´æ¨¡å‹åœ¨çœŸå®æœ¬åœ°è¯­å¢ƒä¸­çš„å¸¸è¯†èƒ½åŠ›ä¸å¯è§ã€‚<br>â€¢ é‡è¦æ€§ï¼šç‰©ç†å¸¸è¯†ä¸æ—¥å¸¸çŸ¥è¯†å¼ºä¾èµ–æ–‡åŒ–ä¸ç¯å¢ƒï¼ˆä¸åŒäºæ•°å­¦/é€»è¾‘çš„è·¨æ–‡åŒ–ç»Ÿä¸€æ€§ï¼‰ï¼›ç¼ºå°‘æ–‡åŒ–ç‰¹å®šè¯„æµ‹ä¼šå‰Šå¼±å…¨çƒå¯ç”¨æ€§ä¸å…¬å¹³æ€§ï¼Œå¹¶æ©ç›–ä½èµ„æºè¯­è¨€çš„å¤§å¹…æ€§èƒ½è½å·®ï¼ˆå¦‚è¥¿æ¬§ vs. æ’’å“ˆæ‹‰ä»¥å—éæ´²æœ€é«˜æ¨¡å‹è¾¾95.6% vs. 80.2%ï¼Œæœºä¼šæ°´å¹³50%ï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šç¿»è¯‘åŸºå‡†æ˜“å¼•å…¥è‹±æ–‡åŒ–åè§ä¸ç¿»è¯‘ä¼ªå½±ã€é¢˜ç›®ä¸è‡ªç„¶ï¼›å·²æœ‰æ–‡åŒ–åŸºå‡†è¯­è¨€è¦†ç›–æœ‰é™æˆ–åé‡å­¦ç§‘çŸ¥è¯†è€Œéæ—¥å¸¸å¸¸è¯†ï¼›å¼€æºä¸é—­æºæ¨¡å‹åœ¨å¤šè¯­è¨€æ–‡åŒ–å¸¸è¯†ä¸Šå·®è·æœªè¢«ç³»ç»Ÿé‡åŒ–ï¼ˆæœ€ä½³é—­æºä¸æœ€å¼ºå¼€æºå¹³å‡å­˜åœ¨~9%ç²¾åº¦å·®ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºGlobal PIQAï¼šä»¥å‚ä¸å¼æ–¹å¼ç”±335ä½ç ”ç©¶è€…æ‰‹å·¥ç¼–å†™116ç§è¯­è¨€çš„éå¹³è¡ŒPIQAé¢˜ï¼ˆæ¯è¯­ç§100é¢˜ï¼Œ59.9%ä¸ºæ–‡åŒ–ç‰¹å®šï¼‰ï¼Œç»ç»Ÿä¸€æ¸…æ´—ã€è‹±è¯‘å®¡æŸ¥ã€æ–‡åŒ–æ€§æ ‡æ³¨ä¸å»é‡é‡‡æ ·å½¢æˆå®˜æ–¹åˆ†å‰²ï¼›å¹¶ç»™å‡ºcompletionä¸promptedä¸¤ç§è¯„æµ‹åè®®ï¼Œå¯¹å¼€æº/é—­æºæ¨¡å‹è¿›è¡Œè·¨è¯­è¨€å¯¹æ¯”ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Global PIQA-Parallelï¼šç”¨å¹³è¡Œæ•°æ®åˆ†ç¦»â€œè¯­è¨€èƒ½åŠ›â€ä¸â€œæ–‡åŒ–çŸ¥è¯†â€å¯¹æ¨¡å‹è¡¨ç°çš„ç›¸å¯¹è´¡çŒ®<br>â€¢ ç¼©å°ä½èµ„æºè¯­è¨€å¸¸è¯†å·®è·ï¼šå‚ä¸å¼å°æ ·æœ¬æ‰©å±•ä¸é«˜è´¨é‡åˆæˆè¿‡æ»¤çš„æŒ‡ä»¤è°ƒä¼˜æ¡†æ¶<br>â€¢ å¼€æºæ¨¡å‹çš„æ–‡åŒ–å¸¸è¯†å¯¹é½ï¼šè·¨è¯­è¨€LoRA/Adapterä¸åŒºåŸŸè¯æ±‡å¢å¼ºçš„ç³»ç»Ÿè¯„ä¼°ä¸æ–¹æ³•</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Latent Chain-of-Thought for Visual Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23925" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23925" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Chain-of-thought (CoT) reasoning is critical for improving the interpretability and reliability of Large Vision-Language Models (LVLMs). However, existing training algorithms such as SFT, PPO, and GRPO may not generalize well across unseen reasoning tasks and heavily rely on a biased reward model. To address this challenge, we reformulate reasoning in LVLMs as posterior inference and propose a scalable training algorithm based on amortized variational inference. By leveraging diversity-seeking reinforcement learning algorithms, we introduce a novel sparse reward function for token-level learning signals that encourage diverse, high-likelihood latent CoT, overcoming deterministic sampling limitations and avoiding reward hacking. Additionally, we implement a Bayesian inference-scaling strategy that replaces costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank optimal rationales and answers. We empirically demonstrate that the proposed method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in terms of effectiveness, generalization, and interpretability.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°æœ‰LVLMçš„CoTè®­ç»ƒï¼ˆSFT/PPO/GRPOï¼‰åœ¨è·¨ä»»åŠ¡æ³›åŒ–ä¸è¶³ï¼šSFTä¾èµ–æ•™å¸ˆå¼ºåˆ¶ä»…å¤è¿°å‚è€ƒè½¨è¿¹ï¼ŒPPO/GRPOä¾èµ–åç½®çš„å¥–åŠ±/è¯„è®ºå™¨ï¼Œéš¾ä»¥å»ºæ¨¡å®Œæ•´çš„æ¨ç†å¤šæ ·æ€§ä¸ä¸ç¡®å®šæ€§ã€‚<br>â€¢ å¼ºKLçº¦æŸä¸å‰ªåˆ‡ç›®æ ‡æŠ‘åˆ¶æ¢ç´¢ï¼Œå¯¼è‡´æ¨¡å¼å¡Œç¼©ï¼Œéš¾ä»¥å‘ç°æ–°é¢–ä¸”é«˜è´¨é‡çš„éšå¼æ¨ç†é“¾ï¼ŒåŒæ—¶å­˜åœ¨â€œå¥–åŠ±é»‘å®¢â€å€¾å‘ã€‚<br>â€¢ è§†è§‰æ¨ç†é“¾å¾ˆé•¿ï¼ˆ~1k tokensï¼‰ï¼Œé€tokenå¥–åŠ±éš¾ä»¥é«˜æ•ˆè·å–ï¼Œç°æœ‰AVI/GFlowNetåœ¨å¤šæ¨¡æ€é•¿åºåˆ—ä¸Šçš„æ•ˆç‡ä¸ç¨³å®šæ€§å—é™ã€‚<br>â€¢ æ¨ç†æ—¶çš„BoN/Beam Searchæˆæœ¬é«˜ã€ä¾èµ–å¤–éƒ¨åˆ¤åˆ«å™¨ä¸”æ˜“å—åç½®ï¼Œç¼ºä¹æ¦‚ç‡å­¦ä¸Šçš„æœ€ä¼˜æ€§ä¸å¯è§£é‡Šæ€§ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>å°†è§†è§‰é“¾å¼æ€ç»´è§†ä¸ºæ½œå˜é‡åéªŒæ¨æ–­ï¼šç”¨GFlowNetå®ç°æ‘Šé”€å˜åˆ†æ¨æ–­ï¼Œè®­ç»ƒç†ç”±é‡‡æ ·å™¨qÎ¸(Z|X)ä½¿è½¨è¿¹æ¦‚ç‡ä¸è”åˆä¼¼ç„¶æˆæ¯”ä¾‹ï¼›æå‡ºå‚è€ƒå¼•å¯¼çš„GFlowNetå¾®è°ƒï¼ˆRGFNï¼‰+ ç¨€ç–-æ’å€¼çš„tokençº§å¥–åŠ±è¿‘ä¼¼ä»¥æå‡æ¢ç´¢ä¸ç¨³å®šæ€§ï¼Œå¹¶åœ¨æ¨ç†ç«¯ç”¨è´å¶æ–¯BiNå¯¹Næ¡æ½œåœ¨ç†ç”±è¿›è¡Œé•¿åº¦å½’ä¸€åŒ–è¾¹é™…åŒ–é€‰æ‹©ç­”æ¡ˆï¼Œæ— éœ€å¤–éƒ¨åˆ¤åˆ«å™¨ä¸BoNã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ å±‚çº§åŒ–æ½œå˜é‡è§†è§‰CoTï¼šä»è§†è§‰é”šç‚¹åˆ°æ­¥éª¤è®¡åˆ’çš„åéªŒåˆ†è§£ï¼šå°†Zåˆ†è§£ä¸ºè§†è§‰å®šä½/æ­¥éª¤è®¡åˆ’/å…¬å¼ä¸‰å±‚æ½œå˜é‡ï¼Œæå‡å¯è§£é‡Šæ€§ä¸è·¨ä»»åŠ¡è¿ç§»ã€‚<br>â€¢ è”åˆè®­ç»ƒçš„LaCoTï¼šç†ç”±é‡‡æ ·å™¨ä¸å›ç­”å™¨çš„è¾¹é™…ä¼¼ç„¶æœ€å¤§åŒ–ï¼šç«¯åˆ°ç«¯æœ€å¤§åŒ–P(Y|X)å¹¶å…±äº«è¡¨ç¤ºï¼Œç¼“è§£å¥–åŠ±åç½®ä¸å¹»è§‰ï¼Œç¨³å®šé‡‡æ ·ä¸ä½œç­”ååŒã€‚<br>â€¢ ç½®ä¿¡è‡ªé€‚åº”çš„è´å¶æ–¯æ¨ç†æ—¶æ‰©å±•ï¼šåŠ¨æ€å€™é€‰ä¸é‡è¦é‡‡æ ·ï¼šåœ¨BiNä¸­åŠ å…¥é‡è¦/åˆ†å±‚é‡‡æ ·ä¸ä¸ç¡®å®šæ€§æ ¡å‡†ï¼ŒæŒ‰ç½®ä¿¡åº¦åŠ¨æ€è°ƒæ•´Nä¸æ¸©åº¦ï¼Œè¿›ä¸€æ­¥é™æœ¬å¢æ•ˆã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">SPICE: Self-Play In Corpus Environments Improves Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24684" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24684" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Self-improving systems require environmental interaction for continuous adaptation. We introduce SPICE (Self-Play In Corpus Environments), a reinforcement learning framework where a single model acts in two roles: a Challenger that mines documents from a large corpus to generate diverse reasoning tasks, and a Reasoner that solves them. Through adversarial dynamics, the Challenger creates an automatic curriculum at the frontier of the Reasoner's capability, while corpus grounding provides the rich, near-inexhaustible external signal necessary for sustained improvement. Unlike existing ungrounded self-play methods that offer more limited benefits, SPICE achieves consistent gains across mathematical (+8.9%) and general reasoning (+9.8%) benchmarks on multiple model families. Our analysis reveals how document grounding is a key ingredient in SPICE to continuously generate its own increasingly challenging goals and achieve them, enabling sustained self-improvement.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ çº¯è‡ªåšå¼ˆè¯­è¨€æ¨¡å‹ç¼ºä¹å¤–éƒ¨å¯éªŒè¯ä¿¡å·ï¼Œæ˜“å‡ºç°æ€§èƒ½å¹³å°åŒ–æˆ–è®­ç»ƒå´©å¡Œçš„é—®é¢˜<br>â€¢ ä¸æ¥åœ°è‡ªåšå¼ˆä¼šæ”¾å¤§å¹»è§‰ï¼šæ¨¡å‹åœ¨è‡ªç”Ÿé—®é¢˜ä¸ç­”æ¡ˆä¸Šè®­ç»ƒï¼Œé”™è¯¯ä¼šè¢«å¾ªç¯æ”¾å¤§<br>â€¢ å‡ºé¢˜è€…ä¸è§£é¢˜è€…ä¿¡æ¯å¯¹ç§°ï¼Œå¯¼è‡´ç¼ºä¹çœŸæ­£æŒ‘æˆ˜ä¸è‡ªåŠ¨è¯¾ç¨‹ï¼Œé—®é¢˜é€æ¸ç®€å•ä¸”æ¨¡å¼åŒ–<br>â€¢ ç°æœ‰RLVRå¤šä¾èµ–äººå·¥ç­–åˆ’é¢˜åº“ä¸é¢†åŸŸä¸“ç”¨éªŒè¯å™¨ï¼ˆæ•°å­¦/ä»£ç ï¼‰ï¼Œéš¾ä»¥è¦†ç›–é€šç”¨æ¨ç†ï¼Œå—éªŒè¯ç“¶é¢ˆåˆ¶çº¦<br>â€¢ éœ€è¦ä¸€ç§åˆ©ç”¨å¤§è§„æ¨¡æ–‡æ¡£è¯­æ–™ä½œä¸ºâ€œå¤–éƒ¨ç¯å¢ƒâ€ï¼ŒæŒç»­æä¾›å¤šæ ·ä¸”å¯éªŒè¯çš„æŒ‘æˆ˜ä»¥å®ç°è‡ªæˆ‘æå‡</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>SPICEæå‡ºè¯­æ–™ç¯å¢ƒä¸­çš„è‡ªåšå¼ˆRLï¼šåŒä¸€æ¨¡å‹åœ¨å‡ºé¢˜è€…ä¸è§£é¢˜è€…ä¸¤ç§è§’è‰²é—´åˆ‡æ¢ï¼Œå‡ºé¢˜è€…ä»å¤§è§„æ¨¡æ–‡æ¡£ç”Ÿæˆå¸¦è¯­æ–™æ”¯æ’‘çš„MCQæˆ–è‡ªç”±æ ¼å¼é¢˜ç›®ä¸é‡‘ç­”æ¡ˆï¼Œè§£é¢˜è€…åœ¨æ— æ–‡æ¡£æ¡ä»¶ä¸‹ä½œç­”ä»¥å½¢æˆä¿¡æ¯ä¸å¯¹ç§°ã€‚ä»¥æ–¹å·®é©±åŠ¨çš„éš¾åº¦å¥–åŠ±ä¸ºå‡ºé¢˜è€…æ„å»ºè‡ªåŠ¨è¯¾ç¨‹ã€ä»¥äºŒå…ƒæ­£ç¡®æ€§å¥–åŠ±è®­ç»ƒè§£é¢˜è€…ï¼Œé‡‡ç”¨æŒ‰è§’è‰²åˆ†å¼€çš„DrGRPOä¼˜åŠ¿ä¼°è®¡ä¸é€šç”¨è§„åˆ™éªŒè¯å™¨å®ç°æ— äººå·¥æ ‡æ³¨çš„æŒç»­è‡ªæ”¹è¿›ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ SPICE-Mï¼šå¤šæ¨¡æ€è¯­æ–™è‡ªåšå¼ˆæå‡è·¨æ¨¡æ€æ¨ç†â€”â€”å°†å›¾åƒ/è¡¨æ ¼/è§†é¢‘ç­‰çº³å…¥è¯­æ–™ç¯å¢ƒå¹¶è®¾è®¡ç»Ÿä¸€éªŒè¯å™¨<br>â€¢ Retrieval-SPICEï¼šæ£€ç´¢-å‡ºé¢˜è”åˆä¼˜åŒ–çš„è‡ªé€‚åº”è¯­æ–™æŒ–æ˜â€”â€”ç«¯åˆ°ç«¯è”è®­æ£€ç´¢å™¨ä¸å‡ºé¢˜è€…ï¼ŒåŠ¨æ€å¯¹é½éš¾åº¦å‰æ²¿<br>â€¢ VerifierGymï¼šé€šç”¨å¯æ‰§è¡ŒéªŒè¯å™¨é©±åŠ¨çš„è¯­æ–™è‡ªåšå¼ˆâ€”â€”å¼•å…¥æ‰§è¡Œå™¨ä¸å¼±ç¬¦å·å·¥å…·ï¼Œè¦†ç›–ç‰©ç†ã€å› æœä¸çŸ¥è¯†æŸ¥è¯¢<br>â€¢ Tri-Player SPICEï¼šå‡ºé¢˜-å®¡é¢˜-è§£é¢˜çš„å¤šä»£ç†å¯¹æŠ—ååŒâ€”â€”åŠ å…¥å®¡é¢˜è£åˆ¤ä»£ç†åšè´¨é‡æ§åˆ¶ä¸é²æ£’æ€§å¯¹æŠ—<br>â€¢ Online-SPICEï¼šé¢å‘åŠ¨æ€ç½‘é¡µçš„åœ¨çº¿æŒç»­å­¦ä¹ â€”â€”å¤„ç†è¯­æ–™æµå¼æ›´æ–°ä¸‹çš„ç¨³å®š-å¯å¡‘å¹³è¡¡ä¸ç¾éš¾æ€§é—å¿˜<br>â€¢ Safety-SPICEï¼šè¯­æ–™æ¥åœ°è‡ªåšå¼ˆä¸­çš„å®‰å…¨ä¸åå·®æ§åˆ¶â€”â€”æ„å»ºå®‰å…¨å¥–åŠ±ä¸è¿‡æ»¤ä»¥æŠ‘åˆ¶æœ‰å®³å†…å®¹ä¸æ•°æ®æ±¡æŸ“</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22768" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22768" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As Large Vision-Language Models (LVLMs) are increasingly deployed in domains such as shopping, health, and news, they are exposed to pervasive persuasive content. A critical question is how these models function as persuadees-how and why they can be influenced by persuasive multimodal inputs. Understanding both their susceptibility to persuasion and the effectiveness of different persuasive strategies is crucial, as overly persuadable models may adopt misleading beliefs, override user preferences, or generate unethical or unsafe outputs when exposed to manipulative messages. We introduce MMPersuade, a unified framework for systematically studying multimodal persuasion dynamics in LVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs images and videos with established persuasion principles across commercial, subjective and behavioral, and adversarial contexts, and (ii) an evaluation framework that quantifies both persuasion effectiveness and model susceptibility via third-party agreement scoring and self-estimated token probabilities on conversation histories. Our study of six leading LVLMs as persuadees yields three key insights: (i) multimodal inputs substantially increase persuasion effectiveness-and model susceptibility-compared to text alone, especially in misinformation scenarios; (ii) stated prior preferences decrease susceptibility, yet multimodal information maintains its persuasive advantage; and (iii) different strategies vary in effectiveness across contexts, with reciprocity being most potent in commercial and subjective contexts, and credibility and logic prevailing in adversarial contexts. By jointly analyzing persuasion effectiveness and susceptibility, MMPersuade provides a principled foundation for developing models that are robust, preference-consistent, and ethically aligned when engaging with persuasive multimodal content.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç¼ºå°‘ç³»ç»Ÿæ€§æ¡†æ¶ç ”ç©¶LVLMä½œä¸ºâ€œè¢«è¯´æœè€…â€çš„å¤šæ¨¡æ€åŠæœæ˜“æ„Ÿæ€§ï¼›ç°æœ‰ç ”ç©¶å¤šä¸ºæ–‡æœ¬å•æ¨¡æ€ã€å•è½®æˆ–çŸ­å¯¹è¯ï¼Œéš¾ä»¥åæ˜ çœŸå®å¤šæ¨¡æ€ä¼ æ’­æ ¼å±€ã€‚<br>â€¢ å¤šæ¨¡æ€åŠæœåœ¨è´­ç‰©ã€å¥åº·ã€æ–°é—»ç­‰åœºæ™¯æ™®éå­˜åœ¨ï¼Œè¿‡åº¦æ˜“æ„Ÿä¼šå¯¼è‡´é‡‡çº³é”™è¯¯ä¿¡å¿µã€è¦†ç›–ç”¨æˆ·åå¥½å¹¶è§¦å‘ä¸å®‰å…¨è¾“å‡ºï¼Œå°¤å…¶åœ¨é”™è¯¯ä¿¡æ¯ä¸å¯¹æŠ—æ€§åœºæ™¯ä¸‹é£é™©æ›´å¤§ã€‚<br>â€¢ ç¼ºå°‘å¤§è§„æ¨¡ã€ç†è®ºæ”¯æ’‘ä¸”å¯æ§å˜é‡ï¼ˆç­–ç•¥/åœºæ™¯/æ¨¡æ€ï¼‰çš„å¤šæ¨¡æ€æ•°æ®é›†å’ŒåŸºå‡†ï¼Œéš¾ä»¥æ¯”è¾ƒä¸åŒåŠæœç­–ç•¥ä¸æ¨¡æ€çš„æ•ˆåŠ›å·®å¼‚ã€‚<br>â€¢ è¯„ä¼°æ–¹æ³•ä¸æŒ‡æ ‡ä¸è¶³ï¼šå¤šä¾èµ–ç²—ç²’åº¦çš„æœ€ç»ˆåŒæ„åº¦æˆ–äºŒå…ƒæˆè´¥ï¼Œå¿½è§†â€œéšå¼ä¿¡å¿µå˜åŒ–â€ä¸â€œæ˜¾æ€§å£å¤´åŒæ„â€çš„åŒºåˆ†ï¼Œä¹Ÿæ— æ³•åŒæ—¶åˆ»ç”»åŠæœçš„æ—¶æœºä¸å¼ºåº¦ï¼›å¯¹å…ˆéªŒåå¥½ï¼ˆé¡½å›ºåº¦ï¼‰ä¸ç³»ç»Ÿæç¤ºè¯ç­‰å› ç´ çš„å½±å“ç¼ºä¹ç³»ç»Ÿæ£€éªŒã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºMMPERSUADEç»Ÿä¸€æ¡†æ¶ï¼šåœ¨å•†ä¸šã€ä¸»è§‚/è¡Œä¸ºä¸å¯¹æŠ—æ€§åœºæ™¯ä¸­ï¼Œå°†æ–‡æœ¬å¯¹è¯æ‰©å±•ä¸ºé…å¥—å›¾åƒ/è§†é¢‘çš„å¤šè½®å¤šæ¨¡æ€æ•°æ®é›†ï¼ˆç­–ç•¥æ˜ å°„åˆ°Cialdiniå…­åŸåˆ™ä¸Aristotleä¸‰è¯‰æ±‚ï¼‰ï¼Œå¹¶ä»¥é™æ€åŠæœè€…åœ¨ä¸‰ç§æ¨¡æ€è®¾å®šï¼ˆçº¯æ–‡æœ¬/æ–‡æœ¬+å›¾åƒæè¿°/çœŸå¤šæ¨¡æ€ï¼‰ä¸‹ä¸LVLMå¯¹è¯ã€‚ç»“åˆç¬¬ä¸‰æ–¹åŒæ„è¯„åˆ†ä¸è‡ªä¼°tokenæ¦‚ç‡ä¸¤æ¡è¯„ä¼°çº¿ï¼Œå¹¶ä»¥PDCGç»Ÿä¸€é‡åŒ–â€œè¶Šæ—©è¶Šå¼ºâ€çš„åŠæœæ•ˆæœï¼ŒåŒæ—¶æ§åˆ¶åå¥½ï¼ˆé¡½å›ºåº¦ï¼‰ä¸ç­–ç•¥å˜é‡è¿›è¡Œå¯¹æ¯”åˆ†æã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ åŠ¨æ€åšå¼ˆå¼å¤šæ¨¡æ€åŠæœä¸é˜²å¾¡ï¼šå¼•å…¥è‡ªé€‚åº”åŠæœè€…ä¸ååŠæœåŠ©æ‰‹çš„å¯¹æŠ—è®­ç»ƒä¸è¯„æµ‹ï¼Œåˆ»ç”»ä¸æå‡LVLMåœ¨çœŸå®äº¤äº’ä¸­çš„é²æ£’æ€§ã€‚<br>â€¢ é¢å‘åå¥½ä¸€è‡´æ€§çš„å¤šæ¨¡æ€å¯¹é½ï¼šå°†ç”¨æˆ·åå¥½ä¸å®‰å…¨çº¦æŸæ³¨å…¥éšå¼/æ˜¾æ€§å±‚ï¼ˆtokenæ¦‚ç‡ä¸å£å¤´åŒæ„ï¼‰ï¼Œé€šè¿‡æ ¡å‡†ä¸è®­ç»ƒç¡®ä¿æ¨¡å‹åœ¨å¼ºåŠæœä¸‹ä¿æŒåå¥½ä¸€è‡´ã€‚<br>â€¢ åˆ©ç”¨éšå¼ä¿¡å¿µä¿¡å·çš„å¤šæ¨¡æ€è¯¯å¯¼æ£€æµ‹ä¸æ ¡å‡†ï¼šä»¥tokenæ¦‚ç‡æ¼‚ç§»ä¸PDCGä¸ºæ—©æœŸä¿¡å·ï¼Œè”åˆæ³•è¯å¼å¯è§†è¯æ®åˆ†æä¸åäº‹å®è§£é‡Šï¼Œåœ¨çº¿è¯†åˆ«å¹¶æŠ‘åˆ¶é”™è¯¯ä¿¡æ¯åŠæœã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24645" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24645" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Function calling (FC) empowers large language models (LLMs) and autonomous agents to interface with external tools, a critical capability for solving complex, real-world problems. As this ability becomes increasingly central to advanced AI systems, the need for high-quality, multi-turn training data to develop and refine it cannot be overstated. Existing data synthesis methods, such as random environment sampling or multi-agent role-playing, are not powerful enough to generate high-quality data in real-world environments. Practical challenges come in three folds: targeted model training, isolation of tool architecture, and multi-turn logical dependency. To address these structural deficiencies, we present FunReason-MT, a novel data synthesis framework for real-world multi-turn tool use. FunReason-MT resolves the complexity barrier in multi-turn FC data by employing 1) Environment-API Graph Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query Synthesis to simplify hard query construction, and 3) Guided Iterative Chain for sophisticated CoT generation. Evaluations on Berkeley Function-Calling Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built upon FunReason-MT generated data achieves state-of-the-art performance among comparable-sized models, outperforming most close-source models. Further performance improvements on BFCLv4 confirm that FunReason-MT provides a reliable and robust source for agentic learning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šç¼ºä¹å¯æ§ã€å¯ä¿¡ä¸”é«˜å¤æ‚åº¦çš„å¤šè½®å‡½æ•°è°ƒç”¨è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹åœ¨çœŸå®å¤šå·¥å…·åä½œä¸é•¿é“¾æ¨ç†ä¸Šå‡ºç°æ€§èƒ½â€œå¤©èŠ±æ¿â€ã€‚<br>â€¢ é‡è¦æ€§ï¼šå‡½æ•°è°ƒç”¨æ˜¯LLMå¯¹æ¥å¤–éƒ¨å·¥å…·ä¸æ„å»ºè‡ªä¸»ä»£ç†çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œé«˜è´¨é‡å¤šè½®è½¨è¿¹æ˜¯è§£é”å¤æ‚çœŸå®ä»»åŠ¡çš„åŸºç¡€ã€‚<br>â€¢ ç°æœ‰å±€é™1ï¼šéšæœºç¯å¢ƒé‡‡æ ·éš¾ä»¥è¦†ç›–é•¿å°¾ä¸ç¨€æœ‰å¤æ‚äº‹ä»¶ï¼Œæ— æ³•å®šå‘è®­ç»ƒå¯¹ç›®æ ‡å¤æ‚å·¥å…·çš„ååŒæŒæ¡ã€‚<br>â€¢ ç°æœ‰å±€é™2ï¼šå¤šæ™ºèƒ½ä½“è§’è‰²æ‰®æ¼”å€¾å‘â€œå¹¸ç¦è·¯å¾„â€ï¼Œç¼ºä¹å¤šæ ·æ€§ä¸å›°éš¾åº¦ï¼Œéš¾ä»¥ç”Ÿæˆå¸¦é€»è¾‘è·³è·ƒçš„éš¾æŸ¥è¯¢ã€‚<br>â€¢ ç°æœ‰å±€é™3ï¼šå·¥å…·ä½“ç³»æ¨¡å—åŒ–ä¸”ç›¸äº’éš”ç¦»ï¼Œç›´æ¥é‡‡æ ·éš¾ä»¥æ•´åˆå‰ç½®ä¾èµ–ï¼›RLLMåœ¨æœªçŸ¥ç¯å¢ƒä¸‹ç”ŸæˆCoTæ˜“ä¸å®Œæ•´/ä¸ä¸€è‡´ï¼Œå¯¼è‡´å¤šè½®è½¨è¿¹ä¸ç¨³å®šã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºFunReason-MTä¸‰é˜¶æ®µæ¡†æ¶ï¼šå…ˆåœ¨å·¥å…·-ç¯å¢ƒAPIå…³ç³»å›¾ä¸Šåšä¾èµ–åˆæ³•ä¸”æŒ‡å‘ç›®æ ‡å·¥å…·çš„å®šå‘é‡‡æ ·ï¼Œå¾—åˆ°å¯æ‰§è¡Œå¤šæ­¥è½¨è¿¹ï¼›å†å°†å¤šæ­¥è½¨è¿¹æŠ½è±¡ä¸ºâ€œé«˜çº§å·¥å…·â€ï¼Œåå‘åˆæˆå«é€»è¾‘è·³è·ƒçš„ç¡¬æŸ¥è¯¢ï¼›æœ€åä»¥â€œæ¨ç†-æ ¡éªŒ-æ‰¹åˆ¤-é‡è¯•â€çš„å¼•å¯¼å¼è¿­ä»£é“¾æå‡CoTä¸€è‡´æ€§ä¸å‡½æ•°è°ƒç”¨æ­£ç¡®æ€§ï¼Œå¹¶æ‰©å±•è‡³å¤šè½®å¯¹è¯ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ é¢å‘å¼€æ”¾ä¸–ç•Œçš„APIå›¾æ•°æ®ç”Ÿæˆï¼šæ‰©å±•Environment-APIå›¾åˆ°å¤§è§„æ¨¡åŠ¨æ€å·¥å…·ç”Ÿæ€ï¼Œç ”ç©¶è·¨åŸŸä¾èµ–å»ºæ¨¡ä¸åœ¨çº¿æ‹“å±•ç­–ç•¥<br>â€¢ åŸºäºæ‰¹åˆ¤ä¿¡å·çš„å¤šè½®å‡½æ•°è°ƒç”¨å¼ºåŒ–å­¦ä¹ ï¼šå°†Guided Iterative Chainä¸­çš„è¯¯å·®åé¦ˆè½¬åŒ–ä¸ºå¯å­¦ä¹ ä¿¡å·ï¼Œå¼€å±•ç«¯åˆ°ç«¯çš„RLä¼˜åŒ–<br>â€¢ é«˜çº§å·¥å…·è‡ªåŠ¨æŠ½è±¡ä¸å¯è¿ç§»ç»„åˆï¼šå­¦ä¹ è·¨ä»»åŠ¡çš„å·¥å…·å¤åˆä¸æŠ½è±¡æœºåˆ¶ï¼Œå®ç°éš¾æŸ¥è¯¢è‡ªåŠ¨åˆæˆä¸è·¨é¢†åŸŸè¿ç§»</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24591" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24591" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Frontier AI agents show increasing promise as scientific research assistants, and may eventually be useful for extended, open-ended research workflows. However, in order to use agents for novel research, we must first assess the underlying faithfulness and correctness of their work. To evaluate agents as research assistants, we introduce ReplicationBench, an evaluation framework that tests whether agents can replicate entire research papers drawn from the astrophysics literature. Astrophysics, where research relies heavily on archival data and computational study while requiring little real-world experimentation, is a particularly useful testbed for AI agents in scientific research. We split each paper into tasks which require agents to replicate the paper's core contributions, including the experimental setup, derivations, data analysis, and codebase. Each task is co-developed with the original paper authors and targets a key scientific result, enabling objective evaluation of both faithfulness (adherence to original methods) and correctness (technical accuracy of results). ReplicationBench is extremely challenging for current frontier language models: even the best-performing language models score under 20%. We analyze ReplicationBench trajectories in collaboration with domain experts and find a rich, diverse set of failure modes for agents in scientific research. ReplicationBench establishes the first benchmark of paper-scale, expert-validated astrophysics research tasks, reveals insights about agent performance generalizable to other domains of data-driven science, and provides a scalable framework for measuring AI agents' reliability in scientific research.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç¼ºå°‘â€œè®ºæ–‡çº§ã€ç«¯åˆ°ç«¯â€çš„ç§‘ç ”ä»£ç†è¯„æµ‹ï¼šç°æœ‰MMLU/GPQA/HLEç­‰ä»…æµ‹é™æ€çŸ¥è¯†ï¼ŒSWE-Bench/PlanBenchç­‰ä¸è¦æ±‚é¢†åŸŸçŸ¥è¯†ï¼Œéš¾ä»¥è¡¡é‡çœŸå®ç§‘ç ”å¯é æ€§ä¸é•¿æ—¶ç¨‹æ‰§è¡Œã€‚<br>â€¢ éœ€è¦åŒæ—¶è¡¡é‡â€œå¿ å®åº¦ä¸æ­£ç¡®æ€§â€ï¼šåœ¨çœŸå®è®ºæ–‡è¯­å¢ƒä¸­æ£€éªŒä»£ç†æ˜¯å¦éµå¾ªåŸæ–¹æ³•å¹¶å¾—åˆ°æ•°å€¼ä¸€è‡´ç»“æœï¼Œä»¥é™ä½â€œçœ‹èµ·æ¥ä¼šåšã€å®é™…ä¸ä¼šåšâ€çš„é£é™©ã€‚<br>â€¢ ç°æœ‰è¯„æµ‹éš¾ä»¥è§„æ¨¡åŒ–ä¸”æ˜“å—ä½œå¼Š/æ±¡æŸ“ï¼šä¾èµ–äººå·¥rubricæˆ–åŸä»£ç ä»“åº“ï¼Œä¸”è®ºæ–‡æ³„æ¼ç­”æ¡ˆå¯¼è‡´æŠ„å½•ä¸è®°å¿†æ±¡æŸ“ï¼›ç¼ºä¹ç³»ç»Ÿæ€§çš„æ‰‹ç¨¿æ©è”½ä¸æ— è®¡ç®—åŸºçº¿å¯¹ç…§ã€‚<br>â€¢ å¤©ä½“ç‰©ç†æ˜¯ç†æƒ³è¯•éªŒåœºå´ç¼ºåŸºå‡†ï¼šè¯¥é¢†åŸŸé«˜åº¦å¯å¤ç°ã€å…¨æµç¨‹å¯è®¡ç®—ï¼Œä½†ç¼ºè¦†ç›–å¤šè®ºæ–‡ã€å¤šä»»åŠ¡çš„æ ‡å‡†åŒ–å¤ç°è¯„æµ‹ï¼›å®è¯æ˜¾ç¤ºå½“å‰å‰æ²¿æ¨¡å‹åœ¨è¯¥åŸºå‡†ä¸Šå¹³å‡å¾—åˆ†ä½ï¼ˆè¡¨2ï¼Œæœ€ä½³â‰ˆ19%ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºReplicationBenchï¼šé€‰å–19ç¯‡ç»åŒè¡Œè¯„å®¡ä¸”å¯å¤ç°çš„å¤©ä½“ç‰©ç†è®ºæ–‡ï¼Œä½œè€…å…±å»º107ä¸ªå®¢è§‚å¯æ‰“åˆ†ä»»åŠ¡ï¼ˆæ‰©å±•é›†å†å¢11ç¯‡/58ä»»åŠ¡ï¼‰ï¼Œæä¾›æ©è”½æ‰‹ç¨¿ã€æ•°æ®ä¸æ‰§è¡Œå…ƒæ•°æ®ï¼Œåœ¨å—æ§å®¹å™¨ä¸­è®©ä»£ç†ä»é›¶å¤ç°å¹¶æäº¤æ•°å€¼ç»“æœï¼›ä»¥ä½œè€…å®¹å·®è¿›è¡Œè‡ªåŠ¨åŒ–ç»ˆå€¼è¯„åˆ†ï¼Œå¹¶è¾…ä»¥ä¸“å®¶è¯„å®¡ä¸LLMè£åˆ¤æ ‡æ³¨å¤±è´¥æ¨¡å¼å’Œæ§åˆ¶è®°å¿†æ±¡æŸ“ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ä»å¤ç°åˆ°å‘ç°ï¼šé¢å‘å¼€æ”¾å¼å¤©ä½“ç‰©ç†ç ”ç©¶çš„ä»£ç†è¯„æµ‹æ¡†æ¶æ‹“å±•ï¼šå°†å›ºå®šç­”æ¡ˆçš„å¤ç°ä»»åŠ¡æ‰©å±•åˆ°å‡è®¾ç”Ÿæˆã€å®éªŒè®¾è®¡ä¸è¿­ä»£åˆ†æï¼Œè¯„æµ‹ä»£ç†åœ¨åŸåˆ›ç ”ç©¶ä¸­çš„æœ‰æ•ˆæ€§ã€‚<br>â€¢ å·¥å…·ä¸è®°å¿†å¢å¼ºå¯¹è®ºæ–‡çº§å¤ç°çš„å› æœå½±å“ç ”ç©¶ï¼šç³»ç»Ÿæ¶ˆèæµè§ˆã€å¤šæ¨¡æ€è¾“å…¥ã€é•¿æœŸè®°å¿†ã€è§„åˆ’å™¨ä¸äººç±»ç›‘ç£ç­‰èƒ½åŠ›ï¼Œå¯¹å¤ç°æˆåŠŸç‡ä¸å¤±è´¥æ¨¡å¼çš„å½±å“ã€‚<br>â€¢ è·¨é¢†åŸŸReplicationBenchï¼šé¢å‘æ•°æ®é©±åŠ¨ç§‘å­¦çš„å¯æ‰©å±•ä»»åŠ¡ç”Ÿæˆä¸è‡ªåŠ¨è¯„åˆ†ï¼šå°†æ¡†æ¶è¿ç§»è‡³ç”Ÿç‰©ã€æ°”å€™ã€ææ–™ç­‰é¢†åŸŸï¼Œæ”¹è¿›äººæœºæ··åˆä»»åŠ¡ç”Ÿæˆä¸é˜²ä½œå¼Šã€å¯éªŒè¯è¯„åˆ†æœºåˆ¶ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Rethinking Visual Intelligence: Insights from Video Pretraining</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24448" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24448" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problem-solving. We investigate Video Diffusion Models (VDMs) as a promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design a controlled evaluation in which both a pretrained LLM and a pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šè§†è§‰é¢†åŸŸå°šæœªåƒè¯­è¨€é¢†åŸŸé‚£æ ·é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒè·å¾—å¼ºçš„ç»„åˆæ€§ç†è§£ã€æ ·æœ¬æ•ˆç‡ä¸é€šç”¨é—®é¢˜æ±‚è§£èƒ½åŠ›ï¼›ç°æœ‰æ¨¡å‹åœ¨æŠ½è±¡æ¨ç†ä¸ç»“æ„åŒ–è§†è§‰ç†è§£ä¸Šæ˜æ˜¾ä¸è¶³ï¼ˆè§ç¬¬1é¡µæ‘˜è¦ä¸ç¬¬1èŠ‚ï¼‰ã€‚<br>â€¢ å±€é™æ€§ï¼šå¤šæ•°å·¥ä½œå°†è§†è§‰ä»»åŠ¡è½¬ä¸ºæ–‡æœ¬æˆ–ä»…å…³æ³¨é«˜ä¿çœŸç”Ÿæˆï¼Œå¼±åŒ–ç©ºé—´ä¸æ—¶åºå…ˆéªŒï¼›ç¼ºå°‘åœ¨ç›¸åŒé€‚é…ç­–ç•¥ä¸‹ã€è·¨æ¨¡æ€å…¬å¹³å¯¹ç…§æ¥éš”ç¦»â€œé¢„è®­ç»ƒå…ˆéªŒâ€çš„è´¡çŒ®ï¼ˆç¬¬1â€“3é¡µï¼‰ã€‚<br>â€¢ é‡è¦æ€§ï¼šè¦è¿ˆå‘è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œéœ€è¦èƒ½å¿«é€Ÿä»æå°‘ç¤ºä¾‹ä¹ å¾—æ–°ä»»åŠ¡çš„æ¡†æ¶ï¼›è§†é¢‘é¢„è®­ç»ƒè•´å«çš„ç©ºé—´ç»“æ„ä¸æ—¶é—´åŠ¨æ€å…ˆéªŒæœ‰æœ›æå‡æ•°æ®æ•ˆç‡ä¸æ³›åŒ–èƒ½åŠ›ï¼Œæœ¬æ–‡æä¾›å—æ§éªŒè¯ï¼ˆç¬¬2é¡µå›¾1å±•ç¤ºåœ¨ConceptARCå¤šæ¦‚å¿µä¸Šçš„ä¼˜åŠ¿è½®å»“ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>å°†å›¾åƒåˆ°å›¾åƒä»»åŠ¡é‡æ„ä¸ºâ€œè¾“å…¥â†’è¾“å‡ºâ€çš„çŸ­è§†é¢‘è¿‡æ¸¡åºåˆ—ï¼Œç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰åœ¨ä»…å†»ç»“éª¨å¹²çš„å‰æä¸‹æ’å…¥LoRAè¿›è¡Œè½»é‡é€‚é…ï¼›å¹¶ä¸åŒæ ·ä»…æ’å…¥LoRAã€ä»¥JSONåˆ°JSONå½¢å¼è®­ç»ƒçš„LLMè¿›è¡Œå¯¹ç…§ï¼Œç»Ÿä¸€è¯„ä¼°å°‘æ ·æœ¬çš„æŠ€èƒ½è·å–æ•ˆç‡ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ åŸºäºè§†é¢‘é¢„è®­ç»ƒçš„è§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ ï¼šè®©VDMåœ¨æ—¶åºæç¤ºä¸­å®ç°ç±»LLMçš„in-contextå­¦ä¹ ï¼Œæå‡ç»„åˆæ€§ä¸å°‘æ ·æœ¬æ³›åŒ–<br>â€¢ è·¨æ¨¡æ€å…±äº«é€‚é…å™¨çš„è”åˆè®­ç»ƒï¼šæ„å»ºVDM+LLMå…±äº«LoRA/æç¤ºæ§½ä½çš„ååŒæ¡†æ¶ï¼Œå¼ºåŒ–æ–‡æœ¬-è§†é¢‘é—´çš„å¯è¿ç§»æ¨ç†<br>â€¢ ä»è¿‡æ¸¡è§†é¢‘åˆ°ä¸–ç•Œæ¨¡å‹ï¼šå°†VDMç”¨äºè§„åˆ’/æ§åˆ¶ï¼ˆå¦‚è·¯å¾„è§„åˆ’ã€å…ƒèƒè‡ªåŠ¨æœºï¼‰ï¼Œç ”ç©¶æ—¶ç©ºå…ˆéªŒå¯¹å†³ç­–ä¸é¢„æµ‹çš„ä¸€ä½“åŒ–å»ºæ¨¡</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Batch Speculative Decoding Done Right</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22876" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22876" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Speculative decoding speeds up LLM inference by using a small draft model to propose multiple tokens that a target model verifies in parallel. Extending this idea to batches is essential for production serving, but it introduces the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, breaking right-alignment and corrupting position IDs, attention masks, and KV-cache state. We show that several existing batch implementations violate output equivalence-the fundamental requirement that speculative decoding must produce identical token sequences to standard autoregressive generation. These violations occur precisely due to improper handling of the ragged tensor problem. In response, we (1) characterize the synchronization requirements that guarantee correctness, (2) present a correctness-first batch speculative decoding EQSPEC that exposes realignment as consuming 40% of overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences and dynamically forms same-length groups, to reduce the realignment overhead while preserving per-sequence speculative speedups. On the SpecBench dataset, across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our approach achieves up to 3times throughput improvement at batch size 8 compared to batch size 1, with efficient scaling through batch size 8, while maintaining 95% output equivalence. Our method requires no custom kernels and integrates cleanly with existing inference stacks. Our code is available at https://github.com/eBay/spec_dec.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ æ‰¹é‡æ¨æµ‹è§£ç çš„â€œå‚å·®å¼ é‡â€é—®é¢˜ï¼šåŒä¸€æ‰¹æ¬¡å†…å„åºåˆ—æ¥æ”¶çš„è‰ç¨¿tokenæ•°ä¸åŒï¼Œç ´åå³å¯¹é½å¹¶å¯¼è‡´ä½ç½®IDã€æ³¨æ„åŠ›maskä¸KV-cacheä¸åŒæ­¥ï¼Œä½¿GPUæ— æ³•ä»¥è§„åˆ™çŸ©é˜µæ‰¹å¤„ç†ï¼ˆå›¾2ï¼Œç¬¬3é¡µï¼‰ã€‚<br>â€¢ æ­£ç¡®æ€§è¦æ±‚çš„ç¼ºå¤±ï¼šæ¨æµ‹åŠ é€Ÿå¿…é¡»æ»¡è¶³ä¸æ ‡å‡†è‡ªå›å½’ç”Ÿæˆçš„â€œè¾“å‡ºç­‰ä»·æ€§â€ï¼Œä½†ç°æœ‰æ‰¹é‡å®ç°ï¼ˆå¦‚BSPã€DSDï¼‰åœ¨æ‰¹é‡>1æ—¶å‡ºç°é‡å¤è¯æˆ–<unk>ç­‰é”™è¯¯ï¼Œè¾“å‡ºè¢«ç ´åï¼ˆå›¾1ï¼Œç¬¬2é¡µï¼›è¡¨1ï¼Œç¬¬7é¡µï¼‰ã€‚<br>â€¢ æ—¢è¦â€œå¯¹â€åˆè¦â€œå¿«â€çš„çŸ›ç›¾ï¼šç»´æŒæ­£ç¡®æ€§çš„å¯¹é½ä¸åŒæ­¥å¼€é”€åœ¨æ‰¹é‡ä¸‹æ˜¾è‘—ä¸Šå‡ï¼ˆä½ç½®IDã€maské‡ç®—ä¸KV-cacheé‡æ’ï¼‰ï¼Œæˆä¸ºååç“¶é¢ˆï¼ˆå›¾5bæ˜¾ç¤ºå¯¹é½å¼€é”€æœ€é«˜æ¥è¿‘40%ï¼Œç¬¬7é¡µï¼›ç¬¬4â€“5é¡µç®—æ³•æè¿°ï¼‰ã€‚<br>â€¢ ç°æœ‰ä¸‰ç±»æ–¹æ¡ˆçš„å±€é™ï¼šæ©ç /éè¿ç»­ä½ç½®IDæ–¹æ¡ˆéœ€å®šåˆ¶CUDAä¸”æ˜“å¤±åºï¼ˆBSPï¼‰ï¼›å›é€€åˆ°æ‰¹å†…æœ€å°æ¥å—é•¿åº¦ä¼šä¸¢å¼ƒå·²éªŒè¯tokenå¯¼è‡´ååå¡Œç¼©ï¼›åŠ¨æ€å¡«å……è‹¥å®ç°ä¸å½“ä¼šå‡ºç°bonus tokenå–æ ·é”™è¯¯ï¼ˆä»è‰ç¨¿è€Œéç›®æ ‡ï¼‰ã€KV-cacheè†¨èƒ€ä¸å¤±å¯¹é½ï¼ˆDSDï¼‰ï¼ˆç¬¬3â€“4é¡µï¼‰ã€‚<br>â€¢ ç”Ÿäº§ç³»ç»Ÿè½åœ°å›°éš¾ï¼šè¿ç»­æ‰¹å¤„ç†+åˆ†é¡µæ³¨æ„åŠ›åœ¨æ¨æµ‹ä¸‹å½¢æˆâ€œåµŒå¥—å‚å·®â€ï¼Œä½¿vLLMä¸SGLangåœ¨å¤§å¹¶å‘ä¸‹å‡ºç°è´ŸåŠ é€Ÿï¼ˆå›¾6ï¼Œç¬¬8é¡µï¼‰ï¼ŒäºŸéœ€å…¼é¡¾æ­£ç¡®æ€§ä¸å¯æ‰©å±•æ€§çš„ç³»ç»Ÿè®¾è®¡ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>è®ºæ–‡æå‡ºEQSPECä¸EXSPECä¸¤é˜¶æ®µè·¯çº¿ï¼šEQSPECä»¥æ­£ç¡®æ€§ä¼˜å…ˆï¼Œé€šè¿‡â€œUnpadâ€“Appendâ€“Repadâ€æœ€å°åŒ–é‡å¯¹é½ï¼Œä¸¥æ ¼åŒæ­¥ä½ç½®IDã€æ³¨æ„åŠ›maskä¸KV-cacheï¼Œå¹¶åœ¨é¦–ä¸ªå¤±é…å¤„ç”±ç›®æ ‡æ¨¡å‹é‡‡æ ·bonus tokenï¼›EXSPECä»¥è·¨æ‰¹è°ƒåº¦å‡å°‘å¯¹é½å¼€é”€ï¼Œç»´æŠ¤æ»‘åŠ¨çª—å£å¹¶åŠ¨æ€ç»„æˆåŒé•¿åˆ†ç»„ï¼Œèƒ½ç›´æ¥æ‹¼æ¥å…é‡å¯¹é½ï¼Œå¤±è´¥æ—¶å†å›é€€EQSPECã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ SpecServeï¼šè·¨æ‰¹åŒé•¿åˆ†ç»„ä¸è¿ç»­æ‰¹å¤„ç†çš„ç»Ÿä¸€è°ƒåº¦æ¡†æ¶â€”â€”å°†EXSPECèå…¥ç”Ÿäº§çº§continuous batchingä¸paged attentionï¼ŒåŒ–è§£â€œåµŒå¥—å‚å·®â€ï¼Œæ¢å¤prefillâ€“decodeåˆ†ç¦»å¹¶æå‡å¤§å¹¶å‘å¯æ‰©å±•æ€§ã€‚<br>â€¢ KV-Poolerï¼šé¢å‘æ‰¹æ¨æµ‹çš„KV-cacheäº²å’Œå¸ƒå±€ä¸é›¶æ‹·è´é‡å¯¹é½â€”â€”è®¾è®¡æ–°çš„KVåˆ†é¡µã€ç´¢å¼•ä¸æ¬ç§»åŸè¯­ï¼Œé™ä½ckv(B)ä¸é‡æ’æˆæœ¬ï¼Œç¼“è§£EXSPECå¼•å…¥çš„å±€éƒ¨æ€§æŸå¤±ï¼ˆå‘¼åº”è¡¨2çš„æ—¶å»¶æƒè¡¡ï¼‰ã€‚<br>â€¢ LenBucket+ï¼šåŸºäºé•¿åº¦ä¸æ¥å—ç‡ç»Ÿè®¡çš„åŠ¨æ€åˆ†æ¡¶ä¸åœ¨çº¿æ’åºâ€”â€”ä»å›¾5(c)çš„åˆ†ç»„ç‡è¯æ®å‡ºå‘ï¼Œå»ºç«‹åˆ†ç»„ç‡-ååæ¨¡å‹ï¼Œåœ¨çº¿é€‰æ‹©çª—å£Wã€æ‰¹å¤§å°Bä¸è‰ç¨¿é•¿åº¦kï¼Œæœ€å¤§åŒ–åŒé•¿åˆ†ç»„ç‡å¹¶æœ€å°åŒ–coverhead(B)ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-11">

    <div class="paper">
        <h2 class="paper-title">SAO-Instruct: Free-form Audio Editing using Natural Language Instructions</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22795" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22795" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Generative models have made significant progress in synthesizing high-fidelity audio from short textual descriptions. However, editing existing audio using natural language has remained largely underexplored. Current approaches either require the complete description of the edited audio or are constrained to predefined edit instructions that lack flexibility. In this work, we introduce SAO-Instruct, a model based on Stable Audio Open capable of editing audio clips using any free-form natural language instruction. To train our model, we create a dataset of audio editing triplets (input audio, edit instruction, output audio) using Prompt-to-Prompt, DDPM inversion, and a manual editing pipeline. Although partially trained on synthetic data, our model generalizes well to real in-the-wild audio clips and unseen edit instructions. We demonstrate that SAO-Instruct achieves competitive performance on objective metrics and outperforms other audio editing approaches in a subjective listening study. To encourage future research, we release our code and model weights.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç¼ºå°‘æ”¯æŒâ€œè‡ªç”±æ–‡æœ¬æŒ‡ä»¤â€çš„é«˜ä¿çœŸéŸ³é¢‘ç¼–è¾‘èƒ½åŠ›ï¼šç°æœ‰ç”Ÿæˆæ¨¡å‹å¯¹ç®€çŸ­/å«ç³Šæç¤ºæ˜“åç¦»æ„å›¾ï¼Œéš¾ä»¥åšå®šå‘å¾®è°ƒä¸”ä¿ç•™æ•´ä½“è¯­å¢ƒï¼ˆè§æ–‡ä¸­å¯¹é€‰æ‹©æ€§ç¼–è¾‘ä¸èƒŒæ™¯ä¿æŒçš„è®¨è®ºï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šé›¶æ ·æœ¬åæ¼”éœ€å®Œæ•´ç›®æ ‡æè¿°ã€å¯¹æªè¾æ•æ„Ÿä¸”ä¸ç›´è§‚ï¼›æŒ‡ä»¤å¼æ–¹æ³•å¤šå—é™äºå›ºå®šä»»åŠ¡é›†åˆï¼ˆå¦‚ADD/REMOVEç­‰ï¼‰ï¼Œç¼ºä¹çµæ´»è‡ªç”±çš„ç¼–è¾‘è¡¨è¾¾ã€‚<br>â€¢ æŠ€æœ¯æŒ‘æˆ˜ï¼šéŸ³é¢‘å…·æœ‰é•¿æ—¶åºä¾èµ–ä¸é«˜ç»´åº¦ç‰¹æ€§ï¼Œç¼–è¾‘éœ€åœ¨å±€éƒ¨ä¿®æ”¹çš„åŒæ—¶ç»´æŒå…¨çƒä¸€è‡´æ€§ä¸è‡ªç„¶æ€§ï¼›ç”¨æˆ·æŒ‡ä»¤è·¨åº¦å¤§ä¸”å¸¸ä¸å®Œå…¨æŒ‡å®šï¼Œéš¾ä»¥ä¸å›ºå®šæ“ä½œä¸€ä¸€æ˜ å°„ã€‚<br>â€¢ æ•°æ®åŒ®ä¹ï¼šç¼ºå°‘â€œè¾“å…¥éŸ³é¢‘â€”ç¼–è¾‘æŒ‡ä»¤â€”ç›®æ ‡éŸ³é¢‘â€ä¸‰å…ƒç»„æ•°æ®ï¼Œé˜»ç¢ç›‘ç£å¼å­¦ä¹ è‡ªç”±æŒ‡ä»¤ç¼–è¾‘èƒ½åŠ›ã€‚<br>â€¢ å®åŠ¡éœ€æ±‚å¼ºï¼šçœŸå®å½•éŸ³å¸¸å«å™ªå£°/å¤±çœŸ/æ„å›¾é—®é¢˜ï¼Œäººå·¥é€æ­¥ç¼–è¾‘æˆæœ¬é«˜ï¼ŒæœŸæœ›ä»¥è‡ªç„¶è¯­è¨€å¿«é€Ÿè¿­ä»£ç²¾ä¿®ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºSAO-Instructï¼šåœ¨Stable Audio Openä¸Šè¿›è¡ŒæŒ‡ä»¤è·Ÿéšå¾®è°ƒï¼Œåˆ©ç”¨LLMç”Ÿæˆçš„ç¼–è¾‘æŒ‡ä»¤å’Œä¸‰å…ƒç»„æ•°æ®ï¼ˆPrompt-to-Promptå…¨åˆæˆã€DDPMåæ¼”åŠåˆæˆã€ç¡®å®šæ€§æ‰‹å·¥ç¼–è¾‘ï¼‰è¿›è¡Œè®­ç»ƒï¼›æ¨ç†æ—¶å°†è¾“å…¥éŸ³é¢‘ç¼–ç å¹¶åŠ å™ªä½œä¸ºåˆå§‹çŠ¶æ€ï¼Œç»“åˆæŒ‡ä»¤ä¸æ—¶é•¿æ¡ä»¶å»å™ªç”Ÿæˆä»¥å®ç°é€‰æ‹©æ€§ç¼–è¾‘å¹¶ä¿ç•™ä¸Šä¸‹æ–‡ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Multilingual SAO-Instruct: Toward Cross-Lingual Free-Form Audio Editingï¼šæ‰©å±•å¤šè¯­è¨€æŒ‡ä»¤ç†è§£ä¸ç¼–è¾‘ä¸€è‡´æ€§ï¼Œè¦†ç›–éè‹±è¯­åœºæ™¯ã€‚<br>â€¢ Chain-of-Edits for Complex Audio Workflowsï¼šæ”¯æŒå¤šæ­¥å¤åˆç¼–è¾‘ä¸å¯å›æº¯å†å²ï¼Œæå‡å¤æ‚åœºæ™¯ç¼–è¾‘çš„å¯ç¼–æ’æ€§ã€‚<br>â€¢ Human-in-the-Loop Bayesian Control for Edit Strengthï¼šåœ¨æ¨ç†ä¸­è‡ªé€‚åº”ä¼˜åŒ–CFG/å™ªå£°/æ³¨æ„åŠ›æ³¨å…¥ä»¥ç»†ç²’åº¦æ§åˆ¶ç¼–è¾‘å¹…åº¦ã€‚<br>â€¢ Scene-Aware Seamless Mixing for Added Elementsï¼šå­¦ä¹ å¼èƒ½é‡/ç©ºé—´/æ··å“åŒ¹é…ï¼Œä½¿æ–°å¢å£°éŸ³ä¸èƒŒæ™¯è‡ªç„¶èåˆã€å‡å°‘â€œè´´ç‰‡æ„Ÿâ€ã€‚<br>â€¢ Scaling Triplet Data via Weak Supervision and Self-Trainingï¼šåˆ©ç”¨å¼±æ ‡æ³¨ä¸è‡ªè®­ç»ƒæ‰©å……ä¸‰å…ƒç»„æ•°æ®è§„æ¨¡ä¸å¤šæ ·æ€§ï¼Œæå‡æ³›åŒ–ä¸é²æ£’æ€§ã€‚<br>â€¢ Beyond CLAP: Comprehensive Metrics for Instructional Audio Editingï¼šæ„å»ºè¶…è¶ŠCLAPçš„ç¼–è¾‘ç›¸å…³æ€§ä¸ä¿çœŸåº¦å®¢è§‚æŒ‡æ ‡ä¸ä¸»è§‚åè®®ã€‚<br>â€¢ Music-Specific Instruct Editing with Harmonic Constraintsï¼šé¢å‘éŸ³ä¹åŸŸå¼•å…¥å’Œå£°/èŠ‚å¥ä¸€è‡´æ€§çº¦æŸï¼Œæ”¯æŒç¼–æ›²çº§è‡ªç”±æŒ‡ä»¤ç¼–è¾‘ã€‚<br>â€¢ Real-time Instruct Audio Diffusionï¼šä¼˜åŒ–æ¨ç†åŠ é€Ÿä¸è’¸é¦ï¼Œå®ç°ä½æ—¶å»¶å®æ—¶æŒ‡ä»¤ç¼–è¾‘ä¸äº¤äº’å¼é¢„è§ˆã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22590" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22590" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In today's rapidly expanding data landscape, knowledge extraction from unstructured text is vital for real-time analytics, temporal inference, and dynamic memory frameworks. However, traditional static knowledge graph (KG) construction often overlooks the dynamic and time-sensitive nature of real-world data, limiting adaptability to continuous changes. Moreover, recent zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance on prebuilt ontologies often suffer from instability across multiple runs, as well as incomplete coverage of key facts. To address these challenges, we introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that builds and continuously updates Temporal Knowledge Graphs (TKGs) from unstructured texts. ATOM splits input documents into minimal, self-contained "atomic" facts, improving extraction exhaustivity and stability. Then, it constructs atomic TKGs from these facts while employing a dual-time modeling that distinguishes when information is observed from when it is valid. The resulting atomic TKGs are subsequently merged in parallel. Empirical evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17% better stability, and over 90% latency reduction compared to baseline methods, demonstrating a strong scalability potential for dynamic TKG construction.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°å®çŸ¥è¯†å…·æœ‰åŠ¨æ€ä¸æ—¶é—´æ•æ„Ÿç‰¹æ€§ï¼Œé™æ€KGéš¾ä»¥æ”¯æŒå˜åŒ–è¿½è¸ªä¸æ—¶åºæ¨ç†ï¼ŒäºŸéœ€èƒ½æŒç»­æ›´æ–°çš„æ—¶é—´çŸ¥è¯†å›¾ï¼ˆTKGï¼‰ã€‚<br>â€¢ ç°æœ‰é›¶/å°æ ·æœ¬LLMå»ºå›¾æ–¹æ³•åœ¨é•¿ä¸Šä¸‹æ–‡ä¸‹å­˜åœ¨â€œé—å¿˜æ•ˆåº”â€ï¼Œå¯¼è‡´äº‹å®è¦†ç›–ä¸å…¨ä¸ä¿¡æ¯é—æ¼ï¼Œä¸”è·¨æ¬¡è¿è¡Œç»“æœä¸ç¨³å®šï¼›è®ºæ–‡åœ¨å›¾2ï¼ˆç¬¬7é¡µï¼‰æ˜¾ç¤ºä¸Šä¸‹æ–‡å˜é•¿æ—¶äº‹å®ä¸æ—¶é—´æŠ½å–çš„RMATCHæ˜¾è‘—ä¸‹é™ã€‚<br>â€¢ å¤šæ•°æ–¹æ³•å¿½ç•¥æˆ–é”™è¯¯å¤„ç†æ—¶é—´ç»´ï¼Œå¸¸å°†â€œè§‚å¯Ÿæ—¶é—´â€å’Œâ€œäº‹å®æœ‰æ•ˆæœŸâ€æ··æ·†ï¼Œé€ æˆæ—¶é—´é”™é…ï¼›é™„å½•å›¾F.2â€“F.4ï¼ˆç¬¬14â€“16é¡µï¼‰å¯¹æ¯”æ˜¾ç¤ºåŸºçº¿å°†è§‚å¯Ÿæ—¶é—´å½“ä½œtstartå¼•å‘è¯¯å½’å› ã€‚<br>â€¢ ä¾èµ–LLMè¿›è¡Œå¢é‡å¼å®ä½“/å…³ç³»/æ—¶é—´æ¶ˆæ­§ä¸åˆå¹¶ä¼šå¼•å‘é«˜æˆæœ¬ã€ä¸Šä¸‹æ–‡çˆ†ç‚¸ä¸å¯æ‰©å±•æ€§å·®çš„é—®é¢˜ï¼›å›¾3ï¼ˆç¬¬8é¡µï¼‰è¡¨æ˜ATOMç›¸å¯¹åŸºçº¿æ–¹æ³•é™ä½äº†>90%æ„å›¾å»¶è¿Ÿã€‚<br>â€¢ ç°æœ‰æµç¨‹åœ¨å¤§è§„æ¨¡æµå¼æ–‡æœ¬åœºæ™¯ä¸‹ç¼ºä¹ç¨³å®šã€å¯å¹¶è¡Œçš„æ•´ä½“æ¶æ„ï¼Œéš¾ä»¥åŒæ—¶ä¿è¯æŠ½å–çš„å®Œå¤‡æ€§ã€ç¨³å®šæ€§ä¸ä½æ—¶å»¶ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>ATOMå…ˆå°†æ–‡æ¡£åˆ†è§£ä¸ºæœ€å°è‡ªå«çš„â€œåŸå­äº‹å®â€ï¼Œå¹¶è¡ŒæŠ½å–æ—¶é—´äº”å…ƒç»„(es, rp, eo, tstart, tend)ï¼Œé‡‡ç”¨åŒæ—¶é—´å»ºæ¨¡æ˜¾å¼åŒºåˆ†è§‚å¯Ÿæ—¶é—´ä¸äº‹å®æœ‰æ•ˆæœŸã€‚éšåä»¥åµŒå…¥+é˜ˆå€¼çš„æ— LLMå¹¶è¡Œåˆå¹¶å®Œæˆå®ä½“/å…³ç³»/æ—¶é—´å¯¹é½ï¼Œå¹¶å°†â€œç»ˆæ­¢åŠ¨ä½œâ€è§„èŒƒåŒ–ä¸ºä»…æ›´æ–°tendçš„è‚¯å®šå¼ï¼Œå®ç°å¯æ‰©å±•çš„DTKGæŒç»­æ›´æ–°ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ FineFactï¼šé¢å‘åŸå­äº‹å®åˆ†è§£çš„æŒ‡ä»¤å¾®è°ƒä¸å¯¹é½ï¼šé™ä½å¹»è§‰ä¸æ—¶é—´é—æ¼ï¼Œæå‡åˆ†è§£çš„ç¨³å®šæ€§ä¸ä¸€è‡´æ€§ã€‚<br>â€¢ Learn2Resolveï¼šç›‘ç£å¼å®ä½“/å…³ç³»æ¶ˆæ­§æ›¿ä»£é˜ˆå€¼åˆå¹¶ï¼šå¼•å…¥ç±»å‹æ„ŸçŸ¥ä¸å¯¹æ¯”å­¦ä¹ ï¼Œå‡å°‘é”™åˆå¹¶å¹¶æå‡æ‰©å±•æ€§ã€‚<br>â€¢ TimeBench-Dualï¼šåŒæ—¶é—´å»ºæ¨¡çš„æ—¶é—´è§£æåŸºå‡†ä¸æŒ‡æ ‡ï¼šé‡åŒ–è¯„æµ‹tobsä¸(tstart,tend)è§£æçš„å‡†ç¡®æ€§ä¸ä¸€è‡´æ€§ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Generalization or Memorization: Dynamic Decoding for Mode Steering</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22099" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22099" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) exhibit a troubling duality, capable of both remarkable generalization and brittle, verbatim memorization of their training data. This unpredictability undermines their reliability in high-stakes applications. In this work, we propose a unified framework to understand, identify, and control these distinct reasoning modes. First, we introduce a theoretical model based on the Information Bottleneck (IB) principle, formalizing generalization as the learning of a compressed, task-relevant representation and memorization as a failure to compress. Building on this theory, we develop Dynamic Mode Steering (DMS), a novel inference-time algorithm which comprises two components: (1) a lightweight, causally-grounded linear probe that identifies the model's instantaneous reliance on memorization, and (2) a dynamic activation steering mechanism that nudges the model's computation towards pre-identified generalization circuits. We frame DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning and faithfulness tasks demonstrate that DMS significantly improves logical consistency and factual accuracy, thereby offering a principled approach to enhancing LLM reliability.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ä¸é‡è¦æ€§ï¼šLLM åœ¨â€œæ³›åŒ–â€ä¸â€œè®°å¿†åŒ–â€ä¸¤ç§æ¨¡å¼é—´ä¸å¯é¢„æµ‹åˆ‡æ¢ï¼Œæ˜“äº§ç”Ÿé€»è¾‘ä¸ä¸€è‡´ã€æœºæ¢°èƒŒä¹¦å¼è°¬è¯¯ä¸è®­ç»ƒæ•°æ®å¤è¿°ï¼Œå‰Šå¼±é«˜é£é™©åœºæ™¯ä¸‹çš„å¯é æ€§ä¸å®‰å…¨æ€§ã€‚<br>â€¢ ç†è®ºä¸åº¦é‡ç¼ºå£ï¼šç¼ºå°‘ç»Ÿä¸€ã€å¯æ“ä½œçš„ç†è®ºæ¡†æ¶æ¥åˆ»ç”»å¹¶åŒºåˆ†ä¸¤ç§æ¨¡å¼ï¼›éœ€è¦ä¸€ç§èƒ½åœ¨æ¨ç†æ—¶åˆ»åŠ¨æ€è¯†åˆ«æ¨¡å‹æ‰€å¤„æ¨¡å¼çš„ä¿¡å·ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šæ ‡å‡†è§£ç ä»…ä½œç”¨äºæœ«ç«¯æ¦‚ç‡åˆ†å¸ƒï¼Œæ— æ³•é€‰æ‹©å†…éƒ¨è®¡ç®—è·¯å¾„ï¼›æ—¢æœ‰æ¿€æ´»ç‰µå¼•å¤šä¸ºé™æ€ã€ç¼ºä¹å› æœéªŒè¯ä¸è‡ªé€‚åº”æ€§ï¼›å¸¸è§„æ¢é’ˆå¤šä¸ºç›¸å…³è€Œéå› æœï¼›è®­ç»ƒæœŸæ­£åˆ™/å¾®è°ƒæˆæœ¬é«˜ã€éš¾ä»¥è¦†ç›–å·²éƒ¨ç½²æ¨¡å‹ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºåŠ¨æ€æ¨¡å¼ç‰µå¼•ï¼ˆDMSï¼‰ï¼šåœ¨å› æœå®šä½çš„å…³é”®å±‚ä¸Šï¼Œç”¨è½»é‡çº¿æ€§æ¢é’ˆå®æ—¶ä¼°è®¡â€œè®°å¿†åŒ–â€ç¨‹åº¦ï¼Œå¹¶æŒ‰è¯¥åˆ†æ•°è‡ªé€‚åº”åœ°å‘æ®‹å·®æµæ³¨å…¥ç”±â€œæ³›åŒ–â€”è®°å¿†â€ç±»å‡å€¼å·®æ„æˆçš„å¼•å¯¼å‘é‡ï¼Œä»è€ŒæŠŠæ¨ç†è½¨è¿¹ä»è®°å¿†å›è·¯ç‰µå¼•è‡³æ³›åŒ–å›è·¯ã€‚æ•´ä½“å¯è§†ä¸ºä¸€ç§å†…ç”Ÿçš„è‡ªå¯¹æ¯”å¼è§£ç ï¼Œè®­ç»ƒå…è°ƒæ•´ã€çº¯æ¨ç†æœŸç”Ÿæ•ˆã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ æ— ç›‘ç£æ¨¡å¼å‘é‡å‘ç°ï¼šåŸºäºç¨€ç–å­—å…¸/å› å­åˆ†æåœ¨æ¿€æ´»ç©ºé—´è‡ªåŠ¨æŒ–æ˜â€œè®°å¿†â€”æ³›åŒ–â€è½´ï¼Œæ‘†è„±æ ‡æ³¨ä¸å¯å‘å¼æ ‡è®°ä¾èµ–ã€‚<br>â€¢ å¤šå±æ€§è‡ªé€‚åº”è”åˆç‰µå¼•ï¼šå¤šæ¢é’ˆ+å¤šå‘é‡çš„åŠ¨æ€ååŒï¼ˆå¦‚è¯šå®ã€æ— å®³ã€æ¨ç†ä¸‰ç»´ï¼‰ï¼Œå«å†²çªè°ƒåº¦ä¸ç¨³å®šæ€§æ§åˆ¶ç­–ç•¥ã€‚<br>â€¢ ä¿¡æ¯ç“¶é¢ˆä¸å‡ ä½•ç»Ÿä¸€ç†è®ºï¼šå°†IBä¸â€œç¥ç»å¡Œç¼©â€å‡ ä½•ç»Ÿä¸€å»ºæ¨¡ï¼Œç»™å‡ºåŠ¨æ€ç‰µå¼•ä¸‹çš„æ³›åŒ–ç•Œä¸åˆ†å¸ƒå¤–ç¨³å®šæ€§ä¿è¯ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-12">

    <div class="paper">
        <h2 class="paper-title">S-Chain: Structured Visual Chain-of-Thought For Medicine</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22728" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22728" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Faithful reasoning in medical vision-language models (VLMs) requires not only accurate predictions but also transparent alignment between textual rationales and visual evidence. While Chain-of-Thought (CoT) prompting has shown promise in medical visual question answering (VQA), no large-scale expert-level dataset has captured stepwise reasoning with precise visual grounding. We introduce S-Chain, the first large-scale dataset of 12,000 expert-annotated medical images with bounding boxes and structured visual CoT (SV-CoT), explicitly linking visual regions to reasoning steps. The dataset further supports 16 languages, totaling over 700k VQA pairs for broad multilingual applicability. Using S-Chain, we benchmark state-of-the-art medical VLMs (ExGra-Med, LLaVA-Med) and general-purpose VLMs (Qwen2.5-VL, InternVL2.5), showing that SV-CoT supervision significantly improves interpretability, grounding fidelity, and robustness. Beyond benchmarking, we study its synergy with retrieval-augmented generation, revealing how domain knowledge and visual grounding interact during autoregressive reasoning. Finally, we propose a new mechanism that strengthens the alignment between visual evidence and reasoning, improving both reliability and efficiency. S-Chain establishes a new benchmark for grounded medical reasoning and paves the way toward more trustworthy and explainable medical VLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šåŒ»ç–—VLMåœ¨é«˜é£é™©åœºæ™¯éœ€è¦â€œå¯è§£é‡Šä¸”å¯è¿½æº¯â€çš„æ¨ç†ï¼Œä½†ç°æœ‰æ¨¡å‹å¤šä¸ºé»‘ç›’ï¼Œåªç»™æœ€ç»ˆç­”æ¡ˆè€Œç¼ºå°‘å°†æ–‡æœ¬ç†ç”±ä¸å›¾åƒè¯æ®ï¼ˆROIï¼‰ç²¾ç¡®å¯¹é½çš„è¿‡ç¨‹ï¼ˆè§å›¾1ï¼Œç¬¬3é¡µï¼‰ã€‚<br>â€¢ é‡è¦æ€§ï¼šä¸´åºŠå†³ç­–è¦æ±‚ä»å®šä½â†’æè¿°â†’æ ‡å‡†åŒ–åˆ†çº§ï¼ˆå¦‚MTA/GCA/Koedamï¼‰â†’è¯Šæ–­çš„å¯éªŒè¯é“¾è·¯ï¼Œå¦åˆ™éš¾ä»¥å»ºç«‹ä¿¡ä»»ä¸å¯å®¡æ ¸æ€§ï¼ˆç¬¬3â€“5é¡µï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šå¤šæ•°CoTæ•°æ®ä¸ºLLMè‡ªåŠ¨ç”Ÿæˆã€ä»…æ–‡æœ¬æˆ–å¼±è§†è§‰å¯¹é½ï¼Œæ˜“å‡ºç°å¹»è§‰/äº‹å®é”™è¯¯ä¸ä¸å¯é çš„æ¡†é€‰ï¼Œç¼ºä¹ä¸“å®¶æ ¸éªŒä¸å¤šè¯­è¦†ç›–ï¼ˆè¡¨1ï¼Œç¬¬4é¡µï¼›å›¾10ï¼Œç¬¬21é¡µï¼‰ã€‚<br>â€¢ æ•°æ®ç“¶é¢ˆï¼šä¸“å®¶çº§å¤šæ¨¡æ€ã€å¯è§†åŒ–è½åœ°ï¼ˆbounding boxï¼‰çš„é€æ­¥æ¨ç†æ ‡æ³¨æ˜‚è´µç¨€ç¼ºï¼Œé™åˆ¶äº†å¯è®­ç»ƒä¸å¯è¯„æµ‹çš„æ•°æ®è§„æ¨¡ä¸è´¨é‡ï¼ˆç¬¬2â€“5é¡µï¼‰ã€‚<br>â€¢ è®­ç»ƒé”™ä½ï¼šè‡ªå›å½’è®­ç»ƒä¸­ï¼Œæ¨¡å‹çš„æ–‡å­—æ¨ç†ä¸çœŸå®ROIå¯èƒ½è„±è€¦ï¼Œå¯¼è‡´â€œçœ‹ä¼¼åˆç†â€çš„ç†ç”±å´æœªä¾æ®å¯¹åº”å›¾åƒè¯æ®ï¼ˆç¬¬8â€“10é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºS-Chainï¼šä¸€ä¸ªå«12kå›¾åƒã€ä¸“å®¶æ ¸éªŒçš„ç»“æ„åŒ–è§†è§‰CoTï¼ˆSV-CoTï¼‰æ•°æ®é›†ï¼Œå››é˜¶æ®µé“¾è·¯ï¼ˆQ1å®šä½ROIæ¡†â†’Q2ç—…ç¶æè¿°â†’Q3æ ‡å‡†åŒ–åˆ†çº§â†’Q4è¯Šæ–­ï¼‰ï¼Œå¹¶ä»¥è‡ªå›å½’SFTå¼ºåˆ¶æ¨¡å‹æŒ‰æ­¥éª¤ç”Ÿæˆï¼›è¿›ä¸€æ­¥ç»“åˆåŒ»å­¦RAGä¸è½»é‡å¯¹é½æ­£åˆ™ï¼ˆROIé”šå®šInfoNCE+ç›‘ç£å¯¹æ¯”ï¼‰å¼ºåŒ–â€œç†ç”±â€”è¯æ®â€ä¸€è‡´æ€§ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ä»2Dåˆ°3Dçš„ç»“æ„åŒ–è§†è§‰é“¾ï¼šå¤šåºåˆ—/ä½“æ•°æ®ä¸Šçš„SV-CoTå­¦ä¹ ä¸è¯„ä¼°ï¼šå°†å››é˜¶æ®µSV-CoTæ‰©å±•åˆ°3Dä½“ç´ ä¸å¤šåºåˆ—MRIï¼Œç ”ç©¶å¯¹åˆ†çº§ä¸è¯Šæ–­çš„ç³»ç»Ÿæ€§æ”¶ç›Šã€‚<br>â€¢ å¿ å®è§£ç å™¨ï¼šçº¦æŸROIä¸€è‡´æ€§çš„å¤šæ¨¡æ€æ¨ç†ä¸è§£ç ç­–ç•¥ï¼šè®¾è®¡å¼ºåˆ¶å¼•ç”¨å·²å®šä½ROIçš„æ³¨æ„åŠ›/è§£ç çº¦æŸä¸å¯¹é½æŸå¤±ï¼Œé™ä½å¹»è§‰å¹¶æå‡ç†ç”±â€”è¯æ®ä¸€è‡´æ€§ã€‚<br>â€¢ æ£€ç´¢å¯¹é½çš„ä¸´åºŠVLMï¼šSV-CoTä¸åŠ¨æ€åŒ»å­¦RAGçš„è”åˆä¼˜åŒ–ï¼šæ„å»ºä»»åŠ¡è‡ªé€‚åº”æ£€ç´¢â€”æ¨ç†é—­ç¯ï¼Œå­¦ä¹ ä½•æ—¶æ£€ç´¢ã€æ£€ç´¢ä½•ç‰©ä¸å¦‚ä½•åœ¨é“¾å¼æ¨ç†ä¸­èåˆã€‚<br>â€¢ å¤šè¯­è·¨åŸŸè’¸é¦ï¼šç”¨å¤šè¯­è¨€SV-CoTæå‡ä½èµ„æºåŒ»å­¦åœºæ™¯ï¼šç ”ç©¶è·¨è¯­è¨€ä¸€è‡´æ€§æ­£åˆ™ä¸æ•™å¸ˆâ€”å­¦ç”Ÿè’¸é¦ï¼Œå¢å¼ºä½èµ„æºè¯­è¨€ä¸æ–°æ¨¡æ€çš„è¿ç§»æ³›åŒ–ã€‚<br>â€¢ äººæœºå…±æ ‡æ³¨çš„ä¸»åŠ¨SV-CoTæ„å»ºï¼šé™ä½ä¸“å®¶è´Ÿæ‹…çš„è½»é‡æ ‡æ³¨æ¡†æ¶ï¼šé€šè¿‡ä¸»åŠ¨å­¦ä¹ /ä¸ç¡®å®šæ€§é©±åŠ¨é€‰æ‹©å…³é”®æ ·æœ¬ä¸å…³é”®æ­¥éª¤ï¼ˆå¦‚ä»…æ ¡æ­£ROIæˆ–åˆ†çº§ï¼‰ï¼Œé™æœ¬å¢æ•ˆã€‚<br>â€¢ å¯ä¿¡è¯„æµ‹åŸºå‡†ï¼šé¢å‘è§†è§‰â€”æ¨ç†ä¸€è‡´æ€§çš„æŒ‡æ ‡ä¸å‹åŠ›æµ‹è¯•ï¼šå»ºç«‹è¡¡é‡CoT-ROIå¯¹é½ã€ä¸´åºŠå¯æ ¸éªŒæ€§ä¸é²æ£’æ€§çš„æ ‡å‡†åŒ–æŒ‡æ ‡ä¸åŸºå‡†å¥—ä»¶ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23667" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23667" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Structural topology optimization (TO) is central to engineering design but remains computationally intensive due to complex physics and hard constraints. Existing deep-learning methods are limited to fixed square grids, a few hand-coded boundary conditions, and post-hoc optimization, preventing general deployment. We introduce Optimize Any Topology (OAT), a foundation-model framework that directly predicts minimum-compliance layouts for arbitrary aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines a resolution- and shape-agnostic autoencoder with an implicit neural-field decoder and a conditional latent-diffusion model trained on OpenTO, a new corpus of 2.2 million optimized structures covering 2 million unique boundary-condition configurations. On four public benchmarks and two challenging unseen tests, OAT lowers mean compliance up to 90% relative to the best prior models and delivers sub-1 second inference on a single GPU across resolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These results establish OAT as a general, fast, and resolution-free framework for physics-aware topology optimization and provide a large-scale dataset to spur further research in generative modeling for inverse design. Code & data can be found at https://github.com/ahnobari/OptimizeAnyTopology.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ä¼ ç»Ÿæœ€å°æŸ”åº¦æ‹“æ‰‘ä¼˜åŒ–éœ€åå¤FEAä¸æ¢¯åº¦è¿­ä»£ï¼Œè®¡ç®—å¼€é”€å¤§ã€éš¾ä»¥æ”¯æŒäº¤äº’å¼æˆ–å¤§è§„æ¨¡è®¾è®¡æ¢ç´¢ã€‚<br>â€¢ ç°æœ‰æ·±åº¦å­¦ä¹ ç›´æ¥é¢„æµ‹æ–¹æ³•æ³›åŒ–å·®ï¼šå±€é™å›ºå®šæ–¹å½¢ç½‘æ ¼ä¸åˆ†è¾¨ç‡ã€å°‘é‡æ‰‹å·¥è¾¹ç•Œæ¡ä»¶ã€å•è½½è·é…ç½®ï¼Œé‡åˆ°ä»»æ„å½¢çŠ¶/åˆ†è¾¨ç‡/å†…è½½è·å³å¤±æ•ˆã€‚<br>â€¢ è¡¨ç¤ºæ–¹å¼å—é™ï¼šå¤šä¾èµ–åƒç´ /FEAåœºä½œä¸ºè¾“å…¥ï¼Œåˆ†è¾¨ç‡ä¸å½¢çŠ¶ç»‘å®šã€æ¨ç†æ…¢ï¼Œä¸”å¸¸éœ€äº‹åè¿­ä»£ä¼˜åŒ–ä¿®è¡¥ï¼Œéš¾ä»¥å·¥ç¨‹éƒ¨ç½²ã€‚<br>â€¢ æ•°æ®é›†ç‹­çª„ï¼šå…¬å¼€æ•°æ®è§„æ¨¡å°ã€é…ç½®å•ä¸€ï¼Œæ— æ³•æ”¯æ’‘å…·å¤‡â€œåŸºç¡€æ¨¡å‹â€èƒ½åŠ›çš„è®­ç»ƒä¸è¯„æµ‹ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºOATï¼šä»¥åˆ†è¾¨ç‡ä¸å½¢çŠ¶æ— å…³çš„è‡ªç¼–ç å™¨ï¼ˆéšå¼ç¥ç»åœºæ¸²æŸ“ï¼‰æ„é€ å›ºå®šç»´æ½œç©ºé—´ï¼Œå¹¶åœ¨å…¶ä¸­è®­ç»ƒæ¡ä»¶æ½œç©ºé—´æ‰©æ•£æ¨¡å‹ï¼›ç”¨BPOMå°†è¾¹ç•Œæ¡ä»¶ä¸è½½è·ç‚¹äº‘ç¼–ç ï¼Œè”åŒä½“ç§¯åˆ†æ•°ã€åƒç´ å°ºå¯¸ã€é•¿å®½æ¯”åµŒå…¥å®ç°ç«¯åˆ°ç«¯æœ€å°æŸ”åº¦æ‹“æ‰‘ç”Ÿæˆï¼Œå¿…è¦æ—¶è¾…ä»¥å°‘æ­¥SIMPç²¾ä¿®ã€‚è®­ç»ƒäºæ¶µç›–ä»»æ„åŸŸå½¢çŠ¶/åˆ†è¾¨ç‡ä¸éšæœºå†…å¤–è½½è·/çº¦æŸçš„OpenTOï¼ˆ220ä¸‡æ ·æœ¬ï¼‰ï¼Œå®ç°64â€“256åˆ†è¾¨ç‡ä¸‹è¿‘å¸¸æ•°æ—¶å»¶ã€äºšç§’çº§æ¨ç†ä¸æ˜¾è‘—æ›´ä½æŸ”åº¦è¯¯å·®ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ä¼˜åŒ–å™¨å¼•å¯¼çš„æ‹“æ‰‘ç”Ÿæˆæ‰©æ•£å¯¹é½ï¼šç»“åˆSIMP/FEAåé¦ˆä¸åå¥½ä¼˜åŒ–/å¼ºåŒ–å­¦ä¹ ï¼Œé™ä½å¤±æ•ˆç‡å¹¶æå‡ç‰©ç†åˆè§„æ€§<br>â€¢ è·¨ç‰©ç†å¤šç›®æ ‡çš„OATï¼šå°†æœ€å°æŸ”åº¦æ‰©å±•è‡³åº”åŠ›çº¦æŸã€å±ˆæ›²ä¸çƒ­è€¦åˆï¼Œæ„å»ºç»Ÿä¸€çš„å¤šç‰©ç†/å¤šç›®æ ‡ç”Ÿæˆæ¡†æ¶<br>â€¢ å°‘æ ·æœ¬é«˜æ•ˆè¿ç§»çš„æ‹“æ‰‘åŸºç¡€æ¨¡å‹ï¼šåŸºäºLoRA/Adapter/å…ƒå­¦ä¹ ï¼Œå®ç°å¯¹æ–°ææ–™ã€åˆ¶é€ çº¦æŸä¸ä¸‰ç»´åœºæ™¯çš„å¿«é€Ÿé€‚é…</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22373" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22373" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Visualization, a domain-specific yet widely used form of imagery, is an effective way to turn complex datasets into intuitive insights, and its value depends on whether data are faithfully represented, clearly communicated, and aesthetically designed. However, evaluating visualization quality is challenging: unlike natural images, it requires simultaneous judgment across data encoding accuracy, information expressiveness, and visual aesthetics. Although multimodal large language models (MLLMs) have shown promising performance in aesthetic assessment of natural images, no systematic benchmark exists for measuring their capabilities in evaluating visualizations. To address this, we propose VisJudge-Bench, the first comprehensive benchmark for evaluating MLLMs' performance in assessing visualization aesthetics and quality. It contains 3,090 expert-annotated samples from real-world scenarios, covering single visualizations, multiple visualizations, and dashboards across 32 chart types. Systematic testing on this benchmark reveals that even the most advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a correlation with human ratings of only 0.429. To address this issue, we propose VisJudge, a model specifically designed for visualization aesthetics and quality assessment. Experimental results demonstrate that VisJudge significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a 19.8% reduction) and increasing the consistency with human experts to 0.681 (a 58.7% improvement) compared to GPT-5. The benchmark is available at https://github.com/HKUSTDial/VisJudgeBench.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šç°æœ‰MLLMåœ¨è‡ªç„¶å›¾åƒå®¡ç¾è¯„ä¼°ä¸Šè¡¨ç°ä¸é”™ï¼Œä½†éš¾ä»¥åŒæ—¶åˆ¤åˆ«æ•°æ®å¯è§†åŒ–çš„â€œæ•°æ®å¿ å®åº¦ï¼ˆFidelityï¼‰â€”ä¿¡æ¯è¡¨è¾¾åŠ›ï¼ˆExpressivenessï¼‰â€”è§†è§‰ç¾æ„Ÿï¼ˆAestheticsï¼‰â€ä¸‰ç»´è´¨é‡ï¼Œæ˜“è¢«â€œå¥½çœ‹ä½†è¯¯å¯¼â€çš„å›¾è¡¨è¿·æƒ‘ï¼ˆè§å›¾2ï¼Œç¬¬2é¡µï¼‰ã€‚<br>â€¢ é‡è¦æ€§ï¼šå¯è§†åŒ–å¹¿æ³›ç”¨äºå°†å¤æ‚æ•°æ®è½¬åŒ–ä¸ºè®¤çŸ¥ä¸å†³ç­–ï¼Œè´¨é‡å¤±çœŸä¼šå¸¦æ¥è¯¯å¯¼ï¼ˆè½´ç¼©æ”¾ã€é…è‰²æš—ç¤ºã€ä¿¡æ¯è¿‡è½½ç­‰ï¼‰ï¼Œå½±å“æ´å¯Ÿä¸å†³ç­–å¯é æ€§ï¼ˆè§å›¾1ï¼Œç¬¬2é¡µï¼‰ã€‚<br>â€¢ ç°æœ‰åŸºå‡†å±€é™ï¼šChartQA/ChartInsightsç­‰ä¸»è¦è€ƒæŸ¥â€œè¯»æ‡‚å›¾è¡¨â€è€Œéâ€œè®¾è®¡è´¨é‡â€ï¼›AVA/ArtiMuseç­‰åªè¯„ç¾æ„Ÿä¸è¯„ä¿¡æ¯æœ‰æ•ˆæ€§ï¼›VisEval/VIS-Shepherdèšç„¦NL2VISæ˜¯å¦ç¬¦åˆæ–‡æœ¬æ„å›¾ï¼Œå¿½ç•¥å›¾è¡¨å†…åœ¨è®¾è®¡è´¨é‡çš„å…¨é¢è¯„ä»·ï¼ˆè§è¡¨1ï¼Œç¬¬3é¡µï¼‰ã€‚<br>â€¢ æ–¹æ³•ç¼ºå£ï¼šç¼ºä¹è¦†ç›–å•å›¾ã€å¤šè§†å›¾ä¸ä»ªè¡¨æ¿ã€å¹¶åœ¨ä¸‰ç»´åº¦ä¸Šç³»ç»Ÿæ‰“åˆ†çš„çœŸå®åœºæ™¯åŸºå‡†ï¼›é€šç”¨MLLMä¸äººç±»ä¸“å®¶åœ¨åˆ¤æ–­ä¸Šå­˜åœ¨æ˜¾è‘—åå·®ï¼ˆè®ºæ–‡æŠ¥å‘Šæœ€å¼ºåŸºçº¿MAE=0.551ï¼Œç›¸å…³ç³»æ•°=0.429ï¼Œç¬¬1-2é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æ„å»ºVisJudge-BenchåŸºå‡†ï¼šå›´ç»•â€œå¿ å®åº¦-è¡¨è¾¾åŠ›-ç¾å­¦â€çš„å¤šç»´æ¡†æ¶ï¼Œé’ˆå¯¹å•å›¾/å¤šå›¾/ä»ªè¡¨æ¿å…±3090ä¸ªçœŸå®æ ·æœ¬ä¸32ç±»å›¾è¡¨ï¼Œé‡‡ç”¨è‡ªé€‚åº”é—®é¢˜ç”Ÿæˆä¸ä¸“å®¶æ ‡æ³¨ï¼Œå½¢æˆå…­ä¸ªå¯é‡åŒ–å­ç»´åº¦çš„è¯„åˆ†ä½“ç³»ï¼›å¹¶æå‡ºä¸“é—¨çš„VisJudgeæ¨¡å‹ç”¨äºå¯è§†åŒ–è´¨é‡åˆ¤åˆ«ã€‚å®éªŒè¯æ˜å…¶æ˜æ˜¾ä¼˜äºé€šç”¨MLLMï¼Œä¸äººç±»ä¸“å®¶æ›´ä¸€è‡´ï¼ˆMAEé™è‡³0.442ï¼Œç›¸å…³æå‡è‡³0.681ï¼Œç¬¬1-2ã€4-5é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ä»â€œè¯„ä¼°â€åˆ°â€œä¿®å¤â€ï¼šåŸºäºVisJudgeçš„å¯è§†åŒ–è‡ªåŠ¨æ”¹å†™ä¸çº åæ¡†æ¶ï¼šå°†è¯„å®¡ä¿¡å·ä½œä¸ºå¥–åŠ±/çº¦æŸï¼Œé—­ç¯ä¼˜åŒ–è½´åŸŸã€æ ‡æ³¨ã€å¸ƒå±€ä¸é…è‰²ã€‚<br>â€¢ ä»»åŠ¡ä¸ç”¨æˆ·æƒ…å¢ƒæ„ŸçŸ¥çš„ä»ªè¡¨æ¿è´¨é‡è¯„ä¼°ï¼šå¼•å…¥ä»»åŠ¡éš¾åº¦ã€äº¤äº’ä¸å—ä¼—ç”»åƒï¼Œæ„å»ºæƒ…å¢ƒåŒ–çš„å¤šç»´è¯„åˆ†ä¸è§£é‡Šã€‚<br>â€¢ è·¨æ–‡åŒ–ä¸å¤šè¯­è¨€çš„å¯è§†åŒ–å®¡ç¾å¯¹é½å­¦ä¹ ï¼šæ‰©å±•åŸºå‡†åˆ°å¤šæ–‡åŒ–è¯­å¢ƒï¼Œç ”ç©¶å®¡ç¾å·®å¼‚å¯¹è®¾è®¡å»ºè®®ä¸è¯„åˆ†ä¸€è‡´æ€§çš„å½±å“ã€‚<br>â€¢ ç”Ÿæˆ-è¯„å®¡è”åˆè®­ç»ƒçš„å¯è§†åŒ–æ™ºèƒ½ä½“ï¼šå°†NL2VISç”Ÿæˆå™¨ä¸VisJudgeè”åˆä¼˜åŒ–ï¼Œå®ç°â€œç”Ÿæˆ-è¯„å®¡-æ”¹å†™â€çš„ååŒæå‡ã€‚<br>â€¢ é¢å‘å¯è§£é‡Šåˆ¤æ®çš„å› æœå¯è§†åŒ–è¯„ä¼°ï¼šä»¥å› æœåˆ¤æ®æ‹†è§£å¤±çœŸæ¥æºï¼ˆè½´ç¼©æ”¾ã€ç¼–ç å†²çªç­‰ï¼‰ï¼Œè¾“å‡ºç»“æ„åŒ–è§£é‡Šä¸å¯æ“ä½œä¿®å¤å»ºè®®ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-13">

    <div class="paper">
        <h2 class="paper-title">GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22319" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22319" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recently, GRPO-based reinforcement learning has shown remarkable progress in optimizing flow-matching models, effectively improving their alignment with task-specific rewards. Within these frameworks, the policy update relies on importance-ratio clipping to constrain overconfident positive and negative gradients. However, in practice, we observe a systematic shift in the importance-ratio distribution-its mean falls below 1 and its variance differs substantially across timesteps. This left-shifted and inconsistent distribution prevents positive-advantage samples from entering the clipped region, causing the mechanism to fail in constraining overconfident positive updates. As a result, the policy model inevitably enters an implicit over-optimization stage-while the proxy reward continues to increase, essential metrics such as image quality and text-prompt alignment deteriorate sharply, ultimately making the learned policy impractical for real-world use. To address this issue, we introduce GRPO-Guard, a simple yet effective enhancement to existing GRPO frameworks. Our method incorporates ratio normalization, which restores a balanced and step-consistent importance ratio, ensuring that PPO clipping properly constrains harmful updates across denoising timesteps. In addition, a gradient reweighting strategy equalizes policy gradients over noise conditions, preventing excessive updates from particular timestep regions. Together, these designs act as a regulated clipping mechanism, stabilizing optimization and substantially mitigating implicit over-optimization without relying on heavy KL regularization. Extensive experiments on multiple diffusion backbones (e.g., SD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard significantly reduces over-optimization while maintaining or even improving generation quality.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šGRPOåœ¨æµåŒ¹é…/æ‰©æ•£æ¨¡å‹ä¸­çš„é‡è¦æ€§æ¯”ç‡åˆ†å¸ƒå…ˆå¤©â€œå·¦ç§»â€ï¼ˆå‡å€¼<1ï¼‰ä¸”è·¨æ­¥é•¿æ–¹å·®ä¸ä¸€è‡´ï¼Œå¯¼è‡´PPOè£å‰ªå¯¹æ­£ä¼˜åŠ¿æ ·æœ¬å‡ ä¹å¤±æ•ˆï¼Œè®­ç»ƒè¿…é€Ÿè¿›å…¥â€œéšå¼è¿‡ä¼˜åŒ–â€ï¼Œä»£ç†åˆ†æ•°ä¸Šå‡è€Œé‡‘æŒ‡æ ‡ï¼ˆå›¾åƒè´¨é‡ã€æ–‡å›¾ä¸€è‡´æ€§ï¼‰æ˜¾è‘—ä¸‹é™ï¼ˆè§ç¬¬1é¡µå›¾1å·¦ã€å³ï¼›ç¬¬5é¡µå›¾2ï¼‰ã€‚<br>â€¢ é‡è¦æ€§ï¼šè¿‡ä¼˜åŒ–ä¼šé€ æˆå¤šæ ·æ€§ã€ç»†èŠ‚ä¸è´¨é‡å´©åï¼Œä½¿ç­–ç•¥åœ¨çœŸå®åº”ç”¨ä¸­ä¸å¯ç”¨ï¼ˆç¬¬1é¡µå›¾1å³å¯è§†åŒ–å¯¹æ¯”ç›´è§‚å±•ç¤ºé€€åŒ–ï¼‰ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ä»£ç†-é‡‘æŒ‡æ ‡èƒŒç¦»ä¸¥é‡ï¼Œéš¾ä»¥åœ¨çº¿ç¨³å¥ç›‘æ§ï¼ˆç¬¬7é¡µè¡¨1ä¸ç¬¬8é¡µå›¾4ï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šä¾èµ–é‡KLæ­£åˆ™è™½èƒ½æŠ‘åˆ¶è¿‡ä¼˜åŒ–ä½†æ˜¾è‘—æ‹–æ…¢å­¦ä¹ ï¼›æ—¢æœ‰å·¥ä½œå¤šå…³æ³¨æ•ˆç‡æˆ–é‡‡æ ·ä¸€è‡´æ€§ï¼ˆå¦‚TempFlowGRPOã€MixGRPOã€Flow-CPSï¼‰ï¼Œæœªè§¦åŠè£å‰ªå¤±æ•ˆçš„æ ¹å› ï¼ˆæ‰©æ•£é‡‡ç”¨é«˜æ–¯è½¬ç§»æ¦‚ç‡è€ŒéLLMç¦»æ•£tokenæ¦‚ç‡ï¼Œå¯¼è‡´æ¯”ç‡åˆ†å¸ƒå¼‚å¸¸ï¼‰ï¼›æ­¥é—´å™ªå£°ä¸è°ƒåº¦é€ æˆæ¢¯åº¦ä¸å¹³è¡¡ï¼Œé«˜å™ªå£°æ­¥å‡ ä¹ä¸è¢«è£å‰ªã€ä½å™ªå£°æ­¥è¢«è¿‡è£å‰ªï¼ˆç¬¬5é¡µå›¾2cã€ç¬¬12é¡µå›¾12ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºGRPO-Guardï¼šåœ¨æ¯ä¸ªå»å™ªæ­¥å¯¹logé‡è¦æ€§æ¯”ç‡è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆRatioNormï¼‰ä»¥å°†å‡å€¼æ‹‰å›â‰ˆ0å¹¶å¯¹é½æ–¹å·®ï¼Œä½¿PPOè£å‰ªèƒ½å¯¹æ­£/è´Ÿä¼˜åŠ¿æ ·æœ¬å¯¹ç§°ç”Ÿæ•ˆï¼›åŒæ—¶åŠ å…¥è·¨æ­¥é•¿çš„æ¢¯åº¦å†åŠ æƒï¼ˆÎ´=1/dtï¼ŒDanceGRPOä¸ºÎ²/dtï¼‰å‡è¡¡å„æ­¥æ¢¯åº¦ï¼ŒæŠ‘åˆ¶å•ä¸€æ­¥æ¡ä»¶ä¸»å¯¼è®­ç»ƒï¼Œä»è€Œåœ¨æ— éœ€é‡KLçš„å‰æä¸‹æ˜¾è‘—ç¼“è§£è¿‡ä¼˜åŒ–ï¼ˆç¬¬5-6é¡µå¼(8)ï¼Œç¬¬7é¡µå›¾3ã€è¡¨1ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Step-Adaptive Regulated Clipping for Flow RLï¼šå­¦ä¹ å¼/è‡ªé€‚åº”çš„é€æ­¥è£å‰ªåŒºé—´ä¸æ¸©åº¦æ ‡å®šï¼Œè¿›ä¸€æ­¥å¢å¼ºåœ¨ä¸åŒé‡‡æ ·è°ƒåº¦ä¸ä»»åŠ¡ä¸‹çš„ç¨³å®šæ€§ã€‚<br>â€¢ Unified Multi-Proxy Reward for KL-free Flow Alignmentï¼šå°†HPSv2/ImageReward/UnifiedRewardç­‰å¤šé‡‘æŒ‡æ ‡è’¸é¦ä¸ºé«˜æ•ˆç»Ÿä¸€å¥–åŠ±æ¨¡å‹ï¼Œç¼©å°ä»£ç†-é‡‘æŒ‡æ ‡é¸¿æ²Ÿï¼Œæºå¤´é™ä½è¿‡ä¼˜åŒ–ã€‚<br>â€¢ GRPO-Guard-V: Temporally Granular Clipping for Video Rectified Flowsï¼šå°†å—æ§è£å‰ªä¸æ¢¯åº¦å‡è¡¡æ‰©å±•è‡³æ—¶åº/è§†é¢‘ç”Ÿæˆï¼Œç»“åˆæ—¶é—´ç²’åº¦å¥–åŠ±åˆ†é…ï¼ˆå¦‚G2RPO/TempFlowGRPOï¼‰æå‡æ—¶åºä¸€è‡´æ€§ä¸ç¨³å¥æ€§ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21323" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21323" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The alignment of vision-language representations endows current Vision-Language Models (VLMs) with strong multi-modal reasoning capabilities. However, the interpretability of the alignment component remains uninvestigated due to the difficulty in mapping the semantics of multi-modal representations into a unified concept set. To address this problem, we propose VL-SAE, a sparse autoencoder that encodes vision-language representations into its hidden activations. Each neuron in its hidden layer correlates to a concept represented by semantically similar images and texts, thereby interpreting these representations with a unified concept set. To establish the neuron-concept correlation, we encourage semantically similar representations to exhibit consistent neuron activations during self-supervised training. First, to measure the semantic similarity of multi-modal representations, we perform their alignment in an explicit form based on cosine similarity. Second, we construct the VL-SAE with a distance-based encoder and two modality-specific decoders to ensure the activation consistency of semantically similar representations. Experiments across multiple VLMs (e.g., CLIP, LLaVA) demonstrate the superior capability of VL-SAE in interpreting and enhancing the vision-language alignment. For interpretation, the alignment between vision and language representations can be understood by comparing their semantics with concepts. For enhancement, the alignment can be strengthened by aligning vision-language representations at the concept level, contributing to performance improvements in downstream tasks, including zero-shot image classification and hallucination elimination. Codes are available at https://github.com/ssfgunner/VL-SAE.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šéš¾ä»¥å°†è§†è§‰ä¸è¯­è¨€è¡¨ç¤ºçš„è¯­ä¹‰æ˜ å°„åˆ°â€œç»Ÿä¸€æ¦‚å¿µé›†â€ï¼Œå¯¼è‡´æ— æ³•ç›´æ¥æ¯”è¾ƒä¸¤æ¨¡æ€è¯­ä¹‰å¹¶è§£é‡ŠVLMçš„å¯¹é½æœºåˆ¶ï¼ˆé¡µ1â€“2ï¼‰<br>â€¢ é‡è¦æ€§ï¼šè§†è§‰â€”è¯­è¨€å¯¹é½æ˜¯VLMæ¨ç†çš„æ ¸å¿ƒï¼Œè¯¯å¯¹é½ä¼šå¼•å‘å¯¹è±¡å¹»è§‰ç­‰å¤±è¯¯ï¼Œé™åˆ¶é›¶æ ·æœ¬åˆ†ç±»ä¸å¤šæ¨¡æ€é—®ç­”çš„å¯é æ€§ä¸å®‰å…¨æ€§ï¼ˆé¡µ1ã€8â€“9ï¼‰<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šé¢„å®šä¹‰æ¦‚å¿µé›†è¦†ç›–ä¸è¶³ã€æ ‡æ³¨æˆæœ¬é«˜ã€å¯æ‰©å±•æ€§å·®ï¼›å¯å­¦ä¹ SAEç«¯åˆ°ç«¯è·å–æ¦‚å¿µä½†ä¸å¯æ§ï¼Œç”¨ä¸¤å¥—SAEå„è‡ªå­¦ä¹ ä¼šäº§ç”Ÿâ€œæ¦‚å¿µä¸åŒ¹é…â€ï¼›å…±äº«å•ä¸€SAEåˆå—ä¸¤æ¨¡æ€åˆ†å¸ƒå·®å¼‚ä¸LVLMéšå¼å¯¹é½ä¸‹â€œç›¸ä¼¼åº¦éš¾åº¦é‡â€çš„åŒé‡å½±å“ï¼›å•è§£ç å™¨ä¼šæŠŠæ¨¡æ€åˆ†å¸ƒä¿¡æ¯æŒ¤å…¥æ¿€æ´»ï¼Œé™ä½è·¨æ¨¡æ€ä¸€è‡´æ€§ï¼ˆé¡µ2ã€5â€“6ï¼‰</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºVL-SAEï¼šåœ¨åŒä¸€ç¨€ç–è‡ªç¼–ç å™¨ä¸­ä½¿ç”¨åŸºäºå½’ä¸€åŒ–æ¬§æ°è·ç¦»çš„ç¼–ç å™¨ä¸ä¸¤å¥—æ¨¡æ€ç‰¹å®šè§£ç å™¨ï¼Œç»“åˆTop-Kç¨€ç–ï¼Œä½¿è¯­ä¹‰ç›¸è¿‘çš„å›¾æ–‡è¡¨ç¤ºè·å¾—ä¸€è‡´æ¿€æ´»å¹¶å½¢æˆç»Ÿä¸€æ¦‚å¿µé›†ï¼›å¯¹LVLMå…ˆä»¥è¾…åŠ©è‡ªç¼–ç å™¨+InfoNCEå°†éšå¼å¯¹é½æ˜¾å¼åŒ–ï¼ˆä½™å¼¦ç›¸ä¼¼å¯åº¦é‡ï¼‰ï¼Œå†ç”¨VL-SAEå­¦ä¹ æ¦‚å¿µå¹¶ç”¨äºè§£é‡Šä¸å¯¹é½å¢å¼ºã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ å‡åŒ€ç¨€ç–VL-SAEï¼šæŠ‘åˆ¶æ­»ç¥ç»å…ƒä¸é«˜é¢‘æ¦‚å¿µçš„ç»“æ„æ­£åˆ™â€”â€”é€šè¿‡å‡åŒ€æ¿€æ´»ä¸ç«äº‰æœºåˆ¶æå‡æ¦‚å¿µè¦†ç›–ä¸ç¨³å¥æ€§<br>â€¢ æ¦‚å¿µå›¾å¯¹é½ï¼šåŸºäºæ¦‚å¿µå…³ç³»å›¾çš„è·¨æ¨¡æ€æ¨ç†ä¸å¯¹é½å¢å¼ºâ€”â€”åœ¨æ¦‚å¿µå±‚å»ºå›¾å»ºæ¨¡ç»„åˆ/å› æœå…³ç³»ä»¥åŠ å¼ºå¯¹é½ä¸å¯è§£é‡Šæ¨ç†<br>â€¢ æŒç»­å­¦ä¹ çš„ç»Ÿä¸€æ¦‚å¿µé›†ï¼šé¢å‘å¢é‡ä»»åŠ¡ä¸åŸŸè¿ç§»çš„è‡ªé€‚åº”VL-SAEâ€”â€”æ”¯æŒæ¦‚å¿µå¢é‡ä¸é—å¿˜/åé—å¿˜ï¼Œç¨³å®šæ›´æ–°å¹¶ç»´æŒè·¨æ¨¡æ€ä¸€è‡´</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20155" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20155" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Understanding objects at the level of their constituent parts is fundamental to advancing computer vision, graphics, and robotics. While datasets like PartNet have driven progress in 3D part understanding, their reliance on untextured geometries and expert-dependent annotation limits scalability and usability. We introduce PartNeXt, a next-generation dataset addressing these gaps with over 23,000 high-quality, textured 3D models annotated with fine-grained, hierarchical part labels across 50 categories. We benchmark PartNeXt on two tasks: (1) class-agnostic part segmentation, where state-of-the-art methods (e.g., PartField, SAMPart3D) struggle with fine-grained and leaf-level parts, and (2) 3D part-centric question answering, a new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary part grounding. Additionally, training Point-SAM on PartNeXt yields substantial gains over PartNet, underscoring the dataset's superior quality and diversity. By combining scalable annotation, texture-aware labels, and multi-task evaluation, PartNeXt opens new avenues for research in structured 3D understanding.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ éƒ¨ä»¶çº§3Dç†è§£æ˜¯è§†è§‰ã€å›¾å½¢ä¸æœºå™¨äººä»»åŠ¡çš„å…³é”®ä¸­é—´è¡¨ç¤ºï¼Œæ”¯æ’‘åˆ†å‰²ã€ç”Ÿæˆä¸æ“æ§ç­‰èƒ½åŠ›ï¼ˆè§ç¬¬2é¡µå¼•è¨€ï¼‰ã€‚<br>â€¢ ç°æœ‰ä¸»æµæ•°æ®é›†ï¼ˆå¦‚PartNetï¼‰ä¾èµ–æ— çº¹ç†é‡ç½‘æ ¼ï¼Œå¯¼è‡´çº¹ç†ç¼ºå¤±ä¸å‡ ä½•å½¢å˜ï¼Œé¢œè‰²/æè´¨çº¿ç´¢ä¸å¯ç”¨ï¼Œä¸”æ ‡æ³¨éœ€ä¸“ä¸š3Dæ“ä½œï¼ˆç”»åˆ‡å‰²çº¿ã€æ£€è§†æˆªé¢ï¼‰ï¼Œéš¾ä»¥ä¼—åŒ…ä¸æ‰©å±•ï¼ˆç¬¬2â€“3é¡µï¼‰ã€‚<br>â€¢ 3Déƒ¨ä»¶æ ‡æ³¨æœ¬èº«å›°éš¾ï¼šå†…éƒ¨/é®æŒ¡éƒ¨ä»¶éš¾å¯è§†åŒ–ã€ç•Œé¢ä¸ç›´è§‚ã€ç»†ç²’åº¦ä¸€è‡´æ€§å’Œè·¨ç±»å±‚çº§ä½“ç³»éš¾è®¾è®¡ä¸ç»´æŠ¤ï¼ˆç¬¬2â€“3é¡µï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•åœ¨ç»†ç²’åº¦ä¸å¶èŠ‚ç‚¹éƒ¨ä»¶ä¸Šè¡¨ç°ä¸ä½³ï¼ˆPartFieldã€SAMeshã€SAMPart3Dï¼‰ï¼Œéš¾ä»¥åœ¨å¼±çº¹ç†ä¸è¿æ¥åŒºåŸŸå–å¾—å…¼é¡¾è¯­ä¹‰ä¸ç²’åº¦çš„åˆ†å‰²ï¼ˆç¬¬7â€“8é¡µï¼‰ã€‚<br>â€¢ 3Då¤šæ¨¡æ€/LLMåœ¨å¼€æ”¾è¯æ±‡éƒ¨ä»¶å®šä½ä¸é—®ç­”ä¸Šèƒ½åŠ›ä¸è¶³ï¼Œè®¡æ•°ä¸å®šä½è¯¯å·®æ˜æ˜¾ï¼Œéš¾ä»¥éµå¾ªæ ¼å¼åŒ–è¾“å‡ºï¼ˆç¬¬8â€“9é¡µï¼Œé™„å½•Eï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºPartNeXtï¼šåŒ…å«23,519ä¸ªé«˜è´¨é‡çº¹ç†ç½‘æ ¼ã€è¦†ç›–50ç±»ã€35ä¸‡éƒ¨ä»¶çš„ç»†ç²’åº¦å±‚çº§æ•°æ®é›†ï¼›é‡‡ç”¨å¯ä¼—åŒ…çš„Webæ ‡æ³¨ç³»ç»Ÿï¼ˆå±‚çº§å¼æµç¨‹+åŒé¢æ¿è§†å›¾+è¿é€š/æ¡†é€‰/é€é¢é€‰æ‹©ï¼‰ï¼Œç›´æ¥åœ¨å¸¦çº¹ç†ç½‘æ ¼ä¸Šé€é¢æ ‡æ³¨ï¼Œå¹¶ä»¥CLIPç­›é€‰èµ„äº§ã€GPT-4oè¾…åŠ©æ„å»ºè·¨ç±»å±‚çº§ä¸ç¤ºä¾‹ã€‚åŸºäºæ­¤å»ºç«‹ä¸¤ä¸ªåŸºå‡†ï¼ˆç±»åˆ«æ— å…³éƒ¨ä»¶å®ä¾‹åˆ†å‰²ä¸éƒ¨ä»¶ä¸­å¿ƒ3Dé—®ç­”ï¼šè®¡æ•°/åˆ†ç±»/å®šä½ï¼‰ï¼Œå¹¶éªŒè¯è®­ç»ƒPoint-SAMå¯æ˜¾è‘—ä¼˜äºç”¨PartNetè®­ç»ƒçš„ç‰ˆæœ¬ï¼ˆç¬¬1ã€3â€“6ã€8â€“9é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ OpenPart-LLM: å¼€æ”¾è¯æ±‡çš„è·¨ç±»3Déƒ¨ä»¶å±‚çº§ä¸æ ‡æ³¨æ¡†æ¶â€”â€”èåˆVLMè‡ªåŠ¨æ‰©å±•å±‚çº§ä¸åŒä¹‰è¯è§„èŒƒï¼Œå‡å°‘äººå·¥æ¨¡æ¿ä¾èµ–å¹¶æå‡å¼€æ”¾è¯æ±‡è¦†ç›–ã€‚<br>â€¢ TexGeo-SAM3D: èåˆçº¹ç†-å‡ ä½•-å¤šè§†å›¾çš„å¯æç¤º3Dåˆ†å‰²åŸºç¡€æ¨¡å‹â€”â€”åœ¨PartNeXtä¸Šè”åˆç‚¹äº‘/ç½‘æ ¼/è´´å›¾è®­ç»ƒï¼Œæ”»å…‹å¼±çº¹ç†ä¸å¶èŠ‚ç‚¹è¿‡/æ¬ åˆ†å‰²é—®é¢˜ã€‚<br>â€¢ PartReasonerQA: é¢å‘å±‚çº§ç»“æ„ä¸åŠŸèƒ½çš„3Då¤šæ­¥éƒ¨ä»¶æ¨ç†åŸºå‡†â€”â€”åœ¨è®¡æ•°/åˆ†ç±»/å®šä½ä¹‹ä¸Šå¼•å…¥å±‚çº§è·¯å¾„æ¨ç†ã€å¯ä¾›æ€§ä¸åŠŸèƒ½é—®ç­”ï¼Œå¹¶é…å¥—å¯è§£é‡Šè¯„æµ‹ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-14">

    <div class="paper">
        <h2 class="paper-title">Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23828" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23828" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present a comprehensive evaluation of the ability of large language models (LLMs) to process culturally grounded language, specifically to understand and pragmatically use figurative expressions that encode local knowledge and cultural nuance. Using figurative language as a proxy for cultural nuance and local knowledge, we design evaluation tasks for contextual understanding, pragmatic use, and connotation interpretation in Arabic and English. We evaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms, multidialectal Arabic proverbs, and English proverbs. Our results show a consistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower than for English proverbs, and performance for Egyptian idioms is 10.28% lower than for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07% relative to understanding, though providing contextual idiomatic sentences improves accuracy by 10.66%. Models also struggle with connotative meaning, reaching at most 85.58% agreement with human annotators on idioms with 100% inter-annotator agreement. These findings demonstrate that figurative language serves as an effective diagnostic for cultural reasoning: while LLMs can often interpret figurative meaning, they face challenges in using it appropriately. To support future research, we release Kinayat, the first dataset of Egyptian Arabic idioms designed for both figurative understanding and pragmatic use evaluation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç¼ºå°‘å¯¹LLMâ€œè¯­ç”¨èƒ½åŠ›â€çš„ç³»ç»Ÿè¯„æµ‹ï¼šæ—¢æœ‰å·¥ä½œå¤šåœç•™åœ¨èƒ½å¦è§£é‡Šæˆè¯­/è°šè¯­çš„â€œç†è§£â€å±‚é¢ï¼Œæœªè¯„ä¼°åœ¨å…·ä½“è¯­å¢ƒä¸­çš„æ°å½“ä½¿ç”¨ä¸ç¤¾ä¼šé€‚é…ï¼›æœ¬æ–‡é‡åŒ–å‡ºè¯­ç”¨ä½¿ç”¨å‡†ç¡®ç‡è¾ƒç†è§£å¹³å‡ä¸‹é™14.07%ï¼ŒåŠ å…¥ä¸Šä¸‹æ–‡å¯æå‡10.66%ï¼ˆè¡¨3ï¼Œç¬¬8é¡µï¼›å›¾22ï¼Œç¬¬20é¡µï¼‰ã€‚<br>â€¢ è·¨è¯­ç§/æ–¹è¨€æ–‡åŒ–æ¨ç†å·®è·æœªè¢«åˆ»ç”»ï¼šæ¨¡å‹åœ¨è‹±è¯­è°šè¯­ > é˜¿æ‹‰ä¼¯è¯­è°šè¯­ > åŸƒåŠé˜¿æ‹‰ä¼¯è¯­æˆè¯­ä¸Šå‘ˆç¨³å®šå±‚çº§ï¼Œé˜¿è¯­è°šè¯­è¾ƒè‹±è¯­ä½4.29%ï¼ŒåŸƒåŠæˆè¯­è¾ƒé˜¿è¯­è°šè¯­ä½10.28%ï¼ˆæ‘˜è¦ï¼Œç¬¬1é¡µï¼›å›¾2ï¼Œç¬¬6é¡µï¼‰ã€‚<br>â€¢ ç°æœ‰åŸºå‡†å¿½è§†â€œå†…æ¶µ/æ„Ÿæƒ…è‰²å½©â€ç»´åº¦ä¸å…¶ä¸»è§‚æ€§ï¼šå³ä¾¿åœ¨100%äººç±»ä¸€è‡´æ ·æœ¬ä¸Šï¼Œæ¨¡å‹å†…æ¶µåˆ¤å®šæœ€é«˜ä»…85.58%ï¼ˆæˆè¯­ï¼‰ä¸74.04%ï¼ˆè°šè¯­ï¼‰ï¼Œæ˜¾ç¤ºå¯¹éšå«æƒ…æ„Ÿä¸ç¤¾ä¼šè§„èŒƒæŠŠæ¡ä¸è¶³ï¼ˆç¬¬7é¡µï¼›è¡¨11ï¼Œç¬¬21é¡µï¼‰ã€‚<br>â€¢ å¤šé€‰é¢˜æ˜“å—å¼±å¹²æ‰°é¡¹ä¸å¦å®šå¼è®¾å®šå½±å“ï¼šä¼ ç»Ÿé”™è¯¯é€‰é¡¹ä¸å¤Ÿè¿·æƒ‘ï¼Œä¸”åœ¨â€œé€‰é”™ç­”æ¡ˆâ€çš„å¦å®šè®¾å®šä¸‹æ˜¾è‘—é™çº§ï¼ˆæˆè¯­ä»76.29%â†’70.97%ï¼Œè°šè¯­ä»86.57%â†’82.71%ï¼›è¡¨6ï¼Œç¬¬18é¡µï¼‰ï¼Œéš¾ä»¥çœŸå®åŒºåˆ†ç†è§£æ·±åº¦ã€‚<br>â€¢ è®°å¿†åç½®ä¸è¯­æ–™ä¸å‡è¡¡ï¼šè°šè¯­è¡¥å…¨è‹±æ–‡æ˜¾è‘—é«˜äºé˜¿æ–‡ï¼ˆ75.43% vs 10.65%ï¼Œè¡¨7ï¼Œç¬¬18é¡µï¼‰ï¼Œæç¤ºæ¨¡å‹å¯èƒ½ä¾èµ–è‹±è¯­è®°å¿†è€Œéè·¨æ–‡åŒ–æ¨ç†ï¼Œéœ€åŒºåˆ†è®°å¿†ä¸æ¨ç†èƒ½åŠ›ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>ä»¥æˆè¯­/è°šè¯­ä¸ºæ–‡åŒ–è¯­ä¹‰ä»£ç†ï¼Œæ„å»ºè·¨è¯­è¨€ç»Ÿä¸€è¯„æµ‹æ¡†æ¶ï¼ˆç†è§£MCQ/ä¸Šä¸‹æ–‡MCQ/å¦å®šå¼MCQã€è¯­ç”¨ä½¿ç”¨ã€å†…æ¶µåˆ¤å®šã€è°šè¯­è¡¥å…¨ï¼‰ï¼Œå¹¶åœ¨22ä¸ªLLMä¸Šç³»ç»Ÿæµ‹è¯„ï¼›æå‡ºâ€œè¯­ç”¨ä½¿ç”¨â€æ–°ä»»åŠ¡ã€é‡‡ç”¨é€šç”¨+SRLä¸¤ç±»é«˜è´¨å¹²æ‰°é¡¹ç”Ÿæˆä¸äººå·¥å¤æ ¸ï¼Œå‘å¸ƒåŸƒåŠé˜¿æ‹‰ä¼¯è¯­æˆè¯­æ•°æ®é›†Kinayatæ”¯æ’‘ç†è§£ä¸è¯­ç”¨è¯„æµ‹ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ PragmaticsBench-Genï¼šå¤šæ–¹è¨€æˆè¯­çš„è‡ªç”±ç”Ÿæˆè¯­ç”¨è¯„æµ‹ä¸åå¥½å¯¹é½â€”â€”ä»é€‰æ‹©é¢˜æ‰©å±•è‡³å¼€æ”¾å¼ç”Ÿæˆï¼Œæ„å»ºäººç±»åå¥½æ•°æ®ä¸DPO/RLHFå¯¹é½ï¼Œè¯„æµ‹æ°å½“æ€§ã€ç¤¼è²Œä¸å—ä¼—é€‚é…ã€‚<br>â€¢ æ£€ç´¢å¢å¼ºçš„æ–‡åŒ–è®°å¿†ï¼šè·¨æ–¹è¨€è¯­ç”¨é€‚é…çš„RAGä¸å·¥å…·åŒ–è®­ç»ƒâ€”â€”å¼•å…¥åœ°æ–¹çŸ¥è¯†åº“/å…¸æ•…æ£€ç´¢ä¸å¯è§£é‡Šè¯æ®ï¼Œæé«˜è¯­ç”¨ä½¿ç”¨ä¸å†…æ¶µåˆ¤å®šåœ¨ä½èµ„æºæ–¹è¨€ä¸Šçš„è¡¨ç°ä¸å¯æ§æ€§ã€‚<br>â€¢ å¤šè§†è§’å†…æ¶µå»ºæ¨¡ä¸ç¨³å¥å¯¹æŠ—è¯„æµ‹ï¼šåŸºäºè¯­ä¹‰è§’è‰²ä¸å› æœå›¾çš„å¼ºå¹²æ‰°ç”Ÿæˆâ€”â€”é‡‡ç”¨â€œé€è§†å¼â€å¤šç¾¤ä½“æ ‡ç­¾åˆ†å¸ƒï¼Œç»“åˆSRL/å› æœæ‰°åŠ¨è‡ªåŠ¨åŒ–æ„é€ å¼ºå¹²æ‰°ï¼Œç³»ç»Ÿè¡¡é‡è·¨æ–‡åŒ–è¯­ç”¨é²æ£’æ€§ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22264" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22264" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Patent text embeddings enable prior art search, technology landscaping, and patent analysis, yet existing benchmarks inadequately capture patent-specific challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15 tasks across retrieval, classification, paraphrase, and clustering, with 2.06 million examples. PatenTEB employs domain-stratified splits, domain specific hard negative mining, and systematic coverage of asymmetric fragment-to-document matching scenarios absent from general embedding benchmarks. We develop the patembed model family through multi-task training, spanning 67M to 344M parameters with context lengths up to 4096 tokens. External validation shows strong generalization: patembed-base achieves state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445 previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM. Systematic ablations reveal that multi-task training improves external generalization despite minor benchmark costs, and that domain-pretrained initialization provides consistent advantages across task families. All resources will be made available at https://github.com/iliass-y/patenteb. Keywords: patent retrieval, sentence embeddings, multi-task learning, asymmetric retrieval, benchmark evaluation, contrastive learning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç¼ºå°‘è¦†ç›–ä¸“åˆ©çœŸå®éœ€æ±‚çš„åµŒå…¥è¯„æµ‹ï¼šé€šç”¨åŸºå‡†ï¼ˆå¦‚MTEBï¼‰æ— ä¸“åˆ©ä»»åŠ¡ï¼Œç°æœ‰ä¸“åˆ©æ•°æ®é›†å¤šä¸ºå•ä»»åŠ¡æˆ–ç¼ºç³»ç»Ÿåè®®ï¼Œæ— æ³•è¯„ä¼°é•¿æ–‡ã€éå¯¹ç§°ç‰‡æ®µâ†’æ–‡æ¡£åŒ¹é…ä¸è·¨é¢†åŸŸæ£€ç´¢ç­‰å…³é”®åœºæ™¯ï¼ˆè§è®ºæ–‡Â§1â€“Â§2.3ï¼‰ã€‚<br>â€¢ ä¸“åˆ©æ–‡æœ¬ç‰¹æ®Šæ€§å¸¦æ¥å»ºæ¨¡éš¾é¢˜ï¼šæ–‡æ¡£è¶…é•¿ä¸”å¼ºç»“æ„åŒ–ã€æœ¯è¯­å¯†é›†ã€è¯­ä¹‰è§’è‰²ï¼ˆProblem/Effect/Solutionï¼‰æ˜ç¡®ï¼Œè·¨æŠ€æœ¯é¢†åŸŸè¯­ä¹‰è¿ç§»å›°éš¾ï¼ˆÂ§1ï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šPatentSBERTaä¾§é‡åˆ†ç±»ã€PAECTERä¾èµ–å¼•æ–‡å­¦åˆ°æ–‡æ¡£ç›¸ä¼¼ã€BERT-for-Patentsä»…åŸŸé¢„è®­ç»ƒï¼›ç¼ºç»Ÿä¸€å¤šä»»åŠ¡æ¡†æ¶ã€éå¯¹ç§°æ£€ç´¢è¦†ç›–å’Œé•¿ä¸Šä¸‹æ–‡æ”¯æŒï¼ˆÂ§2.2ï¼‰ã€‚<br>â€¢ è¯„æµ‹ä¸è®­ç»ƒæ˜“æ³„æ¼ä¸ä¸è´´è¿‘å®åŠ¡ï¼šç¼ºåŸŸåˆ†å±‚åˆ‡åˆ†ä¸åŸŸæ„ŸçŸ¥ç¡¬è´Ÿä¾‹ï¼Œå¯¼è‡´è¯„æµ‹é«˜ä¼°ä¸ä¸ç¨³å®šï¼ˆÂ§3ï¼ŒFig.1ï¼‰ã€‚<br>â€¢ åŸºå‡†ä¼˜åŒ–ä¸æ³›åŒ–è„±èŠ‚ï¼šå•ä¸€ä»»åŠ¡æœ€ä¼˜å¹¶ä¸ç­‰äºå¤–éƒ¨æ³›åŒ–æœ€ä¼˜ï¼Œå¤šä»»åŠ¡è™½ç•¥é™åŸºå‡†åˆ†ä½†æ˜¾è‘—æå‡å¤–éƒ¨è¡¨ç°ï¼ˆÂ§6.2ï¼ŒÂ§7.1ï¼‰ã€‚<br>â€¢ è·¨é¢†åŸŸæ£€ç´¢æ˜¾è‘—å—æŒ«ï¼šåŒåŸŸ(IN)åˆ°å¼‚åŸŸ(OUT)æ€§èƒ½ä¸‹é™è¾¾3â€“6å€ï¼Œè¯æ±‡/è¯­ä¹‰é¸¿æ²Ÿä»æ˜¯ä¸»è¦ç“¶é¢ˆï¼ˆÂ§6.2ï¼ŒFig.5ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºPatenTEBåŸºå‡†ï¼šæ¶µç›–15ä¸ªä»»åŠ¡ï¼ˆæ£€ç´¢ã€åˆ†ç±»ã€é‡Šä¹‰ã€èšç±»ï¼‰ï¼Œçº¦206ä¸‡æ ·æœ¬ï¼ŒæŒ‰IPC3åŸŸåˆ†å±‚åˆ‡åˆ†ã€é˜²æ³„æ¼ä¸åŸŸæ„ŸçŸ¥ç¡¬è´Ÿä¾‹ï¼Œå¹¶ç³»ç»Ÿè¦†ç›–éå¯¹ç§°ç‰‡æ®µâ†’æ–‡æ¡£åŒ¹é…ã€‚æå‡ºpatembedæ¨¡å‹æ—ï¼ˆ67Mâ€“344Mï¼‰ï¼šä»¥ä¸“åˆ©åŸŸé¢„è®­ç»ƒåˆå§‹åŒ–ï¼Œç»“åˆå¤šä»»åŠ¡å¯¹æ¯”/åˆ†ç±»æŸå¤±ä¸æŒ‡ä»¤åŒ–æç¤ºç»Ÿä¸€è®­ç»ƒï¼Œå¹¶é€šè¿‡çŸ¥è¯†è’¸é¦ä¸é•¿ä¸Šä¸‹æ–‡å˜ä½“é€‚é…ä¸åŒèµ„æºï¼›åœ¨BigPatentèšç±»ä¸DAPFAMæ£€ç´¢ä¸Šå–å¾—SOTAã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ çŸ¥è¯†å¢å¼ºçš„è·¨é¢†åŸŸä¸“åˆ©æ£€ç´¢ï¼šç»“åˆIPCæœ¬ä½“ä¸å¼•æ–‡/åŒæ—å›¾çš„å¯¹æ¯”å­¦ä¹ ä»¥ç¼©å°IN/OUTåŸŸæ€§èƒ½é¸¿æ²Ÿ<br>â€¢ å¤šè¯­ç§ä¸è·¨æ³•åŸŸçš„PatenTEB-XLï¼šæ„å»ºå¤šè¯­ä¸“åˆ©åŸºå‡†ä¸æ¨¡å‹ï¼Œè¯„æµ‹è·¨è¯­æ£€ç´¢ä¸å®¶æ—å¯¹é½èƒ½åŠ›<br>â€¢ è§’è‰²æ¡ä»¶åŒ–çš„éå¯¹ç§°ç‰‡æ®µæ£€ç´¢ï¼šå›´ç»•Problem/Effect/Solutionè®¾è®¡ç»†ç²’åº¦æç¤ºä¸æŸå¤±ä»¥æå‡ç‰‡æ®µâ†’æ–‡æ¡£/ç‰‡æ®µâ†’ç‰‡æ®µåŒ¹é…<br>â€¢ è‡ªé€‚åº”å¤šä»»åŠ¡æƒé‡ä¸æŒç»­å­¦ä¹ çš„ä¸“åˆ©åµŒå…¥ï¼šå¼•å…¥åŠ¨æ€æŸå¤±æƒé‡/å…ƒå­¦ä¹ ä¸æ—¶é—´å¢é‡è®­ç»ƒï¼Œç¼“è§£åŸºå‡†-æ³›åŒ–åå·®ä¸æ—¶é—´æ¼‚ç§»<br>â€¢ å›¾æ–‡å¤šæ¨¡æ€ä¸“åˆ©åµŒå…¥ï¼šèåˆè¯´æ˜ä¹¦å›¾çº¸ä¸æ–‡æœ¬ï¼Œå¢å¼ºæŠ€æœ¯å®ä½“ä¸ç»“æ„åŒ–çŸ¥è¯†å¯¹é½ä»¥æ”¹å–„æ£€ç´¢ä¸èšç±»</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 14</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button><button class="page-btn" onclick="goToPage(11)">11</button><button class="page-btn" onclick="goToPage(12)">12</button><button class="page-btn" onclick="goToPage(13)">13</button><button class="page-btn" onclick="goToPage(14)">14</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-10-29 23:22:13 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 14;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>