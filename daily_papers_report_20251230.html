<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - December 30, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">December 30, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23447" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23447" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Routers in standard MoE lack explicit coupling to expert capabilities, leading to misrouting and limiting overall model performance.<br>â€¢ Gradients from misrouted tokens interfere with expert specialization, preventing experts from developing well-differentiated skills.<br>â€¢ Existing coupling methods rely on dense activations or token-dependent compute (e.g., AoE, norm-supervised routers), incurring high overhead that scales with batch size and number of experts.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce an Expert-Router Coupling (ERC) auxiliary loss that treats each router embedding as a proxy token, feeds perturbed proxies through all experts to collect intermediate activation norms, and penalizes off-diagonal activations exceeding a fraction (Î±) of the diagonal. This enforces mutual alignment between experts and their router embeddings with O(n^2) cost independent of batch size and preserves MoE sparsity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Alpha Scheduling for Controllable Expert Specialization in MoE: Develop training curricula and task-aware schedules for Î± to balance specialization and downstream performance over time.<br>â€¢ Learning Proxy Tokens for ERC: Beyond Perturbed Router Embeddings: Replace bounded multiplicative noise with learned or data-driven proxy vectors that more faithfully represent token clusters while maintaining ERCâ€™s efficiency.<br>â€¢ Subquadratic ERC via Sketching and Selective Probing: Use random projections, top-k expert probing, or matrix sketching to approximate the activation matrix M and reduce ERCâ€™s n^2 evaluations without degrading coupling quality.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23576" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23576" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Many-step, bidirectional video diffusion causes minute-level latency and high cost, blocking real-time interactive generation.<br>â€¢ Existing distillation largely targets text-to-video; multimodal (text+image+audio) conditioning for avatars remains unstable and poorly aligned.<br>â€¢ On-policy DMD (Self Forcing) collapses under multimodal inputs, yielding flicker/black frames due to fragile criticâ€“generator coupling, low-quality conditions, weak ODE init, and a short effective learning window.<br>â€¢ Lack of a practical recipe for low-latency streaming with strong audio-visual sync and stable identity over long horizons.<br>â€¢ Missing benchmarks and evaluation for multi-turn coherence in real-time multimodal avatar systems.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>An improved on-policy distillation recipe for multimodal video diffusion: curate high-quality conditions (better reference images, motion-focused text), train ODE initialization to convergence, and use aggressive DMD (higher learning rates, tuned CFG) to achieve stable few-step block-wise autoregressive generation. Integrated with clean KV prefilling (sink+rolling tokens) and a training-free AHIS identity mechanism, LiveTalk produces synchronized, high-fidelity avatar video in real time (4 steps per block, ~20Ã— faster).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Stability-Aware Multimodal DMD: Adaptive critics, guidance schedules, and curriculum on condition quality to provably stabilize on-policy distillation under multimodal inputs.<br>â€¢ End-to-End Co-Training of Audio LMs and Video Diffusion for Unified Talkerâ€“Performer: Joint optimization of speech generation and visual rendering to improve lip-sync, prosody-to-gesture mapping, and latency.<br>â€¢ Personalized Long-Horizon Avatars via Learnable Identity Anchors: Extend AHIS with trainable anchors and continual personalization to preserve identity across minutes-to-hours and scene changes.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Yume-1.5: A Text-Controlled Interactive World Generation Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22096" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22096" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Real-time interactive world generation with diffusion models is hampered by large parameter counts and long sampling schedules, causing high latency that prevents continuous, responsive exploration.<br>â€¢ Historical context grows unbounded during autoregressive generation, leading to exploding memory/computation; existing sliding-window or naive truncation loses long-term consistency, while prior caches/streaming approaches are not optimized for keyboard-controlled settings.<br>â€¢ Prior methods are largely trained on game-like data, creating a domain gap that hurts realism in dynamic urban scenes and real-world exploration tasks.<br>â€¢ Lack of text-controlled event generation and limited control modalitiesâ€”most support only image-conditioned video and low-level keyboard/mouse control without semantic event triggering.<br>â€¢ Camera control in video generation often requires explicit trajectory sequences or pose parameters, which are cumbersome and not intuitive for users seeking simple, discrete keyboard inputs.<br>â€¢ Training-inference mismatch and error accumulation in long-form autoregressive diffusion (fewer steps worsen drift) degrade visual quality and temporal coherence over time.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Yume1.5 is an autoregressive text-/image-conditioned video diffusion framework for keyboard-controlled exploration that introduces joint Temporalâ€“Spatialâ€“Channel Modeling (TSCM) with linear attention to compress and fuse long histories, and a Self-Forcingâ€“inspired teacherâ€“student distillation to enable few-step, real-time sampling while mitigating error accumulation; it further decomposes text into cached action embeddings and event descriptions to support text-driven event generation and editing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ TSCM++: Learnable Adaptive Temporalâ€“Spatialâ€“Channel Compression for Infinite-Context Video Diffusion: Replace rule-based downsampling with a learnable, content-aware compressor/retriever to improve long-term coherence under fixed compute.<br>â€¢ 3D-Consistent Text-Controlled World Generation via NeRF-Augmented Diffusion: Fuse diffusion with 3D radiance/gaussian fields for persistent geometry and view-consistent long-horizon exploration.<br>â€¢ Event-Driven World Generation with Causal Reasoning and Safety Constraints: Integrate causal/logic models to trigger, sequence, and constrain text-specified events with controllable risk and realism.<br>â€¢ Multi-Agent Interactive World Generation with Language and Keyboard Controls: Extend Yume-style control to multiple controllable agents and camera, coordinating via natural language and discrete actions.<br>â€¢ On-Device Real-Time World Generation via Quantized Few-Step Video Diffusion: Combine distribution-matching distillation, low-rank adapters, and quantization/pruning to run interactive worlds on edge GPUs.<br>â€¢ Benchmarking Text-Controlled Interactive World Generators: Tasks, Metrics, and Datasets: Define standardized tasks and metrics for responsiveness, coherence, event fidelity, and long-term memory in interactive settings.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22322" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22322" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Passive, post-hoc verification in agentic RL requires reviewing full, noisy trajectories, leading to high cost/latency and low reliability (hallucinations, false positives).<br>â€¢ Rule-based scripts or outcome reward models depend on task-specific engineering and access to ground-truth states, making them labor-intensive and unscalable for open-ended GUI tasks.<br>â€¢ Existing agents are verification-agnostic and decoupled from judgment, missing the opportunity to use in-situ environment access to proactively prove success.<br>â€¢ Long-context VLM/LLM-as-a-Judge verification strains reasoning, suffers from context rot, and complicates reward design for RL.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SmartSnap introduces a Self-Verifying Agent that proactively curates minimal, decisive evidence as actionâ€“observation tuples guided by the 3C principles (Completeness, Conciseness, Creativity), and trains with GRPO using a structured, dense reward from an LLM verifier to jointly optimize task execution and evidence quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Self-Verifying Agents Beyond Mobile: Extending SmartSnap to Web and Desktop Benchmarks: Adapt the evidence curation and reward shaping to heterogeneous environments (e.g., WebShop, WebArena) with scalable sandbox infrastructure.<br>â€¢ Domain-Adaptive Pretraining for Evidence-Seeking Agents: Use continual pretraining and targeted SFT to inject mobile/web-specific knowledge that enhances RL effectiveness and evidence creativity.<br>â€¢ Learning to Plan Evidence-Oriented Actions: A Hierarchical Controller for 3C Curation: Develop meta-policies that decide when and how to generate additional evidence, balancing completeness and conciseness.<br>â€¢ Multimodal Verifier Design with Uncertainty-Aware, Traceable Reasoning: Build VLM-based verifiers that enforce zero-assumption, evidence-traceable analysis with calibration and voting to reduce hallucinations.<br>â€¢ Efficient Sandbox RL for Self-Verification: Systems and Algorithms for High-Concurrency Training: Create parallelized, sample-efficient RL pipelines (e.g., off-policy, model-based) to scale SmartSnap across large task suites.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23705" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23705" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: "Diffusion knows transparency." Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Depth sensing breaks down on transparent/reflective objects (refraction, reflection, transmission), causing missing regions and unstable estimates in stereo, ToF, and discriminative monocular methods.<br>â€¢ Existing datasets for transparency are limited in diversity, leading to overfitting and poor real-world generalization.<br>â€¢ Single-frame approaches lack temporal consistency across videos, which is critical for robotics tasks requiring stable 3D perception.<br>â€¢ Training robust video models without real-world labels is challenging; adapting generative priors risks catastrophic forgetting.<br>â€¢ Robotics manipulation in dynamic, unstructured environments demands temporally coherent depth/normal estimation under transparency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Repurpose a large pre-trained video diffusion model (WAN) via lightweight LoRA to perform video-to-video translation from RGB to depth/normals by concatenating RGB and (noisy) depth latents within the DiT backbone and co-training on the new synthetic TransPhy3D video dataset plus existing image datasets, yielding temporally consistent predictions for arbitrary-length videos.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Transparency-Aware Generative Depth: Joint Estimation of Depth and Material Optics in Video Diffusion: Extend DKT to jointly predict depth and material properties (e.g., refractive index, roughness) to better resolve transparency-induced ambiguities.<br>â€¢ Self-Supervised Real-World Adaptation of Video Diffusion Depth via Multi-View Consistency and Physics Priors: Adapt DKT on unlabeled real videos using geometric constraints (multi-view, SLAM), temporal smoothness, and physics-based priors to close the sim-to-real gap.<br>â€¢ Sensor-Fusion DKT: Integrating Sparse ToF/Stereo Signals with Video Diffusion Priors for Robust Transparent Object Perception: Fuse sparse or noisy sensor depth with diffusion-based predictions to improve accuracy and reliability across challenging materials.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23709" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23709" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Diffusion-based VSR achieves strong perceptual quality but is impractical for online use due to expensive multi-step denoising and reliance on future frames, causing extreme initial latency.<br>â€¢ Real-time applications (e.g., live streaming, conferencing, AR/VR, game engines) require low per-frame runtime and minimal end-to-end latency, which current diffusion methods cannot meet.<br>â€¢ CNN/Transformer online VSR models are efficient but lag in perceptual detail and temporal coherence compared to diffusion models.<br>â€¢ Existing diffusion VSRs are offline/bidirectional and cannot operate causally on past-only inputs, limiting deployment in latency-sensitive, streaming scenarios.<br>â€¢ Need for a streamable diffusion framework that preserves perceptual quality while enforcing strict causality and drastically reducing denoising steps.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Stream-DiffVSR is a strictly causal, autoregressive diffusion VSR that distills a 50-step process into a 4-step denoiser and augments it with Auto-regressive Temporal Guidance (ARTG) during latent denoising plus a lightweight temporal-aware decoder with a Temporal Processor Module (TPM). This design exploits only past frames to inject motion-aligned cues, enhancing temporal coherence and perceptual fidelity while achieving low latency suitable for online deployment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Toward Lookahead-Bounded Diffusion VSR: Trading One-Frame Lookahead for Further Quality Gains Under Tight Latency Budgets: Explore minimal lookahead (e.g., 1â€“2 frames) to balance quality and latency in a streaming diffusion framework.<br>â€¢ Adaptive-Step Stream Diffusion for Video Restoration: Early-Exit and Content-Aware Denoising: Develop content-adaptive or early-exit schedulers that vary denoising steps per frame/region to further cut latency without degrading quality.<br>â€¢ Motion-Aware Guidance in Autoregressive Diffusion via Learnable Optical Flow and Confidence Modeling: Integrate learnable flow and uncertainty into ARTG to improve motion alignment and robustness under fast or nonrigid motion.<br>â€¢ Real-World Streamable Diffusion VSR via Degradation-Aware Domain Adaptation: Extend to unknown real-world degradations using synthetic-to-real adaptation and degradation-aware conditioning while keeping causal, low-latency constraints.<br>â€¢ Hardware-Aware Stream-DiffVSR: Quantization, Pruning, and Kernel Fusion for Edge GPUs: Co-design model compression and kernel-level optimizations to deploy causal diffusion VSR on resource-constrained devices with strict latency ceilings.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22615" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22615" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as Ï€_0 and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Autoregressive VLM/VLA backbones struggle with long-horizon visual planning and global reasoning due to next-token training and error accumulation.<br>â€¢ Sequential decoding in AR models is inefficient for robotics, requiring architectural changes for action chunking and leading to slower convergence and brittle long-chunk generation.<br>â€¢ Existing diffusion-based VLMs underperform (small data, unstable training), leaving a gap for open, well-aligned dVLM/VLA backbones that leverage bidirectional attention and parallel decoding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Dream-VL and Dream-VLA are built on a diffusion language model (Dream-7B) with bidirectional masked diffusion, aligning a Qwen2ViT vision encoder and text to enable iterative global plan refinement and parallel, chunked action generation. Dream-VLA adds large-scale robotic pretraining on 970k trajectories and supports flexible finetuning objectives (e.g., flow matching) without architectural changes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Scaling Diffusion Vision-Language-Action Models: Data and Model Size Laws for Planning and Control: Systematically study scaling laws for dVLM/dVLA with larger backbones and multimodal data to close gaps with top AR models on general VL tasks while preserving planning gains.<br>â€¢ Unified Diffusion for Language and Continuous Control: Bridging Discrete Masked Diffusion and Flow Matching: Develop a single diffusion framework that jointly models discrete language tokens and continuous robot actions, with adaptive timestep schedules for robust long-horizon control.<br>â€¢ Planning-Centric dVLMs with Tool Use and Symbolic Executors: Integrating Structured Planning, Verification, and Environment Interaction: Augment dVLMs with tool-call APIs, symbolic planners, and iterative self-check to improve multi-step visual grounding and executable plan reliability.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SpotEdit: Selective Region Editing in Diffusion Transformers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22323" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22323" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Full-image regeneration in DiT-based editing uniformly denoises all tokens each step, wasting compute and introducing artifacts in regions that should remain unchanged.<br>â€¢ Lack of an automatic, mask-free way to detect non-edited regions during inference leads to unnecessary processing and degraded background fidelity.<br>â€¢ Existing acceleration methods treat all tokens uniformly (full-token/step reduction), often harming fidelity in semantically important edited areas and failing to exploit editing sparsity.<br>â€¢ Skipping token updates breaks contextual coherence unless temporal consistency is maintained, risking attention drift and boundary artifacts.<br>â€¢ Empirical observation: non-edited regions converge early in diffusion, suggesting selective computation can preserve quality while reducing cost.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SpotEdit is a training-free, region-aware editing framework that uses SpotSelector to identify non-edited tokens via LPIPS-like perceptual scores between reconstructed x0 and the condition image, skipping their DiT computation and reusing condition features. SpotFusion preserves contextual coherence by dynamically blending cached non-edited and condition-image KV features with a cosine schedule and partial attention, so only edited tokens are propagated while full context is maintained.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learnable SpotSelector: End-to-End Token Routing for DiT Editing: Train a token-level classifier to predict edit vs. preserve decisions, improving robustness across prompts, models, and domains beyond heuristic thresholds.<br>â€¢ SpotEdit-Video: Temporally Consistent Region-Aware Editing for Diffusion Transformers: Extend selective token updates to video with temporal KV fusion and flow-guided stabilization to maintain cross-frame coherence.<br>â€¢ Region-Aware Distillation for Efficient DiTs: Distilling Selective Computation into Compact Editors: Distill SpotEditâ€™s selective behavior into a lightweight model that natively skips non-edited tokens with minimal auxiliary caches.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.15560" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.15560" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Notably, under our experimental setup, compared with training a diffusion model from scratch, evaluating with TED-6K is about 750times faster. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our TED-6K dataset and evaluation code are available at the following link: https://anonymous.4open.science/r/GRAN-TED-4FCC/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of an efficient, reliable evaluation framework for text encoders that predicts downstream T2I/T2V generation quality without training full diffusion models<br>â€¢ Difficulty in adapting pretrained LLM/MLLM representations into precise, unambiguous, and DiT-consumable embeddings for visual synthesis<br>â€¢ Misalignment of existing proxies (retrieval single-vector metrics, NLP reasoning benchmarks, QA accuracy) with sequence-level conditioning and visual alignment needs in diffusion models<br>â€¢ Persistent prompt fidelity failures (object counting, relations, attribute binding, negative constraints) despite stronger LLM backbones, indicating insufficient representation quality and fusion strategies</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce TED-6K, a text-only benchmark with a unified sentence-level context aggregator to efficiently and fairly assess text encoders for diffusion models, and propose GRAN-TED, a two-stage encoder (finetuning Qwen3-VL-8B-Instruct plus layer-wise weighting fusion) that produces robust, aligned, and nuanced embeddings correlated with downstream T2I/T2V performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Beyond TED-6K: A Large-Scale Text-Only Diagnostic Suite for Compositional and Aesthetic Alignment in Diffusion Conditioning: Extend TED-6K to broader semantics (style, aesthetics, commonsense) and analyze correlation with diverse generative tasks<br>â€¢ Adaptive Layer-Wise Fusion for Text Encoders in Diffusion Models: Learn prompt- and task-conditioned dynamic weighting over encoder layers to maximize alignment under varying scene complexities<br>â€¢ Cross-Modal Consistency Regularization for Text Encoders: Jointly train encoders with synthetic image/video feedback to enforce consistency between text embeddings and visual outcomes without full end-to-end retraining<br>â€¢ Universal Context Aggregators for Heterogeneous Encoders: Design architecture-agnostic adapters that bridge CLIP/T5/LLM/MLLM tokenization and hidden states to standardized conditioning for DiTs<br>â€¢ Negative-Constraint-Aware Embedding Optimization for Diffusion: Develop training objectives that explicitly encode negation and exclusion semantics to reduce attribute-binding and constraint-violation errors in generation</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Act2Goal: From World Model To General Goal-conditioned Policy</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23541" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23541" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control. Given a current observation and a target visual goal, the world model generates a plausible sequence of intermediate visual states that captures long-horizon structure. To translate this visual plan into robust execution, we introduce Multi-Scale Temporal Hashing (MSTH), which decomposes the imagined trajectory into dense proximal frames for fine-grained closed-loop control and sparse distal frames that anchor global task consistency. The policy couples these representations with motor control through end-to-end cross-attention, enabling coherent long-horizon behavior while remaining reactive to local disturbances. Act2Goal achieves strong zero-shot generalization to novel objects, spatial layouts, and environments. We further enable reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision. Real-robot experiments demonstrate that Act2Goal improves success rates from 30% to 90% on challenging out-of-distribution tasks within minutes of autonomous interaction, validating that goal-conditioned world models with multi-scale temporal control provide structured guidance necessary for robust long-horizon manipulation. Project page: https://act2goal.github.io/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Long-horizon goal-conditioned manipulation is brittle because standard GCPs predict single-step actions without modeling intermediate progress or feasibility.<br>â€¢ Natural language goals are flexible but lack the precision needed for fine-grained, multi-stage manipulation; visual goals are precise yet underexploited in policy learning.<br>â€¢ Policies trained on narrow demonstrations overfit local stateâ€“action correlations and need dense supervision, leading to poor generalization in out-of-distribution tasks.<br>â€¢ Existing controllers struggle to balance global goal consistency with local reactive controlâ€”full-trajectory planning is brittle, while short-horizon control loses alignment over time.<br>â€¢ There is a lack of reward-free online adaptation mechanisms that enable autonomous improvement post-deployment without external rewards or human annotations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Act2Goal couples a goal-conditioned visual world model that imagines intermediate visual states with Multi-Scale Temporal Hashing (dense proximal frames for fine-grained control and sparse distal frames for global consistency), and an action expert uses cross-attention and flow matching to generate coherent multi-scale actions. It is trained offline in two stages (joint flow matching, then end-to-end action-focused finetuning) and adapts online via HER-style hindsight goal relabeling with lightweight LoRA finetuning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Multi-Scale Temporal Hashing: Learning Horizon-Aware Temporal Decompositions for Goal-Conditioned Control: Automatically learn MSTH parameters to adapt proximal/distal sampling to task difficulty and state uncertainty, improving efficiency and robustness.<br>â€¢ Uncertainty-Aware Act2Goal: Probabilistic World Models and Risk-Sensitive Control for Robust Long-Horizon Manipulation: Incorporate uncertainty estimation in imagined trajectories and confidence-weighted action generation to handle stochastic dynamics and safety constraints.<br>â€¢ Language-to-Goal Act2Goal: Generating Visual Goal Trajectories from Natural Language for Open-Ended Tasks: Translate natural language into visual goal images or keyframe sequences, unifying instruction-following with precise visual planning.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Web World Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23676" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23676" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Language agents lack persistent, controllable environments to act, remember, and learn over time.<br>â€¢ Conventional web frameworks are reliable but bounded by pre-defined schemas and database content, limiting open-ended exploration.<br>â€¢ Fully generative world models offer unlimited context but reduce controllability, are hard to debug/scale, and struggle with deterministic, globally consistent state.<br>â€¢ Opaque latent representations hinder observability and tooling; there is a need for typed, debuggable state shared between code and models.<br>â€¢ Scaling infinite worlds requires object permanence without heavy storage and graceful operation under LLM latency/failure.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Web World Models implement world â€˜physicsâ€™ and state transitions in deterministic web code with typed JSON interfaces, while LLMs generate constrained narrative/context on top. Deterministic hashing seeds just-in-time generation for object permanence, and systems degrade gracefully from live generation to cached or templated content when resources are limited.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Typed World Models for Multi-Agent Coordination: Extend WWMs to multi-user, multi-agent settings with conflict resolution, concurrency control, and shared typed state.<br>â€¢ Formal Guarantees for Deterministic Procedural Generation: Analyze and verify hashing/seed schemes for consistency, replayability, and versioning under evolving schemas.<br>â€¢ Measuring Controllability vs. Creativity in WWMs: Define benchmarks and metrics quantifying logical consistency, repeatability, and semantic richness across domains.<br>â€¢ Policy-Safe Imagination: Sandboxing and Governance for LLM-Driven Worlds: Enforce safety, permissions, and capability bounds via typed contracts and runtime policy checks.<br>â€¢ Learning the Physics Layer: Program Synthesis of Rule Engines from Interaction Data: Train models to propose or refine code-level rules that pass type checks and regression tests.<br>â€¢ Cross-Modal Web World Models: Integrating 3D, audio, and UI assets under Typed Interfaces: Generalize typed contracts beyond text to controllable multi-modal generation.<br>â€¢ Graceful Degradation Strategies for LLM-Augmented Systems: Optimize fallback policies, caching, and template design for cost-latency-quality trade-offs in live WWMs.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DiRL: An Efficient Post-Training Framework for Diffusion Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22234" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22234" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remains underdeveloped. Existing methods suffer from computational inefficiency and objective mismatches between training and inference, severely limiting performance on complex reasoning tasks such as mathematics. To address this, we introduce DiRL, an efficient post-training framework that tightly integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. This architecture enables a streamlined online model update loop, facilitating efficient two-stage post-training (Supervised Fine-Tuning followed by Reinforcement Learning). Building on this framework, we propose DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation tailored for dLLMs. We validate our approach by training DiRL-8B-Instruct on high-quality math data. Our model achieves state-of-the-art math performance among dLLMs and surpasses comparable models in the Qwen2.5 series on several benchmarks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Post-training (especially RL) for diffusion LMs (dLLMs) is underdeveloped, limiting their performance on complex reasoning tasks like mathematics.<br>â€¢ Existing methods are computationally inefficient and suffer from trainingâ€“inference objective mismatch.<br>â€¢ Fully bidirectional dLLMs cannot compute exact logits/policies; naive noise injection yields biased logits and misaligned training.<br>â€¢ The absence of KV cache in dLLM RL exacerbates rollout costs and slows optimization.<br>â€¢ Prior work lacks an inference-engine backend, efficient trainingâ€“inference co-design, and fast online rollouts needed for practical GRPO.<br>â€¢ Blockwise dLLMs enable exact logits but do not fully resolve trainâ€“inference consistency or RL efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DiRL integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference to enable consistent, efficient two-stage post-training (SFT followed by RL) with online model updates. It introduces DiPO, the first unbiased GRPO for dLLMs, leveraging exact blockwise logits for fast, scalable rollouts and policy optimization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Unbiased Policy Optimization for Fully Bidirectional Diffusion Language Models: Extend DiPO beyond blockwise models by devising mechanisms to obtain unbiased logits/policies or consistent surrogate objectives for unconstrained generation.<br>â€¢ Cache-Efficient RL for Diffusion LMs via Learned Memory and Block Caching: Design KV-like memory or block-level caching tailored to dLLMs to reduce rollout costs and accelerate training without compromising unbiasedness.<br>â€¢ DiRL-MM: A Unified Post-Training Framework for Multimodal and Long-Context Diffusion LMs: Generalize DiRL to multimodal inputs and long-context settings, co-designing attention, inference, and RL pipelines for consistent objectives and scalable efficiency.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">Training AI Co-Scientists Using Rubric Rewards</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23707" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23707" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Language models struggle to generate rigorous research plans that satisfy explicit constraints and implicit requirements due to the open-ended nature of scientific planning.<br>â€¢ Verification via executing experiments is slow, expensive, and often infeasible (e.g., in medical domains), limiting the availability of rapid training feedback.<br>â€¢ Dominant AI-for-Science paradigms rely on simulators and large-scale trial-and-error, which lack generality and raise ethical/resource concerns in real-world settings.<br>â€¢ There is a need for scalable, automated supervision signals to improve plan quality without relying on human graders or execution feedback.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Automatically extract research goals and goal-specific grading rubrics from existing papers, then finetune a plan-generation model via reinforcement learning with self-grading: a frozen copy of the initial policy acts as a rubric-aware verifier to reward the training policy's plans, creating a generatorâ€“verifier gap that enables improvement without external supervision.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Dynamic Multi-Objective Rubric Rewards for Scientific Plan Generation: Learn adaptive, goal-specific rubrics that capture trade-offs among rigor, feasibility, novelty, and ethical constraints to better guide open-ended planning.<br>â€¢ Human-AI Co-Grading for Research Plan RL: Integrate sparse expert feedback to calibrate and audit the self-grader, reducing grading drift and improving reliability across domains.<br>â€¢ Multimodal Rubric RL for Experimental Sciences: Extend rubric extraction and grading to figures, protocols, datasets, and code to evaluate and train multimodal research plans.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Video-BrowseComp: Benchmarking Agentic Video Research on Open Web</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23044" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23044" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The evolution of autonomous agents is redefining information seeking, transitioning from passive retrieval to proactive, open-ended web research. However, while textual and static multimodal agents have seen rapid progress, a significant modality gap remains in processing the web's most dynamic modality: video. Existing video benchmarks predominantly focus on passive perception, feeding curated clips to models without requiring external retrieval. They fail to evaluate agentic video research, which necessitates actively interrogating video timelines, cross-referencing dispersed evidence, and verifying claims against the open web. To bridge this gap, we present Video-BrowseComp, a challenging benchmark comprising 210 questions tailored for open-web agentic video reasoning. Unlike prior benchmarks, Video-BrowseComp enforces a mandatory dependency on temporal visual evidence, ensuring that answers cannot be derived solely through text search but require navigating video timelines to verify external claims. Our evaluation of state-of-the-art models reveals a critical bottleneck: even advanced search-augmented models like GPT-5.1 (w/ Search) achieve only 15.24\% accuracy. Our analysis reveals that these models largely rely on textual proxies, excelling in metadata-rich domains (e.g., TV shows with plot summaries) but collapsing in metadata-sparse, dynamic environments (e.g., sports, gameplay) where visual grounding is essential. As the first open-web video research benchmark, Video-BrowseComp advances the field beyond passive perception toward proactive video reasoning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Dynamic video on the open web is a modality blind spot for research agents; current systems are optimized for text and static images, not time-evolving visual evidence.<br>â€¢ Existing video benchmarks emphasize passive perception with curated clips and no external retrieval, failing to test active browsing, temporal grounding, and open-web verification.<br>â€¢ Web-browsing agents typically treat videos as static metadata, lacking native capabilities for timeline navigation, multi-step planning, and cross-source reasoning across multiple videos.<br>â€¢ Evaluation protocols often allow text-only shortcuts; there is a need for mandatory video dependency, unique answers, and objective short-form verification to ensure genuine visual grounding.<br>â€¢ Empirical bottleneck: even search-augmented SOTA models (e.g., GPT-5.1 w/ Search) achieve low accuracy (~15%), revealing over-reliance on textual proxies and poor performance in metadata-sparse, dynamic domains (e.g., sports, gameplay).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce Video-BrowseComp, a 210-question benchmark for open-web agentic video reasoning that mandates temporal visual evidence, uses short, objective answers with structured verification, spans 8 genres, and stratifies difficulty into three levels (explicit retrieval, implicit retrieval with entity grounding, and cross-source multi-hop reasoning). Evaluate tool-free and search-augmented models to diagnose reliance on textual proxies and gaps in temporal grounding and multi-hop video research.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning to Browse Videos with Tools: Training Agents for Temporal Navigation and Evidence Verification: Develop tool-integrated agents that control video players (seek, pause, read subtitles) and perform multi-hop cross-video verification.<br>â€¢ From Textual Proxies to Visual Grounding in Open-Web Video Research: Objectives and Data for Robust Temporal and Entity Alignment: Design training signals and datasets that penalize metadata shortcuts and reward spatio-temporal grounding across diverse, dynamic domains.<br>â€¢ Planning with Timelines: Hierarchical Policies for Cross-Source, Long-Horizon Video Reasoning: Create hierarchical planners that decompose queries into search, candidate filtering, timestamp localization, and cross-video aggregation with memory over long contexts.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23646" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23646" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ End-to-end OmniLLMs struggle with fine-grained cross-modal understanding due to difficult audioâ€“video alignment and fixed token budgets that compress visual detail.<br>â€¢ Static or caption-heavy agent workflows are inflexible, costly, and often miss query-relevant cues, leading to noisy or irrelevant context.<br>â€¢ Long-form video analysis is computationally prohibitive without precise event localization; exhaustive visual scanning is inefficient.<br>â€¢ Audio provides concise and reliable temporal cues, yet existing systems underutilize it for guiding fine-grained video reasoning.<br>â€¢ Lack of dynamic planning, reflection, and modality-aware attention allocation prevents accurate, low-cost multimodal comprehension.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>OmniAgent is an audio-guided active perception agent that runs a Thinkâ€“Actâ€“Observeâ€“Reflect loop to dynamically invoke modality-specific tools (video global/clip QA, audio ASR/caption/QA, audio-based event list/location) in a coarse-to-fine mannerâ€”using audio to localize salient times, then performing high-resolution visual inspection and cross-modal consistency checks to produce accurate answers.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ End-to-End Trainable OmniAgent with Tool Self-Calling and KV Memory: Integrate the toolset into a single omnimodal agent that learns when and how to attend, reducing latency and external dependencies via persistent memory and self-invocation.<br>â€¢ Audio-Guided Event Localization Networks: Learnable Temporal Anchoring for Multimodal Agents: Replace heuristic event tools with trainable audio-temporal localization modules that robustly anchor visual inspection windows under diverse acoustic conditions.<br>â€¢ Reinforcement Learning for Adaptive Modality Attention in Active Audio-Video Agents: Optimize planning and tool invocation policies (listen vs. watch, resolution/FPS) under compute budgets to maximize accuracy and efficiency through reward-driven training.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22342" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22342" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In most existing embodied navigation tasks, instructions are well-defined and unambiguous, such as instruction following and object searching. Under this idealized setting, agents are required solely to produce effective navigation outputs conditioned on vision and language inputs. However, real-world navigation instructions are often vague and ambiguous, requiring the agent to resolve uncertainty and infer user intent through active dialog. To address this gap, we propose Interactive Instance Object Navigation (IION), a task that requires agents not only to generate navigation actions but also to produce language outputs via active dialog, thereby aligning more closely with practical settings. IION extends Instance Object Navigation (ION) by allowing agents to freely consult an oracle in natural language while navigating. Building on this task, we present the Vision Language-Language Navigation (VL-LN) benchmark, which provides a large-scale, automatically generated dataset and a comprehensive evaluation protocol for training and assessing dialog-enabled navigation models. VL-LN comprises over 41k long-horizon dialog-augmented trajectories for training and an automatic evaluation protocol with an oracle capable of responding to agent queries. Using this benchmark, we train a navigation model equipped with dialog capabilities and show that it achieves significant improvements over the baselines. Extensive experiments and analyses further demonstrate the effectiveness and reliability of VL-LN for advancing research on dialog-enabled embodied navigation. Code and dataset: https://0309hws.github.io/VL-LN.github.io/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Real-world navigation instructions are often vague and ambiguous, requiring agents to clarify goals via active dialogs rather than passively following fully specified commands.<br>â€¢ Existing interactive benchmarks are small-scale (often room-level), lack large-scale training data, and do not evaluate proactive, targeted question-asking needed for long-horizon, house-scale exploration.<br>â€¢ Agents struggle with instance-level disambiguation and long-horizon exploration; attributeâ€“image alignment is a major bottleneck, and current detectors/policies frequently misidentify targets among same-category distractors.<br>â€¢ There is no unified, scalable agentâ€“oracle framework that supports diverse query types (attribute, route, disambiguation) and enables automatic online evaluation of dialog utility.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces the IION task and the VL-LN benchmark, including an automated pipeline that aggregates MMScan annotations to house-level metadata, instantiates episodes, and collects ~41k dialog-rich trajectories with a frontier-based navigator and scripted GPT-4o+rule oracle answering attribute, route, and disambiguation queries, alongside an MSP metric for dialog utility. Using this benchmark, the authors train a dialog-enabled navigation model that significantly improves IION/ION performance over baselines.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ AttributeGround: Hard-Negative Contrastive Learning for Fine-Grained Attributeâ€“Image Alignment in Instance Navigation: Train with same-category hard negatives and multi-attribute grounding to reduce detection and ambiguity failures.<br>â€¢ Ask to Explore: Information-Theoretic Question Policies for Long-Horizon Navigation: Learn when and what to ask via POMDP/active learning to maximize expected success progress and minimize candidate-set entropy.<br>â€¢ Language-to-Route: End-to-End Grounding of Short-Route Instructions into Low-Level Control: Align natural-language route guidance with action execution using multimodal pretraining and 3D scene context to improve exploration efficiency.<br>â€¢ Oracle-Free Dialog via Self-Play and Scene Graph World Models: Learn an internal oracle that infers attributes and relations from memory and scene graphs, reducing reliance on external oracles and enabling real-world deployment.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Nested Browser-Use Learning for Agentic Information Seeking</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23647" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23647" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Information-seeking (IS) agents have achieved strong performance across a range of wide and deep search tasks, yet their tool use remains largely restricted to API-level snippet retrieval and URL-based page fetching, limiting access to the richer information available through real browsing. While full browser interaction could unlock deeper capabilities, its fine-grained control and verbose page content returns introduce substantial complexity for ReAct-style function-calling agents. To bridge this gap, we propose Nested Browser-Use Learning (NestBrowse), which introduces a minimal and complete browser-action framework that decouples interaction control from page exploration through a nested structure. This design simplifies agentic reasoning while enabling effective deep-web information acquisition. Empirical results on challenging deep IS benchmarks demonstrate that NestBrowse offers clear benefits in practice. Further in-depth analyses underscore its efficiency and flexibility.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing IS agents are constrained to search and static URL fetch, missing dynamic browser-mediated information (client-side rendering, forms, multi-step navigation), which makes web access incomplete for deep tasks.<br>â€¢ Full browser interaction is complex for ReAct-style agents; raw pages are verbose and redundant, exceeding context limits and degrading reasoning, and there is no standard, low-complexity browser action interface.<br>â€¢ IS agents need a framework that decouples reasoning from intra-page exploration to inject only goal-relevant content under practical context constraints, enabling efficient deep-web acquisition.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>NestBrowse introduces a minimal, complete browser toolkit (search, visit, click, fill) and a nested framework that decouples outer-loop agentic reasoning from inner-loop goal-driven page exploration, returning compact useful-info workspaces. These capabilities are trained jointly via weighted multi-task imitation learning with rejection-sampled trajectories.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Beyond Text: Multimodal Nested Browser-Use for Agentic Web Reasoning: Extend NestBrowse to incorporate visual/audio modalities and DOMâ€“vision alignment to handle non-textual UI elements and media-rich pages.<br>â€¢ Adaptive Inner-Loop Exploration via Reinforcement Learning: Learn policies for segment selection, extraction granularity, and stopping to optimize token efficiency and recall under budget constraints.<br>â€¢ Standardized Browser Action APIs and Benchmarks for Information-Seeking Agents: Establish a community-standard minimal browser toolkit and evaluation suite to compare abstractions, measure context efficiency, robustness, and cross-lingual generalization.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23273" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23273" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ YOLO-like real-time detectors use static dense computation, over-processing easy scenes and under-serving complex, crowded ones, leading to inefficiency and suboptimal accuracy.<br>â€¢ The accuracyâ€“latency trade-off is fixed at design time; models lack instance-conditional resource allocation to adapt capacity to input complexity.<br>â€¢ Attention and transformer-based alternatives remain fundamentally dense, while prior MoE efforts focus on classification or heavy ViT detectors, making them unsuitable for real-time object detection.<br>â€¢ Training MoE in dense prediction can suffer expert collapse and unstable routing without tailored load balancing and phased sparsity mechanisms.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>YOLO-Master integrates an Efficient Sparse Mixture-of-Experts (ES-MoE) primarily into the YOLO backbone, using a lightweight GAP-driven gating network with soft Top-K routing during training and hard Top-K routing at inference to selectively activate depthwise-separable convolution experts with diverse kernel sizes. A load-balancing loss complements standard detection losses to encourage expert diversity and stable training, yielding adaptive computation that improves mAP while reducing latency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Region-Conditional MoE for Fine-Grained Real-Time Detection: Introduce spatially localized gating so different image regions activate distinct experts, boosting performance in dense and small-object scenarios.<br>â€¢ Hardware-Co-Designed Sparse MoE for Edge YOLO: Co-optimize routing sparsity, expert kernels, and accelerator scheduling to maximize throughput and energy efficiency on edge devices.<br>â€¢ Temporal MoE Routing for Video Object Detection: Learn frame- and motion-aware gating policies that adapt expert activation over time, improving stability and accuracy in streaming real-time detection.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23162" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23162" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action labels, preventing direct application of imitation learning or VLA training. In this work, we aim to alleviate this problem by learning policy models from SurgWorld, a world model designed for surgical physical AI. We curated the Surgical Action Text Alignment (SATA) dataset with detailed action description specifically for surgical robots. Then we built SurgeWorld based on the most advanced physical AI world model and SATA. It's able to generate diverse, generalizable and realistic surgery videos. We are also the first to use an inverse dynamics model to infer pseudokinematics from synthetic surgical videos, producing synthetic paired video action data. We demonstrate that a surgical VLA policy trained with these augmented data significantly outperforms models trained only on real demonstrations on a real surgical robot platform. Our approach offers a scalable path toward autonomous surgical skill acquisition by leveraging the abundance of unlabeled surgical video and generative world modeling, thus opening the door to generalizable and data efficient surgical robot policies.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Severe scarcity of paired surgical videoâ€“action/kinematics data due to privacy, safety, cost, and regulatory constraints, hindering VLA and imitation learning in surgical robotics<br>â€¢ Existing simulators and synthetic data exhibit large visual/physical domain shifts (especially soft tissue dynamics), while IL suffers from covariate shift under limited data, limiting robust policy transfer<br>â€¢ Prior surgical world-model efforts are narrow, lack rich text grounding and kinematic integration, and cannot leverage abundant unlabeled surgical videos for scalable policy learning</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Curate the SATA dataset with expert text-aligned surgical video clips, train a Cosmos 2.5â€“based diffusion world model (SurgWorld) to generate photorealistic, text-consistent surgical videos, and use an embodiment-specific inverse dynamics model to infer pseudo-kinematics, enabling VLA policy training on mixed real and synthetic videoâ€“action pairs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ SurgWorld-MPC: Closed-loop surgical autonomy using world-model predictive control with safety constraints<br>â€¢ Cross-Embodiment SurgWorld: Text-grounded world models for multi-robot, multi-procedure skill transfer via shared latent action spaces<br>â€¢ Soft-Tissue Aware SurgWorld: Physics-informed world modeling to reduce sim-to-real gaps by incorporating differentiable soft-body dynamics and tissue interaction priors</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Monadic Context Engineering</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22431" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22431" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Brittle imperative agent architectures with tangled control flow, manual state propagation, and boilerplate error handling make systems hard to test, debug, and evolve.<br>â€¢ Lack of principled composition for both sequential dependence and independent concurrent tasks; pure logic is intermingled with side effects without a formal effect system.<br>â€¢ Inadequate error resilience: no intrinsic short-circuiting and clear failure propagation across multi-step tool calls; state can be corrupted on failure.<br>â€¢ Concurrency bottlenecks: ad hoc threading/async management hinder robust parallel I/O and coordinated orchestration.<br>â€¢ Misalignment with emerging standards (e.g., MCP): current frameworks treat state and errors as side channels, making protocol-compliant, observable workflows brittle.<br>â€¢ Scaling to multi-agent teams lacks formal orchestration, leading to unpredictable interactions when dynamically spawning and supervising sub-agents.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Monadic Context Engineering stacks monad transformers (StateT over EitherT over IO) into an AgentMonad that intrinsically threads state, short-circuits failures, and manages side effects; it extends to an AsyncAgentMonad with Applicative combinators (e.g., gather) for principled concurrency/parallelism and a meta-agent layer for generative orchestration.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Formal Verification of AgentMonad Stacks: Proving Safety, Liveness, and Error Propagation: Develop formal semantics and machine-checked proofs (e.g., in Coq/Agda) for state threading, short-circuiting, and effect isolation in MCE.<br>â€¢ Conflict-Free State Merging for Applicative Parallelism in Agents: Design CRDT- or lens-based merge strategies and monoidal state models to reconcile parallel updates from gathered async flows.<br>â€¢ Distributed AsyncAgentMonad: From Local Concurrency to Cluster-Orchestrated Meta-Agents: Extend Applicative parallelism to distributed execution with scheduling, fault tolerance, and protocol-aware tool routing across nodes.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">An Information Theoretic Perspective on Agentic System Design</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21720" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21720" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Agentic language model (LM) systems power modern applications like "Deep Research" and "Claude Code," and leverage multi-LM architectures to overcome context limitations. Beneath their apparent diversity lies a recurring pattern: smaller "compressor" LMs (that can even run locally) distill raw context into compact text that is then consumed by larger "predictor" LMs. Despite their popularity, the design of compressor-predictor systems remains largely ad hoc, with little guidance on how compressor and predictor choices shape downstream performance. In practice, attributing gains to compression versus prediction requires costly, task-specific pairwise sweeps. We argue that these agentic system design questions are, at root, information-theoretic. Viewing the compressor LM as a noisy channel, we introduce a simple estimator of mutual information between the context and its compression to quantify compression quality in a task-independent way. We show that mutual information strongly predicts downstream performance, independent of any specific task. Through an information-theoretic framework, we perform a comprehensive empirical analysis across five datasets and three model families. Results reveal that larger compressors not only are more accurate, but also more token-efficient, conveying more bits of information per token. A 7B Qwen-2.5 compressor, for instance, is 1.6times more accurate, 4.6times more concise, and conveys 5.5times more bits of mutual information per token than its 1.5B sibling. Across datasets, scaling compressors is substantially more effective than scaling predictors, enabling larger on-device compressors to pair with smaller cloud predictors. Applied to a Deep Research system, these principles enable local compressors as small as 3B parameters to recover 99% of frontier-LM accuracy at 26% of API costs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Compressorâ€“predictor agentic systems are designed ad hoc, with no principled way to attribute performance gains to compression versus prediction.<br>â€¢ Practitioners lack task-agnostic metrics to assess how much of the original context a compressor preserves, forcing costly end-to-end sweeps and suffering from context rot.<br>â€¢ Existing work focuses on overall utility and ignores the communication channel; mutual information estimation for text is challenging, leaving a gap in evaluating compressor quality directly.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Model the compressor LM as a noisy channel and introduce a simple, unbiased mutual information estimator between the raw context and its compression that can be computed without full-vocabulary log probabilities. Use rateâ€“distortion analysis to show that information rate (MI per token) strongly predicts downstream performance across datasets and model families, yielding practical design principles (e.g., front-load compute into compressors).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Compressors with Mutual-Information-Regularized Objectives: Train compressor LMs to maximize MI per token under token budget constraints, and evaluate effects on downstream accuracy and cost.<br>â€¢ Adaptive On-Device Compression for Agentic Systems Under Budgeted Compute: Develop policies that dynamically select compressor size and compression granularity based on context complexity and device/cloud resource constraints.<br>â€¢ Theoretical Bounds for Language-Model Channels: Capacity, Rateâ€“Distortion, and Error Exponents: Derive formal bounds linking model size, tokenization entropy, and achievable accuracy to guide compressorâ€“predictor design and scaling.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.20927" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.20927" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses a significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps. Code will be made publicly available.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Dense volume rendering of all intersecting Gaussians makes high-dimensional (e.g., 512-D CLIP) feature rendering computationally prohibitive for real-time use and efficient training.<br>â€¢ Dimensionality reduction via codebooks or compression introduces information loss, degrading open-vocabulary segmentation quality and failing to fundamentally address the compute bottleneck.<br>â€¢ Per-scene optimization yields noisy/local minima in Gaussian distributions and poor generalization, highlighting the need for a transmittance-aware sparse sampling that focuses on influential Gaussians and a generalizable 3D network for feature prediction.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Quantile Rendering (Q-Render) partitions per-ray transmittance into K+1 intervals, selects only Gaussians that cross these boundaries for alpha-blending, and normalizes the accumulated featureâ€”cutting complexity from O(NC) to O(N+KC) without sorting. It is integrated with a Gaussian Splatting Network (GS-Net) that voxelizes Gaussian centers and uses 3D backbones (e.g., PTv3, MinkUNet) to predict high-dimensional Gaussian features distilled from 2D CLIP supervision.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Quantile Rendering: Learning Per-Ray Sampling Policies for Efficient High-Dimensional Splatting: Learn K and thresholding per ray to balance accuracy and speed, enabling scene- and view-adaptive sparsity.<br>â€¢ Multi-Modal Quantile Splatting: Joint RGB, Depth, and Language Feature Rendering for Unified 3D Understanding: Extend Q-Render to co-render multi-modal features for segmentation, detection, and affordance tasks in one pass.<br>â€¢ Generalizable Gaussian Feature Pretraining: End-to-End GS-Net Without Per-Scene Optimization: Pretrain GS-Net across large corpora to predict Gaussian features and geometry end-to-end, removing reliance on per-scene optimization while preserving OVS fidelity.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23703" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23703" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The primary obstacle for applying reinforcement learning (RL) to real-world robotics is the design of effective reward functions. While recently learning-based Process Reward Models (PRMs) are a promising direction, they are often hindered by two fundamental limitations: their reward models lack step-aware understanding and rely on single-view perception, leading to unreliable assessments of fine-grained manipulation progress; and their reward shaping procedures are theoretically unsound, often inducing a semantic trap that misguides policy optimization. To address these, we introduce Dopamine-Reward, a novel reward modeling method for learning a general-purpose, step-aware process reward model from multi-view inputs. At its core is our General Reward Model (GRM), trained on a vast 3,400+ hour dataset, which leverages Step-wise Reward Discretization for structural understanding and Multi-Perspective Reward Fusion to overcome perceptual limitations. Building upon Dopamine-Reward, we propose Dopamine-RL, a robust policy learning framework that employs a theoretically-sound Policy-Invariant Reward Shaping method, which enables the agent to leverage dense rewards for efficient self-improvement without altering the optimal policy, thereby fundamentally avoiding the semantic trap. Extensive experiments across diverse simulated and real-world tasks validate our approach. GRM achieves state-of-the-art accuracy in reward assessment, and Dopamine-RL built on GRM significantly improves policy learning efficiency. For instance, after GRM is adapted to a new task in a one-shot manner from a single expert trajectory, the resulting reward model enables Dopamine-RL to improve the policy from near-zero to 95% success with only 150 online rollouts (approximately 1 hour of real robot interaction), while retaining strong generalization across tasks. Project website: https://robo-dopamine.github.io</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Reward design bottleneck in real-world RL: sparse outcome rewards hinder exploration in long-horizon, contact-rich manipulation; handcrafted dense rewards require expert tuning and donâ€™t scale.<br>â€¢ Existing process reward models are unreliable for fine-grained progress: lack step-aware structure, use near-uniform reward allocations that miss salient sub-steps, and rely on single-view observations vulnerable to occlusion.<br>â€¢ Theoretical flaw in reward shaping (semantic trap): naively using dense process rewards can alter the optimal policy and misguide learning away from true task objectives.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Robo-Dopamine introduces Dopamine-Reward, training a General Reward Model (GRM) to predict step-aware, fine-grained progress from multi-view inputs via hop-based step-wise discretization and multi-perspective reward fusion on a 3,400-hour, 35M-sample dataset. Built on this, Dopamine-RL uses policy-invariant reward shaping to safely exploit GRMâ€™s dense signals for efficient online learning and one-shot task adaptation without changing the optimal policy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Language-Grounded GRMs for Open-World Robotic Manipulation: Extend GRM to condition on natural language goals and object attributes for open-vocabulary task specification and zero-shot generalization.<br>â€¢ Active Multi-View Acquisition for Reward Modeling: Learn camera placement and view-selection policies that co-adapt with GRM to reduce occlusion and maximize observability of fine-grained progress.<br>â€¢ Uncertainty-Aware Policy-Invariant Reward Shaping for Safe Real-World RL: Calibrate GRM uncertainty and integrate risk-sensitive shaping and safety constraints to maintain reliability under distribution shift.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ProGuard: Towards Proactive Multimodal Safeguard</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23573" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23573" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Reactive guard models depend on fixed safety taxonomies, making them unable to detect or describe emerging out-of-distribution (OOD) risks.<br>â€¢ Existing multimodal guards exhibit modality bias due to skewed, text-heavy data, leading to weaker moderation on images and text-image inputs.<br>â€¢ Prior work focuses on binary safety classification and lacks fine-grained unsafe content categorization and proactive reasoning, limiting generalization at test time.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>ProGuard is a vision-language guard trained purely via reinforcement learning on a balanced 87K multimodal dataset, jointly optimizing binary safety classification, unsafe category assignment, and an OOD safety category inference task. It augments the RL objective with a synonym-bank-based similarity reward to produce concise, human-aligned descriptions for unseen unsafe categories, enabling proactive moderation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Open-World Multimodal Safety via Continually Updated Taxonomies: Build systems that dynamically expand and refine safety taxonomies online using proactive guard outputs and human-in-the-loop curation.<br>â€¢ Reward Shaping for Safety Reasoning with Human Preference Learning: Combine preference-based RL/RLHF to better align OOD category inference and explanations with human judgments across modalities.<br>â€¢ Proactive Safety Moderation for Audioâ€“Video and Embodied Agents: Extend proactive guards to audio/video streams and interactive agent settings to evaluate and improve OOD risk detection and description in real-time environments.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">Bridging Your Imagination with Audio-Video Generation via a Unified Director</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23222" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23222" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Existing AI-driven video creation systems typically treat script drafting and key-shot design as two disjoint tasks: the former relies on large language models, while the latter depends on image generation models. We argue that these two tasks should be unified within a single framework, as logical reasoning and imaginative thinking are both fundamental qualities of a film director. In this work, we propose UniMAGE, a unified director model that bridges user prompts with well-structured scripts, thereby empowering non-experts to produce long-context, multi-shot films by leveraging existing audio-video generation models. To achieve this, we employ the Mixture-of-Transformers architecture that unifies text and image generation. To further enhance narrative logic and keyframe consistency, we introduce a ``first interleaving, then disentangling'' training paradigm. Specifically, we first perform Interleaved Concept Learning, which utilizes interleaved text-image data to foster the model's deeper understanding and imaginative interpretation of scripts. We then conduct Disentangled Expert Learning, which decouples script writing from keyframe generation, enabling greater flexibility and creativity in storytelling. Extensive experiments demonstrate that UniMAGE achieves state-of-the-art performance among open-source models, generating logically coherent video scripts and visually consistent keyframe images.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Disjoint script drafting (LLMs) and keyframe design (image diffusion) lead to weak narrativeâ€“visual alignment and limited long-range coherence in multi-shot films.<br>â€¢ Maintaining character identity and visual consistency across long, multi-actor sequences is difficult; existing methods largely address single subjects or short edits.<br>â€¢ Current unified multimodal models and video generators lack mechanisms for coherent long-form planning and flexible script continuation, often relying on per-sample prompt engineering and overlooking storyboard design.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>UniMAGE is a unified director built on a Mixture-of-Transformers that jointly learns text (Next Token Prediction) and image (Rectified Flow) generation, using a â€œfirst interleaving, then disentanglingâ€ training paradigm. It couples Interleaved Concept Learning with In-Context ID Prompting for multi-subject consistency, and Disentangled Expert Learning with Pre-Context Script Splitting to strengthen narrative logic and flexible continuation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Director-AV: Unified Audio-Visual Script and Soundtrack Generation via Mixture-of-Transformers: Extend UniMAGE to jointly plan and synthesize sound design, dialogue, and music aligned with visual narratives.<br>â€¢ LongForm-ID: Memory-Augmented Unified Director for Character Consistency Across Feature-Length Films: Introduce persistent memory/identity embeddings to maintain cast coherence across hundreds of shots and scenes.<br>â€¢ SceneGraph2Director: Integrating 3D Scene Graphs and Camera Planning in Unified Multimodal Script-Keyframe Generation: Incorporate 3D scene representations and camera path planning to produce shot blocking and multi-shot video beyond keyframes.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21734" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21734" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Real-time portrait animation is essential for interactive applications such as virtual assistants and live avatars, requiring high visual fidelity, temporal coherence, ultra-low latency, and responsive control from dynamic inputs like reference images and driving signals. While diffusion-based models achieve strong quality, their non-causal nature hinders streaming deployment. Causal autoregressive video generation approaches enable efficient frame-by-frame generation but suffer from error accumulation, motion discontinuities at chunk boundaries, and degraded long-term consistency. In this work, we present a novel streaming framework named Knot Forcing for real-time portrait animation that addresses these challenges through three key designs: (1) a chunk-wise generation strategy with global identity preservation via cached KV states of the reference image and local temporal modeling using sliding window attention; (2) a temporal knot module that overlaps adjacent chunks and propagates spatio-temporal cues via image-to-video conditioning to smooth inter-chunk motion transitions; and (3) A "running ahead" mechanism that dynamically updates the reference frame's temporal coordinate during inference, keeping its semantic context ahead of the current rollout frame to support long-term coherence. Knot Forcing enables high-fidelity, temporally consistent, and interactive portrait animation over infinite sequences, achieving real-time performance with strong visual stability on consumer-grade GPUs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Non-causal diffusion transformers have high latency due to long-sequence iterative denoising, making them unsuitable for real-time streaming portrait animation.<br>â€¢ Existing autoregressive video diffusion models accumulate errors and exhibit attention context shifts across frames/chunks, causing flicker, motion discontinuities at boundaries, and degraded long-term consistency.<br>â€¢ Maintaining strong identity fidelity and responsiveness to streaming controls (audio/pose) under low latency is challenging, and the trainâ€“inference gap in AR training further degrades stability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Knot Forcing is a chunk-wise causal video diffusion framework that caches the reference imageâ€™s KV states as a global anchor and applies a short sliding window for local temporal modeling. It introduces temporal knots (overlapped boundary frames propagated via I2V mask inpainting) to smooth inter-chunk transitions, and a running-ahead mechanism that advances the reference frameâ€™s RoPE/KV to suppress long-term error accumulation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Bridging the Teacherâ€“Student Gap in Causal Video Diffusion via Context Alignment Theory: Develop formal analyses and objectives to align causal attention contexts with bidirectional teachers, reducing context mismatch and temporal drift.<br>â€¢ Adaptive Temporal Knots: Latency-aware Overlap Scheduling for Real-time AR Video Generation: Learn to adjust knot length and placement based on motion dynamics and latency constraints to optimize coherenceâ€“efficiency trade-offs.<br>â€¢ Generalizing Knot Forcing to World Models: Controllable Long-Horizon Simulation with Running-Ahead Anchors: Extend running-ahead anchoring and knot-based context propagation to interactive world model and game environment simulation tasks.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23236" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23236" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Making deep learning recommendation model (DLRM) training and inference fast and efficient is important. However, this presents three key system challenges - model architecture diversity, kernel primitive diversity, and hardware generation and architecture heterogeneity. This paper presents KernelEvolve-an agentic kernel coding framework-to tackle heterogeneity at-scale for DLRM. KernelEvolve is designed to take kernel specifications as input and automate the process of kernel generation and optimization for recommendation model across heterogeneous hardware architectures. KernelEvolve does so by operating at multiple programming abstractions, from Triton and CuTe DSL to low-level hardware agnostic languages, spanning the full hardware-software optimization stack. The kernel optimization process is described as graph-based search with selection policy, universal operator, fitness function, and termination rule, dynamically adapts to runtime execution context through retrieval-augmented prompt synthesis. We designed, implemented, and deployed KernelEvolve to optimize a wide variety of production recommendation models across generations of NVIDIA and AMD GPUs, as well as Meta's AI accelerators. We validate KernelEvolve on the publicly-available KernelBench suite, achieving 100% pass rate on all 250 problems across three difficulty levels, and 160 PyTorch ATen operators across three heterogeneous hardware platforms, demonstrating 100% correctness. KernelEvolve reduces development time from weeks to hours and achieves substantial performance improvements over PyTorch baselines across diverse production use cases and for heterogeneous AI systems at-scale. Beyond performance efficiency improvements, KernelEvolve significantly mitigates the programmability barrier for new AI hardware by enabling automated kernel generation for in-house developed AI hardware.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ DLRM training and inference must be fast and efficient, but the combination of model architecture diversity, kernel primitive diversity, and hardware heterogeneity creates a vast, complex kernel optimization space.<br>â€¢ Manual, hand-tuned kernel development is slow (weeks), brittle across hardware generations, and does not scale to new accelerators or diverse operators in production.<br>â€¢ Existing libraries/compilers/auto-tuners are tied to single abstractions or architectures, provide incomplete operator coverage, and lack hardware-specific constraintsâ€”especially for proprietary accelerators absent from LLM training corpora.<br>â€¢ Static prompt/code synthesis cannot adapt to runtime execution contexts (shapes, memory layouts, topology), leading to suboptimal performance.<br>â€¢ There is a high programmability barrier for new AI hardware; systems need automated, correct, and performant kernel generation at-scale across heterogeneous accelerators.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>KernelEvolve is an agentic kernel coding framework that takes kernel specifications and autonomously generates and optimizes kernels via a graph-based search (selection policy, universal operator, fitness function, termination rule) across multiple programming abstractions (Triton, CuTe DSL, low-level diagnostic languages), guided by a persistent, hardware-aware knowledge base. It dynamically adapts to runtime contexts through retrieval-augmented prompt synthesis, enabling correct, high-performance kernels on NVIDIA/AMD GPUs and proprietary accelerators like MTIAv3.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Search Policies for Agentic Kernel Optimization on Heterogeneous Accelerators: Integrate reinforcement/meta-learning to learn selection policies that generalize across workloads and hardware, reducing search cost and improving adaptability.<br>â€¢ Knowledge-Base Expansion via On-Device Probing for Proprietary AI Accelerators: Automatically infer, validate, and incorporate hardware constraints through active measurement and diagnostics to enrich KernelEvolveâ€™s persistent knowledge base.<br>â€¢ Topology- and System-Aware Agentic Kernel Co-Optimization for Distributed Recommender Systems: Extend KernelEvolve to jointly optimize kernels, scheduling, and communication for multi-accelerator and multi-node deployments.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22100" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22100" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions. Among these, the evaluation of natural language understanding (NLU) is particularly critical as it serves as a fundamental criterion for assessing model capabilities. Thus, it is essential to establish benchmarks that enable thorough evaluation and analysis of NLU abilities from diverse perspectives. While the GLUE benchmark has set a standard for evaluating English NLU, similar benchmarks have been developed for other languages, such as CLUE for Chinese, FLUE for French, and JGLUE for Japanese. However, no comparable benchmark currently exists for the Turkish language. To address this gap, we introduce TrGLUE, a comprehensive benchmark encompassing a variety of NLU tasks for Turkish. In addition, we present SentiTurca, a specialized benchmark for sentiment analysis. To support researchers, we also provide fine-tuning and evaluation code for transformer-based models, facilitating the effective use of these benchmarks. TrGLUE comprises Turkish-native corpora curated to mirror the domains and task formulations of GLUE-style evaluations, with labels obtained through a semi-automated pipeline that combines strong LLM-based annotation, cross-model agreement checks, and subsequent human validation. This design prioritizes linguistic naturalness, minimizes direct translation artifacts, and yields a scalable, reproducible workflow. With TrGLUE, our goal is to establish a robust evaluation framework for Turkish NLU, empower researchers with valuable resources, and provide insights into generating high-quality semi-automated datasets.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ No standardized Turkish NLU benchmark, making fair comparison and comprehensive evaluation across models and tasks impossible<br>â€¢ Existing Turkish datasets are scattered and lack coverage of paraphrase, similarity, and inference; translation-based resources introduce artifacts, cultural bias, and ignore Turkishâ€™s agglutinative morphology, degrading evaluation quality<br>â€¢ Scarcity and low variability in Turkish training/evaluation corpora (e.g., OSCAR, mC4), with current evaluations lacking reproducibility and transparency (e.g., opaque QA datasets)<br>â€¢ Need for broad sentiment analysis resources, including robust hate speech coverage, to assess model performance and social impact across domains</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce TrGLUE and SentiTurca, built from Turkish-native corpora that mirror GLUE-style tasks, with labels produced via a semi-automated pipeline combining strong LLM-based annotation, cross-model agreement checks, and human validation. Release fine-tuning and evaluation code for transformer models and host benchmarks for reproducible use.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Expanding TrGLUE to Generative and Inference-Rich Turkish Tasks: Extend the benchmark to QA, summarization, and dialogue with Turkish-specific metrics that account for agglutinative morphology<br>â€¢ LLMâ€“Human Co-Annotation for High-Quality Turkish Datasets: Evaluate and optimize semi-automated labeling pipelines for bias, cost, and label quality across diverse Turkish domains<br>â€¢ Cross-Lingual Transfer on TrGLUE: Benchmark multilingual versus Turkish-specific models, analyzing adaptation strategies and robustness against translation-based baselines</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Self-Evaluation Unlocks Any-Step Text-to-Image Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22374" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22374" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce the Self-Evaluating Model (Self-E), a novel, from-scratch training approach for text-to-image generation that supports any-step inference. Self-E learns from data similarly to a Flow Matching model, while simultaneously employing a novel self-evaluation mechanism: it evaluates its own generated samples using its current score estimates, effectively serving as a dynamic self-teacher. Unlike traditional diffusion or flow models, it does not rely solely on local supervision, which typically necessitates many inference steps. Unlike distillation-based approaches, it does not require a pretrained teacher. This combination of instantaneous local learning and self-driven global matching bridges the gap between the two paradigms, enabling the training of a high-quality text-to-image model from scratch that excels even at very low step counts. Extensive experiments on large-scale text-to-image benchmarks show that Self-E not only excels in few-step generation, but is also competitive with state-of-the-art Flow Matching models at 50 steps. We further find that its performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within a single unified model. To our knowledge, Self-E is the first from-scratch, any-step text-to-image model, offering a unified framework for efficient and scalable generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Diffusion and flow models rely on local supervision (score/velocity) that lacks a holistic global view, so they require many sequential steps, making inference slow and costly for time-sensitive applications.<br>â€¢ Few-step distillation approaches depend on strong pretrained teachers and optimize global matching to the teacher, limiting self-contained, from-scratch training and scalability.<br>â€¢ Consistency-based, teacher-free methods are often unstable from scratch or suffer quality degradation, and have not reliably scaled to large text-to-image without distillation.<br>â€¢ Existing systems are not truly any-step: quality often collapses at very low step counts and does not improve monotonically with more steps; there is no unified model supporting both ultra-fast and high-quality long-trajectory sampling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Train a from-scratch Self-E model that jointly learns local velocities (as in Flow Matching) and performs self-evaluation: it scores its own generated samples using its current local estimate to provide a global matching signal, acting as a dynamic self-teacher. This combination of instantaneous local learning and self-driven global supervision enables any-step inference with strong few-step performance and continued improvement at higher step counts.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Self-Evaluation for Any-Step Video Generation: Extend Self-E to text-to-video, using self-evaluation to supervise global temporal distributions while retaining local velocity guidance for consistent, few-step video synthesis.<br>â€¢ Confidence-Weighted Self-E: Stabilizing Teacher-Free Few-Step Training: Add uncertainty estimation and curriculum scheduling to the self-evaluation mechanism to improve stability and quality when scaling from-scratch training on large datasets.<br>â€¢ Theory of Self-E as Global Matching for Flows: Develop formal convergence and error analyses showing how self-evaluation complements local velocity learning to yield few-step performance and any-step guarantees.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22255" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22255" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present the surprising finding that a language model's reasoning capabilities can be improved by training on synthetic datasets of chain-of-thought (CoT) traces from more capable models, even when all of those traces lead to an incorrect final answer. Our experiments show this approach can yield better performance on reasoning tasks than training on human-annotated datasets. We hypothesize that two key factors explain this phenomenon: first, the distribution of synthetic data is inherently closer to the language model's own distribution, making it more amenable to learning. Second, these `incorrect' traces are often only partially flawed and contain valid reasoning steps from which the model can learn. To further test the first hypothesis, we use a language model to paraphrase human-annotated traces -- shifting their distribution closer to the model's own distribution -- and show that this improves performance. For the second hypothesis, we introduce increasingly flawed CoT traces and study to what extent models are tolerant to these flaws. We demonstrate our findings across various reasoning domains like math, algorithmic reasoning and code generation using MATH, GSM8K, Countdown and MBPP datasets on various language models ranging from 1.5B to 9B across Qwen, Llama, and Gemma models. Our study shows that curating datasets that are closer to the model's distribution is a critical aspect to consider. We also show that a correct final answer is not always a reliable indicator of a faithful reasoning process.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing reasoning data pipelines over-prioritize final-answer correctness, overlooking that incorrect CoTs can contain learnable, partially correct reasoning steps.<br>â€¢ Human-written CoTs, though correct, can be far from a modelâ€™s native distribution, reducing fine-tuning efficiency and scalability.<br>â€¢ Incorrect synthetic traces from stronger models are typically discarded, wasting potentially valuable supervision that is closer to the target modelâ€™s distribution.<br>â€¢ Final-answer checking is an incomplete proxy for CoT quality and faithfulness, leading to suboptimal data curation and evaluation.<br>â€¢ There is limited understanding of how tolerant LLMs are to different types and degrees of reasoning flaws during SFT.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Supervised fine-tune smaller LLMs on synthetic chain-of-thought traces from stronger models, explicitly including traces with incorrect final answers to exploit distributional closeness and partially correct steps. Additionally, paraphrase human CoTs to align with model output distribution and progressively inject reasoning flaws to quantify tolerance and performance trade-offs across benchmarks and model families.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Measuring and Optimizing Distributional Closeness for Reasoning SFT: Define and learn metrics/objectives that select and weight training traces by closeness to the target modelâ€™s generative distribution.<br>â€¢ Process-Aware Distillation: Step-Level Weighting and Partial-Credit Learning for CoT: Train models to emphasize valid sub-steps within flawed CoTs via step-wise reliability estimation and dynamic loss weighting.<br>â€¢ Beyond Final Answers: Structural Verifiers for Faithful Reasoning Supervision: Develop verifiers that evaluate intermediate reasoning structure and dependencies rather than only final-answer correctness.<br>â€¢ Curriculum Learning with Controlled Flaw Injection for Robust Reasoning: Systematically introduce localized vs. global errors to shape curricula that improve robustness and generalization in CoT learning.<br>â€¢ Paraphrase-Driven Distribution Alignment at Scale: Automated Rewriting Pipelines for Human CoTs: Build scalable paraphrasing systems that adapt human-written traces to target model distributions while preserving semantic correctness.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-11">

    <div class="paper">
        <h2 class="paper-title">Reverse Personalization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22984" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22984" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent text-to-image diffusion models have demonstrated remarkable generation of realistic facial images conditioned on textual prompts and human identities, enabling creating personalized facial imagery. However, existing prompt-based methods for removing or modifying identity-specific features rely either on the subject being well-represented in the pre-trained model or require model fine-tuning for specific identities. In this work, we analyze the identity generation process and introduce a reverse personalization framework for face anonymization. Our approach leverages conditional diffusion inversion, allowing direct manipulation of images without using text prompts. To generalize beyond subjects in the model's training data, we incorporate an identity-guided conditioning branch. Unlike prior anonymization methods, which lack control over facial attributes, our framework supports attribute-controllable anonymization. We demonstrate that our method achieves a state-of-the-art balance between identity removal, attribute preservation, and image quality. Source code and data are available at https://github.com/hanweikung/reverse-personalization .</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Need to remove identity-specific facial features while preserving non-identity attributes (expression, pose) and scene context to balance privacy and data utility.<br>â€¢ Prompt-based identity editing works only for subjects represented in the model (e.g., celebrities) and depends on textual prompts, failing on arbitrary, unseen identities.<br>â€¢ Personalization methods (e.g., Textual Inversion, DreamBooth) require fine-tuning, multiple reference images, and substantial compute, and often lack fine-grained control over sensitive attributes (age, race, gender).<br>â€¢ Existing GAN/diffusion anonymization approaches struggle to jointly achieve strong identity removal, attribute preservation, and high image realism; many rely on brittle landmarks/masks or identity losses and offer limited controllability.<br>â€¢ Ethical and regulatory needs (e.g., GDPR/CCPA) demand flexible anonymization that can selectively retain or modify demographic attributes depending on application context.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Reverse personalization uses conditional diffusion inversion with a null-identity embedding and identity-guided conditioning (via face embeddings and IP-Adapter) to reconstruct images without text prompts, then applies negative classifier-free guidance to steer generation away from the input identity while preserving scene and controllable attributes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Negative Guidance Schedules for Identity-Safe Diffusion: Learn per-image and per-timestep guidance policies that optimize the privacyâ€“utility trade-off by maximizing identity obfuscation while preserving attributes and realism.<br>â€¢ Temporally Consistent Reverse Personalization for Video: Extend the framework to video by aligning latent trajectories and introducing motion-aware conditioning to ensure frame-to-frame identity removal with stable attributes and scene continuity.<br>â€¢ Generalized Reverse Personalization Across Modalities: Apply the approach to other identity-bearing data (full-body images, gait, voice) using modality-specific embeddings and adapters to achieve attribute-controllable anonymization beyond faces.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 11</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button><button class="page-btn" onclick="goToPage(11)">11</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-12-30 23:53:28 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 11;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>