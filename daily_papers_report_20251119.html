<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 19, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 19, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.14295" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.14295" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Arabic LLM evaluation lacks linguistically grounded tests; existing benchmarks (e.g., ArabicMMLU, EXAMS, CamelEval) emphasize knowledge/factual recall rather than grammar, morphology, orthography, and syntax, leaving core competence unmeasured (Table 2, page 5; Introduction, pages 2‚Äì3).<br>‚Ä¢ Current models show surface-level proficiency but struggle with deeper grammatical and syntactic reasoning; AraLingBench evaluations reveal Syntax as the hardest and large intra-category variance (Table 3 and Figure 3, page 8).<br>‚Ä¢ The Arabic language‚Äôs complex morphology and flexible syntax demand targeted assessment beyond general reasoning, which current practices assume rather than examine (Figure 1, page 2; Section 2.3).<br>‚Ä¢ High scores on general/retrieval-heavy benchmarks can mask weak linguistic mastery; cross-benchmark analysis shows only partial predictiveness and even negative correlation with retrieval-augmented evaluation (Figure 5, page 11).<br>‚Ä¢ The field needs a diagnostic framework that isolates and measures fundamental linguistic skills to guide model development and training (Contributions, page 3).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>AraLingBench is a fully human-authored, 150-item multiple-choice benchmark evenly covering five categories‚Äîgrammar, morphology, spelling, reading comprehension, and syntax‚Äîconstructed via a four-phase pipeline (question generation, diversity/difficulty filtering, expert QC, and difficulty annotation) to ensure validity and balance (Figure 2, page 6). Models are evaluated zero-shot in a standardized A‚ÄìD format across 35 systems, reporting per-category accuracy and analyzing inter-category and cross-benchmark correlations to diagnose true linguistic competence.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Inductive Biases for Arabic Syntax: Designing hierarchical or structure-aware transformer objectives to improve syntactic reasoning, validated on AraLingBench‚Äôs Syntax category (motivated by weak correlations and low scores; Figure 4 and Table 3).<br>‚Ä¢ Morphology-Aware Pretraining for Arabic LLMs: Integrating morphological analyzers, inflectional generation, or contrastive paradigms to strengthen grammar‚Äìmorphology coupling and boost Morphology/Grammar performance on AraLingBench.<br>‚Ä¢ Calibrating Difficulty with Psychometrics for Arabic Benchmarks: Applying item response theory and model-informed calibration to align human difficulty labels with model performance and build curricula, addressing non-monotonic scaling (Figure 6 and Figure 9).</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.08577" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.08577" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Parameter-constrained reasoning: small LLMs (edge/low-cost) hit a ceiling on hard reasoning tokens; early errors derail chains of thought (p.2). This calls for more compute per difficult token without increasing parameters.<br>‚Ä¢ Latent overthinking in fixed-depth recurrent transformers: uniformly ‚Äúthinking twice‚Äù often revises correct first-pass tokens into errors; AlwaysThink causes more errors than corrections (Figure 1(b), p.1; Table 4, p.9).<br>‚Ä¢ Limitations of existing methods: fixed-depth recurrence wastes compute and harms easy tokens; pause/control-token methods give coarse, discrete control; latent optimization sacrifices interpretability or needs heavy retraining; dynamic iteration is hard due to the need for cross-depth context while preserving parallelism, iteration objective shifts with shared weights, and unstable coupled training of decider/backbone (Sections 4.1, 4.2, 4.3).<br>‚Ä¢ Practical importance: Selective iteration improves accuracy at nearly standard FLOPs (‚âà1.06 iterations/token) and constant parameter count; TaH gains 4.0‚Äì5.0% over Standard and 8.1‚Äì11.3% over AlwaysThink while iterating twice on only ~6% tokens (Table 1, p.7; Table 9, p.17).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TaH selectively applies latent iterations only to hard tokens using a lightweight neural decider, while enabling cross-iteration context via duo-causal attention and specializing deeper iterations with depth-only LoRA adapters (Figure 2, p.4). It is trained in two stages with an oracle-guided policy for stability: first align the backbone (with LoRA at d>1) to oracle depths, then distill the oracle‚Äôs continuation decisions into the decider.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning to Halt: Reinforcement Learning for Token-level Compute Allocation in Selective Latent Iteration: Replace oracle imitation with RL to jointly optimize accuracy‚Äìcompute trade-offs and learn decider policies beyond thresholds.<br>‚Ä¢ Online Distillation for Selective Latent Thinkers: On-policy distillation from a stronger teacher to co-train the backbone and decider, reducing oracle mismatch and improving generalization across domains.<br>‚Ä¢ Scaling Duo-Causal Recurrence: Multi-depth Halting with Quantile-based Difficulty and Systems Optimizations: Extend beyond two iterations using non-binary halting (quantile mapping) and design kernels/KV-cache layouts that further accelerate duo-causal attention at scale.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.10555" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.10555" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Prompt-based stylization yields low style consistency‚Äîidentical style prompts often produce visually inconsistent results, making reliable style reproduction hard.<br>‚Ä¢ Image/LoRA-based methods depend on existing style assets, limiting creativity (cannot create unseen styles) and imposing heavy sharing/friction (pixel images or model weights), thus poor reproducibility.<br>‚Ä¢ Existing style representations are complex or entangled with content, hindering a compact, portable style handle that can be easily shared and deterministically reproduced.<br>‚Ä¢ No open-source, academic solution for code-to-style generation exists (industrial-only, e.g., Midjourney), impeding research and democratization.<br>‚Ä¢ Users need a way to generate diverse, novel, and consistent styles from minimal input while keeping strong text-image alignment and aesthetic quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>CoTyle learns a discrete style codebook with contrastive, reconstruction, and vector-quantization losses to extract style tokens from images, conditions a diffusion transformer via a VLM‚Äôs textual branch on these style embeddings, and trains an autoregressive model to generate novel style index sequences deterministically from a numerical code. A high-frequency index suppression strategy improves style intensity and diversity during sampling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Disentangled Style Codebooks: Frequency-Balanced, Semantically Structured Tokenizers for Generative Style Control: Design balanced, interpretable codebooks (e.g., entropy regularization, hierarchical VQ, MoE) to avoid high-frequency placeholders and improve semantic disentanglement.<br>‚Ä¢ CodeNavigator: Interactive Exploration and Editing of Discrete Style Spaces: Build tools that map codes to human-interpretable controls, enable style editing, traversal, and similarity search, and support explainable code-to-attribute mappings.<br>‚Ä¢ StyleLM: Scaling Autoregressive Style Generators for Open-Ended, High-Diversity Code-to-Style Synthesis: Train larger or mixture-of-experts AR generators with diverse corpora to boost creativity, diversity, and controllability of sampled style sequences.<br>‚Ä¢ Text-to-Code: Translating Natural Language Descriptions into Discrete Style Embeddings: Learn a model that converts style prompts into code sequences, enabling promptable, reproducible style creation without reference images.<br>‚Ä¢ CoTyle-Video/Audio: Extending Numerical Style Codes to Video and Audio Stylization: Generalize discrete style codes across modalities for consistent video stylization and audio timbre/style control with temporal coherence.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13853" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13853" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While Chain-of-Thought (CoT) prompting enables sophisticated symbolic reasoning in LLMs, it remains confined to discrete text and cannot simulate the continuous, physics-governed dynamics of the real world. Recent video generation models have emerged as potential world simulators through Chain-of-Frames (CoF) reasoning -- materializing thought as frame-by-frame visual sequences, with each frame representing a physically-grounded reasoning step. Despite compelling demonstrations, a challenge persists: existing benchmarks, focusing on fidelity or alignment, do not assess CoF reasoning and thus cannot measure core cognitive abilities in multi-step planning, algorithmic logic, or abstract pattern extrapolation. This evaluation void prevents systematic understanding of model capabilities and principled guidance for improvement. We introduce Gen-ViRe (Generative Visual Reasoning Benchmark), a framework grounded in cognitive science and real-world AI applications, which decomposes CoF reasoning into six cognitive dimensions -- from perceptual logic to abstract planning -- and 24 subtasks. Through multi-source data curation, minimal prompting protocols, and hybrid VLM-assisted evaluation with detailed criteria, Gen-ViRe delivers the first quantitative assessment of video models as reasoners. Our experiments on SOTA systems reveal substantial discrepancies between impressive visual quality and actual reasoning depth, establishing baselines and diagnostic tools to advance genuine world simulators.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of a rigorous, quantitative benchmark for Chain-of-Frames (CoF) reasoning in video generation models; existing benchmarks focus on fidelity/alignment, not reasoning depth (see Table 1 on page 8 and discussion on pages 1‚Äì3).<br>‚Ä¢ Chain-of-Thought (CoT) is discrete and symbolic and cannot simulate continuous, physics-governed dynamics; real-world tasks require visual, spatiotemporal simulation (pages 1‚Äì2, Figure 1 on page 1).<br>‚Ä¢ Without process-focused evaluation, it is impossible to distinguish genuine reasoning from sophisticated pattern matching or diagnose failure modes (perception vs. physics vs. planning) (pages 2‚Äì3, 7‚Äì8).<br>‚Ä¢ Emerging applications (embodied AI, robotics, autonomous systems) demand dynamic multi-step planning and physical plausibility that current evaluations do not assess (pages 2‚Äì4).<br>‚Ä¢ Conventional vision models are passive perceivers; video models as world simulators need benchmarks that verify goal-directed, temporally coherent, physically plausible frame-by-frame reasoning (pages 2‚Äì3, Figure 3 on page 5).<br>‚Ä¢ Qualitative demos overstate capabilities; there is a documented gap between visual quality and reasoning depth that needs systematic measurement (pages 1‚Äì2, 7‚Äì8).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Gen-ViRe introduces a 6-dimension, 24-subtask benchmark for Generative Visual Reasoning, using multi-source data curation, minimal prompting, and a hybrid VLM-assisted evaluation (Gemini 2.5 Pro as image/video judge) with detailed, subtask-specific criteria to score the entire Chain-of-Frames process. It reports normalized, macro-averaged scores and provides diagnostics that reveal where models fail (perception, physics, planning).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Reinforcement Learning with Verifiable Video Rewards for Chain-of-Frames Reasoning: Train VGMs with process-level rewards that check physics, temporal coherence, and goal completion against Gen-ViRe-like criteria.<br>‚Ä¢ Neuro-Symbolic CoF: Integrating Symbolic Planners with Video Generators for Algorithmic & Logical Tasks: Combine symbolic solvers (e.g., Sudoku/geometry) with video rollouts to ensure rule adherence while maintaining visual plausibility.<br>‚Ä¢ Long-Horizon Gen-ViRe: Evaluating Video Reasoning Beyond Short Clips: Extend benchmarks to minutes-long sequences to test sustained planning, object permanence, and causal consistency.<br>‚Ä¢ Physics-Grounded Critics for Video Models: Automatic Law-Checking Modules for Contact, Continuity, and Conservation: Develop auxiliary evaluators that detect violations like teleportation or non-contact manipulation during generation.<br>‚Ä¢ Interactive Gen-ViRe: Closed-Loop, Action-Conditioned Evaluation for Embodied Agents: Move from passive I2V to interactive tasks where the agent‚Äôs actions alter the environment across frames.<br>‚Ä¢ Minimal-Prompt Curriculum for World Simulators: Teaching Planning Under Sparse Instructions: Study curricula and instruction sparsity to improve autonomous planning noted by the benchmark‚Äôs minimal prompting design.<br>‚Ä¢ CoT‚ÜîCoF Fusion: Interleaved Text-and-Video Reasoning for Complex Multi-Step Tasks: Orchestrate textual CoT with visual CoF, using text to hypothesize and video to verify/execute plans.<br>‚Ä¢ Failure-Mode Diagnosis Toolkit for Generative Visual Reasoning: Standardized probes to localize perceptual grounding, analogical abstraction, and planning errors revealed by Gen-ViRe.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.14159" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.14159" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Evaluating the robustness of Large Vision-Language Models (LVLMs) is essential for their continued development and responsible deployment in real-world applications. However, existing robustness benchmarks typically focus on hallucination or misleading textual inputs, while largely overlooking the equally critical challenge posed by misleading visual inputs in assessing visual understanding. To fill this important gap, we introduce MVI-Bench, the first comprehensive benchmark specially designed for evaluating how Misleading Visual Inputs undermine the robustness of LVLMs. Grounded in fundamental visual primitives, the design of MVI-Bench centers on three hierarchical levels of misleading visual inputs: Visual Concept, Visual Attribute, and Visual Relationship. Using this taxonomy, we curate six representative categories and compile 1,248 expertly annotated VQA instances. To facilitate fine-grained robustness evaluation, we further introduce MVI-Sensitivity, a novel metric that characterizes LVLM robustness at a granular level. Empirical results across 18 state-of-the-art LVLMs uncover pronounced vulnerabilities to misleading visual inputs, and our in-depth analyses on MVI-Bench provide actionable insights that can guide the development of more reliable and robust LVLMs. The benchmark and codebase can be accessed at https://github.com/chenyil6/MVI-Bench.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing robustness evaluations largely ignore misleading visual inputs, focusing instead on hallucinations, adversarial noise, or misleading text, leaving a critical real‚Äëworld failure mode underexplored.<br>‚Ä¢ Prior datasets lack a comprehensive taxonomy; they typically target a single phenomenon (e.g., optical illusions or occlusion) and thus provide a narrow view of robustness.<br>‚Ä¢ Absence of controlled, paired designs (normal vs. misleading images with identical semantics) makes it hard to attribute errors specifically to misleading visual cues.<br>‚Ä¢ Coarse metrics (plain accuracy) cannot quantify how much misleading cues degrade performance; fine‚Äëgrained sensitivity to misleading inputs is missing.<br>‚Ä¢ LVLMs often rely on superficial patterns and struggle with fine‚Äëgrained attributes and spatial reasoning (mirror reflections, occlusion), which are common in real scenes and critical for safe deployment.<br>‚Ä¢ Multiple‚Äëchoice evaluation in LVLMs suffers from option-position selection bias, confounding robustness measurement unless controlled.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes MVI-Bench, a paired multiple-choice VQA benchmark grounded in a three-level taxonomy (Visual Concept, Visual Attribute, Visual Relationship) spanning six categories, containing 624 normal‚Äìmisleading image pairs (1,248 instances) sourced from natural, synthetic, and edited images with controlled semantics and option shuffling. It also introduces MVI-Sensitivity, a fine-grained metric that measures the relative performance drop from normal to misleading inputs to quantify robustness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Causally-Grounded LVLMs via Counterfactual Pair Training: Use MVI-style normal‚Äìmisleading pairs with rationale or attention supervision to penalize shortcut cues and align predictions with causal visual evidence.<br>‚Ä¢ Perception-Preserved Chain-of-Thought for Multimodal Models: Design thinking procedures and RL objectives that prevent visual forgetting, maintaining perceptual grounding during long-form reasoning.<br>‚Ä¢ Reflection- and Occlusion-Aware Vision Encoders via Targeted Curriculum: Build synthetic curricula for mirror reflections and occlusions to strengthen spatial disambiguation and real‚Äìvirtual separation.<br>‚Ä¢ Illusion-Augmented Pretraining for Robust Low-Level Perception: Incorporate diverse optical illusions and deceptive perspectives to improve fine-grained attribute discrimination and geometric reasoning.<br>‚Ä¢ Caption-to-Encoder Distillation for Robust Perception under Misleading Cues: Leverage strong captioners at training time to distill richer visual details into weaker LVLM encoders for test-time robustness.<br>‚Ä¢ Faithfulness Beyond Accuracy: Evaluating Visual Rationale Consistency on MVI-Bench: Develop metrics and protocols (e.g., attention-guided masking tests) that assess whether answers are visually grounded, not just correct.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13026" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13026" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Long-form video understanding involves rich, dynamic visual content; rethinking only text is insufficient to correct reasoning errors.<br>‚Ä¢ Existing reflection mechanisms are text-only, lacking cross-modal interaction and explicit visual re-evaluation, which leads to degraded performance on long videos.<br>‚Ä¢ Token-level, locally dependent generation causes repetitive/incorrect steps; reflection helps but must incorporate visual evidence to be effective.<br>‚Ä¢ RL setups that reward only final-answer correctness provide sparse, non-localized feedback and fail to teach models to select the right video segments.<br>‚Ä¢ There is a need to causally align reasoning with the specific visual segments that support the answer, avoiding reliance on irrelevant or spurious context.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>REVISOR is a two-stage, tool-augmented multimodal reflection framework: an MLLM performs initial reasoning over sparsely sampled frames, proposes a temporal review segment, a visual toolbox densely resamples frames from that segment, and the model reflects with these frames plus its initial trace to refine the answer. Training uses Dual Attribution Decoupled Reward (DADR) within GRPO, combining final-answer verification with a Causal Segment Sufficiency Reward to enforce that the selected segment alone suffices to derive the correct answer.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ REVISOR++: End-to-End Learnable Visual Toolbox for Adaptive Segment Resampling: Replace hand-crafted dense sampling with a trainable sampler that jointly optimizes segment proposal and frame selection.<br>‚Ä¢ Causal-REVISOR: Counterfactual Segment Sufficiency for Robust Video Reasoning: Estimate sufficiency via counterfactual/contrastive interventions to strengthen causal alignment and reduce spurious cues.<br>‚Ä¢ Streaming-REVISOR: Online Multimodal Reflection for Live and Hour-Scale Videos: Extend reflective reasoning to streaming inputs with memory-efficient segment proposal and continual CSSR.<br>‚Ä¢ Hierarchical Multi-Segment Reflection for Multi-Event Long Video QA: Generalize from single to multi-hop segment review with hierarchical planning over events and temporal dependencies.<br>‚Ä¢ Audio-Visual REVISOR: Incorporating Speech and Sound into Multimodal Reflection: Add audio (ASR, sound events) to the reflective loop for richer evidence aggregation in complex scenes.<br>‚Ä¢ AutoDADR: Adaptive Reward Balancing and Curriculum for Multimodal Reflection: Learn to adjust Œª1/Œª2 and difficulty curricula to jointly improve segment localization and answer accuracy.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">VIDEOP2R: Video Understanding from Perception to Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11113" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11113" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement fine-tuning (RFT), a two-stage framework consisting of supervised fine-tuning (SFT) and reinforcement learning (RL) has shown promising results on improving reasoning ability of large language models (LLMs). Yet extending RFT to large video language models (LVLMs) remains challenging. We propose VideoP2R, a novel process-aware video RFT framework that enhances video reasoning by modeling perception and reasoning as distinct processes. In the SFT stage, we develop a three-step pipeline to generate VideoP2R-CoT-162K, a high-quality, process-aware chain-of-thought (CoT) dataset for perception and reasoning. In the RL stage, we introduce a novel process-aware group relative policy optimization (PA-GRPO) algorithm that supplies separate rewards for perception and reasoning. Extensive experiments show that VideoP2R achieves state-of-the-art (SotA) performance on six out of seven video reasoning and understanding benchmarks. Ablation studies further confirm the effectiveness of our process-aware modeling and PA-GRPO and demonstrate that model's perception output is information-sufficient for downstream reasoning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing video RFT for LVLMs is process-agnostic, collapsing perception and reasoning into a single trajectory with a single final reward, which blurs credit assignment and makes it hard to fix perception vs. reasoning errors.<br>‚Ä¢ There is a lack of process-aware CoT supervision; current CoT annotations conflate perception and reasoning, preventing models from learning calibrated, disentangled processes.<br>‚Ä¢ Video reasoning inherently requires coordinated spatial‚Äìtemporal perception and logical inference; coarse, single-reward RL schemes and rigid/frozen preprocessors limit generalization and robustness on diverse video understanding benchmarks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VIDEOP2R separates video understanding into a perception segment (<observation>) and a reasoning segment (<think>/<answer>): SFT trains on a validated process-aware CoT dataset (VIDEOP2R-CoT-162K) generated via a three-step pipeline with observation sufficiency checks. It then applies PA-GRPO, a GRPO variant that assigns separate perception and reasoning rewards (plus format/length) with process-specific advantage normalization and token-level credit assignment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Process-Aware Critics for Video RFT: Replace rule-based/LLM-judge rewards with learned per-process critics to improve reward fidelity and sample efficiency in PA-GRPO.<br>‚Ä¢ Active Perception for Process-Aware Video Reasoning: Integrate uncertainty-aware frame selection/glimpsing driven by the perception segment to ensure observation sufficiency while reducing compute.<br>‚Ä¢ Multimodal Process-Aware RFT for Audio‚ÄìVideo‚ÄìText Narratives: Extend process-separated training and rewards to synchronize perception and reasoning across audio, video, and text for long-form multimodal understanding.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.14366" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.14366" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable "ruler" for progress toward Artificial General Intelligence.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Benchmark saturation: existing leaderboards (e.g., MMLU, MATH) no longer differentiate frontier models, masking real reasoning gaps.<br>‚Ä¢ Narrow or misaligned scope: many high-difficulty sets are either single-discipline (e.g., olympiad math/physics) or general-knowledge, not tailored to integrated scientific reasoning central to AI for Science.<br>‚Ä¢ Data contamination: heavy reuse of public exams/banks lets models exploit memorization rather than genuine reasoning.<br>‚Ä¢ Low-fidelity answer formats: multiple-choice and short strings under-represent real scientific outputs (multi-step derivations, LaTeX, structured answers), creating a fidelity gap.<br>‚Ä¢ Evaluation bottleneck: rule-based grading struggles with complex, multi-part, symbolic outputs; scalable, nuanced judging is missing.<br>‚Ä¢ Need for sustainability: lack of a community-driven, continually updated platform to track progress toward AGI-level scientific reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ATLAS introduces a contamination-resistant, cross-disciplinary benchmark (~800 expert-authored problems across 7 STEM domains) built via a multi-stage human‚ÄìAI pipeline: originality screening with RAG, adversarial filtering by state-of-the-art LRMs (‚â§40% pass), double-blind expert/meta review, and answer refinement. It pairs this with a scalable LRM-as-Judge evaluation workflow using structured JSON outputs and fine-grained sub-answer grading to assess complex, open-ended solutions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ ATLAS++: A Continuously Updated, Community-Driven Benchmark for AI4S Progress Tracking: Periodic new packs with adversarial filtering to prevent overfitting and maintain frontier difficulty.<br>‚Ä¢ JudgeBench: Calibrated LRM-as-Judge with Bias Control and Reliability Guarantees: Meta-evaluation of judge models, uncertainty calibration, tolerance policies, and consistency checks for open-ended scientific answers.<br>‚Ä¢ From Answers to Science: Evaluating Hypothesis Generation and Experimental Design: Extend ATLAS to research-planning tasks (hypotheses, protocols, literature critique) for higher-fidelity scientific assessment.<br>‚Ä¢ Auto-ATLAS: Human‚ÄìAI Co-Creation of Contamination-Resistant, Difficulty-Controlled Problems: Closed-loop generation with RAG originality checks, difficulty tuning via SOTA performance targets, and expert-in-the-loop validation.<br>‚Ä¢ Rewarding Reason: Reinforcement Learning with Verifiable, Structured Rewards from ATLAS: Use fine-grained sub-judgments as stable rewards to train LRMs and measure causal improvements on real benchmarks.<br>‚Ä¢ ATLAS-Multi: Multimodal and Long-Context Scientific Reasoning Evaluation: Incorporate figures, datasets, code, and lab protocols to test multimodal, long-context reasoning in realistic scientific workflows.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.14582" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.14582" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Omnimodal large language models (OmniLLMs) have attracted increasing research attention of late towards unified audio-video understanding, wherein processing audio-video token sequences creates a significant computational bottleneck, however. Existing token compression methods have yet to accommodate this emerging need of jointly compressing multimodal tokens. To bridge this gap, we present OmniZip, a training-free, audio-guided audio-visual token-compression framework that optimizes multimodal token representation and accelerates inference. Specifically, OmniZip first identifies salient audio tokens, then computes an audio retention score for each time group to capture information density, thereby dynamically guiding video token pruning and preserving cues from audio anchors enhanced by cross-modal similarity. For each time window, OmniZip compresses the video tokens using an interleaved spatio-temporal scheme. Extensive empirical results demonstrate the merits of OmniZip - it achieves 3.42X inference speedup and 1.4X memory reduction over other top-performing counterparts, while maintaining performance with no training.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ OmniLLMs suffer heavy compute and memory costs due to very long audio+video token sequences (often 10‚Äì20k tokens) and quadratic attention, hampering practical deployment.<br>‚Ä¢ Existing token compression methods are mostly single-modal; they ignore the joint, time-synchronized nature of audio‚Äìvideo streams with different temporal scales and sparsity, making naive pruning brittle.<br>‚Ä¢ Attention analyses show audio tokens consistently dominate attention and interactions are largely intra-window; global or video-only pruning disrupts temporal alignment and degrades performance.<br>‚Ä¢ Many prior approaches need LLM/ViT attention matrices, conflicting with FlashAttention and causing OOM on long sequences; they add overhead or require training, limiting scalability.<br>‚Ä¢ There is a need for a training-free, window-aware, cross-modal compression that preserves semantic alignment while delivering real speedup and memory reduction.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>OmniZip is a training-free, audio-guided token compressor that operates per time window: it selects salient audio tokens via audio-encoder attention, consolidates non-salient audio into anchors using cross-modal similarity, and then dynamically allocates video pruning using an audio-retention score, compressing video with an interleaved spatio-temporal module (merge temporally similar tokens and prune spatial redundancy via DPC-KNN).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ StreamZip: Streaming Audio-Guided Token Compression for Real-Time OmniLLMs: extend OmniZip to online, arbitrary-length audio‚Äìvideo streams with latency-aware scheduling.<br>‚Ä¢ AutoBalance: Learning Task-Aware Audio‚ÄìVideo Pruning Schedules: train a policy (e.g., RL/meta-learning) to adaptively set per-window audio/video pruning ratios for different tasks and content.<br>‚Ä¢ QuantZip: Joint Token Compression and Quantization/Pruning for OmniLLMs: co-design token compression with weight quantization and structured pruning to maximize end-to-end efficiency on edge GPUs.<br>‚Ä¢ Windowless OmniZip: Cross-Modal Compression Beyond Fixed Time Windows: generalize the approach to architectures without explicit window concatenation using learned event/segment boundaries.<br>‚Ä¢ DistillZip: Knowledge Distillation for Robust High-Ratio Omnimodal Compression: distill from a full-token teacher to recover accuracy under aggressive pruning.<br>‚Ä¢ Hardware-Aware OmniZip: Co-Optimizing Compression with FlashAttention and KV-Cache: tailor pruning to GPU kernels, KV-cache layout, and memory bandwidth for maximal wall-clock gains.<br>‚Ä¢ Event-Aware OmniZip: Multimodal Event Boundary Detection to Drive Dynamic Pruning: replace heuristic audio-retention with learned multimodal event detectors to better preserve salient moments.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13189" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13189" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Extreme multi-label classification (XMC) must balance accuracy and efficiency over label spaces up to millions; most systems still rely on small encoder-only transformers, limiting headroom.<br>‚Ä¢ Decoder-only LLMs produce strong text embeddings broadly, but are not effectively leveraged in XMC due to training/inference cost and lack of a robust Siamese training recipe; prior attempts (e.g., QUEST, MOGIC) show limited gains.<br>‚Ä¢ Text-only inputs (e.g., short product titles) are ambiguous; metadata is needed to disambiguate. Visual metadata remains under-exploited in XMC, and existing VLM-based routes are computationally heavy (hundreds of visual tokens).<br>‚Ä¢ There is no efficient multi-modal method that injects images while keeping sequence lengths short and vision encoders frozen to maintain tractable training/inference.<br>‚Ä¢ Benchmarks are mostly text-only; the field lacks standardized multi-modal datasets linking queries/labels to images.<br>‚Ä¢ Practical constraints matter: single-GPU training, reasonable training time, and low-latency inference for production use cases.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes dual-decoder learning that adapts decoder-only LLMs to Siamese-style XMC via structured prompts and contrastive training, and ViXML, an early-fusion multi-modal framework that concatenates a single projected image embedding (from a frozen vision encoder) to text token embeddings to keep efficiency. Combined with prototypical label representations (PRIME) and PEFT (LoRA), this yields state-of-the-art text-only and multi-modal XMC under manageable compute.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ BiDirectional-LLM4XMC: A Systematic Study of Attention Directionality and Pooling in LLM-based Siamese XMC: Compare uni- vs bi-directional attention and pooling strategies to further boost LLM encoders for XMC.<br>‚Ä¢ Token-Level ViXML: Fine-Grained Visual Token Injection for Efficient Extreme Classification: Replace single pooled image vectors with token-level features or lightweight vision finetuning to capture fine-grained cues under tight budgets.<br>‚Ä¢ RAG-ViXML: Retrieval-Augmented Visual-Text Prompts for Disambiguating Short Titles in XMC: Enrich prompts with retrieved product context to improve representations for short, ambiguous queries and labels.<br>‚Ä¢ Modality-Sparse ViXML: Robust Extreme Classification Under Missing Visual Metadata: Train with modality dropout and cross-modal distillation to handle cases where queries/labels lack images at inference.<br>‚Ä¢ Scaling Dual-Decoder XMC to 70B: Efficient Optimization and PEFT for Massive LLMs: Explore quantization-aware training, curriculum sampling, and advanced PEFT to scale LLM backbones without prohibitive cost.<br>‚Ä¢ Unified Generalist Embeddings for XMC and Retrieval: A Multi-Task Framework: Build a single embedder that preserves XMC quality while performing competitively on general text/multimodal retrieval benchmarks.<br>‚Ä¢ Zero-Shot Extreme Classification via Generative Label Semantics: Predict unseen labels by aligning query embeddings with generated label descriptions and contrastive distillation.<br>‚Ä¢ Fast ViXML Inference via Vision-Encoder Distillation and Compression: Distill heavy vision encoders (e.g., SigLIP2) into compact models and study accuracy‚Äìlatency trade-offs at scale.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.14460" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.14460" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ RL for LLM agents is underexplored: existing RL methods target single-turn, static tasks and do not capture multi-turn interaction, tool use, and stochastic environment feedback (pp. 1‚Äì4; Tables 1‚Äì2).<br>‚Ä¢ Lack of principled formalization: no comprehensive MDP formulation that distinguishes agent actions from environment feedback, separates generative and environmental transitions, and supports process rewards (pp. 2‚Äì5).<br>‚Ä¢ Missing flexible training infrastructure: scarcity of modular, extensible frameworks to train LLM agents end-to-end with multi-turn rollouts and diverse tools/environments (pp. 1, 5‚Äì7; Figs. 3‚Äì4).<br>‚Ä¢ Credit assignment challenges: difficulty attributing rewards to agent-generated tokens versus prompt/environment tokens in multi-turn traces, leading to unstable training and poor generalization (pp. 5, 8‚Äì9).<br>‚Ä¢ Reward design issues: overreliance on sparse final rewards; limited use of intermediate process rewards to guide complex tool-use behaviors (pp. 4‚Äì5, 8).<br>‚Ä¢ Practical gap in performance: naive RAG and base tool-calling underperform on multi-hop QA; need end-to-end RL agents that learn effective retrieval and reasoning (pp. 8‚Äì10; Table 3).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper extends the MDP formulation for LLM agents (state/action spaces, deterministic vs. stochastic transitions PG/PE, and dense process rewards) and introduces Agent-R1, a modular end-to-end RL framework with Tool/ToolEnv for multi-turn rollouts plus action/advantage masking for precise credit assignment, supporting standard RL algorithms (e.g., PPO/GRPO) and validated on multi-hop QA.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Unified Process-Reward Shaping for Tool-Augmented LLM Agents: Design and learn generalizable process rewards that accelerate tool-use competence across heterogeneous environments.<br>‚Ä¢ Action-Aligned Credit Assignment in Multi-Turn LLM Agents: A Theoretical and Empirical Study: Formalize and benchmark masking/attribution schemes beyond action/advantage masks for long-horizon trajectories.<br>‚Ä¢ ToolEnvBench: A Scalable Benchmark Suite for End-to-End RL of LLM Agents: Create standardized environments, metrics, and diagnostics to evaluate PG/PE transitions, reward density, and generalization.<br>‚Ä¢ Hierarchical Agent-RL for Open-Ended Tool Use: Compose Tool/ToolEnv with hierarchical policies to handle long-horizon tasks and curriculum learning across increasing environment complexity.<br>‚Ä¢ Safety-Constrained Reinforcement Learning for Autonomous LLM Agents: Integrate constraint RL and feedback models into Agent-R1 to ensure safe tool invocation and robust behavior under distribution shifts.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.14210" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.14210" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Orion, a visual agent framework that can take in any modality and generate any modality. Using an agentic framework with multiple tool-calling capabilities, Orion is designed for visual AI tasks and achieves state-of-the-art results. Unlike traditional vision-language models that produce descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition, and geometric analysis, to execute complex multi-step visual workflows. The system achieves competitive performance on MMMU, MMBench, DocVQA, and MMLongBench while extending monolithic vision-language models to production-grade visual intelligence. By combining neural perception with symbolic execution, Orion enables autonomous visual reasoning, marking a transition from passive visual understanding to active, tool-driven visual intelligence.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Monolithic VLMs remain passive and descriptive, lacking the precision, determinism, and compositional control needed to execute complex multi-step visual workflows.<br>‚Ä¢ Existing methods have limited fine-grained tool integration, multi-step planning, and structured validation, making them unreliable for production-grade tasks (e.g., document extraction, tracking, medical, compliance).<br>‚Ä¢ Poor support for seamless cross-modal I/O‚Äîingesting and generating images, videos, documents, audio, and text within one coherent agent loop.<br>‚Ä¢ Inadequate reliability mechanisms (reflection, schema validation) lead to brittle outputs and low success rates on complex tasks.<br>‚Ä¢ Inefficient handling of long, multi-turn sessions and large documents; need adaptive context management and pagination.<br>‚Ä¢ Lack of dynamic routing/escalation to specialized tools or stronger models with graceful fallbacks limits scalability and cost-performance.<br>‚Ä¢ Fragmented access to specialized visual skills (OCR, detection, segmentation, geometric analysis, UI parsing, redaction) under a single unified API.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Orion is a multi-modal, ReAct-style agent that plans, executes, and reflects to dynamically orchestrate a standardized library of specialized vision tools (OCR, detection, segmentation, geometric analysis, generation) across images, videos, documents, audio, and text, as illustrated by the architecture on page 4. It couples chain-of-thought planning, dynamic routing, serverless GPU-accelerated tool execution, and VLM-as-a-Judge reflection with schema-validated structured outputs for precise, reliable multi-step visual reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Plug-and-Play Tool Ecosystems for Visual Agents: Open interfaces and adapters for external/user-defined tools, enabling marketplace-style extension and domain-specific integrations.<br>‚Ä¢ Neurosymbolic Code Synthesis for On-the-Fly Vision Tooling: Automatically generate, verify, and execute custom computer-vision tools within the agent loop to meet task-specific requirements.<br>‚Ä¢ Hot-Swappable Multimodal Backbones for Cost-Aware Reasoning: Dynamic selection and switching among frontier/open models based on task difficulty, latency, and cost, with robust fallbacks and evaluation-in-the-loop.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">Mitigating Label Length Bias in Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.14385" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.14385" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLMs used for classification over candidate labels exhibit a previously overlooked label length bias: multi-token labels get penalized by the product-of-token probabilities, and naive length normalization over-corrects by favoring labels with highly predictable continuations (see Figure 2, p.3).<br>‚Ä¢ Existing calibration methods (e.g., contextual calibration and variants) largely assume single-token labels or only calibrate the first token, failing on real multi-token labels; remapping labels to single tokens loses semantics and is often infeasible (Tables 1 and 6).<br>‚Ä¢ Standard CC without length normalization amplifies length bias and can collapse performance on multi-label datasets (e.g., near-zero F1 on TREC-50; Table 2 and Appendix E), showing the need to calibrate at the full-label level.<br>‚Ä¢ Real-world tasks frequently contain multi-token labels (e.g., SST-5, Banking77, CLINC150), and bias harms accuracy, robustness to example selection, and confidence reliability across zero-/few-shot settings and even multiple-choice QA (Figures 3‚Äì5, Table 3).<br>‚Ä¢ Free-generation + similarity approaches (e.g., Gen+SBERT) avoid tokenization issues but introduce retrieval/similarity errors and cannot be calibrated via label probabilities, limiting reliability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Normalized Contextual Calibration (NCC): compute each label‚Äôs geometric-mean (length-normalized) probability and calibrate it by dividing by the corresponding baseline (prior) probability estimated from an ensemble of content-free inputs (empty string, space, ‚ÄúN/A‚Äù, ‚Äú[MASK]‚Äù, ‚ÄúLorem ipsum‚Äù); select the label with the highest calibrated score. This mitigates length bias and common-token priors at the full-label level while preserving probability normalization for reliable confidences.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Approximate Normalized Contextual Calibration for API-Restricted LLMs: Develop NCC variants that estimate full-label priors using only top-k logprobs or proxy models, enabling calibration when complete token distributions are unavailable.<br>‚Ä¢ Dynamic-Label NCC for Open-Ended Generation: Induce a task-specific candidate set on-the-fly (via retrieval/constrained decoding) and apply NCC to the induced labels to bring calibration benefits to generative tasks with evolving label spaces.<br>‚Ä¢ Tokenization- and Language-Robust NCC: Systematically study tokenizer effects across languages and propose subword-robust or byte-level NCC to ensure stable calibration under diverse tokenization schemes.<br>‚Ä¢ Learning the Priors: Meta-Learned Content-Free Probes for NCC: Jointly learn or select the optimal set/weights of content-free inputs and temperature to improve prior estimation and downstream calibration reliability.<br>‚Ä¢ NCC under Domain Shift: Normalized Domain-Context and Batch Generative Priors: Combine NCC with domain-context, batch, or generative prior estimation to maintain calibration and accuracy under distribution drift.<br>‚Ä¢ Uncertainty-Aware NCC for Safety-Critical Use: Integrate NCC with post-hoc uncertainty methods (e.g., temperature scaling, Bayesian ensembling) to further reduce ECE and enhance decision confidence in high-stakes applications.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Agent READMEs: An Empirical Study of Context Files for Agentic Coding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.12884" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.12884" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files ("READMEs for agents") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of empirical understanding and best practices for agent context files (ACFs) that steer coding agents; developers currently rely on fragmented, high-level guidance and trial-and-error.<br>‚Ä¢ Unclear whether ACFs behave as static documentation or living configuration; need to quantify their size, readability, structure, and evolution to improve maintainability and cost.<br>‚Ä¢ Existing SE/LLM research focuses on models and transient prompts, not persistent, project-level context artifacts that directly shape agent behavior.<br>‚Ä¢ Manifests heavily prioritize functional instructions while neglecting non-functional requirements (security, performance, UI/UX), risking insecure or inefficient agent outputs.<br>‚Ä¢ Manual analysis of ACF content does not scale; feasibility of automatic instruction classification is unknown for ecosystem-wide monitoring.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Mine 2,303 ACFs from 1,925 GitHub repositories (Claude Code, OpenAI Codex, GitHub Copilot), quantify length/readability (Flesch Reading Ease) and Markdown header structure, and analyze git histories for maintenance patterns. Construct a 16-category instruction taxonomy via LLM-assisted label creation and human coding, then evaluate automated multi-label classification with GPT-5 (micro-F1 0.79).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Quantifying Context Debt in Agent Manifests: Define context-file smells and metrics beyond readability to measure maintainability and relate them to agent performance.<br>‚Ä¢ Co-evolution of Context Files and Codebases: CI-aware linters that detect drift between Build/Run instructions and actual scripts/configs, and auto-suggest updates during refactorings.<br>‚Ä¢ SWE-NFR-Bench: Benchmarking Agents‚Äô Compliance with Non-Functional Requirements: Create tasks that require agents to satisfy security/performance constraints encoded in ACFs, and measure adherence and trade-offs.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Œ¶eat: Physically-Grounded Feature Representation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11270" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11270" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Foundation models have emerged as effective backbones for many vision tasks. However, current self-supervised features entangle high-level semantics with low-level physical factors, such as geometry and illumination, hindering their use in tasks requiring explicit physical reasoning. In this paper, we introduce Œ¶eat, a novel physically-grounded visual backbone that encourages a representation sensitive to material identity, including reflectance cues and geometric mesostructure. Our key idea is to employ a pretraining strategy that contrasts spatial crops and physical augmentations of the same material under varying shapes and lighting conditions. While similar data have been used in high-end supervised tasks such as intrinsic decomposition or material estimation, we demonstrate that a pure self-supervised training strategy, without explicit labels, already provides a strong prior for tasks requiring robust features invariant to external physical factors. We evaluate the learned representations through feature similarity analysis and material selection, showing that Œ¶eat captures physically-grounded structure beyond semantic grouping. These findings highlight the promise of unsupervised physical feature learning as a foundation for physics-aware perception in vision and graphics. These findings highlight the promise of unsupervised physical feature learning as a foundation for physics-aware perception in vision and graphics.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Semantic SSL features (e.g., DINO/MAE/CLIP) entangle high-level object semantics with physical factors (reflectance, geometry, illumination), limiting performance on physics-aware tasks.<br>‚Ä¢ Many applications (material selection/segmentation, intrinsic decomposition, SVBRDF capture, robotics) need features invariant to extrinsic factors (shape, lighting) yet sensitive to material identity and mesostructure.<br>‚Ä¢ Supervised material methods require large annotated datasets, are task-specific, and scale poorly; prior synthetic pairings of materials/geometries/lighting can be unrealistic and hard to scale.<br>‚Ä¢ Photometric augmentations in standard SSL deprioritize low-level cues (reflectance, transparency, fine texture), failing to induce invariance to physical variation.<br>‚Ä¢ Lack of a scalable, label-free pretraining strategy that yields physically grounded features suitable as a general backbone.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Œ¶eat replaces photometric augmentations with physically grounded multi-render augmentations‚Äîmultiple renderings of the same material across diverse geometries and illuminations‚Äîand fine-tunes a DINOv3 ViT to be invariant to extrinsic factors while sensitive to reflectance and texture. Training combines DINO-style image-level teacher‚Äìstudent alignment, patch-level masked latent reconstruction, Gram anchoring, KoLeo dispersion, and an in-batch InfoNCE that pulls together views of the same material and pushes apart different materials.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Disentangled Physical Feature Learning: Factorize Œ¶eat‚Äôs latent space into separable material, lighting, and geometry axes to enable controllable queries and edits.<br>‚Ä¢ Synthetic-to-Real Material Adaptation: Bridge the domain gap with unsupervised adaptation or generative augmentation for robust deployment on real photographs.<br>‚Ä¢ Open-Vocabulary Material Understanding: Align Œ¶eat with language supervision to enable zero-shot material retrieval and segmentation from text prompts.<br>‚Ä¢ 3D- and Video-Aware Œ¶eat: Extend pretraining to multi-view and temporal data to enforce geometric consistency and enable long-range material tracking.<br>‚Ä¢ Inverse Rendering with Œ¶eat Priors: Use Œ¶eat features as physics-aware regularizers for SVBRDF/intrinsic decomposition to reduce ambiguity and supervision needs.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13954" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13954" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Human emotions are difficult to convey through words and are often abstracted in the process; however, electroencephalogram (EEG) signals can offer a more direct lens into emotional brain activity. Recent studies show that deep learning models can process these signals to perform emotion recognition with high accuracy. However, many existing approaches overlook the dynamic interplay between distinct brain regions, which can be crucial to understanding how emotions unfold and evolve over time, potentially aiding in more accurate emotion recognition. To address this, we propose RBTransformer, a Transformer-based neural network architecture that models inter-cortical neural dynamics of the brain in latent space to better capture structured neural interactions for effective EEG-based emotion recognition. First, the EEG signals are converted into Band Differential Entropy (BDE) tokens, which are then passed through Electrode Identity embeddings to retain spatial provenance. These tokens are processed through successive inter-cortical multi-head attention blocks that construct an electrode x electrode attention matrix, allowing the model to learn the inter-cortical neural dependencies. The resulting features are then passed through a classification head to obtain the final prediction. We conducted extensive experiments, specifically under subject-dependent settings, on the SEED, DEAP, and DREAMER datasets, over all three dimensions, Valence, Arousal, and Dominance (for DEAP and DREAMER), under both binary and multi-class classification settings. The results demonstrate that the proposed RBTransformer outperforms all previous state-of-the-art methods across all three datasets, over all three dimensions under both classification settings. The source code is available at: https://github.com/nnilayy/RBTransformer.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing EEG emotion recognition models largely ignore inter-cortical (inter-electrode) interactions; they treat each channel independently and use attention mainly as a local filtering mechanism rather than to model electrode-to-electrode dynamics.<br>‚Ä¢ Capturing how emotions emerge from coordinated activity across cortical regions is crucial, yet current CNN/RNN/attention approaches don‚Äôt build an explicit electrode √ó electrode dependency structure.<br>‚Ä¢ Many prior works rely on fixed-grid 2D projections or handcrafted pipelines that are weakly neurophysiologically grounded for spatial structure.<br>‚Ä¢ Long-range dependencies are needed, but sequential models (RNN/LSTM) can be inefficient and struggle to model global context across electrodes.<br>‚Ä¢ Robust, consistent performance across datasets and affective dimensions (Valence, Arousal, Dominance) under realistic subject-dependent settings remains challenging.<br>‚Ä¢ Preprocessing often fails to adequately normalize inter-subject/channel variability and disentangle baseline activity from emotion-related signals.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RBTransformer converts EEG into baseline-corrected Band Differential Entropy (BDE) tokens, augments them with learnable Electrode Identity embeddings, and applies stacked inter-cortical multi-head self-attention blocks that explicitly construct an electrode √ó electrode attention matrix to model neural interactions, followed by a classification head. This enables frequency-aware, spatially grounded, global context modeling across electrodes without explicit temporal sequence modeling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Cross-Subject RBTransformer: Domain-Adaptive Inter-Cortical Transformers for Subject-Independent EEG Emotion Recognition: Incorporate adversarial/domain-invariant training, meta-learning, or instance normalization to generalize inter-cortical dynamics across unseen subjects.<br>‚Ä¢ Graph-Guided Inter-Cortical Transformers: Injecting Structural and Functional Connectivity Priors into Attention: Fuse EEG electrode layouts and dynamic graph priors so attention is constrained/regularized by neuroanatomical or functional connectivity information.<br>‚Ä¢ Multi-Modal RBTransformer: Joint EEG‚ÄìECG‚ÄìEDA Fusion with Inter-Modal Attention for Affective Computing: Extend BDE-like tokens and identity embeddings to other biosignals and learn cross-modal interactions alongside inter-cortical dependencies.<br>‚Ä¢ Self-Supervised Pretraining of EEG Inter-Cortical Dynamics: Masked-band modeling and contrastive objectives on large unlabeled EEG to pretrain attention over electrodes, improving downstream affect recognition with limited labels.<br>‚Ä¢ Interpretable Neuro-Affective Transformers: Causal and Attribution Analyses of Electrode√óElectrode Attention: Map attention patterns to cortical regions, validate against affective neuroscience findings, and perform perturbation/ablation to infer causal contributions.<br>‚Ä¢ Temporal-Interaction Transformers: Dual-Axis Attention for Inter-Cortical and Long-Range Temporal Dynamics: Add a temporal attention axis (or temporal convolutions) to RBTransformer to jointly model evolving inter-cortical dependencies over time.<br>‚Ä¢ Edge-Ready RBTransformer: Real-Time, Low-Power Emotion Recognition via Quantization and Pruning: Optimize the model for streaming inference on wearable/edge devices with latency, memory, and energy constraints.<br>‚Ä¢ Federated RBTransformer: Privacy-Preserving Collaborative Training for Affective EEG: Use federated learning and differential privacy to train robust inter-cortical models across institutions without sharing raw EEG.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Proactive Hearing Assistants that Isolate Egocentric Conversations</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11473" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11473" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce proactive hearing assistants that automatically identify and separate the wearer's conversation partners, without requiring explicit prompts. Our system operates on egocentric binaural audio and uses the wearer's self-speech as an anchor, leveraging turn-taking behavior and dialogue dynamics to infer conversational partners and suppress others. To enable real-time, on-device operation, we propose a dual-model architecture: a lightweight streaming model runs every 12.5 ms for low-latency extraction of the conversation partners, while a slower model runs less frequently to capture longer-range conversational dynamics. Results on real-world 2- and 3-speaker conversation test sets, collected with binaural egocentric hardware from 11 participants totaling 6.8 hours, show generalization in identifying and isolating conversational partners in multi-conversation settings. Our work marks a step toward hearing assistants that adapt proactively to conversational dynamics and engagement. More information can be found on our website: https://proactivehearing.cs.washington.edu/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Cocktail party problem: isolating the voices relevant to the wearer in crowded, multi-party settings is hard, causing cognitive overload and listening fatigue for many users, especially those with hearing loss.<br>‚Ä¢ Today‚Äôs hearables are reactive: they require manual enrollment or spatial selection and do not scale to multi-party, dispersed speakers or dynamic conversations.<br>‚Ä¢ Need proactive assistants that infer who the wearer is engaging with (conversation partners) without explicit prompts, and that can seamlessly suppress other speakers.<br>‚Ä¢ Strict real-time, on-device constraints: sub-20 ms end-to-end latency and limited compute/memory make long-context modeling difficult; standard attention scales quadratically with sequence length.<br>‚Ä¢ Lack of suitable egocentric, multi-party datasets with interfering conversations; models must generalize from synthetic training to real-world, spatialized audio.<br>‚Ä¢ Limitations of prior art: Target Conversation Extraction (TCE) operates offline with future context, uses explicit speaker embeddings, and assumes monaural recordings‚Äîunsuitable for real-time egocentric use.<br>‚Ä¢ Alternative modalities (EEG/AV) are impractical or bulky for real-world deployment; commercial modes (e.g., Conversation Awareness) reduce volume but do not separate or track speakers.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A dual-model, real-time pipeline anchors on the wearer‚Äôs self-speech (via a causal binaural beamformer) and leverages turn-taking cues: a slow attention-based model (updated every ~1 s) produces conversational embeddings, which guide a fast low-latency LSTM streaming separator (every 12.5 ms) to isolate conversation partners on-device. Trained on spatialized, synthetic multi-party mixtures with staged training and augmentation, it achieves large SISDR gains and high partner-selection accuracy while meeting embedded latency/memory budgets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Content-Aware Proactive Hearing via Lightweight Speech‚ÄìLanguage Models: Add semantic cues and dialogue-act signals to resolve overlapping speech and simultaneous turn-changes that challenge turn-taking-only models.<br>‚Ä¢ Silence-Robust Proactive Hearing with Multi-Modal Anchors (Gaze/AV-ASD/ASL): Maintain tracking during extended wearer silence by fusing egocentric video, gaze, or head-pose cues with audio to sustain the conversational state.<br>‚Ä¢ Cross-Lingual and Cross-Cultural Adaptation of Turn-Taking for Proactive Hearing: Learn culture- and language-specific turn-taking dynamics (via meta-learning or adaptive priors) to improve robustness across diverse conversational norms.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Error-Driven Scene Editing for 3D Grounding in Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.14086" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.14086" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite recent progress in 3D-LLMs, they remain limited in accurately grounding language to visual and spatial elements in 3D environments. This limitation stems in part from training data that focuses on language reasoning rather than spatial understanding due to scarce 3D resources, leaving inherent grounding biases unresolved. To address this, we propose 3D scene editing as a key mechanism to generate precise visual counterfactuals that mitigate these biases through fine-grained spatial manipulation, without requiring costly scene reconstruction or large-scale 3D data collection. Furthermore, to make these edits targeted and directly address the specific weaknesses of the model, we introduce DEER-3D, an error-driven framework following a structured "Decompose, Diagnostic Evaluation, Edit, and Re-train" workflow, rather than broadly or randomly augmenting data as in conventional approaches. Specifically, upon identifying a grounding failure of the 3D-LLM, our framework first diagnoses the exact predicate-level error (e.g., attribute or spatial relation). It then executes minimal, predicate-aligned 3D scene edits, such as recoloring or repositioning, to produce targeted counterfactual supervision for iterative model fine-tuning, significantly enhancing grounding accuracy. We evaluate our editing pipeline across multiple benchmarks for 3D grounding and scene understanding tasks, consistently demonstrating improvements across all evaluated datasets through iterative refinement. DEER-3D underscores the effectiveness of targeted, error-driven scene editing in bridging linguistic reasoning capabilities with spatial grounding in 3D LLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ 3D-LLMs struggle to spatially ground language to fine-grained visual attributes and relations (e.g., confusing colors or ‚Äònear‚Äô vs. ‚Äòfar‚Äô), which is critical for embodied AI and robotic manipulation.\n‚Ä¢ Scarce 3D training data induces strong co-occurrence biases (e.g., pillows are often white and near lamps), causing models to rely on linguistic shortcuts instead of 3D geometry (see example on page 2, Figure 1).\n‚Ä¢ Existing improvements largely use text-only augmentations that cannot change the 3D visuals, leaving visual biases unresolved and often applying blind, non-targeted data augmentation.\n‚Ä¢ Prior counterfactual augmentation mainly targets 2D/text; existing 3D scene editing tools are photorealism/user-driven oriented, not predicate-isolated or scalable for controlled edits of 3D geometry (orientation, distance).\n‚Ä¢ Building large new 3D datasets or performing heavy manual edits is costly; there is a need for low-cost, targeted 3D counterfactual edits that directly supervise failed predicates.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DEER-3D is a closed-loop, error-driven framework that decomposes each instruction into atomic predicates, diagnoses which predicate (appearance or spatial) caused the failure, and applies minimal Clone‚ÄìReplace‚ÄìModify edits (recolor/rotate/reposition) to produce targeted 3D counterfactuals with aligned QA supervision. The 3D-LLM is iteratively retrained on these counterfactuals to progressively correct its grounding weaknesses.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning to Edit: Reinforcement-Learned Policies for Predicate-Aligned 3D Counterfactual Generation: Learn an edit policy that selects what/where/how to edit to maximize grounding gains per unit cost.\n‚Ä¢ DEER-3D in the Wild: Error-Driven Scene Editing for Dynamic, Outdoor, and Deformable 3D Environments: Extend predicate-isolated edits beyond indoor static scenes to motion, weather, humans, and deformables.\n‚Ä¢ Uncertainty-Aware Diagnostic Evaluators for 3D-LLMs: Integrate perception and spatial uncertainty into the diagnosis and edit selection to better handle noisy reconstructions and ambiguous predicates.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11831" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11831" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LVLMs have a visual-perception bottleneck: lossy tokenization, resizing/patching, and token reduction in the visual encoder distort or discard global structure, making models fragile to topological information.<br>‚Ä¢ Existing benchmarks are semantically rich and rife with local/textual shortcuts, confounding perception with language reasoning and allowing correct answers without truly using the image.<br>‚Ä¢ There is no shortcut-free, granularity-controlled evaluation to directly quantify whether LVLMs preserve global visual features (e.g., loops and enclosure) independent of local cues.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TopoPerception is a diagnostic benchmark using synthetic topological images (uniform spanning trees on grid graphs) and a fixed multiple-choice question to remove textual cues and local visual shortcuts while classifying the number/nesting of closed loops. By scaling image partition granularity (resolution), it isolates and stresses global perception across levels to test whether LVLMs preserve topological class.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Topology-Preserving Visual Encoders for LVLMs: Design and analyze encoders/tokenization schemes that provably retain connectivity, loop counts, and enclosure relations under resizing and token compression.<br>‚Ä¢ CoT-Check: Visually-Grounded Chain-of-Thought with Topological Consistency: Integrate iterative image re-reading and topology checks into reasoning to prevent language priors from overriding fragile visual signals.<br>‚Ä¢ TopoAug: Training LVLMs with Topology-Aware Objectives and Synthetic-to-Natural Transfer: Use synthetic topology data and topology-preservation losses (e.g., persistent homology) to learn global structure retention and transfer it to natural-image tasks.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07865" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07865" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Chaos Engineering (CE) is an engineering technique aimed at improving the resilience of distributed systems. It involves intentionally injecting faults into a system to test its resilience, uncover weaknesses, and address them before they cause failures in production. Recent CE tools automate the execution of predefined CE experiments. However, planning such experiments and improving the system based on the experimental results still remain manual. These processes are labor-intensive and require multi-domain expertise. To address these challenges and enable anyone to build resilient systems at low cost, this paper proposes ChaosEater, a system that automates the entire CE cycle with Large Language Models (LLMs). It predefines an agentic workflow according to a systematic CE cycle and assigns subdivided processes within the workflow to LLMs. ChaosEater targets CE for software systems built on Kubernetes. Therefore, the LLMs in ChaosEater complete CE cycles through software engineering tasks, including requirement definition, code generation, testing, and debugging. We evaluate ChaosEater through case studies on small- and large-scale Kubernetes systems. The results demonstrate that it consistently completes reasonable CE cycles with significantly low time and monetary costs. Its cycles are also qualitatively validated by human engineers and LLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Chaos Engineering (CE) still relies on manual hypothesis definition, experiment planning, and post-incident remediation, which are labor-intensive and demand multi-domain expertise (Kubernetes, SRE, testing).<br>‚Ä¢ Existing CE tooling focuses on automating fault injection and metric collection/validation, but not the end-to-end cycle (hypothesis ‚Üí experiment ‚Üí analysis ‚Üí improvement), creating a critical automation gap.<br>‚Ä¢ Microservice/Kubernetes systems exhibit complex, emergent failure propagation; predicting behavior is hard and minor faults can cascade, making proactive resilience improvement essential.<br>‚Ä¢ The time and monetary costs of expert-driven CE limit accessibility; a low-cost, fully automated approach would broaden adoption and enable continuous resilience improvement.<br>‚Ä¢ Prior algorithmic approaches struggle with the creative, cross-domain nature of CE tasks; recent LLM capabilities suggest full-cycle automation is now feasible.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ChaosEater is an agentic LLM workflow that fully automates CE on Kubernetes: specialized LLM agents define steady states and failure scenarios, generate Validation-as-Code scripts (via K8s API and k6), plan and execute Chaos Mesh experiments, analyze failures, and iteratively reconfigure K8s manifests until hypotheses pass. It orchestrates pre-processing, hypothesis, experiment (pre-/during-/post-validation), analysis, improvement loops, and post-processing, deploying via Skaffold and validating end-to-end at low cost and latency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Safe Production-Grade Autonomous Chaos Engineering with Guardrails and Blast-Radius Control: Design policy-aware agents with safety constraints, impact scoping, canarying, and supervisory monitoring to enable secure CE in production clusters.<br>‚Ä¢ End-to-End Resilience Refactoring Beyond Manifests: Joint Optimization of Application, Frontend, and IaC: Extend reconfiguration from K8s manifests to app/frontend code and Terraform, optimizing resilience under SLO/SLA and cost constraints.<br>‚Ä¢ Long-Horizon Autonomous CE: Multi-Cycle Vulnerability Discovery with Memory and Hypothesis Refinement: Build agents that iteratively refine hypotheses across cycles, maintain memory of prior runs, adapt to temporal drift, and explore broader failure spaces.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 7</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-19 23:08:52 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 7;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>