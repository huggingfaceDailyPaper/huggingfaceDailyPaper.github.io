<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - December 16, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">December 16, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13586" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13586" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Autoregressive LLMs are bottlenecked by sequential left-to-right decoding, limiting inference throughput and parallelization.<br>â€¢ Existing masked diffusion models require bidirectional attention that prevents KV cache reuse, forcing full recomputation each iteration and resulting in high latency.<br>â€¢ Token-level parallel decoding in MDMs learns dependencies over an intractable token-combination space, leading to incoherent generation and difficult training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>ReFusion elevates parallel decoding from tokens to fixed-length slots via an iterative plan-and-infill process: a diffusion-based planner selects weakly dependent slots, and an autoregressive infiller decodes these slots in parallel under a unified causal framework. This slot-based design enables full KV cache reuse and reduces learning complexity from token combinations to slot-level permutations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Slot Boundaries for Diffusion-Planned Parallel Decoding: Learn context-dependent, variable-length slot segmentation to maximize independence and coherence while improving speed.<br>â€¢ Speculative ReFusion: Teacher-Guided Slot Planning and Parallel Infill for Faster LLM Generation: Combine speculative decoding with ARM guidance to refine slot selection and parallel infilling.<br>â€¢ Multimodal ReFusion: Slot-Based Plan-and-Infill for Text, Image, and Speech with KV Cache Reuse: Extend the framework to multimodal generation, evaluating efficiency and quality gains from slot-level parallel decoding.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Towards Scalable Pre-training of Visual Tokenizers for Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13687" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13687" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Reconstruction-only pre-training biases the latent space toward low-level details; improved pixel reconstruction does not translate to better generation and can even degrade it as compute scales.<br>â€¢ Existing visual tokenizers exhibit poor scalability for downstream generationâ€”their performance quickly saturates with more compute, parameters, or data in pre-training.<br>â€¢ Distillation or fixed-representation approaches are constrained by foundation models, leading to low performance ceilings or large reconstruction losses; a unified, semantics-aware tokenizer pre-training paradigm is lacking.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>VTP is a ViT-based autoencoder visual tokenizer jointly optimized with imageâ€“text contrastive learning (CLIP-style), self-supervised learning (masked image modeling and self-distillation), and pixel-level reconstruction, using a weighted multi-task loss and tailored batch sampling; the pixel decoder is later fine-tuned with a GAN objective while freezing the tokenizer.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Scaling Laws for Multi-Objective Visual Tokenizer Pre-training: Theoretically and empirically model how compute, parameters, and data scale each objectiveâ€™s contribution and predict optimal allocations to maximize downstream generative quality.<br>â€¢ Adaptive Curriculum and Loss Weighting for Semantics-Preserving Tokenizer Training: Dynamically schedule CLIP/SSL/REC losses and batch sizes (Î»_rec, Î»_ssl, Î»_clip) to balance semantic alignment and reconstruction, improving convergence and generative performance.<br>â€¢ VTP for Video and Multimodal Generation: Extend the unified tokenizer to video by incorporating motion-aware objectives (e.g., optical flow) and cross-modal alignment (audio/text), evaluating scalability and generative gains across modalities.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Memory in the Age of AI Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13564" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13564" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Conceptual fragmentation in agent memory research: heterogeneous motivations, implementations, assumptions, and evaluation protocols make results hard to compare and reuse.<br>â€¢ Insufficient traditional taxonomies (e.g., short/long-term) that fail to capture the diversity, roles, and temporal dynamics of modern agent memory systems.<br>â€¢ Confusion between agent memory and related constructs (LLM memory, RAG, context engineering), leading to ambiguous scope and misapplied methodologies.<br>â€¢ Lack of a unified formalism for how memory is formed, evolves, and is retrieved within and across tasks in single- and multi-agent settings.<br>â€¢ Limited consolidation of benchmarks and open-source frameworks, hindering empirical progress and practical development.<br>â€¢ Need for guidance on emerging frontiers (automation-oriented design, RL integration, multimodal and multi-agent shared memory, trustworthiness) to steer future work.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces a unified 'Formsâ€“Functionsâ€“Dynamics' taxonomy and a formal agentâ€“memory framework with lifecycle operators for formation, evolution, and retrieval, spanning single- and multi-agent systems. It delineates agent memory from LLM memory, RAG, and context engineering, maps representative systems to the taxonomy, and consolidates benchmarks and frameworks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Automation-Oriented Memory Design for AI Agents: Methods to automatically select, structure, and manage memory content and policies with minimal human intervention.<br>â€¢ Reinforcement Learning with Adaptive Memory Lifecycles: Deep integration of RL to learn when and how to form, evolve, and retrieve memory for improved long-horizon performance.<br>â€¢ Unified Multimodal Memory for Perceptionâ€“Action Agents: Architectures that store and retrieve cross-modal traces (text, code, vision, audio) to support complex interactive tasks.<br>â€¢ Shared Memory Architectures for Multi-Agent Collaboration: Design and governance of shared memory buffers enabling coordination, role alignment, and conflict resolution among agents.<br>â€¢ Trustworthy Memory: Robustness, Privacy, and Auditability in Agent Systems: Techniques for reliable memory curation, bias/error mitigation, privacy preservation, and transparent auditing across agent lifecycles.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.12967" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.12967" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of a mature, end-to-end post-training recipe for long-context reasoning; most work focuses on pre/mid-training or architectural context extension<br>â€¢ Scarcity of scalable, verifiable long-context training data beyond simple retrieval (NIAH) and single-hop RAG; human authoring/verification is infeasible at >32K tokens<br>â€¢ Instability in long-context multi-task RL (reward bias, entropy spikes, response-length explosion), causing training collapse and poor scalability<br>â€¢ Advantage estimation in GRPO is biased across heterogeneous tasks; random sampling induces domain/task distribution drift within batches<br>â€¢ Physical context window limits prevent processing 1Mâ€“4M token inputs; need memory management to extend single-pass reasoning to ultra-long streams</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>QwenLong-L1.5 introduces a unified post-training recipe combining a large-scale long-context data synthesis pipeline (KG-guided multi-hop QA, structural tabular numerical QA, and multi-agent self-evolving QA with robustness checks), stabilized RL (task-balanced sampling, task-specific advantage estimation, negative-gradient clipping, and AEPO for entropy control), and a memory-augmented agent architecture with multi-stage fusion RL and expert merging to handle inputs beyond the context window.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Entropy-Calibrated Advantage Estimation for Long-Context RL: Generalize AEPO with adaptive, task-aware advantage scaling to further reduce reward bias and stabilize exploration-exploitation dynamics<br>â€¢ Robust Graph-and-Table-Grounded Synthesis for Long-Context QA: Automate KG/table construction with self-verification and formal robustness guarantees to improve data quality at scale<br>â€¢ Hierarchical Memory Agents for 4Mâ€“10M Token Reasoning: Develop multi-level memory and planning with joint full-context and memory RL to extend ultra-long reasoning while controlling compute</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LongVie 2: Multimodal Controllable Ultra-Long Video World Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13604" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13604" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing video world models offer only low-level/local controllability, lacking global, semantic-level control over evolving scenes.<br>â€¢ Visual fidelity and temporal stability degrade over long horizons (beyond ~1 minute), leading to drift and quality collapse.<br>â€¢ Dense control signals (e.g., depth) dominate sparse cues (e.g., keypoints), causing imbalanced guidance and suboptimal long-term semantics.<br>â€¢ A trainingâ€“inference gap arises because long-form generation starts from degraded predicted frames and repeated VAE reconstructions, accumulating artifacts.<br>â€¢ Advancing toward unified video world models requires simultaneous controllability, long-term fidelity, and long-context consistencyâ€”critical for general spatiotemporal intelligence and practical applications.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>LongVie 2 is an autoregressive, controllable long-video diffusion framework trained in three progressive stages: multi-modal control injection (dense depth + sparse point maps) with feature/data-level degradations to balance guidance, first-frame degradation-aware training to bridge the trainingâ€“inference gap and sustain visual quality, and history-context guidance that feeds tail frames from preceding clips to enforce long-range temporal consistency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive World-Level Control Balancing for Long Video Models: Learn dynamic weighting of dense and sparse modalities (e.g., via meta-learning or reinforcement) to preserve semantic alignment across multi-minute sequences.<br>â€¢ Memory-Augmented Ultra-Long Video Generation Beyond Ten Minutes: Integrate scalable memory/state modules and hierarchical context to maintain coherence and avoid drift over ultra-long horizons.<br>â€¢ Interactive Agent-Conditioned Video World Models with Action Feedback: Extend controllability to structured actions and camera trajectories for closed-loop simulation and embodied interaction in complex environments.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13168" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13168" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management. We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work. We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of realistic, enterprise-grade benchmarks for finance and accounting that reflect spreadsheet-centric, messy, long-horizon, and multimodal workflows spanning text, tables, formulas, pivots, charts, code, and images.<br>â€¢ Existing evaluations are dominated by short, clean, single-modality or synthetic tasks, failing to capture interleaved activities like web search, cross-file retrieval, calculation/modeling, validation, visualization, translation, and reporting.<br>â€¢ Absence of principled workflow construction from authentic enterprise artifacts; prior datasets often ignore email threads, spreadsheet version histories, and collaborative context, losing real-world complexity.<br>â€¢ Need to quantify the gap between current frontier AI agents and enterprise workflow requirements; current systems show low pass rates even with large time budgets, underscoring limitations in long-horizon reasoning, tool use, and cross-artifact grounding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>FINCH is a benchmark built via LLM-assisted, expert-verified derivation of workflows from real email threads and spreadsheet version histories, followed by meticulous expert annotation, yielding 172 composite workflows across multimodal artifacts. The benchmark is used for human and automated evaluation of frontier AI agents on authentic F&A tasks sourced from Enron and other institutions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Agent Orchestration for Spreadsheet-Centric Enterprise Workflows: Design agent architectures that dynamically coordinate tools (spreadsheets, web search, PDFs, charts) to handle long-horizon, interleaved F&A tasks.<br>â€¢ Cross-Artifact Retrieval and Grounded Reasoning over Enterprise Assets: Develop retrieval-augmented models that jointly reason over emails, spreadsheets, version histories, and PDFs with precise cross-file grounding and provenance.<br>â€¢ Curriculum Learning on FINCH for Robust Financial Modeling and Validation: Use FINCH to scaffold curricula that progressively teach models formula synthesis, pivot manipulation, modeling, validation, and reporting under real-world noise.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.12730" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.12730" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing benchmarks largely test short-horizon tasks (function-level generation, scaffolded completion, or repair), failing to assess end-to-end repository construction from natural language.<br>â€¢ Current evaluations often rely on LLM judges or strong structural priors (scaffolding/signatures), obscuring true functional correctness and reducing demands on global planning, architecture design, and cross-file consistency.<br>â€¢ There is no rigorous, execution-based, apples-to-apples testbed that measures sustained agentic competence over hundreds of steps with binary correctness grounded in upstream tests.<br>â€¢ Long-horizon failure modes in current agents (premature termination, loss of global coherence, brittle dependency handling, inadequate planning) remain unaddressed and hinder progress toward autonomous software engineering.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>NL2Repo-Bench requires agents to construct a complete, installable Python library from an empty workspace using only a single natural-language requirements document, and evaluates correctness strictly by running the upstream repositoryâ€™s official pytest suite within a standardized environment. Tasks are curated from real-world projects under principled selection criteria to ensure complexity, maturity, recency, and verifiable testability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Hierarchical Planning and Global State Tracking for Long-Horizon Coding Agents: Develop multi-level planners and persistent memory mechanisms to maintain architectural coherence and cross-file consistency over extended build trajectories.<br>â€¢ Self-Verification Loops and Persistent Execution for Repository-Scale Code Generation: Design agents with iterative test-runâ€“diagnoseâ€“repair cycles and termination safeguards to reduce premature stopping and improve sustained correctness.<br>â€¢ Dependency- and Packaging-Aware Tool Augmentation for From-Scratch Library Construction: Integrate specialized tools for dependency resolution, environment isolation, and packaging workflows to robustly manage cross-file dependencies and installation fidelity.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.12602" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.12602" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ The quadratic time/memory complexity of softmax attention bottlenecks long-context processing, throughput, and real-time interactivity in LLMs, especially in RL and tool-use scenarios.<br>â€¢ Existing linear-time attention/SSM methods discretize an underlying continuous-time process with low-order schemes (typically Euler), causing truncation error, instability, and error accumulation on long sequences and stiff dynamics.<br>â€¢ Heuristic fixes (decay, gating, adaptive forgetting) stabilize training but cannot eliminate discretization error; a principled, numerically stable, error-free, and fully parallel linear-time attention is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>EFLA reformulates the delta-rule/linear attention as a continuous-time ODE and derives an exact closed-form solution by exploiting the rank-1 dynamics matrix, yielding an RK-infinity-equivalent update that is computable in linear time with full parallelism and no numerical integration error.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Error-Free Linear Attention for Long and Heterogeneous Contexts: Learn content-dependent time constants/decay and variable-step integration atop the exact solution to better handle diverse sequence regimes.<br>â€¢ EFLA-SSM: Unifying Exact Attention and State Space Models: Integrate EFLA with modern SSMs (e.g., Mamba) and multi-head architectures to study scaling, training dynamics, and theoretical equivalences.<br>â€¢ Provable Robustness and Quantization of Exact Linear Attention: Analyze and optimize EFLA under noise and low-precision arithmetic, providing robustness guarantees and hardware-aware training/implementation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">KlingAvatar 2.0 Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13313" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13313" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Inefficiency and instability in generating long-duration, high-resolution avatar videos, causing temporal drifting, visual degradation, and weak adherence to prompts as length increases.<br>â€¢ Poor multimodal fusion and instruction alignment over long horizons; modality conflicts between audio, image, and text degrade coherence.<br>â€¢ General video diffusion models lack robust audio conditioning, leading to inaccurate lipâ€“teeth synchronization, limited emotional expressiveness, and weaker identity preservation and camera control.<br>â€¢ Limited support for multi-character, multi-audio scenarios; difficulty in per-character synchronization and region-specific control.<br>â€¢ Coarse, global negative prompting fails to suppress shot-specific artifacts, implausible motions, and emotion mismatches in long-form generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A spatio-temporal cascade pipeline first generates low-resolution blueprint keyframes capturing global semantics and motion, then progressively refines and upscales them into high-resolution, temporally coherent sub-clips via firstâ€“last frame conditioning, guided by a multimodal Co-Reasoning Director with positive and negative prompts. Deep DiT-based mask prediction enables ID-specific, region-gated audio injection for multi-character control, and trajectory-preserving distillation accelerates inference.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Negative Prompting for Shot-Level Emotional Control in Avatar Diffusion: Learn context-aware, per-shot negative constraints that automatically stabilize facial expressions, gestures, and camera motion across extended durations.<br>â€¢ Self-Supervised Character Masking in DiT for Scalable Multi-Person Avatar Synthesis: Replace external detectors with self-supervised segmentation and cross-attention consistency losses to learn robust identity-aware masks directly from audioâ€“visual cues.<br>â€¢ Memory-Augmented Co-Reasoning Directors for Hour-Scale, Multi-Scene Avatar Narratives: Integrate long-term memory and world-state tracking into the director to maintain identity, props, and camera continuity across scenes and minutes-to-hours videos.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.09636" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.09636" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Deploying LLMs in mental-health contexts is risky when their reasoning is incomplete, inconsistent, or ungrounded, potentially causing harm.<br>â€¢ Existing psychological LLMs emphasize empathy or knowledge recall but lack clinically aligned, step-wise reasoning across appraisal, diagnosis, intervention, abstraction, and verification.<br>â€¢ Current evaluations and training pipelines focus on task accuracy and overlook reasoning quality (conciseness, coherence, hallucination avoidance, task understanding, internal consistency), with few mechanisms to enforce consistency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MentraSuite introduces MentraBenchâ€”a benchmark spanning six task types and five core clinical reasoning aspects with reasoning-quality metricsâ€”and Mindora, a post-trained LLM trained via joint SFTâ€“RL (CHORD) using an auxiliary inconsistency detector and structured, verifier-guided reasoning trajectories to enforce faithful, concise, and coherent reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ MentraBench-Clinical: Human-in-the-loop Evaluation of Reasoning Quality and Safety in Real Counseling Dialogues: Extend MentraBench with clinician-annotated safety and calibration metrics on real multi-turn cases.<br>â€¢ Retrieval-Augmented Consistency Rewards for Mental Health LLMs: Combine external evidence retrieval with adaptive inconsistency detection to ground and verify reasoning chains.<br>â€¢ Multimodal MentraSuite: Integrating Speech and Physiological Signals for Robust Appraisal and Diagnosis: Incorporate voice prosody and physiological markers to enhance cognitive-pattern recognition and severity assessment.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.10071" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.10071" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The 2025 BEHAVIOR Challenge is designed to rigorously track progress toward solving long-horizon tasks by physical agents in simulated environments. BEHAVIOR-1K focuses on everyday household tasks that people most want robots to assist with and these tasks introduce long-horizon mobile manipulation challenges in realistic settings, bridging the gap between current research and real-world, human-centric applications. This report presents our solution to the 2025 BEHAVIOR Challenge in a very close 2nd place and substantially outperforms the rest of the submissions. Building on Ï€_{0.5}, we focus on systematically building our solution by studying the effects of training techniques and data. Through careful ablations, we show the scaling power in pre-training and post-training phases for competitive performance. We summarize our practical lessons and design recommendations that we hope will provide actionable insights for the broader embodied AI community when adapting powerful foundation models to complex embodied scenarios.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Current VLA policies excel at short-horizon control but struggle on long-horizon household tasks where error accumulation and skill chaining degrade performance.<br>â€¢ Subtask decomposition with separate local policies does not model reliable transitions and is often incompatible with large-scale, offline end-to-end training.<br>â€¢ The BEHAVIOR simulatorâ€™s slowness and heterogeneous compute needs make online RL/adaptation sample-inefficient and impractical.<br>â€¢ Adapting generic foundation VLA models to complex, human-centric environments requires principled data scaling and training/inference design.<br>â€¢ Official datasets are skill-imbalanced and tasks vary widely in horizon and complexity, stressing generalization and robustness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Build on the Ï€0.5 VLA backbone, scale multi-task pre-training using BEHAVIOR human demos plus planner and offline RL trajectories, then apply rejection sampling fine-tuning with pose perturbations to create an offline data flywheel. At inference, use receding-horizon control with high-resolution RGB (and optional 3D) inputs and carefully chosen action horizons/representations to improve long-horizon stability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ DAgger-Enhanced Post-Training for VLAs in Slow Simulators: Combine DAgger/on-policy distillation with privileged experts to replace or augment RFT and boost sampling efficiency under BEHAVIOR-like constraints.<br>â€¢ Hierarchical VLAs with Learned Skill-Transition Models: Integrate explicit option-level or transition-aware modules within end-to-end VLAs to address skill chaining without sacrificing scalability.<br>â€¢ Adaptive Horizon and Perception for Long-Horizon Control: Develop policies that dynamically adjust action horizon, control mode, and input resolution/geometry based on scene difficulty to balance foresight and stability.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13080" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13080" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ VLA models perceive with 2D inputs but must act in 3D, causing weak correspondence between visual features and physical action space and poor spatial grounding.<br>â€¢ Existing 3D VLMs improve perception but do not explicitly align 3D understanding with action generation, limiting transfer to policy learning and generalization.<br>â€¢ Monocular 3D cues often have relative scale mismatches, breaking metric consistency needed for precise manipulation.<br>â€¢ Large-scale robot data is costly; human videos are abundant but prior approaches either learn implicit features or suffer from embodiment mismatch without actionable 3D alignment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>VIPA-VLA performs Spatial-Aware VLA Pretraining from human videos using a dual encoder (semantic vision + 3D Cut3R) fused via cross-attention: Stage-1 aligns 2D features to calibrated 3D visual annotations; Stage-2 extends the LLM with motion tokens to learn discretized 3D trajectories, followed by post-training with a diffusion-transformer action head using flow matching.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Embodiment-Conditioned Visual-Physical Alignment for Robot Kinematics: Introduce robot-specific constraints and kinematic mappings during pretraining to translate human 3D trajectories into feasible robot actions.<br>â€¢ Hybrid Humanâ€“Robot VLA Pretraining: Combining Visual-Physical Alignment with Robot Experience: Co-train VIPA-VLA on Hand3D and robot datasets to jointly learn spatial grounding and execution priors for stronger generalization.<br>â€¢ Metric-Consistent Multi-Sensor 3D Pretraining for VLA: Fuse multi-view RGB, depth/LiDAR, and egocentricâ€“exocentric cues to improve absolute scale calibration and robust 3D grounding across diverse environments.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.12692" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.12692" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLM-based WebAgents are myopic, selecting actions greedily from current observations without considering long-term consequences or alternative paths.<br>â€¢ Web environments are partially observable and include irreversible actions, making missteps hard to undo; the lack of explicit, safe backtracking undermines error correction and systematic exploration.<br>â€¢ Existing tree-search approaches assume reversibility and lack mechanisms for safe backtracking, leading to unintended side effects and reduced effectiveness in realistic web tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>WebOperator introduces an action-aware, best-first tree search that ranks candidate actions by both reward estimates and safety, coupled with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying to avoid side effects. It diversifies and curates actions by generating candidates from multiple reasoning contexts, filtering invalid actions pre-execution, and merging semantically equivalent actions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Reversibility and Side-Effect Models for Web Actions: Train predictive models of action reversibility and safety across diverse sites and integrate them as priors into action-aware search.<br>â€¢ Belief-State Planning for Partially Observable Web Environments: Extend WebOperator with POMDP-style belief tracking of hidden server-side state to improve long-horizon planning and recovery.<br>â€¢ Cross-Domain Action-Aware Search for UI Agents: Adapt the safe backtracking and action curation framework to mobile and desktop applications with platform-specific constraints and irreversible operations.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.11995" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.11995" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ VLMs excel at single-turn, well-specified questions but struggle with open-ended, multi-step visual tasks that require active exploration and adaptive planning.<br>â€¢ Existing benchmarks primarily evaluate final answers and overlook intermediate exploration steps, obscuring shortcut use, redundancy, and the ability to recover from mid-chain errors.<br>â€¢ The exploration space for free-form question/answer paths is prohibitively large, hindering reliable, objective, and fine-grained evaluation.<br>â€¢ Current datasets emphasize math/puzzle or toy environments solvable in language space and neglect question-space exploration (the ability to propose helpful sub-questions), limiting real-world relevance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>V-REX introduces a Chain-of-Questions (CoQ) framework that casts multi-step visual reasoning into finite multiple-choice sub-questions, disentangling Planning (selecting informative sub-questions) from Following (answering them) for reliable, fine-grained assessment. The benchmark spans 702 samples across 15 scenarios and 4 categories, using curated distractor questions/answers to quantify capabilities step-by-step.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning to Plan: Training VLMs for Question-Space Exploration with CoQ Supervision: Develop supervised/RL pipelines that explicitly optimize sub-question selection policies to improve Planning performance on V-REX.<br>â€¢ Recovering from Missteps: Self-Verification and Error-Correction for Robust Following in CoQ Chains: Introduce mechanisms to detect, backtrack, and repair incorrect intermediate answers to enhance final-task robustness.<br>â€¢ Beyond MCQ CoQ: Open-Ended Chain Generation and Reliable Process Evaluation for Exploratory Visual Reasoning: Automate free-form CoQ creation and judgment via calibrated process reward models and reference-aligned path scoring.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13250" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13250" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ VLMs excel at static-image VQA but lack ambulatory vision, i.e., the ability to actively move to acquire more informative viewpoints for answering questions.<br>â€¢ Prior EQA systems emphasize exploration, scene memory, and commonsense reasoning, while underexploring the core computer-vision challenge of localization and fine-grained perception based solely on current visual evidence.<br>â€¢ Existing approaches often rely on coarse, discrete actions and external knowledge or memorized scene representations; they do not learn continuous, visually grounded control (heading rotation, forward translation, viewing rotation) from partial observations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes Visually-Grounded Active View Selection (VG-AVS), fine-tuning pretrained VLMs to predict continuous viewpoint adjustments (heading, translation, viewing rotation) from a single image, trained on a ProcTHOR-based dataset of paired queryâ€“target views via supervised fine-tuning followed by RL-based policy optimization, and using the refined view with a VLM verifier for answering.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Memory-Augmented VG-AVS: Bridging Visual Grounding and Short-Term Scene Memory â€” Integrate lightweight episodic memory to support multi-step, continuous view selection while remaining grounded in current observations.<br>â€¢ Sim2Real Ambulatory Vision: Real-World Dataset and Policy Adaptation for Continuous Active View Selection â€” Curate a large real-world egocentric dataset and develop domain adaptation techniques to deploy VG-AVS on physical robots with robust continuous control.<br>â€¢ Uncertainty-Driven Active Perception for VLMs: Learning View Selection via Information Gain â€” Use self-supervised or RL objectives based on uncertainty/entropy reduction to select informative viewpoints without reliance on explicit action labels.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.12799" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.12799" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ VLA MLLMs lack fine-grained spatial outputs (3D occupancy and occupancy flow), weakening reliability, interpretability, and safety in autonomous driving.<br>â€¢ VA models provide precise spatial perception but cannot interact via language; mainstream VLA models rely mainly on images and miss LiDARâ€™s precise 3D geometry.<br>â€¢ The VAâ€“VLA paradigm split prevents a unified, end-to-end framework that jointly optimizes understanding, perception, prediction, and planning.<br>â€¢ Standard next-token generation in MLLMs struggles to produce dense 3D/4D outputs, calling for task-specific architectures and supervision.<br>â€¢ There is a shortage of datasets/benchmarks linking language with 3D occupancy and flow for training and evaluating linguistic 4D spatial reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DrivePI is a spatial-aware 4D MLLM that fuses LiDAR and multi-view images into latent BEV features, projects them into vision tokens, and jointly processes them with text tokens in a compact MLLM to produce text QA, 3D occupancy, occupancy flow, and action trajectories via four task-specific heads under end-to-end training. A data engine generates text-occupancy and text-flow QA pairs to supervise 4D linguistic spatial understanding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ DrivePI-Large: Scaling Spatial-aware 4D MLLMs with High-capacity Backbones and Synthetic 4D Pretraining: Scale the backbone beyond 0.5B and pretrain on large simulated 4D occupancy/flow corpora to boost precision and generalization.<br>â€¢ SafeDrivePI: Language-grounded Formal Safety Constraints and Uncertainty-aware Planning in 4D MLLMs: Integrate safety rules and calibrated uncertainty into the planning head to provide verifiable, risk-aware decision making.<br>â€¢ CrossSensor-DrivePI: Domain and Sensor Adaptation for Unified 4D MLLMs Across Cities and Modalities: Develop adaptation methods (e.g., self-supervised BEV alignment, test-time tuning) to handle new domains and sensors (radar, missing LiDAR) without extensive relabeling.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.11891" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.11891" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ VLA policies lack explicit safety guarantees in unstructured environments, leading to potential collisions, hardware damage, and injury.<br>â€¢ Existing safety methods often rely on retraining via reinforcement learning, which is computationally expensive, not plugâ€‘andâ€‘play, and treats safety as a soft reward penalty rather than a hard constraint with inference-time guarantees.<br>â€¢ Classical planners/reactive methods (e.g., A*/RRT*/APF) either override end-to-end semantic intent or lack rigorous, task-aware safety guarantees, making them ill-suited to preserve instruction-following.<br>â€¢ Control barrier functions require precise geometric states, creating a perception gap from raw images; conventional barriers are semantic-agnostic and cannot prioritize task-relevant obstacles.<br>â€¢ There is no suitable benchmark to evaluate safety-critical VLA behavior; standard LIBERO rarely induces collisions, limiting assessment of safety mechanisms.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>AEGIS adds a plug-and-play safety constraint layer to any VLA, using a VLM-guided obstacle identifier plus GroundingDINO and RGB-D fusion to fit minimum volume enclosing ellipsoids of obstacles and the end-effector, then solving a CBF-based QP that minimally adjusts the VLA action only when safety is at risk. This yields forward-invariance (hard) safety guarantees while preserving instruction-following performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Probabilistic VLSA: Safety-Constrained Control under Perception Uncertainty: Incorporate detection/depth uncertainty into chance-constrained or stochastic CBF-QP to maintain guarantees with noisy sensors.<br>â€¢ VLSA for Dynamic and Human-in-the-Loop Environments: Extend to moving obstacles and proxemics-aware human safety using time-varying CBFs and predictive tracking.<br>â€¢ Whole-Body VLSA: Extending CBF Constraints to Full Manipulator Geometry: Generalize from end-effector ellipsoids to link-level and tool-level constraints for comprehensive collision avoidance.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Image Diffusion Preview with Consistency Solver</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13592" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13592" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Slow, multi-step diffusion inference impedes interactive use and is impractical on resource-constrained devices, necessitating rapid previews for fast iteration.<br>â€¢ Previews must be high-fidelity and consistent with the final PF-ODE output so users can trust that refining a satisfactory preview yields the desired high-quality image.<br>â€¢ Training-free ODE solvers rely on assumptions misaligned with real model dynamics, often producing low-quality, low-step previews that lack fidelity and consistency.<br>â€¢ Distillation-based accelerations require costly retraining, break deterministic noise-to-image mapping of PF-ODE, accumulate trajectory errors, and reduce flexibility (e.g., step selection, score estimation).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Diffusion Preview introduces a two-stage workflowâ€”fast low-step preview followed by full-step refinementâ€”powered by ConsistencySolver, a lightweight trainable high-order linear multistep PF-ODE solver. ConsistencySolverâ€™s coefficients are optimized via reinforcement learning to match the modelâ€™s sampling dynamics, yielding high-fidelity, consistent low-step previews without modifying the base model.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Consistency Solvers for Prompt- and Style-Aware Diffusion Preview: Develop online or context-conditioned RL to adapt solver coefficients per prompt/style and content complexity, improving fidelity under diverse inputs.<br>â€¢ Extending Diffusion Preview to Video and 3D Generative Models with Temporal/Geometric Consistency: Generalize RL-optimized multistep solvers to video and 3D PF-ODEs, ensuring previewâ€“final consistency across time and viewpoints.<br>â€¢ Theoretical Guarantees for RL-Optimized Linear Multistep Solvers in PF-ODEs: Establish stability, convergence, and error bounds for learned solvers, linking preview fidelity to final-step consistency.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.12751" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.12751" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Black-box action-to-video diffusion models lack explicit physical modeling, leading to biased and physically inconsistent driving videos (e.g., favoring going straight even when commanded to turn).<br>â€¢ Autonomous driving needs physics-aware world models for planning, long-tailed data synthesis, and closed-loop evaluation; existing methods struggle to provide controllable, accurate 4D scene understanding.<br>â€¢ Efficient and accurate compression of high-resolution 4D occupancy is hard; prior VQ/VAE designs either lose information (discrete tokens) or are computationally heavy (diffusion), and training VAE and predictors separately causes representation misalignment.<br>â€¢ Multi-view video generation often lacks cross-view consistency; naÃ¯ve cross-view attention is computationally prohibitive and destabilizes pretrained priors during fine-tuning.<br>â€¢ Long-horizon forecasting and generation degrade sharply over time in existing methods and demand large models and resources.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>GenieDrive is a two-stage physics-aware world model: it forecasts future 4D occupancy with a compact tri-plane VAE and Mutual Control Attention trained end-to-end, then splats occupancy into semantic maps to guide a pretrained video generator augmented with Normalized Multi-View Attention for consistent, controllable multi-view driving videos.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ GenieDrive-ClosedLoop: Integrating Physics-Aware World Models with Planning and Control for Autonomous Driving: Couple occupancy forecasting with a planner to evaluate, optimize, and learn controls in simulation via closed-loop training.<br>â€¢ Uncertainty-Aware GenieDrive: Probabilistic 4D Occupancy and Multi-View Video Generation with Calibration: Model and propagate uncertainty in the tri-plane latent and video generation, using Bayesian/ensemble methods for calibrated risk-aware predictions.<br>â€¢ Cross-Domain GenieDrive: Domain-Adaptive Physics-Aware World Modeling for Diverse Cities and Sensors: Develop adaptation and self-supervised objectives to generalize GenieDrive across datasets, sensor suites, and weather/lighting conditions while maintaining physics consistency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological "Censorship"</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.11883" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.11883" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Over-alignment to generalized aesthetic preference conflicts with instruction following, causing models to ignore requests for anti-aesthetic outputs needed for artistic, critical, or technical purposes.<br>â€¢ Reward models encode mainstream beauty bias, penalizing anti-aesthetic images even when they perfectly match prompts, thereby imposing developer-centered values and reducing aesthetic pluralism and user autonomy.<br>â€¢ Existing pipelines assume a universal aesthetic standard and lack benchmarks for wide-spectrum (anti-aesthetic) cases, obscuring systemic bias and de facto ideological "censorship."</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Construct a wide-spectrum aesthetics dataset and benchmark state-of-the-art image generation and reward models on instruction-following tasks, quantifying bias via reward scores (e.g., HPSv3), image-to-image editing experiments, and comparisons against real abstract artworks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Decoupling Instruction Following from Aesthetic Preference: Designing Reward Models that Condition on Explicit User Intent â€” Develop reward functions that prioritize prompt fidelity when user requests conflict with mainstream aesthetics.<br>â€¢ The Wide-Spectrum Aesthetics Benchmark: Metrics and Dataset for Evaluating Aesthetic Pluralism in Generative Models â€” Establish standardized datasets and metrics to systematically assess anti-aesthetic performance and bias.<br>â€¢ Multi-Objective Alignment for Controllable Aesthetic Bias in Image Generation â€” Use multi-objective RLHF with tunable weights to balance instruction following and aesthetic preferences per user or task.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Towards Interactive Intelligence for Digital Humans</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13674" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13674" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Most digital humans are imitative and offline, lacking personality-aligned expression, adaptive responsiveness, and long-term narrative coherence.<br>â€¢ Standard LLM responses drift from persona and violate narrative causality over extended interactions due to weak memory and context control.<br>â€¢ Existing TTS/tokenizers entangle semantic and acoustic cues, run at high frame rates/bitrate, and are optimized for offline MOS rather than low-latency, dialogue-centric generation.<br>â€¢ Facial animation suffers from â€œzombie-faceâ€ listening stiffness because listener motion is weakly correlated with audio and lacks a learned internal motion prior.<br>â€¢ Real-time, editable full-body motion is hard: autoregressive models accumulate errors; diffusion is too slow; smooth mid-stream prompt switches and long-horizon coherence are unmet.<br>â€¢ Rendering by image-driven diffusion compromises multi-view identity consistency, parameter-faithful control (FLAME/SMPL), and temporal stability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Mio is a unified, real-time multimodal framework with a Thinker coordinating a Kodama speech stack (semanticâ€“acoustic disentangled tokenizer + LLM-based TTS), a UniLS two-stage facial animator (audio-free motion priors then dual-audio cross-attention finetuning), a FloodDiffusion body animator (causal VAE latent + lower-triangular vectorized noise schedule with frame-wise text conditioning), and an AvatarDiT renderer (parameter-driven vDiT with FLAME/SMPL adapters for identity-consistent, multi-view video).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Self-Evolving Digital Personas with Long-Horizon Memory Graphs: Learn online updates to hierarchical memory and diegetic knowledge graphs (via RLHF and constrained optimization) to adapt persona without drift over multi-session interactions.<br>â€¢ Streaming Diffusion Forcing for Real-Time Embodied Control: Formalize and generalize FloodDiffusionâ€™s lower-triangular schedules and bi-directional window attention, proving stability/latency bounds and extending to other streaming modalities (gesture, gaze, locomotion).<br>â€¢ Neural Parameter-to-Video Rendering with 3D Consistency: Fuse FLAME/SMPL control with explicit 3D scene representations (NeRF/GS) and camera-aware losses to further improve multi-view identity preservation, lighting realism, and temporal coherence.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">RecTok: Reconstruction Distillation along Rectified Flow</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13421" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13421" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Latent space dimensionality in visual tokenizers imposes a trade-off: low dimensions ease diffusion training but limit reconstruction fidelity and semantic expressiveness, while high dimensions destabilize generation.<br>â€¢ Existing VFM-based distillation enriches only the clean latent x0, yet diffusion transformers train on forward-flow states xt where semantics degrade, hurting convergence and generative quality.<br>â€¢ High-dimensional tokenizers underperform low-dimensional ones and converge slowly; approaches like RAE improve generation but sacrifice reconstruction by freezing VFMs and lack a principled treatment of semantics along the flow.<br>â€¢ There is a need for a tokenizer that simultaneously improves reconstruction, generation, and discriminative semantics as dimensionality scales, accelerates training, and achieves strong performance without classifier-free guidance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>RecTok distills VFM semantics into the entire rectified forward flow trajectory via Flow Semantic Distillation (FSD) using a lightweight semantic decoder, and strengthens semantics with Reconstructionâ€“Alignment Distillation (RAD) by masked reconstruction on noisy latents aligned to VFM featuresâ€”making xt semantically rich and enabling high-dimensional latents with improved reconstruction and generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Flow-Aware Semantic Tokenizers for Video Diffusion: Extend FSD and RAD to temporally coherent forward flows to maintain semantics across frames and improve video generation quality.<br>â€¢ Curriculum Flow Distillation: Learning Timestep Schedules for High-Dimensional Latents: Develop adaptive or learned timestep sampling and curriculum strategies that optimize semantic retention and convergence as latent dimensionality scales.<br>â€¢ Multi-Teacher Flow Distillation for Robust Semantics: Combine complementary VFMs with gating/ensembles to distill diverse semantics along the flow without degrading generation, improving robustness across tasks and domains.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.10655" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.10655" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Text-to-image diffusion models can unintentionally reproduce training images (memorization), posing privacy and copyright risks.<br>â€¢ Training-time fixes are impractical for deployed systems because they require dataset access and substantial compute.<br>â€¢ Most inference-time methods steer via CFG, prompt perturbation, or cross-attention tweaks, often reducing prompt alignment and visual quality.<br>â€¢ Existing approaches typically act globally and lack precise localization of when (timesteps) and where (spatial regions) memorization occurs.<br>â€¢ There is a need for a training-free, inference-time method that targets memorized regions/timesteps while preserving semantic fidelity by operating directly in latent space.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>CAPTAIN is a training-free inference framework that operates in latent space: it initializes with frequency-decomposed latents (low-frequency structure from a non-memorized reference image plus high-frequency Gaussian noise), then finds an effective timestep window and spatial masks by combining Bright Ending and concept-specific attention, and injects semantically aligned reference features into masked regions during those steps to suppress memorization without altering CFG or prompts.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Retrieval-Free CAPTAIN: Self-Generating Non-Memorized Semantic References for Diffusion Safety: Replace web retrieval with on-the-fly, text-conditioned synthetic references to eliminate dependency and improve privacy.<br>â€¢ Temporal CAPTAIN: Memorization Mitigation in Text-to-Video Diffusion via Motion-Aware Feature Injection: Extend localized feature injection with temporally consistent masks and motion cues to handle video generative models.<br>â€¢ Learning When and Where to Inject: A Meta-Controller for Timestep and Region Selection in Diffusion Models: Train a lightweight controller to predict optimal injection windows and masks per prompt/image, reducing reliance on heuristic BE signals.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13690" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13690" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4times real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Video diffusion models are slow, imprecise, and opaque during generation, preventing users from seeing progress or correcting course until late.<br>â€¢ Limited controllability stems from inherent stochasticity; users cannot steer trajectories mid-denoising.<br>â€¢ Existing efficiency methods (e.g., distillation, cascades, MoE, sparse attention) often degrade quality/diversity, alter model capacity, or require complex pipelines, and do not provide real-time previews.<br>â€¢ Control via extra conditioning (e.g., depth/edges) changes base model behavior and complicates training/inference.<br>â€¢ Reinforcement-learning or aesthetic-reward-based guidance needs full roll-outs, making interactive guidance impractical.<br>â€¢ There is a lack of tools to probe when and how scene intrinsics (geometry/appearance) emerge during denoising.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Attach a lightweight, model-agnostic multi-branch decoder to intermediate diffusion features (timesteps or blocks) to generate fast, consistent multi-modal previews (RGB, albedo, normals, depth) with negligible overhead and without altering the base generator. Use these previews to enable early termination and interactive steering of denoising via stochasticity reinjection and modal (color/depth/normal) guidance while preserving final video fidelity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Preview-Guided Tree Search for Video Diffusion: Leverage multi-modal previews as node evaluations to automatically branch, prune, and select denoising trajectories for quality/diversity objectives.<br>â€¢ Universal Intrinsic Decoders Across Backbones: Train generalizable preview heads that transfer across diverse video diffusion architectures and datasets without per-model finetuning.<br>â€¢ Human-in-the-Loop Reward-Free Alignment via Preview Steering: Formalize interactive, mid-denoising steering with previews as preference optimization to align outputs without expensive roll-outs.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">LitePT: Lighter Yet Stronger Point Transformer</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13689" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13689" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has 3.6times fewer parameters, runs 2times faster, and uses 2times less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of principled guidelines on how to combine convolution and attention in 3D point cloud backbones; current models indiscriminately use both at all hierarchy levels.<br>â€¢ Inefficiency of state-of-the-art architectures (e.g., PTv3) where attention is costly at high-resolution early stages and sparse convolutions dominate parameter budgets (â‰ˆ67%), inflating memory and latency.<br>â€¢ Positional encoding reliance on learnable convolutional modules (CPE) adds substantial parameters; existing PE schemes (RPE, cRPE, CPE) are either computationally heavy or parameter-inefficient.<br>â€¢ Need for lightweight, high-performance backbones that reduce training/inference memory and latency while preserving or improving accuracy across semantic/instance segmentation and detection.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>LitePT is a stage-aware hybrid backbone that uses sparse convolutions in early, high-resolution stages and switches to attention in deeper, low-resolution stages, augmented by a parameter-free 3D rotary positional embedding (PointROPE). This design removes redundant attention where local geometry dominates and replaces convolutional positional encoding with PointROPE, achieving fewer parameters, faster runtime, and lower memory while matching or surpassing PTv3.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive LitePT: Input-Driven Stage Scheduling for Hybrid 3D Backbones: Learn a controller that dynamically selects convolution vs. attention per stage based on token density and semantic complexity to optimize accuracy-efficiency trade-offs.<br>â€¢ PointROPE++: Learnable Frequency and Axis Coupling for Rotary 3D Positional Encoding: Extend PointROPE with lightweight learnable frequency scales and cross-axis coupling to improve robustness to anisotropy and rotations while retaining near-zero parameter overhead.<br>â€¢ Spatiotemporal LitePT for Streaming LiDAR: Stage-Aware Hybrid Modeling with Temporal PointROPE: Generalize LitePT to sequences by integrating causal local attention and temporal rotary embeddings to efficiently capture motion and long-range temporal context.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Directional Textual Inversion for Personalized Text-to-Image Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13672" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13672" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Textual Inversion (TI) often fails on complex prompts, reducing controllability and text fidelity.<br>â€¢ Learned TI tokens exhibit embedding norm inflation, drifting to out-of-distribution magnitudes that degrade prompt conditioning in pre-norm Transformers.<br>â€¢ Inflated norms attenuate positional information and hinder residual updates, harming contextualization.<br>â€¢ TI requires long per-concept fine-tuning, limiting practical scalability.<br>â€¢ TI variants that expand embedding spaces improve fidelity but introduce significant computational and storage overhead, undermining TIâ€™s efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Directional Textual Inversion (DTI) fixes the token embedding magnitude to an in-distribution scale and optimizes only the direction on the unit hypersphere via Riemannian SGD, casting direction learning as MAP with a von Misesâ€“Fisher prior that yields a simple constant-direction prior gradient. This direction-only parameterization improves prompt faithfulness while preserving subject similarity and enables coherent concept interpolation via slerp.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Hyperspherical Textual Inversion: Extend DTI to multi-token and multi-layer settings with learned per-layer radii and directional couplings to better capture complex concepts.<br>â€¢ Direction-Only Personalization Across Encoders: Generalize DTI to non-CLIP text encoders and multimodal architectures to test the universality of direction-dominant semantics and norm sensitivity.<br>â€¢ Slerp-Based Concept Composition for Text-to-Image: Leverage hyperspherical slerp to compose and interpolate multiple personalized concepts, studying controllability and semantic coherence in composite prompts.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Few-Step Distillation for Text-to-Image Generation: A Practical Guide</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13006" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13006" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ High latency and compute cost of diffusion-based text-to-image (T2I) models due to hundreds of NFEs, limiting real-time and resource-constrained applications<br>â€¢ Lack of a systematic, fair adaptation and comparison of few-step distillation methods for open-ended T2I; most public results focus on class-conditional or unconditional image generation<br>â€¢ Training instability and unclear scalability of trajectory-based methods (sCM, IMM, MeanFlow) when moving from discrete labels to free-form language prompts<br>â€¢ Absence of a unified framework, interoperable formulations (e.g., FM â†” TrigFlow), and practical recipes (input scaling, architecture, hyperparameters) with reproducible code and pretrained students</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Unifies distribution- and trajectory-based few-step distillation for T2I, adapting sCM, MeanFlow, and IMM to open-ended prompts and deriving interoperable mappings between Flow Matching and TrigFlow. Provides practical training recipes, an open-source codebase with pretrained students distilled from FLUX.1-lite, and a MeanFlow distillation variant tailored for T2I.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Prompt-Conditioned Few-Step Distillation for Text-to-Image: Learn prompt-aware guidance schedules and losses to handle long, compositional, and style-specific prompts within 1â€“4 NFEs<br>â€¢ Hybrid Adversarialâ€“Consistency Distillation for Fast T2I: Combine latent adversarial objectives (LADD) with trajectory consistency (sCM/MeanFlow) to jointly optimize fidelity, stability, and speed<br>â€¢ Scalable Theory and Benchmarking of FMâ€“TrigFlow Interoperability in T2I: Establish unified benchmarks and derive error bounds for samplerâ€“model conversions and few-step budgets in open-vocabulary settings</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">Flowception: Temporally Expansive Flow Matching for Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.11438" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.11438" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Autoregressive (AR) video generators suffer from exposure bias and error accumulation/drift, as models are trained on ground truth but must condition on their own imperfect outputs at inference.<br>â€¢ Causal attention in AR models limits expressiveness and long-term context modeling; efficient sampling depends on KV caching, which constrains architecture design.<br>â€¢ Full-sequence diffusion/flow models require fixed-length generation, incur quadratic attention cost over all frames, and cannot stream (frames must be fully denoised before display).<br>â€¢ Existing approaches poorly handle variable-length generation and struggle to jointly learn video length and content.<br>â€¢ Training and sampling are computationally expensive for full-sequence models; scaling to long videos is limited by memory and compute.<br>â€¢ Current paradigms are not unified across tasks (image-to-video, video-to-video, interpolation), lacking a single model that flexibly inserts or preserves frames.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Flowception interleaves continuous flow-matching denoising with stochastic frame insertions by predicting per-frame velocity fields and insertion rates, yielding a coupled ODEâ€“jump process for variable-length, non-autoregressive video generation. This schedule marginalizes unrevealed frames to cut attention FLOPs (â‰ˆ3Ã— lower in training; â‰ˆ1.5Ã— in sampling vs full-sequence), supports any-order conditioning, and improves robustness over AR while enabling streaming-like behavior.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Insertion Policies for Flowception: Learn content- and context-aware insertion schedules and priorities to optimize quality/latency trade-offs and long-horizon coherence.<br>â€¢ Hierarchical Flowception with Multi-Scale Temporal Blocks: Introduce coarse-to-fine temporal insertion and denoising to handle very long videos with improved efficiency and global consistency.<br>â€¢ Multimodal Flowception for Audio-Text-Video Co-Generation: Extend the ODEâ€“jump framework to incorporate audio and rich text prompts, aligning frame insertion and denoising with multimodal temporal cues.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">What matters for Representation Alignment: Global Information or Spatial Structure?</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.10794" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.10794" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its global semantic information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of spatial information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in <4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Misaligned assumptions: The field assumes higher global semantic performance (e.g., ImageNet-1K linear probing) yields better generation with REPA, but empirical evidence shows this often fails.<br>â€¢ Poor target selection: Practitioners lack reliable criteria to choose vision encoders for representation alignment; global accuracy mispredicts generative quality.<br>â€¢ Architectural bottlenecks: Standard MLP projection layers in REPA degrade spatial information, and current alignment setups underutilize spatial structure.<br>â€¢ Missing diagnostics: No simple, fast metric exists to quantify spatial self-similarity in target representations and predict downstream generative performance.<br>â€¢ Training efficiency: Need methods that consistently accelerate convergence of diffusion models by leveraging the right aspects of external representations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper performs a large-scale correlation study across 27 encoders showing spatial structure, not global accuracy, predicts REPAâ€™s generative performance, introduces a simple Spatial Structure Metric (SSM) based on self-similarity vs lattice distance, and proposes iREPAâ€”replacing MLP with a convolutional projection and adding a spatial normalization layerâ€”to better preserve and transfer spatial information, yielding faster convergence across encoders and recipes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Spatial-First Alignments for Diffusion Training: Design alignment losses and architectures that explicitly optimize spatial self-similarity transfer, adaptively weighting tokens by spatial proximity and noise level.<br>â€¢ Beyond Global Accuracy: A Benchmark of Spatial Structure Metrics for Generative Pretraining: Systematically compare spatial metrics (e.g., LDS variants, graph kernels) as predictors of generative performance across datasets, scales, and training recipes.<br>â€¢ Representation Alignment without External Encoders via Self-Spatial Regularization: Develop internal spatial regularizers and token-topology-preserving projections to replace external encoders, achieving iREPA-like benefits end-to-end.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">START: Spatial and Textual Learning for Chart Understanding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07186" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07186" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ MLLMs struggle with chartsâ€™ dual natureâ€”structured spatial layouts (axes, legends, subplots) and underlying textual/data representationsâ€”leading to poor grounding and fine-grained reasoning.<br>â€¢ Most chart QA training omits explicit chart-element grounding and chart-to-code recovery, limiting precise interpretation and step-by-step reasoning.<br>â€¢ Existing code-based synthetic datasets are visually simple, lack layout/style diversity, and mismatch real chart distributions; real-image datasets lack executable code and element locations.<br>â€¢ There is no dedicated benchmark to evaluate spatial understanding in charts, hindering comprehensive assessment.<br>â€¢ Current models underperform on complex chart tasks (even strong vision reasoning models), highlighting the need for spatialâ€“textual learning across SFT and RL paradigms.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>START jointly trains chart-element grounding (spatial learning) and chart-to-code generation (textual learning) alongside chart QA under both supervised finetuning and GRPO-based reinforcement learning. It builds the START-Dataset by translating real chart images into executable Python code with an MLLM and evolving code with an LLM to derive precise element positions, and introduces CS-Bench to evaluate spatial understanding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ START-Doc: Unified Spatialâ€“Textual Learning for Complex Scientific Figures and Multi-Panel Documents: Extend START to figures containing multiple charts, tables, and captions, with hierarchical grounding and cross-panel reasoning.<br>â€¢ Code-Grounded RL for Chart Understanding: Aligning Execution Traces with Visual Grounding: Use execution-based rewards (code render fidelity, data-value consistency) jointly with IoU grounding to improve robustness and faithfulness.<br>â€¢ CS-Bench++: A Comprehensive Benchmark for Spatial Reasoning in Charts and Dashboards: Expand CS-Bench to interactive dashboards and multi-step spatial reasoning, covering dynamic legends, filters, and temporal updates.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-11">

    <div class="paper">
        <h2 class="paper-title">Inferring Compositional 4D Scenes without Ever Seeing One</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05272" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05272" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Scenes in the real world are often composed of several static and dynamic objects. Capturing their 4-dimensional structures, composition and spatio-temporal configuration in-the-wild, though extremely interesting, is equally hard. Therefore, existing works often focus on one object at a time, while relying on some category-specific parametric shape model for dynamic objects. This can lead to inconsistent scene configurations, in addition to being limited to the modeled object categories. We propose COM4D (Compositional 4D), a method that consistently and jointly predicts the structure and spatio-temporal configuration of 4D/3D objects using only static multi-object or dynamic single object supervision. We achieve this by a carefully designed training of spatial and temporal attentions on 2D video input. The training is disentangled into learning from object compositions on the one hand, and single object dynamics throughout the video on the other, thus completely avoiding reliance on 4D compositional training data. At inference time, our proposed attention mixing mechanism combines these independently learned attentions, without requiring any 4D composition examples. By alternating between spatial and temporal reasoning, COM4D reconstructs complete and persistent 4D scenes with multiple interacting objects directly from monocular videos. Furthermore, COM4D provides state-of-the-art results in existing separate problems of 4D object and composed 3D reconstruction despite being purely data-driven.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Jointly reconstructing complete, persistent 4D scenes with multiple static and dynamic objects from monocular videos without test-time optimization or category-specific priors<br>â€¢ Learning spatio-temporal reasoning when large-scale, in-the-wild 4D multi-object data is scarce or unavailable<br>â€¢ Achieving object-aware decomposition and consistency across occlusions, interactions, and large viewpoint changes, where prior methods are fragile<br>â€¢ Overcoming limitations of existing approaches that focus on single objects, rely on parametric models or controlled setups, and struggle with temporal coherence and object persistence</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>COM4D trains a single Diffusion Transformer via Attention Parsing: alternating spatial multi-instance attention on static multi-object scenes and temporal multi-frame attention on single-object dynamic sequences, with Diffusion Forcing to handle mixed-noise latents. At inference, Attention Mixing alternates spatial and temporal reasoningâ€”conditioned on DINOv2 embeddings and SAM-derived object masksâ€”to reconstruct compositional 4D scenes from a single video.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Attention Mixing without Object Masks: Replace SAM-based masks with self-supervised, temporally consistent object discovery to enable fully mask-free compositional 4D inference<br>â€¢ Physics-Guided Diffusion for 4D Scene Dynamics: Integrate differentiable physical constraints into temporal attention and diffusion forcing to improve motion realism and stability<br>â€¢ Real-Time COM4D for Streaming Video: Optimize attention mixing and diffusion to achieve low-latency, online 4D reconstruction for interactive and robotic applications</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13330" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13330" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Finnish LLMs lack comprehensive, high-quality evaluation suites; most benchmarks are English-centric, hindering progress for a low-resource language.<br>â€¢ Existing multilingual resources suffer from data quality issues (e.g., machine-translated samples without human review) and lack task quality assessment across model sizes.<br>â€¢ Task formulations are simplistic, ignore prompt sensitivity, and are poorly compatible with evaluating non-instruction-tuned models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>FIN-bench-v2 consolidates Finnish versions of widely used benchmarks and an expanded FIN-bench into standardized HuggingFace datasets with cloze and multiple-choice prompt variants (five per task) and human-reviewed translations. It selects robust tasks via learning-curve metrics from pretrained 2.15B decoder models (monotonicity, signal-to-noise, non-random performance, model-ordering consistency) and evaluates larger instruction-tuned models, releasing all resources via the LM Evaluation Harness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Prompt Variant Selection for Robust Finnish LLM Evaluation: Learn to choose or ensemble prompt formulations per task to reduce prompt sensitivity and improve reliability across model families.<br>â€¢ Cross-Lingual Transfer of Learning-Curve Metrics for Benchmark Curation: Generalize FIN-bench-v2â€™s robustness criteria to curate unified, high-quality suites for other low-resource languages and domains.<br>â€¢ Human-in-the-Loop Scaling of Machine-Translated Benchmarks: Build semi-automatic pipelines combining MT quality estimation with targeted human review to expand and maintain verified Finnish evaluation datasets.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">State over Tokens: Characterizing the Role of Reasoning Tokens</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.12777" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.12777" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Reasoning tokens often appear as human-readable explanations but empirically lack faithfulness to the modelâ€™s actual computation.<br>â€¢ Existing metaphors (e.g., Chain-of-Thought, Scratchpad) mislead by conflating narrative text with computational process, risking unwarranted trust.<br>â€¢ Interpretability methods largely ignore the multi-cycle, state-passing nature of autoregressive generation, leaving a conceptual vacuum about token roles.<br>â€¢ Empirical limitations include incompleteness of reasoning text and semantic mismatch between what models use and what humans read.<br>â€¢ Training setups optimize final answers while leaving reasoning tokens unconstrained, reinforcing the gap between appearance and function.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce the State over Tokens (SoT) framework that formalizes LLM generation as repeated applications of a function M on a token prefix, where the token sequence Sk is the sole persistent, forward-looking computational state across stateless cycles. Clarify properties of state, dispel misconceptions (completeness and shared meaning), and argue the ontological divergence between text for humans and state for models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Decoding State in Reasoning Tokens: Methods to map internal computation to external token state and recover what information is actually encoded.<br>â€¢ Cycle-Aware Interpretability for LLMs: Tracing how state is constructed and used across autoregressive steps and model components.<br>â€¢ Measuring Information Propagation in SoT: Identifying which prefixes and tokens are critical for the final answer and how influence flows.<br>â€¢ Consistency of Encodings Across Problems: Testing whether models reuse stable state-encoding schemes within and across tasks.<br>â€¢ Is Natural Language a Special State Medium?: Comparing language tokens to vectors or structured formats as substrates for state.<br>â€¢ Training for Faithful SoT: Dual-objective protocols to make tokens both efficient computational state and faithful human explanations.<br>â€¢ Perturbation Studies of SoT: Causal interventions on reasoning tokens to examine sensitivity and trajectory changes.<br>â€¢ Separate Channels for State and Explanation: Interface designs that decouple machine state from human-readable logs.<br>â€¢ Trust Calibration Under SoT: Detecting and mitigating plausible-but-unfaithful reasoning text in high-stakes settings.<br>â€¢ Formal Capacity of Multi-Cycle SoT: Theoretical limits and constructions showing how accumulated tokens expand computational power.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-12">

    <div class="paper">
        <h2 class="paper-title">Rethinking Expert Trajectory Utilization in LLM Post-training</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.11470" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.11470" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ The optimal mechanism to utilize expert trajectories (SFT data) for maximizing LLM postâ€‘training performance remains unclear.<br>â€¢ Synchronized SFTâ€‘RL methods show claimed efficiency but rely on small SFT sets and exhibit instability and sensitivity to model priors at larger scales.<br>â€¢ There is no principled criterion for when to switch from SFT to RL; premature or late switching can reduce RL plasticity.<br>â€¢ The impact of SFT data scale and trajectory difficulty on the final postâ€‘training ceiling is underdefined; â€œLess is Moreâ€ may hinder RL scaling.<br>â€¢ A unified framework linking SFT performance, RL plasticity, and compute to predict ceilings is missing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes the Plasticityâ€‘Ceiling Framework, decomposing postâ€‘training performance into realized SFT performance and subsequent RL plasticity, modeled via sigmoidal powerâ€‘law scaling with compute. Using this framework and largeâ€‘scale experiments, it establishes sequential SFTâ€‘thenâ€‘RL as superior, recommends switching to RL at the Stable or Mildâ€‘Overfitting SFT subâ€‘phase, shows data scale as the primary driver with difficulty as a multiplier, and identifies minimum SFT validation loss as a reliable predictor of the final ceiling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive SFTâ€‘toâ€‘RL Scheduler via Online Plasticity Estimation: Learn a controller that triggers the SFTâ†’RL switch using realâ€‘time validation loss and projected plasticity.<br>â€¢ Curriculum Difficulty Schedules for Maximizing RL Plasticity: Design and evaluate difficultyâ€‘aware SFT curricula that optimize both Psft and subsequent PLrl.<br>â€¢ Generalizing Plasticityâ€‘Ceiling Beyond Math: Test the framework on code, multiâ€‘turn dialogue, and multilingual reasoning to assess universality.<br>â€¢ Stabilized Synced SFTâ€‘RL at Scale with Variance Control: Develop varianceâ€‘reduced, KLâ€‘regularized synchronized methods that remain stable on large SFT datasets.<br>â€¢ Active Expert Trajectory Selection via Lossâ€‘Proxy Optimization: Use minimum validation loss and uncertainty signals to actively curate trajectories that maximize the final ceiling.<br>â€¢ Theoretical Bounds on Plasticity Under Data and Model Scaling: Derive formal relationships between data scale, model size, and asymptotic postâ€‘training ceilings.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.10927" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.10927" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Current video VLMs struggle with fine-grained motion and spatial reasoning, often recognizing "what" but failing to explain "how" motions occur.<br>â€¢ There is a scarcity of large-scale, high-quality motion datasets; manual annotation of trajectories and interactions is costly and unscalable.<br>â€¢ Existing benchmarks focus on basic motion recognition and miss spatial relations (trajectories, relative positions, geometric constraints).<br>â€¢ Camera motion complicates tracking and annotation, degrading model learning without targeted filtering/handling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A fully automated pipeline that detects and tracks objects (including human-centric parts) to produce structured trajectory JSONs, filters heavy camera motion, and uses LLMs to generate motion-centric captions and multi-choice QAs; these auto-labeled pairs are then used to fine-tune open-source VLMs, yielding state-of-the-art motion understanding without hurting other capabilities.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Ego-Motion-Aware FoundationMotion: Joint Modeling of Camera and Object Trajectories: Integrate camera pose estimation and 3D scene geometry to disentangle ego-motion from object motion and enable robust motion reasoning in dynamic views.<br>â€¢ Trajectory-Conditioned Pretraining for Video-Language Models: Use structured bounding-box trajectories as auxiliary supervision (prediction, contrastive alignment, temporal ordering) to pretrain VLMs for stronger spatio-temporal grounding.<br>â€¢ From Auto-Labels to Action: Motion-Centric Fine-Tuning for Embodied Agents: Leverage FoundationMotion data to train robots and autonomous systems on action understanding and planning, coupling motion QA with control policies and imitation learning.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Learning Robot Manipulation from Audio World Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08405" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08405" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling a bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose a generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into a robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Many manipulation tasks (e.g., bottle filling) have ambiguous or insufficient visual cues, making audio essential for state estimation and stopping conditions.<br>â€¢ Existing multimodal policies typically treat audio as auxiliary and lack future lookahead, missing intrinsic physical dynamics (pitch/rhythm) needed for planning.<br>â€¢ Video-based world models are computationally heavy and slow; efficient latent models with temporally consistent predictions are needed for real-time control.<br>â€¢ Audio-driven world models are underexplored; robots need accurate predictions of future audio states to reason about long-term consequences and improve action selection.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A transformer-based latent flow matching model predicts future audio latents (spectrograms or MIDI) from recent audio, trained with vector-field regression and a velocity loss for temporal consistency, then decoded to audio and fed with current vision/audio into a robot policy (flow-matching or SAC) in a modular, multi-stage pipeline.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Audio-Visual-Tactile Latent Flow World Models for Contact-Rich Manipulation: Fuse audio, vision, and touch in a unified latent flow model to anticipate multimodal future states for tasks like cutting, grinding, or scooping.<br>â€¢ Uncertainty-Aware Audio World Models for Robust Robot Planning in Noisy Environments: Extend flow matching with probabilistic outputs and uncertainty calibration to enable risk-sensitive planning under variable acoustic conditions.<br>â€¢ Closed-Loop Active Acoustic Probing for Manipulation via Predictive Audio World Models: Learn action policies that actively elicit informative sounds (tapping, pouring, shaking) and update the audio world model online for improved state estimation and control.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-13">

    <div class="paper">
        <h2 class="paper-title">Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08400" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08400" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Accurate fisheries data are crucial for effective and sustainable marine resource management. With the recent adoption of Electronic Monitoring (EM) systems, more video data is now being collected than can be feasibly reviewed manually. This paper addresses this challenge by developing an optimized deep learning pipeline for automated fish re-identification (Re-ID) using the novel AutoFish dataset, which simulates EM systems with conveyor belts with six similarly looking fish species. We demonstrate that key Re-ID metrics (R1 and mAP@k) are substantially improved by using hard triplet mining in conjunction with a custom image transformation pipeline that includes dataset-specific normalization. By employing these strategies, we demonstrate that the Vision Transformer-based Swin-T architecture consistently outperforms the Convolutional Neural Network-based ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. An in-depth analysis reveals that the primary challenge is distinguishing visually similar individuals of the same species (Intra-species errors), where viewpoint inconsistency proves significantly more detrimental than partial occlusion. The source code and documentation are available at: https://github.com/msamdk/Fish_Re_Identification.git</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ EM systems generate more video than can be manually reviewed, demanding automated, reliable fish identity tracking for accurate catch documentation.<br>â€¢ Frequent occlusions and viewpoint changes on conveyor belts cause identity loss, leading to counting errors even when species classification is correct.<br>â€¢ Existing aquatic Re-ID research largely targets species with distinctive markings and relies on localized features that fail under occlusion; transferability of person/vehicle Re-ID (especially transformers) to visually homogeneous fish is underexplored.<br>â€¢ Fine-grained re-identification of visually similar individuals within the same species is a core unmet challenge for fisheries EM.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>An optimized deep metric-learning pipeline fine-tunes a Swin-T backbone with a 512-D embedding head using PK sampling and hard triplet mining, paired with a resize-and-pad-to-square transform and dataset-specific normalization to preserve fine-grained cues. Compared to ResNet-50, Swin-T consistently yields higher R1 and mAP@39, and analyses show viewpoint inconsistency is more detrimental than partial occlusion.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Viewpoint-Invariant Fish Re-Identification via Multi-View Contrastive Learning on EM Data: Learn embeddings robust to flipped sides using multi-view supervision and synthetic viewpoint augmentation, reducing intra-species confusions.<br>â€¢ Occlusion-Aware Fish Re-ID with Part-Based Transformers and Weak Supervision: Introduce part-level attention without manual landmarks to maintain identity under partial occlusion and clutter on conveyor belts.<br>â€¢ From Lab to Deck: Domain Adaptation and Temporal Modeling for Fish Re-ID in Real EM Videos: Integrate detectionâ€“tracking with sequence models and unsupervised domain adaptation to handle compression artifacts, debris, and inter-camera, long-term re-identification.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.12768" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.12768" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing 3D generators are imitation-based and lack explicit reasoning, leading to failures on relational, counting, geometric, and physical-contact cues implied by language.<br>â€¢ There is a need to tightly couple linguistic intent with spatial synthesis to improve reliability, interpretability, and cross-modal alignment in 3D tasks.<br>â€¢ Prevailing 3D representations are suboptimal: flat voxels waste compute and miss structured spatial dependencies, while part-based schemes require fixed ontologies and generalize poorly.<br>â€¢ Few models unify 3D understanding and generation; prior 3D-LLMs focus on recognition or rely on symbolic planning rather than token-level 3D reasoning.<br>â€¢ Supervision for reasoning traces is scarce; training requires RL that can elicit plans, assign granular process credit with 3D-specific rewards, and mitigate reward hacking via multi-critic feedback.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>CoRe3D unifies semantic chain-of-thought planning with an octant-based geometric chain-of-thought, using a 3D VQ-VAE to tokenize a 64Â³ volume into discrete octant tokens that are autoregressively constructed by a unified 3D-LLM. Both reasoning streams are jointly optimized via 3D Co-GRPO with multi-critic rewards to align linguistic intent, visual fidelity, and physical coherence.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Octree-CoT: Adaptive Hierarchical Geometric Reasoning for Scalable 3D Generation: Replace uniform octants with adaptive octrees to enable coarse-to-fine CoT scheduling and hierarchical attention for efficiency and fidelity.<br>â€¢ SceneCoRe3D: Collaborative Reasoning for Multi-Object 3D Scene Synthesis and Understanding: Extend collaborative reasoning to full scenes with relational layouts, contacts, and global constraints.<br>â€¢ PhysCoT: Integrating Differentiable Physics into Multi-Critic RL for Physically Plausible 3D Generation: Embed differentiable physics and stability metrics into the critic suite to enforce dynamics and structural soundness.<br>â€¢ Uni3D-IO: Unified Bidirectional 3Dâ€“Language Tasks via Shared Semantic and Geometric CoT: Generalize the framework to captioning, VQA, instruction following, and editing with shared reasoning traces.<br>â€¢ Self-Play Co-GRPO: Data-Efficient Bootstrapping of 3D Reasoning without Paired Supervision: Use self-play, cycle consistency, and synthetic reward shaping to reduce reliance on labeled data and human preferences.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.09069" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.09069" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD- OCT.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Clinical deployment of high-accuracy OCT classifiers is hindered by the heavy computational cost of state-of-the-art models (e.g., ConvNeXtV2-Large ~197M parameters), limiting real-time use in resource-constrained settings.<br>â€¢ Existing OCT methods (multi-scale, feature-fusion, Transformer variants) often trade efficiency for accuracy and struggle with patient-level generalization, class imbalance, and subtle lesions (drusen vs. CNV).<br>â€¢ A gap exists in clinical-grade, cross-architecture, real-time knowledge distillation for OCT AMD classification that integrates patient-disjoint validation, tailored augmentations, and robust optimization to transfer teacher performance to lightweight students.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>KD-OCT trains a high-capacity ConvNeXtV2-Large teacher with focal loss, stochastic weight averaging, and heavy augmentations, then performs real-time temperature-scaled distillation to an EfficientNet-B2 student using a weighted KL divergence + cross-entropy loss. The framework is evaluated with patient-level cross-validation and test-time augmentation, achieving near-teacher accuracy with 25.5Ã— fewer parameters.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Semi-Supervised KD-OCT: Label-Efficient Retinal OCT Classification with Teacher-Guided Pseudo-Labels and Consistency â€” Reduce annotation needs by leveraging unlabeled OCT volumes via pseudo-labeling and consistency regularization within the KD pipeline.<br>â€¢ Multi-Modal KD-OCT: Distilling Joint Knowledge from OCT and Fundus Images for Robust AMD and DME Detection â€” Combine fundus priors with OCT through teacher ensembles and cross-modal distillation to improve accuracy and cross-device generalization.<br>â€¢ Federated KD-OCT: Privacy-Preserving Cross-Site Distillation and Model Compression for Edge Deployment â€” Use federated KD across hospitals without sharing raw data, coupled with quantization/pruning to enable real-time inference on portable OCT devices.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 13</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button><button class="page-btn" onclick="goToPage(11)">11</button><button class="page-btn" onclick="goToPage(12)">12</button><button class="page-btn" onclick="goToPage(13)">13</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-12-16 23:08:15 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 13;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>