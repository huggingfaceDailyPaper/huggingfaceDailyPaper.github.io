<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - January 02, 2026</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">January 02, 2026</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23959" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23959" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing multi-step RAG memories are passive, unstructured stores of isolated facts that fail to capture higher-order correlations needed for complex relational reasoning.<br>‚Ä¢ Static memory designs limit representational strength and hinder knowledge evolution across steps, causing fragmented reasoning and weak global sense-making in long contexts.<br>‚Ä¢ Plaintext summaries of interaction history are hard to manipulate computationally, offering little proposition-level guidance for subsequent retrieval and reasoning.<br>‚Ä¢ Single-step RAG is inadequate for long-context tasks, motivating iterative retrieval‚Äìreasoning with a more expressive and dynamic memory.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>HGMEM models working memory as a hypergraph where hyperedges serve as memory units that progressively bind multiple facts and thoughts, enabling explicit higher-order relations and an evolving situated knowledge structure. This dynamic memory provides strong propositions to guide subsequent retrieval and reasoning for improved global sense-making.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning to Form and Update Hyperedges: End-to-End Training of Hypergraph Memory in RAG: Develop differentiable or reinforcement learning policies that automatically construct and revise hyperedges from LLM signals and retrieval outcomes to optimize global reasoning.<br>‚Ä¢ Efficient Hypergraph Memory Compression for Streaming Long Contexts: Create pruning, summarization, and compaction algorithms that preserve higher-order relational structure while scaling hypergraph memory to very long, streaming inputs.<br>‚Ä¢ Multi-Agent RAG with Shared Hypergraph Memory: Explore collaborative agents that read/write to a shared hypergraph memory to coordinate complex retrieval and reasoning across sub-tasks and domains.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24617" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24617" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first compression-aware scaling law, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a decoupled ŒºP parametrization that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting (R=4, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a +2.69\% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Uniform token-level computation in LLMs ignores non-uniform information density, wasting compute on predictable spans and under-serving high-information transitions where reasoning is hardest.<br>‚Ä¢ Token-level autoregression lacks explicit abstraction, forcing models to repeatedly infer high-level structure at every layer, causing misallocation of capacity and inefficiency under fixed FLOPs budgets.<br>‚Ä¢ Prior approaches fall short: latent reasoning lacks learned, content-adaptive segmentation and can struggle with symbolic precision; sentence-level LCMs rely on fixed human-defined boundaries and require frozen encoders/decoders; MoE/Universal Transformer focus on parameter/compute routing but not semantic information density; H-NET learns boundaries but operates at byte-level and isn‚Äôt validated for modern next-token LLMs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DLCM dynamically segments token sequences into variable-length concepts via learned boundary detection on encoder representations, pools tokens into concept embeddings, performs deep reasoning with a high-capacity concept-level transformer, and decodes tokens through causal cross-attention to the concept stream. It introduces a compression-aware scaling law to balance token- and concept-level capacity under fixed FLOPs and a decoupled ¬µP parametrization for stable training across heterogeneous widths and compression regimes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning-to-Parse with Task Rewards for Concept Boundaries: Use reinforcement/self-training to align boundary detection with downstream loss and latency constraints, improving task-optimal segmentation.<br>‚Ä¢ Multimodal Dynamic Large Concept Models: Extend DLCM to jointly segment and reason over text, code, and vision/audio streams in a shared adaptive concept space.<br>‚Ä¢ Adaptive Per-Document Compression Control for DLCMs: Learn policies that set compression ratio R at inference time conditioned on content entropy and compute budgets.<br>‚Ä¢ MoE-Augmented Concept Backbones for Conditional Reasoning: Combine dynamic segmentation with expert routing at the concept level to scale parameters without increasing per-token FLOPs.<br>‚Ä¢ Interpreting and Evaluating Learned Concepts: Develop diagnostics and benchmarks to measure semantic coherence of concepts and their causal impact on reasoning accuracy.<br>‚Ä¢ Theory of Compression-Aware Scaling Laws: Derive and validate optimal compute allocation across token capacity, concept capacity, and compression ratio with formal guarantees.<br>‚Ä¢ Streaming DLCM with Long-Context Concept Memory: Maintain and update concept-level memory for million-token contexts with subquadratic cost.<br>‚Ä¢ Cross-Lingual and Low-Resource Transfer via Concept Spaces: Investigate how learned segmentation enables efficient transfer to new languages/domains with minimal data.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24165" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24165" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\%) and Gemini-3-Flash (+111.6\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Text-centric MLLMs rely on lengthy Chain-of-Thought, leading to uncontrollable generation length, high latency, and poor tracking of evolving visual states in long-horizon, vision-centric tasks.<br>‚Ä¢ "Thinking with Image" pipelines depend on multi-turn tool calls, which inflate interaction cost and still struggle with logical consistency and spatial precision.<br>‚Ä¢ Autoregressive reasoning lacks native parallelism and controllable inference budgets, limiting efficiency and robustness in complex search spaces.<br>‚Ä¢ Closed-source video-based reasoning is opaque and computationally prohibitive; there is a need for an open, efficient paradigm that reasons natively in visual space.<br>‚Ä¢ Evaluation across diverse tasks requires parseable visual outputs to ensure fair, leakage-free comparison with symbolic ground-truth.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DiffThinker reformulates multimodal reasoning as an image-to-image generative task using a diffusion model trained with Flow Matching on an MMDiT backbone in VAE latent space, producing solution images that are parsed into symbolic outputs for evaluation. This design emphasizes efficiency, controllability, native parallel sampling, and collaborative integration with MLLMs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ End-to-End Differentiable Parsing for Generative Reasoners: Jointly learn the visual generator and parser to optimize symbolic accuracy and reduce post-hoc parsing errors.<br>‚Ä¢ DiffThinker-Video: Efficient Long-Horizon Multimodal Reasoning via Lightweight Video Diffusion: Scale the paradigm to temporal reasoning with compute-aware solvers and curriculum scheduling.<br>‚Ä¢ Hybrid Co-Reasoning Between Diffusion Generators and MLLMs: Adaptive delegation policies that decide when to reason in visual vs. symbolic space to minimize cost and maximize accuracy.<br>‚Ä¢ Controllable Diffusion Inference for Reasoning with Guarantees: Develop budgeted ODE solvers and constraint-aware guidance to bound compute and enforce validity.<br>‚Ä¢ Parallel Hypothesis Sampling in Visual Space for Uncertainty and Verification: Exploit native parallel samples to estimate uncertainty, prune invalid paths, and perform constraint checking.<br>‚Ä¢ Generative Multimodal Reasoning for Embodied Robotics and 3D Scenes: Extend DiffThinker to sensor-rich 3D environments for spatial planning and manipulation.<br>‚Ä¢ Benchmarking Logical Consistency and Spatial Precision in Generative Visual Reasoning: Create standardized datasets, parsers, and metrics to evaluate visual-solution quality across domains.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">On the Role of Discreteness in Diffusion LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22630" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22630" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion models offer appealing properties for language generation, such as parallel decoding and iterative refinement, but the discrete and highly structured nature of text challenges the direct application of diffusion principles. In this paper, we revisit diffusion language modeling from the view of diffusion process and language modeling, and outline five properties that separate diffusion mechanics from language-specific requirements. We first categorize existing approaches into continuous diffusion in embedding space and discrete diffusion over tokens. We then show that each satisfies only part of the five essential properties and therefore reflects a structural trade-off. Through analyses of recent large diffusion language models, we identify two central issues: (i) uniform corruption does not respect how information is distributed across positions, and (ii) token-wise marginal training cannot capture multi-token dependencies during parallel decoding. These observations motivate diffusion processes that align more closely with the structure of text, and encourage future work toward more coherent diffusion language models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Diffusion assumptions (smooth, continuous corruption) are misaligned with the discrete, dependency-rich nature of language, requiring a clear definition of what an ideal diffusion LM should satisfy.<br>‚Ä¢ Existing approaches make structural trade-offs: continuous DLMs preserve diffusion mechanics but break discreteness and rely on implicit structure; discrete DLMs keep tokens but lose smoothness and depend on marginal, token-wise training.<br>‚Ä¢ Uniform corruption ignores uneven information distribution across positions, causing abrupt information loss and frequency collapse toward high-frequency tokens.<br>‚Ä¢ Token-wise marginal training cannot capture joint multi-token constraints during parallel decoding (the marginal trap), yielding locally plausible but globally inconsistent outputs.<br>‚Ä¢ A unified framework is needed to diagnose these issues and guide designs that retain diffusion benefits (parallel editing, compute‚Äìlength decoupling, data efficiency) while respecting linguistic structure.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes a five-property framework separating diffusion-side requirements (smooth corruption, tractable intermediates, iterative refinement) from language-side requirements (discreteness, structural dependency), and uses it to systematically analyze continuous vs. discrete DLMs and their trade-offs. It provides theoretical arguments and probing experiments demonstrating position-dependent information loss under uniform corruption and the marginal trap under token-wise, parallel denoising.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Information-Smooth Corruption for Text Diffusion: Design context-/MI-aware noise schedules that equalize recoverable information across positions rather than applying uniform masking.<br>‚Ä¢ Semantic Transition Kernels for Discrete Diffusion: Replace binary masks with structured, small-step categorical transitions (specific ‚Üí general ‚Üí [MASK]) to preserve token identity longer.<br>‚Ä¢ Hybrid Discrete‚ÄìContinuous Diffusion with Identity Channels: Decouple a discrete identity-preserving process from a continuous refinement pathway to retain L1 while achieving D1‚ÄìD3.<br>‚Ä¢ From Marginals to Joints: Sequence-Level Objectives for Diffusion LMs: Train with energy-based/contrastive or structured losses that directly score joint token configurations to enforce global coherence.<br>‚Ä¢ Uncommitted Soft-State Refinement for Parallel Decoding: Maintain soft token distributions during denoising to enable joint revisions and avoid early hard commitments.<br>‚Ä¢ Certainty-Driven Parallel Decoding via Distillation: Encourage early high-confidence convergence at many positions (e.g., certainty-forcing objectives) to reduce incoherent mixtures under parallel updates.<br>‚Ä¢ Context-Adaptive Token-Level Noise Scheduling: Learn position-aware rescheduling (generalizing CART) that upweights recoverable positions and downweights near-random ones during training.<br>‚Ä¢ Dependency-Guided Blockwise Decoding Schedules: Update mutually dependent token blocks sequentially (or with constrained parallelism) using learned syntax/semantic graphs to couple decisions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24724" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24724" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In this work, we show that the impact of model capacity varies across timesteps: it is crucial for the early and late stages but largely negligible during the intermediate stage. Accordingly, we propose FlowBlending, a stage-aware multi-model sampling strategy that employs a large model and a small model at capacity-sensitive stages and intermediate stages, respectively. We further introduce simple criteria to choose stage boundaries and provide a velocity-divergence analysis as an effective proxy for identifying capacity-sensitive regions. Across LTX-Video (2B/13B) and WAN 2.1 (1.3B/14B), FlowBlending achieves up to 1.65x faster inference with 57.35% fewer FLOPs, while maintaining the visual fidelity, temporal coherence, and semantic alignment of the large models. FlowBlending is also compatible with existing sampling-acceleration techniques, enabling up to 2x additional speedup. Project page is available at: https://jibin86.github.io/flowblending_project_page.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ High computational cost of video diffusion sampling due to many denoising steps and increasingly large model sizes<br>‚Ä¢ Existing acceleration methods (efficient solvers, distillation) assume uniform capacity needs across timesteps, using a single model throughout or compressing the entire model, which can hurt quality<br>‚Ä¢ Small-capacity video models are much faster but degrade semantic alignment, temporal coherence, and visual fidelity compared to large models<br>‚Ä¢ In video (unlike image), model capacity importance varies by stage: early steps set global structure/motion and late steps refine details/artifacts‚Äîboth are capacity-sensitive‚Äîsuggesting an opportunity for stage-aware capacity allocation<br>‚Ä¢ Prior multi-model sampling insights from images do not transfer to video; there is a need for a video-specific, training-free, multi-model allocation strategy</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>FlowBlending is a stage-aware multi-model sampling strategy that runs a large model in early and late (capacity-sensitive) denoising stages and a small model in the intermediate stage, preserving large-model quality while cutting compute. Stage boundaries are selected using simple criteria (e.g., semantic similarity of latents and a fine-detail quality indicator) and guided by velocity-divergence analysis, requiring no retraining and remaining compatible with other acceleration methods.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Boundary FlowBlending via Velocity-Divergence-Guided Online Scheduling: Learn per-sample, per-prompt boundaries that switch models dynamically using real-time velocity divergence and semantic cues to maximize quality-efficiency trade-offs.<br>‚Ä¢ Learning-to-Blend: A Gating Network for Per-Timestep Model Selection in Video Diffusion: Train a lightweight policy to choose between small and large models at each step, integrating content-, motion-, and detail-aware signals for finer-grained capacity allocation.<br>‚Ä¢ Cross-Model Consistency Distillation for Enhanced FlowBlending: Distill intermediate-stage velocities and late-stage refinements from the large to the small model to expand the replaceable step range and further reduce FLOPs without perceptual loss.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24766" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24766" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Bridging the embodiment gap: human-centric video generations produce plausible interactions but in a human action space, making direct transfer to robot control difficult.<br>‚Ä¢ Translating high-level visual predictions into low-level robot commands without task-specific demonstrations for open-world, zero-shot manipulation.<br>‚Ä¢ Needing a general, embodiment-agnostic intermediate representation that captures diverse object interactions (rigid, articulated, deformable, granular).<br>‚Ä¢ Limitations of existing task specification: symbolic methods require manual modeling; language-to-action and outcome-based approaches need large, task-specific data and may struggle to generalize; keypoint/affordance interfaces can be brittle or incomplete across object types.<br>‚Ä¢ Lack of a scalable mechanism to ground open-ended language instructions into actionable goals in unseen scenes using pre-trained models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Dream2Flow generates a text-conditioned video in the robot‚Äôs scene, reconstructs 3D object flow from it via depth estimation and point tracking, and formulates manipulation as object trajectory tracking. The extracted 3D flow is then translated into low-level actions through trajectory optimization or reinforcement learning, enabling zero-shot execution without task-specific demos.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Uncertainty-Aware Object-Flow Planning for Robust Open-World Manipulation: Incorporate confidence estimation on reconstructed flow and detect generation failures to trigger replanning and filtering.<br>‚Ä¢ Hierarchical Flow Chaining for Long-Horizon, Multi-Object Tasks: Decompose videos into subgoal flows and compose them with a high-level planner for extended, multi-step manipulation.<br>‚Ä¢ Cross-Embodiment Flow Retargeting from Humans to Diverse Robots: Learn general retargeters that map object-flow goals to feasible control across varied kinematics and end-effectors.<br>‚Ä¢ Closed-Loop Manipulation with Online 3D Flow Fusion: Fuse onboard sensing with generated flow for feedback control and active perception under distribution shift.<br>‚Ä¢ Control-Aware Video Generation for Trackable Object Flows: Co-train video models with planners so generated motions better satisfy robot dynamics and contact constraints.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24007" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24007" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), a novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages a short-term Tabu List to prevent cycling and encourage diversification, and a long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates a dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESO's effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: https://github.com/bulentsoykan/TESO.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Noisy, stochastic simulation outputs make it hard to reliably assess improvements under limited replication budgets.<br>‚Ä¢ High computational cost and black-box models (no gradients) hinder frequent evaluations and derivative-based optimization.<br>‚Ä¢ Complex, often multimodal and high-dimensional landscapes exacerbate premature convergence and cycling.<br>‚Ä¢ Existing methods have limitations: surrogate-based BO/SBO can be costly and sensitive to model/hyperparameter choices; Ranking & Selection fits finite/discrete sets; GA/SA/PSO are parameter-sensitive and prone to premature convergence; classic Tabu Search assumes deterministic evaluations and lacks adaptations for noise.<br>‚Ä¢ Effective exploration‚Äìexploitation balance is difficult under noise and tight evaluation budgets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TESO integrates a short-term Tabu List with an aspiration criterion and a long-term Elite Memory, generating candidates via probabilistic diversification (random sampling) and intensification (adaptive perturbations around elite solutions) under a decaying noise schedule. Each candidate is evaluated with multiple replications to obtain mean estimates, which update the memories and best solution while preventing cycling and guiding search in noisy black-box settings.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ TESO-Bayes: Hybrid Surrogate-Assisted Tabu-Enhanced Simulation Optimization: Combine Gaussian-process/Bayesian acquisition with TESO‚Äôs dual-memory to select points, adapt replication counts by uncertainty, and improve efficiency on expensive models.<br>‚Ä¢ Risk-Aware TESO: Adaptive Replication and Aspiration under Simulation Noise: Use variance estimates to dynamically allocate replications, set risk-sensitive aspiration criteria, and robustly compare candidates in heteroskedastic environments.<br>‚Ä¢ TESO-X: Extending Tabu-Enhanced Simulation Optimization to Mixed-Discrete and High-Dimensional Spaces: Design attribute-based tabu and structured perturbations for combinatorial and mixed-variable problems, with scalability and multimodality benchmarks.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 3</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2026-01-02 23:49:41 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 3;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>