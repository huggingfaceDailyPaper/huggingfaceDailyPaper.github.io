<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - October 27, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "SF Pro Display", -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Helvetica Neue", "Arial", "Noto Sans", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", sans-serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">October 27, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">DeepAgent: A General Reasoning Agent with Scalable Toolsets</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21618" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21618" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To address the challenges of long-horizon interactions, particularly the context length explosion from multiple tool calls and the accumulation of interaction history, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. This work takes a step toward more general and capable agents for real-world applications. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°å®ä»»åŠ¡éœ€è¦åœ¨é•¿æ—¶äº¤äº’ä¸­çµæ´»ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼Œä½†ä¼ ç»ŸåŸºäºé¢„è®¾æµç¨‹çš„ä»£ç†ï¼ˆå¦‚ReActã€Plan-and-Solveï¼‰ç¼ºä¹å…¨å±€è‡ªä¸»æ€§ä¸è¿‡ç¨‹å†…å·¥å…·å‘ç°èƒ½åŠ›ï¼Œä¸”é€šå¸¸ä»…æ”¯æŒå°‘é‡ç ”ç©¶å‹å·¥å…·ï¼ˆæœç´¢/æµè§ˆ/ä»£ç ï¼‰ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤§è§„æ¨¡å¼€æ”¾å·¥å…·é›†ï¼ˆè§å›¾2ï¼Œç¬¬2é¡µï¼‰ã€‚<br>â€¢ å¤šè½®å·¥å…·è°ƒç”¨ä¸å†å²ç´¯ç§¯å¸¦æ¥ä¸Šä¸‹æ–‡é•¿åº¦çˆ†ç‚¸ä¸è¯¯å·®ç´¯ç§¯ï¼Œç°æœ‰æ–¹æ³•ç¼ºå°‘ç¨³å¥çš„è®°å¿†ç®¡ç†ä¸ä¿¡æ¯å‹ç¼©ï¼Œéš¾ä»¥åœ¨é•¿ç¨‹æ¨ç†ä¸­â€œæ­¢æŸé‡æ•´â€ï¼ˆè®ºæ–‡æå‡ºçš„è®°å¿†æŠ˜å å³ä¸ºå¯¹ç­–ï¼Œè§ç¬¬3.4èŠ‚ï¼Œç¬¬4é¡µï¼‰ã€‚<br>â€¢ è®­ç»ƒå±‚é¢ï¼ŒçœŸå®APIä¸ç¨³å®šã€æ˜‚è´µä¸”é«˜å»¶è¿Ÿï¼Œä¸”ä»…ä¾èµ–æœ€ç»ˆç»“æœçš„ç¨€ç–å¥–åŠ±éš¾ä»¥å­¦åˆ°ç²¾ç¡®çš„å·¥å…·è°ƒç”¨ä¸æ—¶æœºæ§åˆ¶ï¼›å› æ­¤éœ€è¦æ›´ç¨³å®šé«˜æ•ˆçš„è®­ç»ƒç¯å¢ƒä¸ç»†ç²’åº¦ä¿¡ç”¨åˆ†é…ï¼ˆè§ToolPOï¼Œç¬¬3.5èŠ‚ï¼Œç¬¬5é¡µï¼‰ã€‚æ­¤å¤–ï¼Œè¯¥é—®é¢˜å¯¹å®é™…è½åœ°è‡³å…³é‡è¦ï¼šåœ¨å¼€æ”¾å·¥å…·é›†ä¸ä¸‹æ¸¸åº”ç”¨ä¸Šï¼Œç»Ÿä¸€æ·±åº¦æ¨ç†èŒƒå¼æ˜¾è‘—ä¼˜äºæµç¨‹æ³•ï¼ˆè§å›¾1ä¸è¡¨1/è¡¨2ï¼Œç¬¬1ã€5ã€6é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºDeepAgentï¼šå°†è‡ªä¸»æ€è€ƒã€åŠ¨æ€å·¥å…·æ£€ç´¢ä¸è°ƒç”¨ã€ä»¥åŠâ€œè®°å¿†æŠ˜å â€æ•´åˆä¸ºå•ä¸€è¿ç»­çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶ä»¥ToolPOç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œåˆ©ç”¨LLMå·¥å…·æ¨¡æ‹Ÿå™¨ä¸å·¥å…·è°ƒç”¨ä¼˜åŠ¿å½’å› å®ç°ç¨³å®šé«˜æ•ˆã€ç»†ç²’åº¦çš„é€šç”¨ç”¨å·¥å…·èƒ½åŠ›ï¼ˆè§æ¡†æ¶å›¾ï¼Œç¬¬3é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ End-to-End Differentiable Tool Retrieval for Deep Agentsï¼šè”åˆä¼˜åŒ–æ£€ç´¢å™¨ä¸ç­–ç•¥æ¨¡å‹ï¼Œä½¿å·¥å…·æ£€ç´¢ä¸è°ƒç”¨åœ¨åŒä¸€ç›®æ ‡ä¸‹ç«¯åˆ°ç«¯å­¦ä¹ ï¼Œæå‡å¼€æ”¾é›†å·¥å…·å‘ç°çš„å‡†ç¡®ç‡ä¸é²æ£’æ€§ã€‚<br>â€¢ Learning to Fold: è‡ªé€‚åº”è®°å¿†æŠ˜å ç­–ç•¥ä¸ç†è®ºä¿è¯ï¼šå­¦ä¹ â€œä½•æ—¶/å¦‚ä½•â€æŠ˜å ä¸æŠ˜å ç²’åº¦ï¼Œå»ºç«‹å‹ç¼©-æ€§èƒ½çš„ç†è®ºç•Œä¸ä»£ä»·æ„ŸçŸ¥ç­–ç•¥ï¼Œé™ä½é•¿ç¨‹äº¤äº’ä¸­çš„è¯¯å·®ç´¯ç§¯ä¸ç®—åŠ›æˆæœ¬ã€‚<br>â€¢ From Simulation to Reality: é¢å‘çœŸå®APIçš„è¯¾ç¨‹å¼å®‰å…¨è®­ç»ƒï¼šåœ¨ToolPOä¸­å¼•å…¥Sim2Realè¯¾ç¨‹ã€è°ƒç”¨å‰åéªŒè¯ä¸å®‰å…¨çº¦æŸã€æ•…éšœæ³¨å…¥ä¸æ¢å¤ï¼Œæå‡çœŸå®APIç¯å¢ƒä¸‹çš„ç¨³å¥æ€§ä¸å®‰å…¨æ€§ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Video-As-Prompt: Unified Semantic Control for Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20888" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20888" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Unified, generalizable semantic control in video generation remains a critical open challenge. Existing methods either introduce artifacts by enforcing inappropriate pixel-wise priors from structure-based controls, or rely on non-generalizable, condition-specific finetuning or task-specific architectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes this problem as in-context generation. VAP leverages a reference video as a direct semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via a plug-and-play Mixture-of-Transformers (MoT) expert. This architecture prevents catastrophic forgetting and is guided by a temporally biased position embedding that eliminates spurious mapping priors for robust context retrieval. To power this approach and catalyze future research, we built VAP-Data, the largest dataset for semantic-controlled video generation with over 100K paired videos across 100 semantic conditions. As a single unified model, VAP sets a new state-of-the-art for open-source methods, achieving a 38.7% user preference rate that rivals leading condition-specific commercial models. VAP's strong zero-shot generalization and support for various downstream applications mark a significant advance toward general-purpose, controllable video generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ è¯­ä¹‰æ§åˆ¶è§†é¢‘ç”Ÿæˆç¼ºä¹ç»Ÿä¸€ã€å¯æ³›åŒ–çš„æ¡†æ¶ï¼Œéš¾ä»¥åŒæ—¶å¤„ç†æ¦‚å¿µã€é£æ ¼ã€è¿åŠ¨ã€æœºä½ç­‰éåƒç´ å¯¹é½æ¡ä»¶ã€‚<br>â€¢ å°†ç»“æ„æ§åˆ¶èŒƒå¼ç›´æ¥è¿ç§»åˆ°è¯­ä¹‰æ§åˆ¶ä¼šå¼ºåŠ åƒç´ çº§æ˜ å°„å…ˆéªŒï¼Œäº§ç”Ÿå¤åˆ¶/ç²˜è´´å¼ä¼ªå½±ï¼›è€ŒæŒ‰æ¡ä»¶å¾®è°ƒæˆ–ä»»åŠ¡ç‰¹å®šè®¾è®¡æˆæœ¬é«˜ã€æ˜“é—å¿˜ã€é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›å¼±ã€‚<br>â€¢ ç¼ºå°‘é¢å‘è¯­ä¹‰æ§åˆ¶çš„è§„æ¨¡åŒ–é…å¯¹æ•°æ®ä¸åŸºå‡†ï¼Œé™åˆ¶äº†æ–¹æ³•è®­ç»ƒä¸å®¢è§‚è¯„æµ‹ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºVideo-As-Promptï¼ˆVAPï¼‰ï¼šå°†åŒ…å«ç›®æ ‡è¯­ä¹‰çš„å‚è€ƒè§†é¢‘ä½œä¸ºâ€œè§†é¢‘æç¤ºâ€ï¼Œé€šè¿‡å¯æ’æ‹”çš„Mixture-of-Transformersä¸“å®¶å¹¶è”åˆ°å†»ç»“çš„Video DiTï¼Œä»¥å…¨æ³¨æ„åŠ›è·¨åˆ†æ”¯äº¤äº’å®ç°åœ¨ä¸Šä¸‹æ–‡çš„ç»Ÿä¸€è¯­ä¹‰æ§åˆ¶ï¼›å¹¶å¼•å…¥æ—¶é—´åç½®çš„RoPEï¼ˆæ—¶é—´ä¸Šå°†å‚è€ƒç½®äºç›®æ ‡ä¹‹å‰ã€ç©ºé—´ä½ç½®å›ºå®šï¼‰ä»¥æ¶ˆé™¤é”™è¯¯çš„åƒç´ å¯¹é½å…ˆéªŒå¹¶æå‡è¯­å¢ƒæ£€ç´¢çš„ç¨³å¥æ€§ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ç»“æ„ä¸è¯­ä¹‰ä¸€ä½“åŒ–çš„å¤šæ¡ä»¶è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼šç»Ÿä¸€åƒç´ å¯¹é½ç»“æ„æ¡ä»¶ä¸éå¯¹é½è¯­ä¹‰æ¡ä»¶ï¼Œç ”ç©¶å†²çªå…ˆéªŒçš„è§£è€¦ä¸ååŒæ§åˆ¶ã€‚<br>â€¢ è‡ªé€‚åº”æ—¶é—´åç½®ä½ç½®ç¼–ç ä¸åŠ¨æ€ä¸“å®¶è·¯ç”±ï¼šå­¦ä¹ å¼æ—¶é—´åç½®RoPEä¸å¯å­¦ä¹ MoTè·¯ç”±/ç¨€ç–åŒ–ï¼Œæå‡é•¿è§†é¢‘ä¸å¤šè¯­ä¹‰åœºæ™¯ä¸‹çš„æ£€ç´¢ä¸æ³›åŒ–ã€‚<br>â€¢ æ— å‚è€ƒè§†é¢‘çš„è½»é‡åŒ–è¯­ä¹‰æ§åˆ¶è’¸é¦ï¼šå°†VAPçš„åœ¨ä¸Šä¸‹æ–‡èƒ½åŠ›è’¸é¦åˆ°æ–‡æœ¬/å›¾åƒ/å°‘å¸§æç¤ºï¼Œå®ç°ä½å¼€é”€ä¸å®æ—¶è¯­ä¹‰å¯æ§ç”Ÿæˆã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">WorldGrow: Generating Infinite 3D World</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21682" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21682" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We tackle the challenge of generating the infinitely extendable 3D world -- large, continuous environments with coherent geometry and realistic appearance. Existing methods face key challenges: 2D-lifting approaches suffer from geometric and appearance inconsistencies across views, 3D implicit representations are hard to scale up, and current 3D foundation models are mostly object-centric, limiting their applicability to scene-level generation. Our key insight is leveraging strong generation priors from pre-trained 3D models for structured scene block generation. To this end, we propose WorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our method features three core components: (1) a data curation pipeline that extracts high-quality scene blocks for training, making the 3D structured latent representations suitable for scene generation; (2) a 3D block inpainting mechanism that enables context-aware scene extension; and (3) a coarse-to-fine generation strategy that ensures both global layout plausibility and local geometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset, WorldGrow achieves SOTA performance in geometry reconstruction, while uniquely supporting infinite scene generation with photorealistic and structurally consistent outputs. These results highlight its capability for constructing large-scale virtual environments and potential for building future world models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç›®æ ‡ï¼šç”Ÿæˆå¯æ— é™æ‰©å±•ã€å‡ ä½•è¿è´¯ä¸”å¤–è§‚é€¼çœŸçš„3Dä¸–ç•Œï¼ŒæœåŠ¡äºæ¸¸æˆ/VRã€å½±è§†åˆ¶ä½œåŠä¸–ç•Œæ¨¡å‹ä¸å…·èº«æ™ºèƒ½ç­‰åº”ç”¨ã€‚<br>â€¢ å±€é™1ï¼ˆ2DæŠ¬å‡ï¼‰ï¼šä¾èµ–2Dæ‰©æ•£å†â€œæŠ¬å‡â€åˆ°3Dçš„æ–¹æ³•ç¼ºä¹å…¨å±€3Dç†è§£ï¼Œè·¨è§†è§’/è·¨åŒºåŸŸå‡ºç°å‡ ä½•ä¸å¤–è§‚ä¸ä¸€è‡´ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤§å°ºåº¦åœºæ™¯ã€‚<br>â€¢ å±€é™2ï¼ˆ3Dè¡¨ç¤ºï¼‰ï¼šéšå¼/å…¨å±€3Dè¡¨ç¤ºåœ¨è®¡ç®—ä¸å†…å­˜ä¸Šéš¾ä»¥å¤§è§„æ¨¡æ‰©å±•ï¼Œä¸”å—é™äºåœºæ™¯çº§è®­ç»ƒæ•°æ®è§„æ¨¡ä¸å¤šæ ·æ€§ã€‚<br>â€¢ å±€é™3ï¼ˆå¯¹è±¡ä¸­å¿ƒï¼‰ï¼šç°æœ‰å¼ºå¤§3DåŸºç¡€æ¨¡å‹å¤šä¸ºå•ç‰©ä½“ç”Ÿæˆï¼Œéš¾ä»¥ç›´æ¥ç”¨äºåœºæ™¯çº§ã€å¯æ‹¼æ¥çš„è¿ç»­ä¸–ç•Œç”Ÿæˆã€‚<br>â€¢ å…³é”®æŒ‘æˆ˜ï¼šåœ¨è¿­ä»£ç”Ÿé•¿ä¸­åŒæ—¶ä¿è¯ç›¸é‚»å—çš„å‡ ä½•/é£æ ¼è¿ç»­æ€§ã€å…¨å±€å¸ƒå±€çš„åˆç†æ€§ä¸å±€éƒ¨å‡ ä½•çº¹ç†çš„é«˜ä¿çœŸã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>WorldGrowæå‡ºåŸºäºåœºæ™¯å—çš„å±‚çº§å¼ç”Ÿæˆï¼šé€šè¿‡æ•°æ®ç­–åˆ’è·å¾—ç²—/ç»†ä¸¤çº§å—ä¸â€œåœºæ™¯å‹å¥½SLATâ€ï¼Œä»¥æ©ç å¼•å¯¼çš„3Då—çº§è¡¥å…¨ï¼ˆå…ˆç»“æ„ã€åæ½œç‰¹å¾ï¼‰è¿›è¡ŒåŒºåŸŸç”Ÿé•¿ï¼Œå…ˆç”¨ç²—å°ºåº¦å»ºç«‹å…¨å±€å¸ƒå±€ï¼Œå†ç”¨ç»†å°ºåº¦ç»†åŒ–å‡ ä½•ä¸å¤–è§‚ï¼Œæœ€ç»ˆè§£ç ä¸ºå¯æ¸²æŸ“3Dä¸–ç•Œï¼Œå®ç°æ— é™æ‰©å±•ä¸è·¨å—è¿è´¯ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Z-Growï¼šä¸‰ç»´çºµå‘å¯æ‰©å±•ä¸–ç•Œç”Ÿæˆâ€”â€”å°†åŒºåŸŸç”Ÿé•¿ä»å¹³é¢æ‰©å±•åˆ°Zè½´ï¼Œæ”¯æŒå¤šå±‚å»ºç­‘/ç«‹ä½“åŸå¸‚çš„è¿ç»­ç”Ÿæˆ<br>â€¢ SemGrowï¼šè¯­ä¹‰å¯æ§çš„æ— é™3Dä¸–ç•Œç”Ÿæˆâ€”â€”å¼•å…¥LLM/åœºæ™¯å›¾çº¦æŸï¼ŒæŒ‰æˆ¿å‹/åŠŸèƒ½åŒº/é£æ ¼è¿›è¡Œå¯æ§å¸ƒå±€ä¸ç»†èŠ‚ç”Ÿæˆ<br>â€¢ OutdoorGrowï¼šè·¨åŸŸå¤§å°ºåº¦æˆ·å¤–ä¸–ç•Œç”Ÿæˆâ€”â€”èåˆèˆªæ‹/è¡—æ™¯/ç¨‹åºåŒ–èµ„äº§ï¼Œæå‡åŸå¸‚ä¸è‡ªç„¶åœºæ™¯çš„å¤šæ ·æ€§ä¸ç¨³å®šæ€§<br>â€¢ UniLat-Growï¼šå‡ ä½•-å¤–è§‚ç»Ÿä¸€æ½œç©ºé—´çš„å•é˜¶æ®µä¸–ç•Œç”Ÿæˆâ€”â€”å°†ç»“æ„ä¸çº¹ç†ç»Ÿä¸€å»ºæ¨¡ï¼Œå‡å°‘ä¸¤é˜¶æ®µè¯¯å·®å¹¶åŠ é€Ÿæ¨ç†<br>â€¢ Agent-in-the-Loop WorldGrowï¼šé¢å‘å¯å¯¼èˆªæ€§çš„ä¸–ç•Œç”Ÿæˆâ€”â€”å°†å¯è¡Œèµ°æ€§ä¸è·¯å¾„è§„åˆ’æŒ‡æ ‡çº³å…¥è®­ç»ƒ/é€‰æ‹©ï¼Œç”Ÿæˆå¯¹æ™ºèƒ½ä½“å‹å¥½åœºæ™¯<br>â€¢ RefineGrowï¼šå¯ç¼–è¾‘çš„å±€éƒ¨è¶…ç»†èŠ‚é‡ç”Ÿæˆâ€”â€”æ”¯æŒå±€éƒ¨è¶…åˆ†ä¸é‡å»ºï¼Œè¾¹ç•Œæ— ç¼èåˆï¼Œå®ç°äº¤äº’å¼ç¼–è¾‘ä¸è¿­ä»£ä¼˜åŒ–</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21583" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21583" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Group Relative Policy Optimization (GRPO) has shown strong potential for flow-matching-based text-to-image (T2I) generation, but it faces two key limitations: inaccurate advantage attribution, and the neglect of temporal dynamics of generation. In this work, we argue that shifting the optimization paradigm from the step level to the chunk level can effectively alleviate these issues. Building on this idea, we propose Chunk-GRPO, the first chunk-level GRPO-based approach for T2I generation. The insight is to group consecutive steps into coherent 'chunk's that capture the intrinsic temporal dynamics of flow matching, and to optimize policies at the chunk level. In addition, we introduce an optional weighted sampling strategy to further enhance performance. Extensive experiments show that ChunkGRPO achieves superior results in both preference alignment and image quality, highlighting the promise of chunk-level optimization for GRPO-based methods.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ GRPOåœ¨æµåŒ¹é…T2Iä¸­å°†ç»„å†…ç›¸å¯¹ä¼˜åŠ¿å‡åŒ€åˆ†é…åˆ°æ‰€æœ‰æ—¶é—´æ­¥ï¼Œå¯¼è‡´ä¼˜åŠ¿å½’å› ä¸å‡†ç¡®ï¼šæ•´ä½“æ›´ä¼˜çš„è½¨è¿¹åœ¨æŸäº›æ­¥åè€Œæ›´å·®ï¼Œç»Ÿä¸€èµ‹å€¼ä¼šè¯¯å¯¼æ›´æ–°ï¼ˆç¤ºä¾‹è§ç¬¬2é¡µFigure 2ï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å¿½è§†æµåŒ¹é…çš„æ—¶é—´åŠ¨æ€ï¼šä¸åŒå™ªå£°æ°´å¹³/æ—¶é—´æ­¥å¯¹æœ€ç»ˆå›¾åƒè´¡çŒ®ä¸åŒï¼Œlatentå˜åŒ–å‘ˆæç¤ºè¯æ— å…³çš„é˜¶æ®µæ€§æ¨¡å¼ï¼ˆç›¸å¯¹L1è·ç¦»æ­ç¤ºçš„æ¨¡å¼ï¼Œè§ç¬¬4é¡µFigure 3ï¼‰ï¼Œæ­¥çº§ä¼˜åŒ–æ— æ³•åˆ©ç”¨è¿™ä¸€æœ¬è´¨ç»“æ„ã€‚<br>â€¢ ç²’åº¦ä¸å½“å¸¦æ¥è®­ç»ƒå™ªå£°ä¸ä¸ç¨³å®šï¼šæ­¥çº§åˆ«é‡è¦æ€§æ¯”å€¼æ›´æ–°å™ªå£°å¤§ã€ä»·å€¼ä¼ æ’­ç¼“æ…¢ï¼›åºåˆ—çº§ï¼ˆå¦‚GSPOï¼‰åˆè¿‡äºç²—ç³™æœªåˆ©ç”¨ç»†ç²’åº¦åŠ¨æ€ï¼›å³ä¾¿æ—¶é—´åŠ æƒä»åœç•™åœ¨æ­¥çº§ï¼Œé™åˆ¶äº†åå¥½å¯¹é½ä¸å›¾åƒè´¨é‡æå‡ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºChunk-GRPOï¼šä¾æ®ç›¸å¯¹L1åŠ¨æ€å°†è¿ç»­æ—¶é—´æ­¥åˆ’åˆ†ä¸ºè‹¥å¹²chunkï¼Œåœ¨chunkä¸Šä»¥å‡ ä½•å¹³å‡çš„é‡è¦æ€§æ¯”å®šä¹‰GRPOç›®æ ‡å¹¶è¿›è¡Œä¼˜åŒ–ï¼Œå®ç°ä»æ­¥çº§åˆ°å—çº§çš„ä¿¡ç”¨åˆ†é…ä¸æ›´æ–°ï¼›å¹¶æä¾›å¯é€‰çš„åŸºäºåŠ¨æ€çš„åŠ æƒæŠ½æ ·ï¼Œåå‘é«˜å™ªé˜¶æ®µä»¥å¼ºåŒ–æœ‰æ•ˆå­¦ä¹ ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ AutoChunk: è‡ªé€‚åº”æ—¶é—´åŠ¨æ€é©±åŠ¨çš„Chunkåˆ’åˆ†ä¸è”åˆä¼˜åŒ–ï¼šåœ¨çº¿å­¦ä¹ chunkè¾¹ç•Œä¸å¤§å°ï¼Œéšè®­ç»ƒä¿¡å·è‡ªé€‚åº”æ›´æ–°ä»¥æå‡ç¨³å®šæ€§ä¸æ•ˆæœã€‚<br>â€¢ Hetero-Reward Chunk-GRPO: åŸºäºå™ªå£°é˜¶æ®µçš„å¼‚æ„å¥–åŠ±èåˆï¼šåœ¨ä¸åŒchunkä½¿ç”¨å·®å¼‚åŒ–/å¤šç›®æ ‡å¥–åŠ±ï¼ˆå®¡ç¾ã€ç»“æ„ã€è¯­ä¹‰ï¼‰ä»¥åˆ†é˜¶æ®µä¼˜åŒ–ã€‚<br>â€¢ Stability-Aware Chunk Sampling: é¢å‘é«˜å™ªåŒºçš„ç¨³å¥åŠ æƒä¸æ­£åˆ™ï¼šå¼•å…¥ä¸ç¡®å®šæ€§åº¦é‡ä¸ç»“æ„çº¦æŸï¼Œç¼“è§£é«˜å™ªé˜¶æ®µåŠ æƒé‡‡æ ·å¸¦æ¥çš„ç»“æ„åå¡Œã€‚<br>â€¢ On the Theory of Chunked Policy Optimization for Flow Matching: chunkå¤§å°-ä¿¡ç”¨å½’å› -æ”¶æ•›æ€§çš„ç†è®ºåˆ†æä¸æœ€ä¼˜ç²’åº¦é€‰æ‹©å‡†åˆ™ã€‚<br>â€¢ Chunk-GRPO for Video/3D Diffusion: æ—¶åŸŸå—ä¼˜åŒ–çš„è·¨å¸§/è§†è§’ä¸€è‡´æ€§ï¼šå°†chunkæ€æƒ³æ‰©å±•è‡³è§†é¢‘ä¸3Dç”Ÿæˆï¼Œå¢å¼ºæ—¶é—´/è§†è§’ä¸€è‡´æ€§ã€‚<br>â€¢ BranchChunk-GRPO: åˆ†æ”¯é‡ç”¨ä¸Chunkä¼˜åŒ–çš„é«˜æ•ˆè”åˆï¼šç»“åˆBranchGRPOçš„å‰ç¼€é‡ç”¨ä¸chunkçº§ä¼˜åŒ–ï¼Œé™ä½è®¡ç®—å¹¶æå‡æ ·æœ¬æ•ˆç‡ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.19871" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.19871" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Discrete diffusion models have emerged as a promising direction for vision-language tasks, offering bidirectional context modeling and theoretical parallelization. However, their practical application is severely hindered by a train-inference discrepancy, which leads to catastrophic error cascades: initial token errors during parallel decoding pollute the generation context, triggering a chain reaction of compounding errors and leading to syntactic errors and semantic hallucinations. To address this fundamental challenge, we reframe the generation process from passive denoising to active refining. We introduce ReDiff, a refining-enhanced diffusion framework that teaches the model to identify and correct its own errors. Our approach features a two-stage training process: first, we instill a foundational revision capability by training the model to revise synthetic errors; second, we implement a novel online self-correction loop where the model is explicitly trained to revise its own flawed drafts by learning from an expert's corrections. This mistake-driven learning endows the model with the crucial ability to revisit and refine its already generated output, effectively breaking the error cascade. Extensive experiments demonstrate that ReDiff significantly improves the coherence and factual accuracy of generated content, enabling stable and efficient parallel generation far superior to traditional denoising methods. Our codes and models are available at https://rediff-hku.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ è®­ç»ƒ-æ¨ç†ä¸ä¸€è‡´å¯¼è‡´å¹¶è¡Œè§£ç ä¸­çš„é”™è¯¯çº§è”ï¼šåˆå§‹å°‘é‡tokené”™è¯¯ä¼šæ±¡æŸ“å…¨å±€ä¸Šä¸‹æ–‡ï¼Œåç»­æ­¥éª¤éš¾ä»¥çº æ­£ï¼Œå‡ºç°é‡å¤ã€è¯­æ³•æ··ä¹±ä¸è§†è§‰å¹»è§‰ã€‚<br>â€¢ ç°æœ‰Mask-Predæ‰©æ•£å°†å·²è§£ç tokenè§†ä¸ºå›ºå®šæ¡ä»¶ï¼Œç¼ºä¹â€œå›çœ‹å¹¶é‡å†™â€çš„èƒ½åŠ›ï¼›åœ¨å°‘æ­¥ï¼ˆå¤štoken/æ­¥ï¼‰å¹¶è¡Œç”Ÿæˆæ—¶è´¨é‡æ˜¾è‘—å´©å¡Œï¼Œæ•ˆç‡ä¸ç¨³å®šæ€§éš¾ä»¥å…¼å¾—ã€‚<br>â€¢ æ¨¡å‹ä»…åœ¨å¹²å‡€çœŸå€¼ä¸Šè®­ç»ƒï¼Œæ¨ç†æ—¶å´è¦å»ºç«‹åœ¨è‡ªèº«å™ªå£°è‰ç¨¿ä¸Šï¼Œç¼ºå°‘ä»è‡ªèº«é”™è¯¯ä¸­å­¦ä¹ ä¸çº åçš„æœºåˆ¶ï¼Œå¯¼è‡´äº‹å®ä¸€è‡´æ€§ä¸è¿è´¯æ€§ä¸è¶³ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºReDiffï¼Œå°†ç¦»æ•£æ‰©æ•£ä»â€œè¢«åŠ¨å»å™ªâ€è½¬ä¸ºâ€œä¸»åŠ¨ç²¾ä¿®â€ï¼šå…ˆé€šè¿‡å‘çœŸå€¼æ³¨å…¥åˆæˆè¯­æ³•/å¹»è§‰é”™è¯¯è¿›è¡ŒåŸºç¡€ç²¾ä¿®è®­ç»ƒï¼Œå†åœ¨åœ¨çº¿è‡ªçº ç¯ä¸­ç”¨ä¸“å®¶å¯¹æ¨¡å‹è‰ç¨¿ç”Ÿæˆâ€œè‰ç¨¿-ä¿®è®¢â€å¯¹å¹¶ä»…å¯¹è¢«çº é”™ç‰‡æ®µç›‘ç£ï¼›æ¨ç†æ—¶æ¯æ­¥åŒæ—¶è§£é”æ–°tokenå¹¶å…è®¸è¦†ç›–å¼é‡å†™å·²ç”Ÿæˆtokenï¼Œä»è€Œæ‰“æ–­é”™è¯¯çº§è”ã€æå‡å¹¶è¡Œç”Ÿæˆç¨³å®šæ€§ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„åŠ¨æ€é‡å†™æ‰©æ•£ï¼šå¼•å…¥ç½®ä¿¡åº¦/æ ¡å‡†ä¸ç¡®å®šæ€§ï¼ŒæŒ‰ä½é€‰æ‹©é‡å†™æˆ–ä¿ç•™ï¼Œè”åŠ¨æ­¥é•¿ä¸é‡æ©ç­–ç•¥ä»¥ä¼˜åŒ–å¹¶è¡Œç¨³å®šæ€§ã€‚<br>â€¢ ä¸“å®¶å…æˆ–å¼±ç›‘ç£çš„è‡ªçº è’¸é¦æ¡†æ¶ï¼šç”¨è‡ªä¸€è‡´æ€§ã€å¯¹æ¯”å­¦ä¹ æˆ–RLHFæ›¿ä»£å¤–éƒ¨ä¸“å®¶ï¼Œå®ç°æŒç»­åœ¨çº¿è‡ªçº å­¦ä¹ ä¸ä»£ä»·ä¸‹é™ã€‚<br>â€¢ AR-æ‰©æ•£æ··åˆè‡ªçº è§£ç å™¨ï¼šèåˆè‡ªå›å½’ä¸æ‰©æ•£çš„åŒå‘/å›é€€èƒ½åŠ›ï¼Œè®¾è®¡å¯å›æ»šä¸å¯ç¼–è¾‘çš„æ··åˆæ¨ç†ä»¥å…¼é¡¾é€Ÿåº¦ã€è¿è´¯æ€§ä¸äº‹å®åº¦ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">A Definition of AGI</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.18212" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.18212" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The lack of a concrete definition for Artificial General Intelligence (AGI) obscures the gap between today's specialized AI and human-level cognition. This paper introduces a quantifiable framework to address this, defining AGI as matching the cognitive versatility and proficiency of a well-educated adult. To operationalize this, we ground our methodology in Cattell-Horn-Carroll theory, the most empirically validated model of human cognition. The framework dissects general intelligence into ten core cognitive domains-including reasoning, memory, and perception-and adapts established human psychometric batteries to evaluate AI systems. Application of this framework reveals a highly "jagged" cognitive profile in contemporary models. While proficient in knowledge-intensive domains, current AI systems have critical deficits in foundational cognitive machinery, particularly long-term memory storage. The resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify both rapid progress and the substantial gap remaining before AGI.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šAGIæ¦‚å¿µâ€œç§»åŠ¨é—¨æ§›â€ã€å®šä¹‰å«ç³Šï¼Œå¯¼è‡´è¿›å±•è¯„ä¼°ä¸æ”¿ç­–è®¨è®ºç¼ºä¹ç»Ÿä¸€å°ºåº¦ï¼Œéš¾ä»¥å›ç­”â€œç¦»äººç±»æ°´å¹³è¿˜æœ‰å¤šè¿œâ€ã€‚<br>â€¢ é‡è¦æ€§ï¼šéœ€è¦å…¼é¡¾å¹¿åº¦ï¼ˆå¤šåŸŸã€å¤šæ¨¡æ€ï¼‰ä¸ç†Ÿç»ƒåº¦ï¼ˆäººç±»æ°´å¹³ï¼‰çš„å¯é‡åŒ–æ¡†æ¶ï¼Œé¿å…ä»…å‡­å•ä¸€æˆ–çª„åŸŸåŸºå‡†å¤¸å¤§èƒ½åŠ›ã€‚<br>â€¢ ç°æœ‰å±€é™1ï¼šä¸»æµåŸºå‡†å¤šæ˜¯ä»»åŠ¡/æ•°æ®é›†å¯¼å‘ï¼Œæ˜“è¢«â€œèƒŒé¢˜/ç›¸ä¼¼åˆ†å¸ƒè®­ç»ƒâ€æ±¡æŸ“ï¼Œä¸”å¸¸å…è®¸ä»¥è¡¥å¿æ€§ç­–ç•¥ï¼ˆå¦‚é•¿ä¸Šä¸‹æ–‡ã€RAGï¼‰æ©ç›–åº•å±‚èƒ½åŠ›ç¼ºé™·ï¼Œåˆ¶é€ â€œé€šç”¨æ€§å¹»è§‰â€ã€‚<br>â€¢ ç°æœ‰å±€é™2ï¼šè¯„æµ‹åé‡çŸ¥è¯†å¯†é›†ä»»åŠ¡ï¼Œå¿½è§†æ ¸å¿ƒè®¤çŸ¥æœºæ¢°ï¼ˆå¦‚é•¿æœŸè®°å¿†å­˜å‚¨/æ£€ç´¢ç²¾åº¦ã€å·¥ä½œè®°å¿†ã€é€Ÿåº¦ã€è§†è§‰/å¬è§‰å¤„ç†ï¼‰ï¼Œå¯¼è‡´æ¨¡å‹å‘ˆâ€œé”¯é½¿å‹ç”»åƒâ€ã€‚<br>â€¢ ç°æœ‰å±€é™3ï¼šç¼ºå°‘ä»¥â€œå—è¿‡è‰¯å¥½æ•™è‚²çš„æˆå¹´äººâ€ä¸ºå‚ç…§çš„ç»Ÿä¸€äººç±»åŸºå‡†ä¸å¤šæ¨¡æ€æ“ä½œåŒ–æ ‡å‡†ï¼Œæ— æ³•æ¨ªå‘æ¯”è¾ƒæ¨¡å‹æ•´ä½“â€œå¿ƒæ™ºå¼•æ“â€çš„ç“¶é¢ˆä¸çŸ­æ¿ï¼ˆå¦‚è®ºæ–‡è¡¨1ç¤ºä¾‹ï¼šGPT-4çº¦27%ï¼ŒGPT-5çº¦57%ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºåŸºäºCHCï¼ˆCattellâ€“Hornâ€“Carrollï¼‰ç†è®ºçš„AGIæ“ä½œåŒ–æ¡†æ¶ï¼šå°†æ™ºèƒ½åˆ†è§£ä¸ºåä¸ªç­‰æƒæ ¸å¿ƒèƒ½åŠ›ï¼ˆçŸ¥è¯†ã€è¯»å†™ã€æ•°å­¦ã€å³æ—¶æ¨ç†ã€å·¥ä½œè®°å¿†ã€é•¿æœŸè®°å¿†å­˜å‚¨/æ£€ç´¢ã€è§†è§‰ã€å¬è§‰ã€é€Ÿåº¦ï¼‰ï¼Œä¸ºæ¯ä¸ªèƒ½åŠ›è®¾è®¡å¯äººå·¥åˆ¤å®šçš„ç»†åŒ–ä»»åŠ¡ç”µæ± ä¸é€šè¿‡é˜ˆå€¼ï¼Œæ±‡æ€»ä¸º0â€“100%çš„â€œAGI Scoreâ€ï¼Œä»¥â€œå—è¿‡è‰¯å¥½æ•™è‚²çš„æˆå¹´äººâ€æ°´å¹³ä¸ºå‚ç…§ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ é¢å‘AGIçš„å¯æŒç»­é•¿æœŸè®°å¿†ï¼šåœ¨è®­ç»ƒåæŒç»­å­¦ä¹ çš„æƒé‡å¯å¡‘æ¨¡å—+ç§æœ‰å¤–å­˜ç´¢å¼•ï¼Œè¯„æµ‹å¹¶æå‡ä¸ªä½“åŒ–è®°å¿†çš„å­˜å‚¨/å·©å›º/å›å¿†ã€‚<br>â€¢ ä»å¤–éƒ¨æ£€ç´¢åˆ°å†…ç”Ÿæ£€ç´¢çš„å¹»è§‰æŠ‘åˆ¶ï¼šæ„å»ºç»Ÿä¸€çš„æ£€ç´¢ç²¾åº¦ä¸å†…æ£€æ ¡æ ¸æœºåˆ¶ï¼Œç³»ç»Ÿæ¯”è¾ƒRAGä¸å†…ç”Ÿè®°å¿†æ£€ç´¢å¯¹å¹»è§‰ç‡ä¸è¦†ç›–ç‡çš„æƒè¡¡ã€‚<br>â€¢ è·¨è¯­è¨€ä¸æ–‡åŒ–çš„CHC-AGIç­‰å€¼åŒ–ï¼šå¤šè¯­è¨€ã€å¤šæ–‡åŒ–ç‰ˆæœ¬çš„ååŸŸæµ‹è¯„ä¸é‡è¡¨ç­‰å€¼åŒ–ï¼Œé¿å…è‹±è¯­ä¸æ–‡åŒ–åç½®ï¼Œå»ºç«‹å›½é™…é€šç”¨åˆ†æ•°çº¿ã€‚<br>â€¢ å¤šæ¨¡æ€é€Ÿåº¦ä¸ååº”æ—¶åŸºå‡†ï¼ˆGs/Gtï¼‰ï¼šæ ‡å‡†åŒ–è§†è§‰/å¬è§‰/æ–‡æœ¬ç«¯åˆ°ç«¯å»¶è¿Ÿä¸ååè¯„æµ‹ï¼Œè¿æ¥æ¨¡å‹æ¨ç†ã€I/Oä¸ç³»ç»Ÿæ ˆä¼˜åŒ–å¯¹â€œå¿ƒæ™ºé€Ÿåº¦â€çš„è´¡çŒ®ã€‚<br>â€¢ èƒ½åŠ›æ‰­ç»“çš„å®šé‡åŒ–ä¸ä»£ä»·æ¨¡å‹ï¼šé‡åŒ–é•¿ä¸Šä¸‹æ–‡ã€å·¥å…·è°ƒç”¨ã€RAGç­‰è¡¥å¿ç­–ç•¥å¯¹æ ¸å¿ƒèƒ½åŠ›ç¼ºé™·çš„â€œé®è”½åº¦/æˆæœ¬â€ï¼Œç»™å‡ºä½•æ—¶ä¸å¯æ›¿ä»£åœ°éœ€è¦åº•å±‚èƒ½åŠ›æ”¹è¿›ã€‚<br>â€¢ AGIè¯„åˆ†çš„é²æ£’èšåˆä¸æ•æ„Ÿæ€§åˆ†æï¼šç ”ç©¶ä¸åŒæƒé‡æ–¹æ¡ˆã€ç“¶é¢ˆæƒ©ç½šä¸ç”»åƒä¼˜å…ˆçš„æŠ¥å‘ŠèŒƒå¼ï¼Œå¯¹æ€»ä½“åˆ†æ•°çš„ç¨³å®šæ€§ä¸å†³ç­–è§£é‡Šæ€§è¿›è¡Œç³»ç»Ÿè¯„ä¼°ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Reasoning with Sampling: Your Base Model is Smarter Than You Think</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.14901" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.14901" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Frontier reasoning models have exhibited incredible capabilities across a wide array of disciplines, driven by posttraining large language models (LLMs) with reinforcement learning (RL). However, despite the widespread success of this paradigm, much of the literature has been devoted to disentangling truly novel behaviors that emerge during RL but are not present in the base models. In our work, we approach this question from a different angle, instead asking whether comparable reasoning capabilites can be elicited from base models at inference time by pure sampling, without any additional training. Inspired by Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened distributions, we propose a simple iterative sampling algorithm leveraging the base models' own likelihoods. Over different base models, we show that our algorithm offers substantial boosts in reasoning that nearly match and even outperform those from RL on a wide variety of single-shot tasks, including MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in diversity over multiple samples that is characteristic of RL-posttraining. Crucially, our method does not require training, curated datasets, or a verifier, suggesting broad applicability beyond easily verifiable domains.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šèƒ½å¦åœ¨ä¸è¿›è¡Œä»»ä½•åè®­ç»ƒï¼ˆRL/å¾®è°ƒï¼‰ã€ä¸ä¾èµ–æ•°æ®ä¸è£åˆ¤å™¨çš„å‰æä¸‹ï¼Œä»…é€šè¿‡æ¨ç†æ—¶é‡‡æ ·ä»åŸºåº§æ¨¡å‹ä¸­æ¿€å‘ä¸RLç›¸å½“çš„å•æ ·æœ¬æ¨ç†èƒ½åŠ›ï¼›RLçš„æå‡æ˜¯å¦ä¸»è¦æ˜¯å¯¹åŸºæ¨¡åˆ†å¸ƒçš„â€œé”åŒ–â€è€Œéå­¦ä¹ å…¨æ–°èƒ½åŠ›ï¼ˆè§å›¾1ï¼Œç¬¬2é¡µï¼‰ã€‚<br>â€¢ é‡è¦æ€§ä¸ç°æœ‰RLå±€é™ï¼šRLVR/GRPOè™½åœ¨æ•°å­¦ä¸ä»£ç ç­‰å¯éªŒè¯ä»»åŠ¡ä¸Šæœ‰æ•ˆï¼Œä½†è®­ç»ƒä»£ä»·é«˜ã€éœ€å¤§è§„æ¨¡ç²¾é€‰æ•°æ®ä¸è‡ªåŠ¨è£åˆ¤å™¨ã€å­˜åœ¨è®­ç»ƒä¸ç¨³å®šä¸è¶…å‚æ•æ„Ÿï¼›ä¸”æ˜“å¯¼è‡´å¤šæ ·æ€§å¡Œç¼©ã€pass@kæ˜¾è‘—ä¸‹é™ã€å‡ºåŸŸæ³›åŒ–æ¬ ä½³ï¼ˆå›¾4ä¸pass@kæ›²çº¿è§ç¬¬9-10é¡µï¼›è¡¨1ç»“æœè§ç¬¬9é¡µï¼‰ã€‚<br>â€¢ ç°æœ‰é‡‡æ ·æ–¹æ³•å±€é™ï¼šå¸¸ç”¨ä½æ¸©é‡‡æ ·é€tokené™æ¸©å¹¶ä¸ç­‰ä»·äºå¯¹æ•´æ®µåºåˆ—çš„å¹‚åˆ†å¸ƒé‡‡æ ·p^Î±ï¼ˆå‘½é¢˜1ï¼Œç¬¬4-5é¡µï¼‰ï¼Œå…¶â€œæ±‚å’Œåå†æŒ‡æ•°â€ä¼šåå‘â€œè·¯å¾„å¤šä½†å•æ¡ä½ä¼¼ç„¶â€çš„ç»­å†™ï¼Œæ— æ³•ä¸ºâ€œæœªæ¥è·¯å¾„â€åšå…¨å±€è§„åˆ’ï¼Œæ˜“åœ¨å…³é”®çª—å£/æ¢çº½tokenä¸Šé€‰é”™ï¼›åŸºæ¨¡çš„é«˜ä¼¼ç„¶/é«˜ç½®ä¿¡åŒºåŸŸä¸æ­£ç¡®æ¨ç†å¼ºç›¸å…³å´è¢«ä½æ•ˆåˆ©ç”¨ï¼ˆå›¾4ï¼Œç¬¬9é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºâ€œå¹‚åˆ†å¸ƒé‡‡æ ·â€ï¼šä»¥åŸºæ¨¡åˆ†å¸ƒpçš„å¹‚p^Î±ä¸ºç›®æ ‡ï¼Œç”¨åˆ†å—è‡ªå›å½’çš„Metropolisâ€“Hastings MCMCåœ¨æ¨ç†æ—¶è¿­ä»£é‡é‡‡æ ·å­åºåˆ—å¹¶æŒ‰åŸºæ¨¡ä¼¼ç„¶æ¯”æ¥å—ï¼Œä»è€Œåœ¨æ— éœ€è®­ç»ƒ/æ•°æ®/è£åˆ¤å™¨çš„å‰æä¸‹ï¼Œæ˜¾å¼å¯¹é«˜ä¼¼ç„¶æ¨ç†è½¨è¿¹è¿›è¡Œåˆ†å¸ƒé”åŒ–ï¼ˆç®—æ³•1ï¼Œç¬¬7é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è‡ªé€‚åº”å¹‚é‡‡æ ·ä¸åŠ¨æ€Î±è°ƒåº¦ï¼šåŸºäºä¸ç¡®å®šæ€§ä¸å…³é”®çª—å£ä¿¡å·åœ¨çº¿è°ƒæ•´Î±ä¸MCMCæ­¥æ•°ï¼Œå…¼é¡¾å‡†ç¡®ç‡ã€è®¡ç®—ä¸å¤šæ ·æ€§ã€‚<br>â€¢ å­¦ä¹ å‹æè®®åˆ†å¸ƒé©±åŠ¨çš„è‡ªå›å½’MCMCï¼šç”¨å°æ¨¡å‹æˆ–ç¦»çº¿è¡Œä¸ºå…‹éš†å­¦ä¹ ppropï¼Œç¼©çŸ­æ··åˆæ—¶é—´ï¼Œæ‰©å±•è‡³é•¿ä¸Šä¸‹æ–‡ä¸å¤šæ¨¡æ€ã€‚<br>â€¢ è®­ç»ƒä¸é‡‡æ ·çš„ç»Ÿä¸€è§†è§’ï¼šå°†RLVRè§†ä¸ºå¯¹p^Î±çš„è®­ç»ƒè¿‘ä¼¼ï¼Œå»ºç«‹å•/å¤šæ ·æœ¬æ€§èƒ½ä¸ä¼¼ç„¶é”åŒ–ä¹‹é—´çš„å®šé‡ç†è®ºä¸è®¡ç®—æ‰©å±•æ³•åˆ™ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Sparser Block-Sparse Attention via Token Permutation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21270" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21270" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Scaling the context length of large language models (LLMs) offers significant benefits but is computationally expensive. This expense stems primarily from the self-attention mechanism, whose O(N^2) complexity with respect to sequence length presents a major bottleneck for both memory and latency. Fortunately, the attention matrix is often sparse, particularly for long sequences, suggesting an opportunity for optimization. Block-sparse attention has emerged as a promising solution that partitions sequences into blocks and skips computation for a subset of these blocks. However, the effectiveness of this method is highly dependent on the underlying attention patterns, which can lead to sub-optimal block-level sparsity. For instance, important key tokens for queries within a single block may be scattered across numerous other blocks, leading to computational redundancy. In this work, we propose Permuted Block-Sparse Attention (PBS-Attn), a plug-and-play method that leverages the permutation properties of attention to increase block-level sparsity and enhance the computational efficiency of LLM prefilling. We conduct comprehensive experiments on challenging real-world long-context datasets, demonstrating that PBS-Attn consistently outperforms existing block-sparse attention methods in model accuracy and closely matches the full attention baseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn achieves an end-to-end speedup of up to 2.75times in long-context prefilling, confirming its practical viability. Code available at https://github.com/xinghaow99/pbs-attn</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ è‡ªæ³¨æ„åŠ›åœ¨åºåˆ—é•¿åº¦ä¸Šçš„O(N^2)å¤æ‚åº¦å¯¼è‡´é•¿ä¸Šä¸‹æ–‡LLMçš„å†…å­˜ä¸æ—¶å»¶æˆæœ¬é™¡å¢ï¼Œå°¤å…¶åœ¨prefillé˜¶æ®µæˆä¸ºä¸»è¦ç“¶é¢ˆã€‚<br>â€¢ ç°æœ‰ç¡¬ä»¶å‹å¥½ä¼˜åŒ–ï¼ˆå¦‚FlashAttentionï¼‰è™½é™ä½å†…å­˜/I-Oå¼€é”€ï¼Œä½†ä¸æ”¹å˜è®¡ç®—é‡é˜¶æ•°ï¼›å—ç¨€ç–æ³¨æ„åŠ›éœ€é¢„å…ˆé€‰å—ï¼Œæ•ˆæœå¼ºä¾èµ–æ³¨æ„åŠ›æ¨¡å¼ã€‚<br>â€¢ å½“åŒä¸€æŸ¥è¯¢å—çš„é‡è¦é”®åˆ†æ•£åœ¨ä¼—å¤šé”®å—æ—¶ï¼Œå—çº§ç¨€ç–æ€§æ˜¾è‘—ä¸‹é™ã€äº§ç”Ÿå†—ä½™è®¡ç®—ï¼Œå¯¼è‡´å—ç¨€ç–æ³¨æ„åŠ›è¿œç¦»æœ€ä¼˜ç¨€ç–ç»“æ„ã€‚<br>â€¢ å…¨å±€ç½®æ¢è™½å¯èšæ‹¢é‡è¦é”®ï¼Œä½†ä¼šç ´åå› æœæ³¨æ„åŠ›çš„ä¸‹ä¸‰è§’ç»“æ„ï¼›ç¼ºä¹åœ¨ä¿æŒå› æœæ€§çš„åŒæ—¶æå‡å—çº§ç¨€ç–çš„é€šç”¨æ–¹æ³•ï¼ˆå›¾ç¤ºè§ç¬¬3é¡µå›¾1ï¼‰ã€‚<br>â€¢ é•¿åºåˆ—æ³¨æ„åŠ›å¸¸å‘ˆâ€œå‚ç›´çº¿â€æ¨¡å¼ï¼ˆè·¨æŸ¥è¯¢å…±äº«çš„å…³é”®é”®ï¼‰ï¼›è‹¥åˆ†å¸ƒé›¶æ•£ä¼šè¿«ä½¿è¦†ç›–å¤§é‡å—ï¼Œéœ€è¦èƒ½èšç±»è¿™äº›å…³é”®é”®çš„æœºåˆ¶ï¼ˆç¬¬4é¡µå›¾2æ˜¾ç¤ºç½®æ¢åå—å¯†åº¦ç”±82.50%é™è‡³32.31%ï¼Œè¦†ç›–ç‡åå‡è‡³96.44%ï¼‰ã€‚<br>â€¢ éœ€è¦ä¸€ç§å¯æ’æ‹”ã€ä¸é«˜æ•ˆå†…æ ¸å…¼å®¹çš„æ–¹æ³•ï¼Œåœ¨ä¸ç‰ºç‰²ç²¾åº¦çš„å‰æä¸‹æ˜¾è‘—æé«˜å—ç¨€ç–åº¦ä¸ç«¯åˆ°ç«¯prefillé€Ÿåº¦ï¼ˆè®ºæ–‡æŠ¥å‘Šè‡³å¤š2.75Ã—åŠ é€Ÿï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºPBS-Attnï¼šåˆ©ç”¨æ³¨æ„åŠ›å¯¹é”®å€¼ç½®æ¢ä¸å˜ã€å¯¹æŸ¥è¯¢ç½®æ¢ç­‰å˜çš„æ€§è´¨ï¼Œè¿›è¡Œâ€œåˆ†æ®µç½®æ¢â€ä»¥åœ¨ä¿æŒè·¨æ®µå› æœæ€§çš„åŒæ—¶æ®µå†…è‡ªç”±é‡æ’ï¼›ç»“åˆâ€œæŸ¥è¯¢æ„ŸçŸ¥â€çš„é”®ç½®æ¢ï¼ˆç”¨æœ€åä¸€å—æŸ¥è¯¢ä¼°è®¡é”®é‡è¦æ€§å¹¶åœ¨æ®µå†…æ’åºï¼‰ï¼Œåœ¨ç½®æ¢åç”Ÿæˆå—ç¨€ç–æ©ç ï¼Œå€ŸåŠ©å®šåˆ¶çš„permuted-FlashAttentionæŒ‰æ©ç è®¡ç®—å¹¶å¯¹è¾“å‡ºé€†ç½®æ¢æ¢å¤é¡ºåºã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ å¯å­¦ä¹ çš„åˆ†æ®µç½®æ¢ä¸å—é€‰æ‹©è”åˆä¼˜åŒ–ï¼šå°†æ®µåˆ’åˆ†ã€ç½®æ¢ä¸å—æ©ç å‚æ•°åŒ–å¹¶ç«¯åˆ°ç«¯è®­ç»ƒï¼Œé€‚é…ä¸åŒå±‚/å¤´ä¸ä»»åŠ¡ä»¥è¿›ä¸€æ­¥é€¼è¿‘å…¨æ³¨æ„åŠ›ç²¾åº¦ã€‚<br>â€¢ æ¨ç†æ—¶è‡ªé€‚åº”åˆ†æ®µä¸åœ¨çº¿é‡æ’çš„ä½å»¶è¿Ÿç®—æ³•ï¼šåŸºäºå®æ—¶æ³¨æ„åŠ›ç»Ÿè®¡åœ¨çº¿è°ƒæ•´æ®µé•¿ä¸ç½®æ¢ç­–ç•¥ï¼Œç»Ÿä¸€åŠ é€Ÿprefillä¸decodeé˜¶æ®µå¹¶ä¿è¯æ—¶å»¶ç¨³å®šæ€§ã€‚<br>â€¢ PBS-Attnä¸ç¡¬ä»¶å…±è®¾è®¡ï¼šé¢å‘GPU/TPU/NPUçš„å†…æ ¸ä¸ç¼–è¯‘ä¼˜åŒ–ï¼Œè”åˆè°ƒåº¦ç½®æ¢ä¸ç¨€ç–è®¿å­˜ï¼Œé‡åŒ–å¸¦å®½/ç®—åŠ›å—é™åŒºé—´ä¸å¯è¾¾æé™åŠ é€Ÿæ¯”ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20286" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20286" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>GUI grounding, which maps natural-language instructions to actionable UI elements, is a core capability of GUI agents. Prior works largely treats instructions as a static proxy for user intent, overlooking the impact of instruction diversity and quality on grounding performance. Through a careful investigation of existing grounding datasets, we find a 23.3% flaw rate in their instructions and show that inference-time exploitation of instruction diversity yields up to a substantial 76% relative performance improvement. In this paper, we introduce the Instruction-as-Reasoning paradigm, treating instructions as dynamic analytical pathways that offer distinct perspectives and enabling the model to select the most effective pathway during reasoning. To achieve this, we propose a two-stage training framework: supervised fine-tuning (SFT) on synthesized, diverse instructions to instill multi-perspective reasoning, followed by reinforcement learning (RL) to optimize pathway selection and composition. Our resulting models, UI-Ins-7B and UI-Ins-32B, achieve state-of-the-art results on five challenging grounding benchmarks and exhibit emergent reasoning, selectively composing and synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model demonstrates strong agentic potential, achieving a 74.1% success rate on AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals additional insights such as how reasoning can be formulated to enhance rather than hinder grounding performance, and how our method mitigates policy collapse in the SFT+RL framework. All code and model checkpoints will be publicly released in https://github.com/alibaba/UI-Ins.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šGUI groundingè¿‡åº¦æŠŠâ€œæŒ‡ä»¤â€å½“ä½œé™æ€è¾“å…¥ï¼Œæ— æ³•åƒäººç±»ä¸€æ ·åœ¨å¤–è§‚ã€åŠŸèƒ½ã€ä½ç½®ã€æ„å›¾ç­‰å¤šè§†è§’ä¹‹é—´çµæ´»åˆ‡æ¢ï¼Œé€ æˆç†è§£ä¸å®šä½èƒ½åŠ›å—é™ã€‚<br>â€¢ æ•°æ®å™ªå£°ï¼šä¸»æµæ•°æ®é›†æŒ‡ä»¤è´¨é‡è¾ƒå·®ï¼Œäººå·¥æŠ½æ£€å‘ç°çº¦23.3%å­˜åœ¨æ­§ä¹‰æˆ–ä¸åŒ¹é…ï¼Œç›´æ¥æ‹‰ä½è®­ç»ƒæ•ˆæœï¼›æ¸…æ´—åæ˜¾è‘—æå‡å‡†ç¡®ç‡ï¼ˆæ–‡ä¸­å¤šåŸºå‡†éªŒè¯ï¼‰ã€‚<br>â€¢ æœºä¼šç‚¹ï¼šåŒä¸€æ ·æœ¬è‹¥èƒ½æ€»æ˜¯é€‰åˆ°æœ€ä½³æŒ‡ä»¤è§†è§’ï¼Œé›¶è®­ç»ƒæƒ…å†µä¸‹ç›¸å¯¹å¯æå‡æœ€é«˜è¾¾76%ï¼Œä½†ç°æœ‰æ¨¡å‹ç¼ºä¹â€œé€‰æ‹©/ç»„åˆè§†è§’â€çš„æœºåˆ¶ã€‚<br>â€¢ è®­ç»ƒå›°å¢ƒï¼šè‡ªç”±å¼ï¼ˆFree-formï¼‰æ¨ç†åœ¨RLä¸­å¸¸å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼›SFT+RLæ˜“å‡ºç°ç­–ç•¥åå¡Œï¼ˆåæ ‡è¾“å‡ºå•ä¸€ã€æ¢ç´¢ä¸è¶³ï¼‰ï¼Œé™åˆ¶å¯æ‰©å±•æ€§ä¸ç¨³å®šæ€§ã€‚<br>â€¢ ç°å®éœ€æ±‚ï¼šåœ¨çº¿ä»£ç†ï¼ˆå¦‚AndroidWorldï¼‰è¦æ±‚æ—¶ç©ºç¨³å®šã€è¯­ä¹‰ä¸€è‡´çš„å®šä½ï¼Œå‘¼å”¤èƒ½ä¸»åŠ¨â€œé€‰è·¯â€çš„ç¨³å¥æ¨ç†èŒƒå¼ä¸æ›´é«˜è´¨é‡çš„æ•°æ®åŸºç¡€ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºInstruction-as-ReasoningèŒƒå¼ï¼šå°†â€œæŒ‡ä»¤â€è§†ä¸ºå¯é€‰æ‹©çš„æ¨ç†è·¯å¾„ã€‚å…ˆç”¨æ•°æ®ç®¡çº¿æ¸…æ´—å¹¶ç”Ÿæˆå¤šè§†è§’é«˜è´¨æŒ‡ä»¤ï¼ŒSFTé˜¶æ®µæ˜¾å¼å­¦ä¹ å¤šè§†è§’æ¨ç†åˆ°åæ ‡çš„ç”Ÿæˆï¼Œå†ç”¨GRPOå¼ºåŒ–å­¦ä¹ åœ¨å¼€æ”¾æ¨ç†ç©ºé—´ä¸­é€‰æ‹©/ç»„åˆæœ€ä¼˜è§†è§’ï¼ˆç‚¹å†…å¥–åŠ±ï¼‰ï¼Œä»è€Œç¨³å¥æå‡å®šä½å¹¶ç¼“è§£ç­–ç•¥åå¡Œã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„Instruction-as-Reasoningï¼šä¸ºè§†è§’é€‰æ‹©å¼•å…¥ç½®ä¿¡åº¦ä¸æ ¡å‡†ï¼Œæé«˜åœ¨å™ªå£°ä¸æ­§ä¹‰åœºæ™¯ä¸‹çš„ç¨³å¥æ€§<br>â€¢ çŸ¥è¯†å¢å¼ºçš„GUI Groundingï¼šèåˆå¤–éƒ¨å®ä½“/å“ç‰Œ/é¢†åŸŸçŸ¥è¯†åº“ï¼Œè¡¥é½ä¸“ä¸šè½¯ä»¶ä¸å“ç‰Œè¯†åˆ«ç­‰ç¼ºå¤±çŸ¥è¯†<br>â€¢ ä»ç‚¹åˆ°ç»“æ„çš„å¸ƒå±€æ¨ç†ï¼šè”åˆUIå¸ƒå±€è§£æä¸å…³ç³»æ¨ç†ï¼ˆçˆ¶å­/åŒçº§/ç¾¤ç»„ï¼‰ä»¥ç¼“è§£å¯ç‚¹å‡»åŒºåŸŸä¸å¹²æ‰°é¡¹ç›¸æ··é—®é¢˜<br>â€¢ ç»„åˆè§†è§’çš„æ¨ç†æœç´¢ä¸èšåˆï¼šæµ‹è¯•æ—¶å¤šè·¯å¾„ç”Ÿæˆ+è‡ªé€‚åº”æŠ•ç¥¨/åŠ æƒèšåˆï¼Œæå‡å›°éš¾æ ·æœ¬çš„ä¸Šé™<br>â€¢ å¤šæ¨¡æ€å¤åˆå¥–åŠ±è®¾è®¡ï¼šåœ¨RLä¸­å¼•å…¥é«˜æ–¯/IoU/ä¸€è‡´æ€§ç­‰å¤åˆä¿¡å·ï¼Œè¶…è¶Šå•ä¸€â€œç‚¹å†…â€å¥–åŠ±ï¼Œç¨³å®šä¼˜åŒ–è¿‡ç¨‹<br>â€¢ ç«¯åˆ°ç«¯è§„åˆ’-å®šä½è”åˆè®­ç»ƒï¼šå°†è§„åˆ’å™¨ä¸å®šä½å™¨è”åˆSFT+RLï¼Œæ‰“é€šâ€œæ„å›¾-æ¨ç†-åŠ¨ä½œâ€å…¨é“¾è·¯<br>â€¢ æŒ‡ä»¤è´¨é‡è¯„æµ‹ä¸åˆæˆåŸºå‡†ï¼šæ„å»ºå¤šè§†è§’ã€å¯éªŒè¯çš„æŒ‡ä»¤è´¨é‡è¯„åˆ†ä½“ç³»ä¸å…¬å¼€åŸºå‡†ï¼Œä¿ƒè¿›æ•°æ®å±‚é¢çš„å¯æŒç»­æ”¹è¿›<br>â€¢ è·¨å¹³å°ä¸è½»é‡åŒ–æ³›åŒ–ï¼šé¢å‘å¤šOS/å¤šåˆ†è¾¨ç‡çš„è¿ç§»ä¸è’¸é¦ï¼Œè®©å¤šè§†è§’æ¨ç†åœ¨ç§»åŠ¨ç«¯å°æ¨¡å‹ä¸ŠåŒæ ·æœ‰æ•ˆ</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Visual Diffusion Models are Geometric Solvers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21697" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21697" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In this paper we show that visual diffusion models can serve as effective geometric solvers: they can directly reason about geometric problems by working in pixel space. We first demonstrate this on the Inscribed Square Problem, a long-standing problem in geometry that asks whether every Jordan curve contains four points forming a square. We then extend the approach to two other well-known hard geometric problems: the Steiner Tree Problem and the Simple Polygon Problem. Our method treats each problem instance as an image and trains a standard visual diffusion model that transforms Gaussian noise into an image representing a valid approximate solution that closely matches the exact one. The model learns to transform noisy geometric structures into correct configurations, effectively recasting geometric reasoning as image generation. Unlike prior work that necessitates specialized architectures and domain-specific adaptations when applying diffusion to parametric geometric representations, we employ a standard visual diffusion model that operates on the visual representation of the problem. This simplicity highlights a surprising bridge between generative modeling and geometric problem solving. Beyond the specific problems studied here, our results point toward a broader paradigm: operating in image space provides a general and practical framework for approximating notoriously hard problems, and opens the door to tackling a far wider class of challenging geometric tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šå¦‚ä½•ç”¨ç»Ÿä¸€ã€ç®€å•çš„å­¦ä¹ æ¡†æ¶è§£å†³ä¸€ç±»ç»å…¸è€Œå›°éš¾çš„å‡ ä½•ä¼˜åŒ–/æ¨ç†é—®é¢˜ï¼ˆå¦‚å†…æ¥æ­£æ–¹å½¢ã€Steineræ ‘ã€æœ€å¤§é¢ç§¯å¤šè¾¹å½¢åŒ–ï¼‰ï¼Œè€Œéä¸ºæ¯ä¸ªé—®é¢˜è®¾è®¡ä¸“é—¨è¡¨ç¤ºä¸ç®—æ³•ã€‚<br>â€¢ é‡è¦æ€§ï¼šè¿™äº›é—®é¢˜è¦ä¹ˆæœªå®Œå…¨è§£å†³ï¼ˆå†…æ¥æ­£æ–¹å½¢åœ¨ä¸€èˆ¬Jordanæ›²çº¿ä»æœªè¯å®ï¼‰ï¼Œè¦ä¹ˆä¸ºNPéš¾ï¼ˆSteineræ ‘ã€æœ€å¤§é¢ç§¯å¤šè¾¹å½¢åŒ–ï¼‰ï¼Œä¸”åœ¨é€šä¿¡/å¸ƒçº¿/åœ°å›¾ä¸è®¡ç®—æœºå›¾å½¢ç­‰åº”ç”¨ä¸­å…³é”®ï¼ˆè§æ–‡ä¸­ç¬¬4ã€5èŠ‚ï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šä»¥å¾€æ‰©æ•£æ±‚è§£å¤šåœ¨ç¬¦å·/å›¾ç©ºé—´ï¼Œéœ€ç‰¹å®šç¼–ç ä¸å™ªå£°è¿‡ç¨‹ï¼›ä¼ ç»Ÿå‡ ä½•ç®—æ³•ä¾èµ–ç²¾ç»†å·¥ç¨‹ã€å¼ºå…ˆéªŒä¸ç‰¹å®šè¡¨ç¤ºï¼Œå¯¹å¤šè§£åˆ†å¸ƒä¸ä¸ç¡®å®šæ€§æ”¯æŒä¸è¶³ï¼›å¯¹è§„æ¨¡æ•æ„Ÿï¼Œæ³›åŒ–åˆ°æ›´å¤§ç‚¹é›†å›°éš¾ã€‚<br>â€¢ ç¼ºå£ä¸æœºé‡ï¼šåƒç´ åŸŸèƒ½è‡ªç„¶æ‰¿è½½å¤šæ¨¡æ€è§£åˆ†å¸ƒä¸”ä¾¿äºå¤§è§„æ¨¡æ•°æ®åˆæˆä¸è®­ç»ƒï¼Œä½†æ˜¯å¦èƒ½ç›´æ¥â€œåœ¨åƒç´ ä¸­æ¨ç†â€å°šç¼ºç³»ç»ŸéªŒè¯ã€‚æœ¬å·¥ä½œæ˜¾ç¤ºæ ‡å‡†è§†è§‰æ‰©æ•£å³å¯åœ¨åƒç´ åŸŸè¿‘ä¼¼æ±‚è§£å¹¶äº§ç”Ÿå¤šæ ·è§£ï¼ˆè§å›¾3/6/9ï¼‰ï¼Œä¸”æ•ˆæœæ¥è¿‘æœ€ä¼˜ï¼ˆSteineræ ‘è¡¨2ã€MAXAPè¡¨3ï¼‰ã€‚<br>â€¢ å®è·µä»·å€¼ï¼šåŒä¸€è§†è§‰æ‰©æ•£æ¡†æ¶å¯è·¨ä»»åŠ¡å¤ç”¨ï¼Œä»…æ›´æ¢è®­ç»ƒæ•°æ®ä¸è½»é‡åå¤„ç†ï¼Œæ˜¾è‘—ç®€åŒ–å·¥ç¨‹å¤æ‚åº¦ï¼ŒåŒæ—¶é€šè¿‡å¤šç§éšæœºç§å­æ¢ç´¢å¤šè§£ï¼ˆå†…æ¥æ­£æ–¹å½¢å›¾4ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>å°†å‡ ä½•é—®é¢˜å®ä¾‹ä¸è§£ç»Ÿä¸€è¡¨ç¤ºä¸ºå›¾åƒï¼Œè®­ç»ƒæ¡ä»¶è§†è§‰æ‰©æ•£æ¨¡å‹ï¼ˆUâ€‘Net+è‡ªæ³¨æ„ï¼Œæ¡ä»¶ä¸ºé—®é¢˜å®ä¾‹å›¾ï¼‰ä»é«˜æ–¯å™ªå£°å»å™ªç”Ÿæˆâ€œè§£å›¾åƒâ€ï¼Œå†ä»¥å‡ ä½•æ„ŸçŸ¥çš„åå¤„ç†ï¼ˆå¦‚é¡¶ç‚¹/è¾¹æå–ã€åˆšæ€§å¯¹é½snappingï¼‰ä»åƒç´ ç»“æœæ¢å¤ç»“æ„åŒ–è§£å¹¶è¯„æµ‹ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ çº¦æŸå¼•å¯¼çš„è§†è§‰æ‰©æ•£ï¼šåœ¨é‡‡æ ·ä¸­æ³¨å…¥å‡ ä½•ä¸å˜é‡ä¸èƒ½é‡å¼•å¯¼ï¼Œæé«˜æœ‰æ•ˆè§£ç‡ä¸å¤§è§„æ¨¡ç¨³å®šæ€§<br>â€¢ åƒç´ åˆ°å‘é‡çš„å¯å¾®çŸ¢é‡åŒ–ï¼šå°†å¯å¾®æ¸²æŸ“/çŸ¢é‡åŒ–ä¸æ‰©æ•£åå¤„ç†è”åˆä¼˜åŒ–ï¼Œè·å¾—äºšåƒç´ ç²¾åº¦ä¸å‚æ•°çº§è¯¯å·®ç•Œ<br>â€¢ å¤šä»»åŠ¡ç»Ÿä¸€å‡ ä½•æ±‚è§£å™¨ï¼šå•æ¨¡å‹å¤šä»»åŠ¡æ¡ä»¶åŒ–è®­ç»ƒï¼Œè·¨å†…æ¥å½¢ã€Steineræ ‘ã€Polygonizationå…±äº«è¡¨ç¤ºä¸è¿ç§»<br>â€¢ åˆ†å±‚å¤šåˆ†è¾¨ç‡ä¸è¯¾ç¨‹å­¦ä¹ ï¼šè‡ªå°è§„æ¨¡åˆ°å¤§è§„æ¨¡ç‚¹é›†/å¤æ‚æ›²çº¿çš„å°ºåº¦æ³›åŒ–ä¸ç»„åˆæ³›åŒ–ç­–ç•¥<br>â€¢ åƒç´ -ç¬¦å·æ··åˆæ¨ç†ï¼šæ‰©æ•£ç”Ÿæˆé«˜è´¨é‡åˆè§£ï¼Œç»“åˆGeoSteiner/ILP/å¯å‘å¼ç²¾ä¿®ä»¥è¾¾æ›´ä¼˜è´¨é‡-æ•ˆç‡æƒè¡¡<br>â€¢ å¤šè§£åˆ†å¸ƒå»ºæ¨¡ä¸è¦†ç›–ä¼˜åŒ–ï¼šåŸºäºæ‰©æ•£é‡‡æ ·çš„è§£é›†å¤šæ ·æ€§åº¦é‡ã€èšç±»ä¸äº¤äº’å¼æ¢ç´¢/æšä¸¾<br>â€¢ ä»2Dåˆ°3Dä¸æµå½¢ï¼šæ‰©å±•åˆ°ä¸‰ç»´/æ›²é¢ä¸Šçš„Steineræ ‘ã€å†…æ¥ç»“æ„ä¸å¤šè¾¹å½¢åŒ–ç­‰æ›´å¹¿æ³›å‡ ä½•ä»»åŠ¡</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20206" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20206" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Prompt design plays a crucial role in text-to-video (T2V) generation, yet user-provided prompts are often short, unstructured, and misaligned with training data, limiting the generative potential of diffusion-based T2V models. We present RAPO++, a cross-stage prompt optimization framework that unifies training-data--aligned refinement, test-time iterative scaling, and large language model (LLM) fine-tuning to substantially improve T2V generation without modifying the underlying generative backbone. In Stage 1, Retrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with semantically relevant modifiers retrieved from a relation graph and refactors them to match training distributions, enhancing compositionality and multi-object fidelity. Stage 2 introduces Sample-Specific Prompt Optimization (SSPO), a closed-loop mechanism that iteratively refines prompts using multi-source feedback -- including semantic alignment, spatial fidelity, temporal coherence, and task-specific signals such as optical flow -- yielding progressively improved video generation quality. Stage 3 leverages optimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing task-specific optimization patterns and enabling efficient, high-quality prompt generation even before inference. Extensive experiments across five state-of-the-art T2V models and five benchmarks demonstrate that RAPO++ achieves significant gains in semantic alignment, compositional reasoning, temporal stability, and physical plausibility, outperforming existing methods by large margins. Our results highlight RAPO++ as a model-agnostic, cost-efficient, and scalable solution that sets a new standard for prompt optimization in T2V generation. The code is available at https://github.com/Vchitect/RAPO.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç”¨æˆ·è‡ªç„¶è¯­è¨€æç¤ºçŸ­å°ã€æ— ç»“æ„ä¸”ä¸è®­ç»ƒæ•°æ®é£æ ¼/é•¿åº¦åˆ†å¸ƒä¸åŒ¹é…ï¼Œé™åˆ¶æ‰©æ•£å¼T2Vçš„è¯­ä¹‰å¯¹é½ä¸æ—¶åºè´¨é‡<br>â€¢ ç°æœ‰T2Iæç¤ºä¼˜åŒ–è¿ç§»åˆ°è§†é¢‘æœ‰é™ï¼Œç¼ºä¹å¯¹è¿åŠ¨å¹³æ»‘ã€æ—¶åºä¸€è‡´æ€§ä¸ç‰©ç†å¯è¡Œæ€§çš„çº¦æŸ<br>â€¢ ç°æœ‰T2Væç¤ºå·¥ç¨‹å¤šä¸ºæ¨¡å‹ç‰¹å®šï¼Œç¼ºå°‘å¯æ³›åŒ–ã€å¯æ‰©å±•çš„ç»Ÿä¸€ä¼˜åŒ–èŒƒå¼<br>â€¢ RLHFç­‰éœ€è¦å¤§é‡ç”Ÿæˆå›æ”¾çš„æ–¹æ¡ˆåœ¨è§†é¢‘åœºæ™¯è®¡ç®—æˆæœ¬è¿‡é«˜ï¼Œéš¾ä»¥åœ¨æ¨ç†æ—¶è½åœ°<br>â€¢ ç›´æ¥ç”¨LLMæ”¹å†™æ˜“åç¦»ç”¨æˆ·æ„å›¾æˆ–å¼•å…¥æ— å…³ä¿®é¥°ï¼Œä¸”ä»ä¸è®­ç»ƒè¯­æ–™åˆ†å¸ƒå¤±é…<br>â€¢ ç¼ºå°‘åˆ©ç”¨å¤šæºè‡ªåŠ¨è¯„ä¼°ä¿¡å·åœ¨æµ‹è¯•æ—¶è¿­ä»£æ”¹å†™æç¤ºçš„é—­ç¯æœºåˆ¶</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>RAPO++æå‡ºè·¨é˜¶æ®µæç¤ºä¼˜åŒ–ï¼šå…ˆç”¨æ£€ç´¢å¢å¼ºä¸åˆ†å¸ƒå¯¹é½çš„æ”¹å†™å¾—åˆ°è®­ç»ƒæ•°æ®ä¸€è‡´çš„å¼ºåŒ–æç¤ºï¼Œå†åœ¨æ¨ç†æ—¶åŸºäºå¤šæºè¯„ä¼°åé¦ˆè¿›è¡Œæ ·æœ¬çº§é—­ç¯è¿­ä»£ä¼˜åŒ–ï¼Œå¹¶å°†è¿­ä»£å¾—åˆ°çš„ä¼˜è´¨æç¤ºå¯¹ç”¨äºLLMå¾®è°ƒä»¥å›ºåŒ–èƒ½åŠ›ã€‚è¯¥æµç¨‹åœ¨ä¸æ”¹åŠ¨ç”Ÿæˆéª¨å¹²çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡è¯­ä¹‰å¯¹é½ã€ç»„åˆæ€§ä¸æ—¶åºç¨³å®šã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Physics-RAPO++: é¢å‘ç‰©ç†ä¸€è‡´æ€§çš„è§†é¢‘æç¤ºä¼˜åŒ–ä¸å¯å¾®è¯„ä¼°ï¼šå°†å…‰æµã€ç¢°æ’/é‡åŠ›ä¸€è‡´æ€§ç­‰ç‰©ç†åº¦é‡ä¸å¯å¾®ç‰©ç†æ¨¡å—èå…¥SSPOåé¦ˆï¼Œç»Ÿä¸€è¯­ä¹‰ä¸ç‰©ç†çº¦æŸ<br>â€¢ Agent-RAPO++: å¤šä»£ç†å¤šæ¨¡æ€ååŒçš„æµ‹è¯•æ—¶æç¤ºè‡ªæ”¹å–„ï¼šå¼•å…¥éŸ³ç”»åŒæ­¥ã€æ·±åº¦/åˆ†å‰²/è·Ÿè¸ªç­‰å¼‚æ„éªŒè¯å™¨ä¸è§„åˆ’-è¯„è®ºåŒä»£ç†ï¼Œè”åˆä¼˜åŒ–é•œå¤´çº§ä¸å¸§çº§æç¤º<br>â€¢ Prompt2Prompt Distillation: è·¨æ¨¡å‹è‡ªé€‚åº”çš„æç¤ºè¿ç§»ä¸é«˜æ•ˆè’¸é¦ï¼šæ±‡é›†ä¸åŒT2Véª¨å¹²ä¸Šçš„æœ€ä¼˜æç¤ºå¯¹ï¼Œå…ƒå­¦ä¹ ç”Ÿæˆå™¨æ¡ä»¶åŒ–çš„æ”¹å†™å™¨ï¼Œå¹¶è’¸é¦ä¸ºä½æ—¶å»¶é‡å†™æ¨¡å‹</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20479" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20479" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We unveil that internal representations in large language models (LLMs) serve as reliable proxies of learned knowledge, and propose RECALL, a novel representation-aware model merging framework for continual learning without access to historical data. RECALL computes inter-model similarity from layer-wise hidden representations over clustered typical samples, and performs adaptive, hierarchical parameter fusion to align knowledge across models. This design enables the preservation of domain-general features in shallow layers while allowing task-specific adaptation in deeper layers. Unlike prior methods that require task labels or incur performance trade-offs, RECALL achieves seamless multi-domain integration and strong resistance to catastrophic forgetting. Extensive experiments across five NLP tasks and multiple continual learning scenarios show that RECALL outperforms baselines in both knowledge retention and generalization, providing a scalable and data-free solution for evolving LLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ è§£å†³çš„é—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨è¿ç»­/å¤šåŸŸå­¦ä¹ ä¸­æ˜“å‘ç”Ÿç¾éš¾æ€§é—å¿˜ï¼Œéœ€è¦åœ¨æ–°ä»»åŠ¡é€‚åº”çš„åŒæ—¶ä¿ç•™æ—¢æœ‰çŸ¥è¯†ï¼ˆè§ç¬¬1é¡µæ‘˜è¦ä¸ç¬¬2é¡µå›¾1ï¼‰<br>â€¢ é‡è¦æ€§ï¼šçœŸå®åº”ç”¨å¸¸æ— æ³•å›æ”¾å†å²æ•°æ®ï¼ˆéšç§/åˆè§„/å­˜å‚¨å—é™ï¼‰ï¼Œä¸”ä»»åŠ¡è¾¹ç•Œä¸æ ‡è¯†ä¸æ˜ç¡®ï¼Œè¿«åˆ‡éœ€è¦ä»»åŠ¡æ— å…³ã€æ•°æ®æ— ä¾èµ–çš„å¯æ‹“å±•æ–¹æ¡ˆï¼ˆç¬¬2é¡µï¼‰<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼ˆæ•°æ®ç±»ï¼‰ï¼šä¾èµ–å†å²æ ·æœ¬é‡æ”¾ï¼Œå®é™…ä¸å¯ç”¨æˆ–æˆæœ¬é«˜ï¼ˆç¬¬2é¡µå›¾1aï¼‰<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼ˆæ¨¡å‹ç±»ï¼‰ï¼šé€šè¿‡æ­£åˆ™/ç»“æ„éš”ç¦»ä¿ç•™æ—§çŸ¥è¯†ï¼Œä½†ä¼˜åŒ–ç©ºé—´å—é™ã€å¸¸ä¾èµ–ä»»åŠ¡IDã€æ¨¡å‹å¤æ‚åº¦éšæ—¶é—´å¢é•¿ï¼Œä¸”è·¨ä»»åŠ¡æ³›åŒ–å—é™ï¼ˆç¬¬2é¡µå›¾1bï¼‰<br>â€¢ å…³é”®æ´å¯Ÿï¼šå±‚é—´/æ¨¡å‹é—´éšè¡¨ç¤ºå…·æœ‰ç³»ç»Ÿæ€§å·®å¼‚â€”â€”æµ…å±‚æ›´é€šç”¨ã€æ·±å±‚æ›´ä»»åŠ¡åŒ–ï¼›ä¸åŒä»»åŠ¡å¾®è°ƒåè¡¨ç¤ºåœ¨æ·±å±‚æ˜æ˜¾åˆ†åŒ–ï¼Œæœ´ç´ å‚æ•°å¹³å‡ä¼šå¼•å…¥è¯­ä¹‰ä¸ä¸€è‡´ä¸å¹²æ‰°ï¼ˆç¬¬3é¡µå›¾2ä¸å›¾3ï¼›é™„å½•Dç›¸ä¼¼åº¦æ›²çº¿ï¼‰</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>RECALLæ˜¯ä¸€ç§è¡¨å¾æ„ŸçŸ¥çš„å±‚çº§æ¨¡å‹èåˆï¼šå…ˆç”¨K-meansä»æ–°ä»»åŠ¡æ•°æ®ä¸­é€‰æ‹©å…¸å‹æ ·æœ¬ï¼Œæå–å„æ¨¡å‹çš„å±‚çº§éšè¡¨ç¤ºå¹¶ä»¥RBFæ ¸è®¡ç®—è·¨æ¨¡å‹å±‚é—´ç›¸ä¼¼åº¦ï¼Œå†å°†ç›¸ä¼¼åº¦ç»softmaxè½¬ä¸ºæ¯å±‚çš„è‡ªé€‚åº”æƒé‡ï¼Œå¯¹å¯¹åº”å±‚å‚æ•°åšåŠ æƒçº¿æ€§æ’å€¼ï¼Œä»è€Œåœ¨æµ…å±‚ä¿ç•™é€šç”¨ç‰¹å¾ã€æ·±å±‚æ³¨å…¥ä»»åŠ¡ä¸“é•¿ï¼Œä¸”æ— éœ€è®¿é—®å†å²æ•°æ®ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ RECALL++ï¼šé¢å‘å¼‚æ„LLMçš„è·¨æ¨¡å‹è¡¨ç¤ºå¯¹é½ä¸èåˆï¼šæ”¯æŒä¸åŒæ¶æ„/è¯è¡¨çš„å¯¹é½ç©ºé—´æˆ–å¯¹é½é€‚é…å™¨ï¼Œçªç ´åŒæ„å‡è®¾<br>â€¢ åŸºäºè‡ªé€‚åº”åŸå‹é€‰æ‹©çš„æ— æ•°æ®è¿ç»­å­¦ä¹ ï¼šè”å­¦ä¹ å…¸å‹æ ·æœ¬é€‰å–ä¸ç›¸ä¼¼åº¦åº¦é‡ï¼ˆå¦‚CKA/å¯¹æ¯”å­¦ä¹ ï¼‰ï¼Œæå‡æƒé‡ä¼°è®¡ç¨³å¥æ€§ä¸æ•ˆç‡<br>â€¢ å¯æ‰©å±•çš„å±‚çº§èåˆï¼šä»ååˆ°ç™¾ä¸ªä¸“å®¶æ¨¡å‹çš„é«˜æ•ˆåˆå¹¶ï¼šç”¨ä½ç§©/ç¨€ç–å¢é‡ã€è‰å›¾åŒ–ç›¸ä¼¼åº¦ä¸åˆ†å±‚è·¯ç”±ï¼Œå®ç°è¿‘çº¿æ€§å¼€é”€ä¸ç†è®ºè¯¯å·®ç•Œ</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">Model Merging with Functional Dual Anchors</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21223" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21223" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Model merging is an efficient post-training strategy for integrating knowledge from multiple finetuned checkpoints of a shared foundation model. Existing methods operate in the parameter space, combining task vectors to mitigate conflicts, but remain constrained by parameter inconsistencies. We propose Functional Dual Anchors (FDAs), a framework that instead models the input-representation space. FDAs are synthetic inputs whose induced gradients align with task vectors, capturing task-specific functional shifts relative to the pretrained model. This perspective bridges joint multi-task training and post-hoc merging, offering both robustness and flexibility. We further introduce a principled initialization scheme and show that FDAs are complementary to parameter-space model merging. Comprehensive experiments demonstrate the effectiveness of FDAs in model merging.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ é—®é¢˜ä¸é‡è¦æ€§ï¼šåœ¨æ— åŸå§‹æ•°æ®çš„ç°å®åœºæ™¯ä¸­ï¼Œå¦‚ä½•å°†å¤šä¸ªåŒæºå¾®è°ƒæ¨¡å‹ç¨³å¥åœ°åˆå¹¶ä¸ºä¸€ä¸ªå…·å¤‡å¤šä»»åŠ¡èƒ½åŠ›çš„æ¨¡å‹ï¼›ç›¸æ¯”å¤šä»»åŠ¡è”åˆè®­ç»ƒ/æŒç»­å­¦ä¹ ï¼Œåè®­ç»ƒåˆå¹¶æ›´é«˜æ•ˆã€éƒ¨ç½²å‹å¥½ï¼Œå› è€Œè‡³å…³é‡è¦ï¼ˆè§ç¬¬1é¡µã€å›¾1ï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼ˆå‚æ•°ç©ºé—´ï¼‰ï¼šä¸»æµåšæ³•åœ¨å‚æ•°ç©ºé—´å¯¹ä»»åŠ¡å‘é‡ï¼ˆå¾®è°ƒå¢é‡ï¼‰åšç¼©æ”¾/åŠ æƒ/å­ç©ºé—´çº¦æŸï¼Œå—åˆå§‹åŒ–ä¸å°ºåº¦æ•æ„Ÿã€éš¾ä»¥å½»åº•åŒ–è§£å‚æ•°ä¸ä¸€è‡´å¯¼è‡´çš„å†²çªï¼Œä¸”å¸¸éœ€æ•°æ®å…ˆéªŒæˆ–å¼ºç»“æ„å…ˆéªŒï¼ˆç¬¬2é¡µï¼‰ã€‚<br>â€¢ ä¼˜åŒ–ä¸é²æ£’æ€§ä¸è¶³ï¼šä»»åŠ¡å‘é‡è§„å®šçš„æ˜¯ä»é¢„è®­ç»ƒç‚¹å‡ºå‘çš„å›ºå®šçº¿æ€§è·¯å¾„ï¼Œæ˜“åç¦»çœŸå®æŸå¤±åœ°å½¢ã€éš¾ä»¥éšä¼˜åŒ–åŠ¨æ€è°ƒæ•´ï¼Œè¡¨ç°å‡ºæ³›åŒ–ä¸ç¨³å¥æ€§ä¸è¶³ï¼›ç›¸è¾ƒä¹‹ä¸‹ï¼Œè¾“å…¥ç©ºé—´æ›´å…·ç»“æ„æ€§ï¼Œåˆ©äºå»ºæ¨¡ï¼ˆå›¾2ã€ç¬¬2é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºåŠŸèƒ½å‹åŒé”šï¼ˆFDAsï¼‰ï¼šä¸ºæ¯ä¸ªä¸‹æ¸¸æ¨¡å‹æ„é€ ä¸€ç»„åˆæˆè¾“å…¥ï¼Œä½¿å…¶åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸Šè¯±å¯¼çš„æ¢¯åº¦ä¸å¯¹åº”ä»»åŠ¡å‘é‡å¯¹é½ï¼Œå°†ä»»åŠ¡çŸ¥è¯†æŠ•å½±åˆ°è¾“å…¥-è¡¨å¾ç©ºé—´ï¼ˆä»¥æ¢¯åº¦åŒ¹é…æœ€å°åŒ–ä½™å¼¦è·ç¦»ï¼Œå¼(1)/(2)å¹¶é‡‡ç”¨åˆ†å±‚æ„é€ ä¸ä¸¤ç§ç¨³å¥åˆå§‹åŒ–ï¼‰ã€‚éšåç”¨FDAså¯¹é½é¢„è®­ç»ƒæˆ–å·²åˆå¹¶æ¨¡å‹åœ¨è¿™äº›é”šç‚¹ä¸Šçš„è¾“å‡ºï¼ˆå¼(5)/(6)ï¼‰ï¼Œå®ç°æ•°æ®è‡ªç”±ä¸”å¯ä¸å‚æ•°ç©ºé—´æ–¹æ³•äº’è¡¥çš„å¤šä»»åŠ¡åˆå¹¶ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Adaptive FDA Construction with Learnable Distance Metricsï¼šå­¦ä¹ å±‚/ä»»åŠ¡è‡ªé€‚åº”çš„è·ç¦»å‡½æ•°ä¸æƒé‡è°ƒåº¦ï¼Œæå‡é”šç‚¹è´¨é‡ã€æ”¶æ•›é€Ÿåº¦ä¸è·¨ä»»åŠ¡é²æ£’æ€§ã€‚<br>â€¢ Generative Functional Anchors for Data-free Mergingï¼šå¼•å…¥æ‰©æ•£/è‡ªå›å½’ç”Ÿæˆå™¨ç”Ÿæˆåˆå§‹é”šå¹¶è”åˆæ¢¯åº¦åŒ¹é…ï¼Œé™ä½äºŒé˜¶è®¡ç®—ä¸å†…å­˜æˆæœ¬ï¼Œå¢å¼ºå¤§æ¨¡å‹å¯æ‰©å±•æ€§ã€‚<br>â€¢ Continual Model Merging via FDA Replayï¼šå°†FDAsä½œä¸ºå¯é‡æ”¾çš„åˆæˆè®°å¿†ï¼Œå®ç°å¢é‡å¹¶å…¥æ–°ä»»åŠ¡æ£€æŸ¥ç‚¹æ—¶çš„ç¨³å®šåˆå¹¶ä¸æŠ—é—å¿˜ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.13251" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.13251" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Video Large Language Models (VideoLLMs) extend the capabilities of vision-language models to spatiotemporal inputs, enabling tasks such as video question answering (VideoQA). Despite recent advances in VideoLLMs, their internal mechanisms on where and how they extract and propagate video and textual information remain less explored. In this study, we investigate the internal information flow of VideoLLMs using mechanistic interpretability techniques. Our analysis reveals consistent patterns across diverse VideoQA tasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frame interactions in early-to-middle layers, (2) followed by progressive video-language integration in middle layers. This is facilitated by alignment between video representations and linguistic embeddings containing temporal concepts. (3) Upon completion of this integration, the model is ready to generate correct answers in middle-to-late layers. (4) Based on our analysis, we show that VideoLLMs can retain their VideoQA performance by selecting these effective information pathways while suppressing a substantial amount of attention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a blueprint on how VideoLLMs perform temporal reasoning and offer practical insights for improving model interpretability and downstream generalization. Our project page with the source code is available at https://map-the-flow.github.io</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç¼ºä¹å¯¹VideoLLMså†…éƒ¨æ—¶åºæ¨ç†æœºåˆ¶çš„ç³»ç»Ÿæ€§ç†è§£ï¼šè§†é¢‘tokenå¦‚ä½•åœ¨æ—©ä¸­å±‚è¿›è¡Œè·¨å¸§äº¤äº’ã€å¦‚ä½•ä¸é—®é¢˜ä¸­çš„æ—¶é—´å…³é”®è¯èåˆå¹¶å°†ä¿¡æ¯ä¼ è‡³æœ€åtokenä»¥ç”Ÿæˆç­”æ¡ˆï¼Œé•¿æœŸæœªè¢«æ­ç¤ºã€‚<br>â€¢ ç°æœ‰ç ”ç©¶ä¾§é‡â€œå¤–éƒ¨è®¾è®¡â€ï¼ˆæ•°æ®è§„æ¨¡ã€å…³é”®å¸§é€‰æ‹©ã€è§†é¢‘tokenå‹ç¼©ï¼‰ï¼Œå›¾åƒMLLMä¸Šçš„å¯è§£é‡Šæ€§ç»“è®ºéš¾ç›´æ¥æ¨å¹¿åˆ°è§†é¢‘åœºæ™¯ï¼Œå¿½è§†è§†é¢‘-æ–‡æœ¬å¯¹é½ä¸­â€œæ—¶é—´æ¦‚å¿µâ€çš„å†…éƒ¨æœºåˆ¶ã€‚<br>â€¢ è§£é‡Šæ€§ä¸æ•ˆç‡éœ€æ±‚è¿«åˆ‡ï¼šè‹¥èƒ½å®šä½æœ‰æ•ˆä¿¡æ¯é€šè·¯ï¼Œå¯åœ¨å‡ ä¹ä¸é™æ€§èƒ½å‰æä¸‹ç¨€ç–æ³¨æ„åŠ›é€šè·¯ï¼ˆè®ºæ–‡æ˜¾ç¤ºå¯¹LLaVAâ€‘NeXTâ€‘7Bâ€‘Videoâ€‘FTä»…ä¿ç•™çº¦42%æ³¨æ„åŠ›è¾¹ä»ä¿æŒæ€§èƒ½ï¼‰ï¼Œä¸ºæ¨¡å‹å‹ç¼©ä¸æ³›åŒ–æä¾›ä¾æ®ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>é‡‡ç”¨æœºåˆ¶å¯è§£é‡Šæ€§ç®¡çº¿ï¼šä»¥Attention Knockoutåœ¨å±‚çº§/è·¨æ¨¡æ€ä¸Šæ–­å¼€ç‰¹å®šæ³¨æ„åŠ›è¾¹å¹¶ç”¨ç­”æ¡ˆæ¦‚ç‡å˜åŒ–é‡é‡åŒ–å…¶å› æœè´¡çŒ®ï¼Œç»“åˆLogit Lensæ¢æµ‹è§†é¢‘tokenä¸­ç©ºé—´/æ—¶é—´æ¦‚å¿µçš„å±‚çº§æ¶Œç°ï¼Œä»è€Œæç»˜â€œè§†é¢‘â†’é—®é¢˜ï¼ˆæ—¶é—´å…³é”®è¯ï¼‰â†’æœ€åtokenâ€çš„ä¸»é€šè·¯ï¼Œå¹¶é€šè¿‡ä»…ä¿ç•™è¯¥é€šè·¯éªŒè¯å…¶å¯¹VideoQAçš„å……åˆ†æ€§ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ å¯è§£é‡Šæ€§å¼•å¯¼çš„VideoLLMé€šè·¯å‰ªæä¸åŠ é€Ÿï¼šå°†è¯†åˆ«å‡ºçš„æœ‰æ•ˆé€šè·¯è½¬åŒ–ä¸ºç»“æ„åŒ–ç¨€ç–æ³¨æ„åŠ›ä¸è’¸é¦ç­–ç•¥ï¼Œå®ç°ä½å¼€é”€é«˜å‡†ç¡®çš„é•¿è§†é¢‘æ¨ç†ã€‚<br>â€¢ æ—¶é—´æ¦‚å¿µå¯¹é½è®­ç»ƒï¼šè®¾è®¡æ˜¾å¼å¯¹é½æŸå¤±/å¯¹æ¯”å­¦ä¹ ï¼Œä½¿è§†é¢‘è¡¨å¾ä¸â€œbegins/ends/first/lastâ€ç­‰æ—¶é—´è¯åµŒå…¥æ›´ä¸€è‡´ï¼Œå¼ºåŒ–ä¸­å±‚è§†é¢‘-è¯­è¨€èåˆä¸é²æ£’æ€§ã€‚<br>â€¢ ç»Ÿä¸€çš„ä¿¡æ¯æµè¯„æµ‹ä¸è¿ç§»ï¼šæ„å»ºé€šè·¯çº§å¹²é¢„åŸºå‡†ï¼Œå°†å‘ç°çš„ä¸»é€šè·¯å…ˆéªŒè¿ç§»åˆ°å¼€æ”¾å¼é—®ç­”ä¸æµå¼é•¿è§†é¢‘åœºæ™¯ï¼Œæå‡è·¨ä»»åŠ¡ä¸è·¨æ¶æ„çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Document Understanding, Measurement, and Manipulation Using Category Theory</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21553" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21553" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We apply category theory to extract multimodal document structure which leads us to develop information theoretic measures, content summarization and extension, and self-supervised improvement of large pretrained models. We first develop a mathematical representation of a document as a category of question-answer pairs. Second, we develop an orthogonalization procedure to divide the information contained in one or more documents into non-overlapping pieces. The structures extracted in the first and second steps lead us to develop methods to measure and enumerate the information contained in a document. We also build on those steps to develop new summarization techniques, as well as to develop a solution to a new problem viz. exegesis resulting in an extension of the original document. Our question-answer pair methodology enables a novel rate distortion analysis of summarization techniques. We implement our techniques using large pretrained models, and we propose a multimodal extension of our overall mathematical framework. Finally, we develop a novel self-supervised method using RLVR to improve large pretrained models using consistency constraints such as composability and closure under certain operations that stem naturally from our category theoretic framework.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ è¯­ä¹‰å±‚æ–‡æ¡£è¡¨ç¤ºç¼ºä½ï¼šç¼ºå°‘ç»Ÿä¸€ã€å¯æ“ä½œçš„ç»“æ„æ¥è¡¨ç¤ºä¸æ“æ§æ–‡æ¡£çš„â€œå«ä¹‰â€ï¼Œéš¾ä»¥è¿›è¡Œåº¦é‡ã€æ¯”è¾ƒã€æ£€ç´¢ä¸ç³»ç»Ÿå˜æ¢ï¼ˆè·¨æ–‡æœ¬ä¸å¤šæ¨¡æ€ï¼‰ã€‚<br>â€¢ ç°æœ‰ä¿¡æ¯è®ºä¸æœ¬ä½“çš„å±€é™ï¼šé¦™å†œä¿¡æ¯ä¾§é‡ç¬¦å·ç»Ÿè®¡ï¼ˆè¯­æ³•å±‚ï¼‰ï¼Œæ‰‹å·¥æœ¬ä½“éš¾æ‰©å±•ï¼Œæ— æ³•â€œç¦»æ•£åŒ–æ„ä¹‰â€ä»¥è¿›è¡Œæµ‹é‡ä¸è¿ç®—ã€‚<br>â€¢ ç»éªŒå¼æ–¹æ³•ç¼ºä¹ç»“æ„ä¿è¯ï¼šRST/QA/æ‘˜è¦å¤šä¸ºç»éªŒå·¥ç¨‹ï¼Œç¼ºå°‘èŒƒç•´ç»“æ„ã€å¯ç»„åˆæ€§ä¸å¯éªŒè¯ä¸€è‡´æ€§ï¼Œéš¾ä»¥æ”¯æŒç³»ç»ŸåŒ–çš„æ€»ç»“ä¸â€œé‡Šç»å¼æ‰©å±•â€ã€‚<br>â€¢ ç¼ºå°‘è¯­ä¹‰ä¿¡æ¯åº¦é‡ä¸æƒè¡¡å·¥å…·ï¼šç¼ºä¹ä¿¡æ¯å«é‡ã€å†…å®¹ç†µã€ä¿¡æ¯å¢ç›Šä¸æ‘˜è¦ç‡å¤±çœŸç­‰è¯­ä¹‰æŒ‡æ ‡ï¼Œæ— æ³•é‡åŒ–æ‘˜è¦é•¿åº¦â€”å‡†ç¡®ç‡çš„æƒè¡¡ä¸æœ€ä¼˜æ€§ã€‚<br>â€¢ å¤§æ¨¡å‹æ”¹è¿›ç¼ºå°‘å¯éªŒè¯ä¿¡å·ï¼šéœ€è¦åˆ©ç”¨LLMéšå¼æœ¬ä½“è¿›è¡Œè‡ªç›‘ç£æå‡ï¼Œä½†ç¼ºå°‘æ¥è‡ªç»“æ„çº¦æŸï¼ˆå¯ç»„åˆã€é—­åŒ…ç­‰ï¼‰çš„å¯éªŒè¯è®­ç»ƒä¿¡å·ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>ä»¥å¤§æ¨¡å‹æŠ½å–ä¿®è¾ç»“æ„ä¸ºæŠ½è±¡DAGï¼Œå°†æ–­è¨€è½¬ä¸ºæ ¸å¿ƒé—®ç­”å¹¶æ„é€ æˆâ€œé—®ç­”èŒƒç•´â€ï¼Œå®šä¹‰åŸºäºå¯å›ç­”é›†åˆäº¤å¹¶çš„Jaccardå¼è·ç¦»ï¼Œå€ŸåŠ©äº¤/å¹¶/è¡¥åˆ†è§£å®ç°æ­£äº¤åŒ–å¾—åˆ°åŸå­QAï¼Œè¿›è€Œæ„å»ºæ ¼ä»¥æšä¸¾å±‚çº§æ‘˜è¦ä¸é‡Šç»å¼æ‰©å±•ï¼Œå¹¶æå‡ºä¿¡æ¯å«é‡/å¯†åº¦ã€äº’ä¿¡æ¯ã€ä¿¡æ¯å¢ç›Šã€å†…å®¹ç†µä¸å¤šæ ·æ€§-æ·±åº¦ç†µç­‰åº¦é‡åŠæ‘˜è¦çš„æ“ä½œæ€§ç‡å¤±çœŸæ›²çº¿ã€‚æœ€åï¼Œåˆ©ç”¨èŒƒç•´çš„å¯ç»„åˆä¸é—­åŒ…ç­‰å¯éªŒè¯ä¸€è‡´æ€§çº¦æŸï¼Œç»“åˆRLVRå¯¹å¤§æ¨¡å‹è¿›è¡Œè‡ªç›‘ç£ä¼˜åŒ–ï¼Œå¹¶æ‰©å±•åˆ°å¤šæ¨¡æ€ä¸æ¦‚ç‡èŒƒç•´ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ å¤šæ¨¡æ€é—®ç­”èŒƒç•´ä¸è¯æ®å¯¹é½ï¼šå°†é«˜å±‚æ–­è¨€ä¸æ–‡æœ¬/å›¾åƒ/éŸ³é¢‘ç­‰è¯æ®å»ºç«‹å‡½å­æ˜ å°„ï¼Œè¯„ä¼°è·¨æ¨¡æ€ä¸€è‡´æ€§ä¸å¯¹é½è´¨é‡ã€‚<br>â€¢ æ¦‚ç‡èŒƒç•´æ–‡æ¡£ç†è§£ï¼šä¸ºQAå¯¹è±¡/æ€å°„èµ‹äºˆæ¦‚ç‡æƒ…æ™¯ï¼Œç ”ç©¶ä¸ç¡®å®šæ€§ä¼ æ’­ä¸è´å¶æ–¯æ›´æ–°ä¸‹çš„æ­£äº¤åŒ–ä¸åº¦é‡ã€‚<br>â€¢ åŸºäºæ ¼ç»“æ„çš„æ€»ç»“â€”æ‰©å±•ç‡å¤±çœŸå‡½æ•°ï¼šç³»ç»Ÿæ¯”è¾ƒä¸åŒæ‘˜è¦/é‡Šç»ç­–ç•¥ï¼Œç»™å‡ºæ“ä½œæ€§ç‡å¤±çœŸæ›²çº¿ä¸ä¸‹ç•Œåˆ†æã€‚<br>â€¢ RLVRé©±åŠ¨çš„èŒƒç•´ä¸€è‡´æ€§å¯¹é½è®­ç»ƒï¼šä»¥å¯ç»„åˆæ€§ã€é—­åŒ…ã€å¹¶äº¤ç¨³å®šæ€§ç­‰çº¦æŸæ„é€ å¯éªŒè¯å¥–åŠ±ï¼Œæå‡LLMä¸€è‡´æ€§ä¸ç¨³å¥æ€§ã€‚<br>â€¢ å†…å®¹ç†µä¸è¯­ä¹‰å†—ä½™çš„å®è¯æ ‡å®šï¼šä»¥é“¾å¤´è®¡æ•°çš„å†…å®¹ç†µä¸ä¿¡æ¯å¯†åº¦è¯„æµ‹å¤šä½“è£æ–‡æ¡£ï¼Œå»ºç«‹åŸºå‡†ä¸ç½®ä¿¡åŒºé—´ã€‚<br>â€¢ Sheafä¸€è‡´æ€§ç”¨äºè·¨æ–‡æ¡£èåˆä¸å†²çªæ£€æµ‹ï¼šå°†æ–‡é›†è§†ä½œæ‹“æ‰‘è¦†ç›–ï¼Œåº¦é‡é¢„å±‚åˆ°å±‚çš„å·®è·ä»¥å®šä½çŸ›ç›¾ä¸ä¿¡æ¯ç¼ºå£ã€‚<br>â€¢ é‡Šç»å¼ä»»åŠ¡è¿ç§»å­¦ä¹ ï¼šæŠŠä»»åŠ¡è¯´æ˜èŒƒç•´åŒ–å¹¶æ‰§è¡Œè¶…æ–‡æ¡£/ç»†åŒ–æ‰©å±•ï¼Œå®ç°ä»ä»»åŠ¡T1åˆ°T2çš„å¯è§£é‡Šè¿ç§»ä¸å¤ç”¨ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">ARC-Encoder: learning compressed text representations for large language models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20535" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20535" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent techniques such as retrieval-augmented generation or chain-of-thought reasoning have led to longer contexts and increased inference costs. Context compression techniques can reduce these costs, but the most effective approaches require fine-tuning the target model or even modifying its architecture. This can degrade its general abilities when not used for this specific purpose. Here we explore an alternative approach: an encoder that compresses the context into continuous representations which replace token embeddings in decoder LLMs. First, we perform a systematic study of training strategies and architecture choices for the encoder. Our findings led to the design of an Adaptable text Representations Compressor, named ARC-Encoder, which outputs x-times fewer continuous representations (typically x!in!{4,8}) than text tokens. We evaluate ARC-Encoder across a variety of LLM usage scenarios, ranging from in-context learning to context window extension, on both instruct and base decoders. Results show that ARC-Encoder achieves state-of-the-art performance on several benchmarks while improving computational efficiency at inference. Finally, we demonstrate that our models can be adapted to multiple decoders simultaneously, allowing a single encoder to generalize across different decoder LLMs. This makes ARC-Encoder a flexible and efficient solution for portable encoders that work seamlessly with multiple LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder , fine-tuning dataset and pretrained models are available at https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šRAG/CoTç­‰åº”ç”¨å¯¼è‡´ä¸Šä¸‹æ–‡æ€¥å‰§å˜é•¿ï¼Œæ³¨æ„åŠ›è®¡ç®—å¼€é”€éšé•¿åº¦äºŒæ¬¡å¢é•¿ï¼Œä¿¡æ¯è¢«ç¨€é‡Šä¸”æ˜“è§¦è¾¾ä¸Šä¸‹æ–‡çª—å£ä¸Šé™ï¼ŒæŸå®³æ¨¡å‹èƒ½åŠ›ï¼ˆç¬¬1â€“2é¡µï¼‰ã€‚<br>â€¢ é‡è¦æ€§ï¼šåœ¨ä¸ç‰ºç‰²å‡†ç¡®æ€§çš„å‰æä¸‹é™ä½æ¨ç†æˆæœ¬ã€æ‰©å±•æœ‰æ•ˆä¸Šä¸‹æ–‡ï¼Œæ˜¯å¤§è§„æ¨¡LLMåœ¨çœŸå®åœºæ™¯è½åœ°ï¼ˆå¦‚RAGã€é˜…è¯»ç†è§£ã€ç¿»è¯‘ã€æ‘˜è¦ï¼‰çš„å…³é”®ï¼ˆç¬¬1â€“2é¡µï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šç¡¬å‹ç¼©ï¼ˆåˆ è¯/æ‘˜è¦ï¼‰å¯è§£é‡Šä½†å‹ç¼©ç‡æœ‰é™ã€æ˜“ä¸¢å…³é”®ä¿¡æ¯ï¼›è½¯å‹ç¼©ï¼ˆgist/memory tokensï¼‰è™½é«˜æ•ˆï¼Œä½†å¸¸éœ€è”è®­/æ”¹é€ è§£ç å™¨ï¼Œç ´åå³æ’å³ç”¨ä¸é€šç”¨æ€§ï¼Œä¸”å›ºå®šæ•°é‡è®°å¿†å‘é‡å¯¹ä¸åŒé•¿åº¦çš„ä¸é€‚é…ï¼ˆç¬¬2é¡µï¼‰ã€‚<br>â€¢ é¢å¤–ç—›ç‚¹ï¼šå¾ˆå¤šæ–¹æ³•é›¶æ ·æœ¬è¯„æµ‹ä¾èµ–æŒ‡ä»¤æ¨¡å‹ï¼Œæ˜“â€œç…§æŠ„ä¸Šä¸‹æ–‡â€è™šé«˜ï¼›é¢„è®¡ç®—å‘é‡æ–¹æ¡ˆçµæ´»æ€§å·®ï¼›éœ€è¦ä¸€ç§æ—¢èƒ½ä¿æŒå°‘æ ·æœ¬ICLã€åˆæ— éœ€æ”¹è§£ç å™¨ã€è¿˜èƒ½è·¨å¤šè§£ç å™¨è¿ç§»çš„å‹ç¼©æ–¹æ¡ˆï¼ˆç¬¬5â€“6é¡µï¼Œè¡¨1ç¬¬6é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºARC-Encoderï¼šç”¨LLMå¼ç¼–ç å™¨åœ¨æœ€åä¸€å±‚è‡ªæ³¨æ„åŠ›å¯¹ç›¸é‚»æŸ¥è¯¢å‘é‡åšå›ºå®šæ­¥é•¿å¹³å‡æ± åŒ–ï¼ˆé”®/å€¼ä¸å˜ï¼‰å°†åºåˆ—ç¼©çŸ­xå€ï¼Œå¹¶é€šè¿‡ä¸¤å±‚æ— æ¿€æ´»MLPæŠ•å½±åˆ°è§£ç å™¨éšç©ºé—´ï¼Œä½œä¸ºæ›¿ä»£è¯åµŒå…¥ç›´æ¥å–‚ç»™å†»ç»“è§£ç å™¨ï¼ˆè§å›¾1ç¬¬3é¡µï¼‰ã€‚é‡‡ç”¨â€œé‡å»º+ç»­å†™â€äº¤æ›¿é¢„è®­ç»ƒé…åˆå°‘é‡ä»»åŠ¡å¾®è°ƒï¼Œå¹¶ä»¥å…±äº«ç¼–ç å™¨+æ¯è§£ç å™¨ä»…<1%å‚æ•°çš„å°MLPå®ç°å¤šè§£ç å™¨é€‚é…ä¸é•¿æ–‡åˆ†å—å¹¶è¡Œå‹ç¼©ï¼ˆè§è¡¨1ç¬¬6é¡µã€è¡¨3ç¬¬8é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è‡ªé€‚åº”å¯å˜å‹ç¼©ARC-Encoderï¼šåŸºäºé‡è¦æ€§ä¼°è®¡/ä¸ç¡®å®šæ€§é©±åŠ¨çš„åŠ¨æ€æ± åŒ–å› å­ï¼ŒæŒ‰æ®µè½/å¥å­è‡ªè°ƒå‹ç¼©ç‡ä»¥ä¼˜åŒ–ç®—åŠ›-ç²¾åº¦æƒè¡¡ã€‚<br>â€¢ é€šç”¨è·¨è§£ç å™¨è¯­ä¹‰å¯¹é½çš„ARCæŠ•å½±å™¨ï¼šç”¨å¯¹æ¯”å­¦ä¹ /éšç©ºé—´è’¸é¦æ„å»ºå…±äº«å‹ç¼©è¯­ä¹‰ç©ºé—´ä¸è½»é‡æŠ•å½±ï¼Œå®ç°æ›´å¼ºçš„â€œä¸€ä¸ªç¼–ç å™¨é€‚é…å¤šè§£ç å™¨â€ã€‚<br>â€¢ ARC-Encoderä¸æ£€ç´¢ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ï¼šå°†æ£€ç´¢å™¨è®­ç»ƒä¸å‹ç¼©è¡¨ç¤ºååŒå­¦ä¹ ï¼ˆå«PQé‡åŒ–ä¸ç¼“å­˜ç­–ç•¥ï¼‰ï¼Œç»Ÿä¸€æå‡RAGåœºæ™¯çš„å¯é¢„è®¡ç®—ã€å­˜å‚¨æ•ˆç‡ä¸ä¸‹æ¸¸æ•ˆæœã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21652" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21652" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>AI agents hold the potential to revolutionize scientific productivity by automating literature reviews, replicating experiments, analyzing data, and even proposing new directions of inquiry; indeed, there are now many such agents, ranging from general-purpose "deep research" systems to specialized science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of these agents is critical for progress. Yet existing benchmarks fall short on several fronts: they (1) fail to provide holistic, product-informed measures of real-world use cases such as science research; (2) lack reproducible agent tools necessary for a controlled comparison of core agentic capabilities; (3) do not account for confounding variables such as model cost and tool access; (4) do not provide standardized interfaces for quick agent prototyping and evaluation; and (5) lack comprehensive baseline agents necessary to identify true advances. In response, we define principles and tooling for more rigorously benchmarking agents. Using these, we present AstaBench, a suite that provides the first holistic measure of agentic ability to perform scientific research, comprising 2400+ problems spanning the entire scientific discovery process and multiple scientific domains, and including many problems inspired by actual user requests to deployed Asta agents. Our suite comes with the first scientific research environment with production-grade search tools that enable controlled, reproducible evaluation, better accounting for confounders. Alongside, we provide a comprehensive suite of nine science-optimized classes of Asta agents and numerous baselines. Our extensive evaluation of 57 agents across 22 agent classes reveals several interesting findings, most importantly that despite meaningful progress on certain individual aspects, AI remains far from solving the challenge of science research assistance.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ è¯„æµ‹ä¸å¤Ÿâ€œçœŸå®ä¸”å…¨é¢â€ï¼šç°æœ‰åŸºå‡†å¤šç¼ºä¹è¦†ç›–å®Œæ•´ç§‘ç ”æµç¨‹ã€è·¨é¢†åŸŸã€ä¸”æºè‡ªçœŸå®äº§å“ä½¿ç”¨åœºæ™¯çš„ä»»åŠ¡ï¼Œéš¾ä»¥åæ˜ å®é™…æ•ˆç”¨ï¼ˆè®ºæ–‡è¡¨1ä¸åŸåˆ™1ï¼‰ã€‚<br>â€¢ ç¼ºå¤±æ ‡å‡†å¯æ§ç¯å¢ƒä¸æˆæœ¬åº¦é‡ï¼šæ²¡æœ‰ç»Ÿä¸€ã€å¯å¤ç°çš„æ£€ç´¢ä¸ä»£ç æ‰§è¡Œå·¥å…·ï¼Œä¸”å¾ˆå°‘ç³»ç»Ÿè®¡å…¥æ¨ç†æˆæœ¬ä¸å·¥å…·å·®å¼‚ç­‰æ··æ‚å› ç´ ï¼Œæ— æ³•å…¬å¹³å¯¹æ¯”ä»£ç†ï¼ˆåŸåˆ™2ä¸3ï¼‰ã€‚<br>â€¢ æ¥å£ä¸åŸºçº¿ä¸è¶³ï¼šä»»åŠ¡æ ¼å¼ä¸åˆ©äºé€šç”¨ä»£ç†å¿«é€Ÿæ¥å…¥ï¼Œæ¬ ç¼ºè¦†ç›–å¹¿æ³›æ¶æ„çš„å¼ºåŸºçº¿ï¼Œå¯¼è‡´å¤ç°ä¸è¿›æ­¥åˆ¤æ–­å›°éš¾ï¼ˆåŸåˆ™4ä¸5ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºAstaBenchï¼šä¸€ä¸ªè¦†ç›–æ–‡çŒ®ç†è§£ã€ä»£ç æ‰§è¡Œã€æ•°æ®åˆ†æä¸ç«¯åˆ°ç«¯å‘ç°çš„11é¡¹åŸºå‡†ï¼ˆè§è¡¨2ï¼‰ï¼Œé…å¥—Asta Environmentï¼ˆæ—¥æœŸå—é™çš„ç”Ÿäº§çº§æ–‡çŒ®æ£€ç´¢ä¸å¯çŠ¶æ€è®¡ç®—ç¬”è®°æœ¬ï¼‰ã€agent-evalå·¥å…·åŒ…ï¼ˆåŸºäºä½¿ç”¨æ—¥å¿—çš„æ—¶é—´ä¸å˜æˆæœ¬æ ¸ç®—ï¼‰ä¸å…¬å¼€æ’è¡Œæ¦œï¼Œå¹¶æä¾›æ ‡å‡†åŒ–Inspectæ¥å£ä¸å¤§è§„æ¨¡å¼€æºåŸºçº¿ä»£ç†å¥—ä»¶ã€‚è¯¥ä½“ç³»åœ¨ç»Ÿä¸€å·¥å…·ä¸æˆæœ¬çº¦æŸä¸‹ï¼Œå¯¹57ä¸ªä»£ç†/22ç±»æ¶æ„è¿›è¡Œå¯å¤ç°å®éªŒä¸æˆæœ¬-æ€§èƒ½å¯¹æ¯”ï¼Œéš”ç¦»ä¿¡æ¯è®¿é—®å·®å¼‚ï¼Œæå‡è¯„æµ‹å…¬æ­£æ€§ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ æˆæœ¬æ„ŸçŸ¥ä¸å·¥å…·å—æ§çš„é€šç”¨ä»£ç†è¯„æµ‹æ¡†æ¶ï¼šå°†AstaBenchèŒƒå¼æ¨å¹¿åˆ°æ›´å¤šé¢†åŸŸï¼Œå»ºç«‹å¯å¤ç°å·¥å…·ã€æ—¶é—´ä¸å˜æˆæœ¬ä¸æ··æ‚å› ç´ æ§åˆ¶çš„ç»Ÿä¸€åŸºå‡†ä½“ç³»ã€‚<br>â€¢ é¢å‘é•¿ç¨‹ç§‘å­¦é¡¹ç›®çš„è®°å¿†ä¸ç¼–æ’å¢å¼ºä»£ç†ï¼šç ”ç©¶æ”¯æŒé•¿æ—¶ç¨‹ã€è·¨é˜¶æ®µå®éªŒçš„è®¡åˆ’-æ‰§è¡Œ-è¿½æº¯æœºåˆ¶ä¸ç¨³å¥ä¸Šä¸‹æ–‡ç®¡ç†ï¼Œæå‡ç«¯åˆ°ç«¯ç§‘ç ”æˆåŠŸç‡ã€‚<br>â€¢ å¯é çš„LLMè£åˆ¤ï¼šé¢å‘ç§‘ç ”è¯„æµ‹çš„å¤šè£åˆ¤ä¸€è‡´æ€§ä¸è¯æ®å¯¹é½æ–¹æ³•ï¼šæ„å»ºå¼•ç”¨å¯¹é½ã€å¤šè¯„å®¡é›†æˆä¸ä¸ç¡®å®šæ€§æ ¡å‡†çš„LLM-as-judgeï¼Œæé«˜å¤æ‚ç§‘ç ”ä»»åŠ¡è¯„åˆ†çš„ç¨³å¥æ€§ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21447" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21447" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Interactive world models that simulate object dynamics are crucial for robotics, VR, and AR. However, it remains a significant challenge to learn physics-consistent dynamics models from limited real-world video data, especially for deformable objects with spatially-varying physical properties. To overcome the challenge of data scarcity, we propose PhysWorld, a novel framework that utilizes a simulator to synthesize physically plausible and diverse demonstrations to learn efficient world models. Specifically, we first construct a physics-consistent digital twin within MPM simulator via constitutive model selection and global-to-local optimization of physical properties. Subsequently, we apply part-aware perturbations to the physical properties and generate various motion patterns for the digital twin, synthesizing extensive and diverse demonstrations. Finally, using these demonstrations, we train a lightweight GNN-based world model that is embedded with physical properties. The real video can be used to further refine the physical properties. PhysWorld achieves accurate and fast future predictions for various deformable objects, and also generalizes well to novel interactions. Experiments show that PhysWorld has competitive performance while enabling inference speeds 47 times faster than the recent state-of-the-art method, i.e., PhysTwin.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ä»çŸ­æ—¶çœŸå®è§†é¢‘ä¸­å­¦ä¹ å¯å˜å½¢ç‰©ä½“çš„â€œä¸–ç•Œæ¨¡å‹â€æ—¢è¦å‡†ç¡®åˆè¦é«˜æ•ˆï¼šå­¦ä¹ å¼æ–¹æ³•éœ€å¤§é‡æ•°æ®ã€ç‰©ç†ä¸€è‡´æ€§å¼±ï¼›ç‰©ç†ä»¿çœŸæ³•ï¼ˆå¦‚MPMï¼‰å‡†ç¡®ä½†éš¾ä»¥å®æ—¶ï¼Œéš¾ä»¥æ”¯æŒè§„åˆ’ä¸äº¤äº’ã€‚<br>â€¢ çœŸå®è§†é¢‘æ•°æ®ç¨€ç¼ºä¸”å­˜åœ¨â€œä»¿çœŸ-ç°å®â€åŸŸå·®ï¼šæ¨¡æ‹Ÿæ•°æ®å¸¸ä¸çœŸå®ç‰©ç†ä¸ä¸€è‡´ï¼Œä¸”è®¸å¤šå¯¹è±¡å…·æœ‰ç©ºé—´å¼‚è´¨çš„ç‰©ç†å±æ€§ï¼Œä¼ ç»Ÿä»¥å…¨å±€å‚æ•°å»ºæ¨¡ï¼ˆå¦‚éƒ¨åˆ†GNNå·¥ä½œï¼‰éš¾ä»¥åˆ»ç”»ï¼Œå¯¼è‡´æ³›åŒ–å—é™ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å¯¹æœªè§äº¤äº’çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼šå•ä¸€çœŸå®è½¨è¿¹éš¾è¦†ç›–ä¸°å¯ŒåŠ¨åŠ›å­¦ï¼›æ—¢æœ‰æ•°å­—å­ªç”Ÿä¼˜åŒ–æ˜“å—åˆå§‹åŒ–ä¸æ¢¯åº¦ä¸ç¨³å®šå½±å“ï¼Œå­¦ä¹ æ¨¡å‹äº¦æ˜“ç§¯ç´¯è¯¯å·®ï¼Œéš¾ä»¥åœ¨é€Ÿåº¦ã€ç²¾åº¦ã€å¯æ‰©å±•æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>å…ˆç”¨VLMè‡ªåŠ¨é€‰æ‹©æœ¬æ„æ¨¡å‹ï¼Œå¹¶åœ¨MPMä¸­å¯¹ç‰©ç†å±æ€§é‡‡ç”¨â€œå…¨å±€åˆ°å±€éƒ¨â€çš„å¯å¾®ä¼˜åŒ–ï¼Œæ„å»ºä¸è§†é¢‘ä¸€è‡´çš„æ•°å­—å­ªç”Ÿï¼›å†é€šè¿‡æ›²ç‡çº¦æŸè´å¡å°”è½¨è¿¹ä¸ä¸‰æ®µå¼é€Ÿåº¦è°±ç”Ÿæˆå¤šæ ·åŠ¨ä½œï¼Œå¹¶ä»¥è¯­ä¹‰éƒ¨ä»¶å¼•å¯¼çš„åæ–¹å·®æ‰°åŠ¨è¿›è¡Œç‰©æ€§æ‰°åŠ¨åˆæˆå¤§é‡ç‰©ç†å¯è¡Œæ¼”ç¤ºï¼›æœ€åè®­ç»ƒä»¥ç©ºé—´å¼‚è´¨ç‰©æ€§ä¸ºæ¡ä»¶çš„è½»é‡GNNä¸–ç•Œæ¨¡å‹ï¼Œå¹¶ç”¨çœŸå®è§†é¢‘å¾®è°ƒç‰©æ€§ä»¥ç¼©å°å®ä»¿å·®è·ï¼ŒåŒæ—¶ç»“åˆ3DGS+LBSå®ç°åŠ¨ä½œæ¡ä»¶è§†é¢‘é¢„æµ‹ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ OnlineSim2Graph: åŸºäºä¸ç¡®å®šæ€§çš„åœ¨çº¿ç‰©æ€§è¾¨è¯†ä¸GNNä¸–ç•Œæ¨¡å‹è‡ªé€‚åº”ï¼Œé¢å‘é•¿æ—¶ä¸åˆ†å¸ƒæ¼‚ç§»åœºæ™¯çš„å®æ—¶æ›´æ–°<br>â€¢ Multi-Object PhysWorld: æ‹“å±•è‡³å¤šç‰©ä½“æ¥è§¦ã€ç²˜é™„ã€æµå›º/æ–­è£‚ç­‰å¤æ‚æœ¬æ„çš„è·¨èŒƒå¼æ•°å­—å­ªç”Ÿä¸ç»Ÿä¸€GNNåŠ¨åŠ›å­¦<br>â€¢ Plan4Deform: å°†å®æ—¶GNNä¸–ç•Œæ¨¡å‹ä¸å¯å¾®/é‡‡æ ·å¼MPCæ·±åº¦è€¦åˆï¼Œå®ç°çœŸå®æœºå™¨äººå¯å˜å½¢ä½“ä»»åŠ¡çš„é—­ç¯è§„åˆ’ä¸æ‰§è¡Œ</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20780" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20780" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advancements in large reasoning models (LRMs) have introduced an intermediate "thinking" process prior to generating final answers, improving their reasoning capabilities on complex downstream tasks. However, the potential of LRMs as evaluators for machine translation (MT) quality remains underexplored. We provides the first systematic analysis of LRM-as-a-judge in MT evaluation. We identify key challenges, revealing LRMs require tailored evaluation materials, tend to "overthink" simpler instances and have issues with scoring mechanisms leading to overestimation. To address these, we propose to calibrate LRM thinking by training them on synthetic, human-like thinking trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this approach largely reduces thinking budgets by ~35x while concurrently improving evaluation performance across different LRM scales from 7B to 32B (e.g., R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These findings highlight the potential of efficiently calibrated LRMs to advance fine-grained automatic MT evaluation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç ”ç©¶ç©ºç™½ï¼šä»¥â€œæ€ç»´é“¾/æ¨ç†â€ä¸ºç‰¹å¾çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰èƒ½å¦èƒœä»»æœºå™¨ç¿»è¯‘è¯„ä¼°è€…ï¼ˆMQMå°ºåº¦ï¼‰å°šæœªç³»ç»Ÿç ”ç©¶ï¼Œè€Œè¯„ä¼°ä»»åŠ¡æœ¬èº«éœ€è¦ç±»ä¼¼â€œç³»ç»Ÿ2â€çš„å®¡æ…æ¨ç†ï¼ˆç¬¬1é¡µ-ç¬¬2é¡µï¼‰<br>â€¢ è¯„ä»·ææ–™ä¸åŒ¹é…ï¼šä¸åŒè§„æ¨¡LRMå¯¹æºæ–‡/å‚è€ƒçš„æ•æ„Ÿæ€§å·®å¼‚æ˜¾è‘—ï¼Œå°æ¨¡å‹æ›´ä¾èµ–å‚è€ƒã€å¤§æ¨¡å‹æ›´å—ç›Šäºæºæ–‡ï¼›ç°æœ‰åšæ³•å¸¸â€œè¿·å¤±äºæºæ–‡â€ä¸”æœªåšè§„æ¨¡æ„ŸçŸ¥è®¾è®¡ï¼ˆè§ç¬¬5é¡µå›¾3ã€è¡¨1ï¼‰<br>â€¢ è¯„åˆ†æœºåˆ¶é™·é˜±ï¼šåŸºäºè¾…åŠ©æ¨¡å‹çš„äºŒæ¬¡è¯„åˆ†éš¾ä»¥å½’å› ã€ä¸”å­˜åœ¨ç³»ç»Ÿæ€§â€œè¿‡é«˜ä¼°åˆ†â€åå·®ï¼›è§„åˆ™æƒé‡å˜åŒ–å¯¹æ’åºå½±å“å°ä½†æ¨¡å‹å¼è¯„åˆ†åå·®æ˜æ˜¾ï¼ˆè§ç¬¬6-7é¡µå›¾4ã€å›¾5ã€è¡¨2ï¼‰<br>â€¢ æ€ç»´é¢„ç®—ä½æ•ˆï¼šLRMåœ¨ç®€æ˜“æ ·ä¾‹ä¸Šâ€œè¿‡åº¦æ€è€ƒâ€ï¼Œæ¨ç†tokenä¸éš¾åº¦ä¸åŒ¹é…ã€æˆæœ¬é«˜è€Œæ”¶ç›Šä¸ç¨³ï¼ˆè§ç¬¬7é¡µå›¾6ï¼‰<br>â€¢ é‡è¦æ€§ï¼šMTå‘å±•ä¾èµ–å¯é ã€ç»†ç²’åº¦ä¸”å¯æ‰©å±•çš„äººç±»ä¸€è‡´æ€§è¯„ä¼°ï¼›éœ€è¦æ—¢é«˜ç›¸å…³æ€§åˆé«˜æ•ˆç‡ã€è¿‡ç¨‹é€æ˜çš„â€œLRM-as-a-judgeâ€ï¼ˆç¬¬2é¡µã€å›¾1ï¼‰</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºThinMQMï¼šåˆæˆè´´è¿‘äººç±»çš„MQMè¯„ä¼°è½¨è¿¹ï¼ˆé”™è¯¯æ ‡æ³¨â†’åŸºäºæƒé‡çš„èšåˆè¯„åˆ†ï¼‰ï¼Œå¯¹LRMè¿›è¡Œåè®­ç»ƒï¼Œä½¿å…¶åœ¨ä¸€æ¬¡ç”Ÿæˆä¸­å®Œæˆâ€œæ€è€ƒ+æ ‡æ³¨+æ‰“åˆ†â€å¹¶æ ¡å‡†æ‰“åˆ†åˆ†å¸ƒã€‚è¯¥æ–¹æ³•åœ¨WMT24ä¸Šå°†æ€ç»´é¢„ç®—å‡å°‘çº¦35å€ï¼ŒåŒæ—¶åœ¨7Bâ€“32Bæ¨¡å‹ä¸Šæå‡ç³»ç»Ÿ/ç‰‡æ®µç›¸å…³æ€§ï¼ˆä¾‹å¦‚+8.7ç‚¹ï¼Œè§è¡¨3ã€å›¾1bï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ é¢å‘å¤šè¯­ä¸ä½èµ„æºçš„ThinMQMæ‰©å±•ï¼šå°†æ€ç»´è½¨è¿¹åˆæˆä¸æ ¡å‡†è¿ç§»åˆ°æ›´å¤šè¯­å¯¹ä¸ä¸“ä¸šåŸŸï¼Œç ”ç©¶è·¨è¯­ç³»/ä½èµ„æºåœºæ™¯çš„é²æ£’æ€§ä¸åŸŸè‡ªé€‚åº”<br>â€¢ è‡ªé€‚åº”æ€ç»´é¢„ç®—ä¸éš¾åº¦æ„ŸçŸ¥è°ƒåº¦ï¼šæ ¹æ®æ ·ä¾‹éš¾åº¦åŠ¨æ€åˆ†é…æ¨ç†æ­¥æ•°/Tokenï¼Œæœ€å°åŒ–è¿‡åº¦æ€è€ƒå¹¶ä¿æŒç›¸å…³æ€§ä¸ç¨³å®šæ€§<br>â€¢ ç«¯åˆ°ç«¯å¯è§£é‡Šçš„MQMè¯„åˆ†å™¨ï¼šè”åˆå­¦ä¹ â€œé”™è¯¯ç±»å‹è¯†åˆ«+ä¸¥é‡åº¦ä¼°è®¡+åˆ†æ•°æ ¡å‡†â€ï¼Œåœ¨ä¿æŒå¯è§£é‡Šæ€§çš„åŒæ—¶æ‘†è„±å¤–éƒ¨è¯„åˆ†å™¨å½’å› ä¸æ¸…çš„é—®é¢˜</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Taming Modality Entanglement in Continual Audio-Visual Segmentation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.17234" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.17234" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recently, significant progress has been made in multi-modal continual learning, aiming to learn new tasks sequentially in multi-modal settings while preserving performance on previously learned ones. However, existing methods mainly focus on coarse-grained tasks, with limitations in addressing modality entanglement in fine-grained continual learning settings. To bridge this gap, we introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to continuously segment new classes guided by audio. Through comprehensive analysis, two critical challenges are identified: 1) multi-modal semantic drift, where a sounding objects is labeled as background in sequential tasks; 2) co-occurrence confusion, where frequent co-occurring classes tend to be confused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework is designed to address these challenges. Specifically, for multi-modal semantic drift, a Multi-modal Sample Selection (MSS) strategy is proposed to select samples with high modal consistency for rehearsal. Meanwhile, for co-occurence confusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed, allowing for the increase of rehearsal sample frequency of those confusable classes during training process. Moreover, we construct three audio-visual incremental scenarios to verify effectiveness of our method. Comprehensive experiments demonstrate that our method significantly outperforms single-modal continual learning methods.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç ”ç©¶ç©ºç™½ï¼šç°æœ‰å¤šæ¨¡æ€æŒç»­å­¦ä¹ å¤šé›†ä¸­åœ¨åˆ†ç±»/åˆ†ç¦»ç­‰ç²—ç²’åº¦ä»»åŠ¡ï¼Œéš¾ä»¥å¤„ç†åƒç´ çº§çš„éŸ³è§†é¢‘å¯¹é½ä¸é—å¿˜ï¼›å°†å•æ¨¡æ€æŒç»­è¯­ä¹‰åˆ†å‰²ç›´æ¥è¿ç§»åˆ°å¤šæ¨¡æ€åœºæ™¯è¡¨ç°ä¸ä½³ï¼ˆé¡µ1-2ï¼‰ã€‚<br>â€¢ ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼ˆæ¨¡æ€çº ç¼ ï¼‰ï¼š(1) å¤šæ¨¡æ€è¯­ä¹‰æ¼‚ç§»â€”â€”å·²å­¦å‘å£°ç›®æ ‡åœ¨åç»­ä»»åŠ¡è¢«æ ‡ä¸ºèƒŒæ™¯ï¼Œå¯¼è‡´éŸ³è§†è¯­ä¹‰å¯¹é½é”™è¯¯å¹¶åŠ å‰§é—å¿˜ï¼›(2) å…±ç°æ··æ·†â€”â€”é«˜å…±ç°ç±»åˆ«åœ¨å­¦ä¹ æ–°ç±»åæ˜“äº’ç›¸è¯¯åˆ¤ï¼ˆå›¾1ï¼Œé¡µ1ï¼‰ã€‚<br>â€¢ å®ç”¨ä¸æ–¹æ³•å±€é™ï¼šçœŸå®åº”ç”¨ï¼ˆå¦‚å…·èº«æ™ºèƒ½ï¼‰éœ€è¦åœ¨å†…å­˜å—é™çš„æ•°æ®æµä¸­æŒç»­å®šä½å£°æºï¼›ç°æœ‰å›æ”¾/ç”Ÿæˆæ–¹æ³•éš¾ä»¥ä¿è¯éŸ³é¢‘ä¸è§†è§‰çš„ç²¾ç¡®å¯¹é½ï¼Œå¯èƒ½æ”¾å¤§æ¨¡æ€çº ç¼ ï¼ˆå®éªŒå¯¹æ¯”æ˜¾ç¤ºä¼ ç»ŸCSS/EIRç­‰åœ¨AVSä¸‹æ˜¾è‘—é€€åŒ–ï¼Œè¡¨1é¡µ6ï¼Œå›¾5é¡µ6ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºç¢°æ’é©±åŠ¨çš„å¤šæ¨¡æ€å›æ”¾æ¡†æ¶CMRï¼šç”¨å¤šæ¨¡æ€æ ·æœ¬é€‰æ‹©ï¼ˆMSSï¼‰ä¾æ®å•æ¨¡æ€ä¸å¤šæ¨¡æ€mIoUå·®å€¼é€‰å–é«˜æ¨¡æ€ä¸€è‡´æ€§æ ·æœ¬ä»¥æŠ‘åˆ¶è¯­ä¹‰æ¼‚ç§»ï¼›ç”¨ç¢°æ’å›æ”¾ï¼ˆCSRï¼‰æŒ‰æ—§æ¨¡å‹é¢„æµ‹ä¸ç°æ ‡æ³¨çš„å†²çªé¢‘ç‡è‡ªé€‚åº”æé«˜æ˜“æ··ç±»çš„å›æ”¾æƒé‡ï¼Œç¼“è§£å…±ç°æ··æ·†ï¼ˆæ¡†æ¶è§å›¾2ï¼Œé¡µ3ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ å› æœè§£è€¦çš„æŒç»­éŸ³è§†é¢‘åˆ†å‰²ï¼šç”¨å› æœå›¾ä¸å¹²é¢„å­¦ä¹ æ¶ˆé™¤å…±ç°ç›¸å…³æ€§ï¼Œå­¦ä¹ ä¸éŸ³é¢‘å› æœç›¸å…³çš„åƒç´ çº§ä¸å˜è¡¨ç¤ºã€‚<br>â€¢ åŠ¨æ€è®°å¿†è°ƒåº¦ä¸å•ç›®æ ‡è§£è€¦å›æ”¾ï¼šå°†å¤šç›®æ ‡æ ·æœ¬è‡ªåŠ¨åˆ‡åˆ†ä¸ºå•ç›®æ ‡ç‰‡æ®µï¼Œå¹¶åŸºäºåœ¨çº¿å†²çªç»Ÿè®¡è‡ªé€‚åº”åˆ†é…å†…å­˜ä¸å›æ”¾é¢‘ç‡ã€‚<br>â€¢ æ— å›æ”¾çš„å‚æ•°é«˜æ•ˆCAVSï¼šåŸºäºå¯åŠ æç¤º/é€‚é…å™¨ä¸è·¨æ¨¡æ€è’¸é¦ï¼Œåœ¨ä¸å­˜åŸå§‹æ•°æ®çš„æ¡ä»¶ä¸‹ä¿æŒæ—§ç±»å¹¶å…¼å®¹Transformer/å¤§æ¨¡å‹ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21581" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21581" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Foley Control is a lightweight approach to video-guided Foley that keeps pretrained single-modality models frozen and learns only a small cross-attention bridge between them. We connect V-JEPA2 video embeddings to a frozen Stable Audio Open DiT text-to-audio (T2A) model by inserting compact video cross-attention after the model's existing text cross-attention, so prompts set global semantics while video refines timing and local dynamics. The frozen backbones retain strong marginals (video; audio given text) and the bridge learns the audio-video dependency needed for synchronization -- without retraining the audio prior. To cut memory and stabilize training, we pool video tokens before conditioning. On curated video-audio benchmarks, Foley Control delivers competitive temporal and semantic alignment with far fewer trainable parameters than recent multi-modal systems, while preserving prompt-driven controllability and production-friendly modularity (swap/upgrade encoders or the T2A backbone without end-to-end retraining). Although we focus on Video-to-Foley, the same bridge design can potentially extend to other audio modalities (e.g., speech).</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ éœ€è¦åœ¨ä¸é‡æ–°è®­ç»ƒåºå¤§éŸ³é¢‘å…ˆéªŒçš„æƒ…å†µä¸‹ï¼Œå®ç°ç”±è§†é¢‘ç²¾å‡†å¼•å¯¼çš„é«˜ä¿çœŸæ‹ŸéŸ³ï¼ˆè¯­ä¹‰ä¸€è‡´ä¸”å¯¹é½åˆ°è§†è§‰äº‹ä»¶çš„ç¬æ€ä¸èŠ‚å¥ï¼‰<br>â€¢ ç«¯åˆ°ç«¯å¤šæ¨¡æ€æ‰©æ•£Transformerä¾èµ–æµ·é‡ä¸¥æ ¼é…å¯¹æ•°æ®ä¸é‡ç®—åŠ›ã€è¿‡æ»¤æµç¨‹ç¹çï¼Œæ¨¡å—åŒ–å’Œå¯ç»´æŠ¤æ€§å·®ï¼Œç”Ÿäº§è½åœ°å›°éš¾<br>â€¢ ç°æœ‰é€‚é…å™¨/æ§åˆ¶å™¨æ–¹æ³•å¸¸åœ¨U-Netä¸Šå †å å¤šæ§åˆ¶æµå¹¶ä¾èµ–å¤–éƒ¨æ—¶é—´æˆ³/åŒæ­¥æ¨¡å—ï¼Œè·¯å¾„å‰²è£‚ã€è®­ç»ƒä¸ç¨³ä¸”æ¨ç†å¤æ‚ï¼Œå‰Šå¼±å¯æ§æ€§<br>â€¢ çœŸå®æ•°æ®å­˜åœ¨é…éŸ³ã€ç¦»å±å£°ã€èƒŒæ™¯éŸ³ä¹ä¸æ—¶é—´æˆ³è¯¯å·®ç­‰å™ªå£°ï¼Œå‰Šå¼±ç›‘ç£ã€éš¾è¦†ç›–é•¿å°¾äº‹ä»¶<br>â€¢ éœ€è¦å¯æ’æ‹”çš„æ¨¡å—åŒ–æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨ä¸ç«¯åˆ°ç«¯é‡è®­çš„å‰æä¸‹æ›¿æ¢/å‡çº§è§†é¢‘ç¼–ç å™¨æˆ–T2Aä¸»å¹²ï¼ŒåŒæ—¶ä¿æŒæ–‡æœ¬æç¤ºçš„å…¨å±€è¯­ä¹‰å¯æ§</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºFoley Controlï¼šåœ¨å†»ç»“çš„Stable Audio Open DiTæ–‡æœ¬åˆ°éŸ³é¢‘ä¸»å¹²å†…ï¼Œç´§éšæ–‡æœ¬è·¨æ³¨æ„åŠ›åæ’å…¥è½»é‡è§†é¢‘è·¨æ³¨æ„åŠ›æ¡¥æ¥ï¼Œä»…è®­ç»ƒè¯¥æ¡¥æ¥ï¼ˆå«å¾®å‹MLPä¸QKV/è¾“å‡ºæŠ•å½±ï¼‰ï¼Œä»¥æ± åŒ–åçš„V-JEPA2è§†é¢‘ä»¤ç‰Œä¸ºæ¡ä»¶ï¼›æ–‡æœ¬å…ˆè®¾å®šå…¨å±€è¯­ä¹‰ï¼Œè§†é¢‘å†ç»†åŒ–æ—¶åºä¸å±€éƒ¨åŠ¨æ€ï¼Œå¹¶ç»“åˆRoPEç¨³å®šè·¨æ¨¡æ€å¯¹é½ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Foley Control++ï¼šè‡ªé€‚åº”è§†é¢‘ä»¤ç‰Œæ±‡èšä¸é¢„ç®—æ„ŸçŸ¥è·¯ç”±ç”¨äºé•¿æ—¶ä¸Šä¸‹æ–‡æ‹ŸéŸ³â€”â€”é€šè¿‡å¯å­¦ä¹ æ± åŒ–/ç¨€ç–æ³¨æ„åŠ›åœ¨å›ºå®šæ˜¾å­˜ä¸‹ä¿ç•™ç»†ç²’åº¦ç©ºé—´/è¿åŠ¨çº¿ç´¢ï¼Œæ”¯æŒæ›´é•¿æ—¶åŸŸ<br>â€¢ Spatial Foley Controlï¼šé¢å‘åŒè€³/å…¨å‘çš„å¯æ§ç©ºé—´æ‹ŸéŸ³ç”Ÿæˆâ€”â€”å°†æ¡¥æ¥æ‰©å±•åˆ°ç©ºé—´å£°å­¦ä¸åœºæ™¯å‡ ä½•ï¼Œç”Ÿæˆå®šä½å‡†ç¡®çš„ç«‹ä½“/ç¯ç»•å£°<br>â€¢ Streaming Foley Controlï¼šä½å»¶è¿Ÿåœ¨çº¿è§†é¢‘åˆ°éŸ³é¢‘çš„å¢é‡å¯¹é½â€”â€”è®¾è®¡å› æœåŒ–æ¡¥æ¥ä¸åŒæ­¥æŸå¤±ï¼Œå®ç°ç›´æ’­/äº¤äº’åœºæ™¯çš„åœ¨çº¿æ‹ŸéŸ³</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">Soft Instruction De-escalation Defense</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21057" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21057" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an external environment; this makes them susceptible to prompt injections when dealing with untrusted data. To overcome this limitation, we propose SIC (Soft Instruction Control)-a simple yet effective iterative prompt sanitization loop designed for tool-augmented LLM agents. Our method repeatedly inspects incoming data for instructions that could compromise agent behavior. If such content is found, the malicious content is rewritten, masked, or removed, and the result is re-evaluated. The process continues until the input is clean or a maximum iteration limit is reached; if imperative instruction-like content remains, the agent halts to ensure security. By allowing multiple passes, our approach acknowledges that individual rewrites may fail but enables the system to catch and correct missed injections in later steps. Although immediately useful, worst-case analysis shows that SIC is not infallible; strong adversary can still get a 15% ASR by embedding non-imperative workflows. This nonetheless raises the bar.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šå·¥å…·å¢å¼ºå‹LLMåœ¨å¤„ç†ä¸å¯ä¿¡å¤–éƒ¨æ•°æ®ï¼ˆç½‘é¡µã€é‚®ä»¶ã€APIï¼‰æ—¶ææ˜“é­é‡é—´æ¥æç¤ºæ³¨å…¥ï¼Œè¿›è€Œè¢«è¯±å¯¼æ‰§è¡Œé”™è¯¯/æ¶æ„çš„å·¥å…·è°ƒç”¨ã€‚<br>â€¢ é‡è¦æ€§ï¼šç°å®ç³»ç»Ÿå·²å¤šæ¬¡æš´éœ²æ­¤ç±»é£é™©ï¼Œæ”»å‡»å¾€å¾€è·¨å¤šæ­¥é“¾æ¡è§¦å‘æ•æ„Ÿæ“ä½œï¼Œå•ç‚¹é˜²æŠ¤éš¾ä»¥å¥æ•ˆï¼Œéœ€åœ¨ä¿è¯å¯ç”¨æ€§çš„åŒæ—¶æ˜¾è‘—å‹ä½ASRã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šæ£€æµ‹å™¨/å•æ¬¡è¿‡æ»¤æŠŠå®‰å…¨é—®é¢˜ç®€åŒ–ä¸ºâ€œäºŒåˆ†ç±»â€ï¼Œæ˜“è¢«å¯¹æŠ—æ”¹å†™å’Œè¿­ä»£æ”»å‡»ç»•è¿‡ï¼Œä¸”è¯¯æŠ¥é«˜ã€æ•ˆç”¨é™ï¼ˆè®ºæ–‡æŒ‡å‡ºRUP/SWDåœ¨å¤šä»»åŠ¡ASRä»é«˜ï¼›è§ç¬¬6é¡µå›¾2ä¸è¡¨1ï¼‰ã€‚<br>â€¢ æç¤ºå¢å¼ºï¼ˆåˆ†éš”ç¬¦ã€é‡å¤ç”¨æˆ·æç¤ºã€è¿½åŠ å®‰å…¨è¯­ï¼‰éƒ¨ç½²ç®€å•ä½†å®‰å…¨æ€§æœ‰é™ï¼Œç³»ç»Ÿçº§æ–¹æ¡ˆï¼ˆéš”ç¦»ã€ä¿¡æ¯æµ/æƒé™æ§åˆ¶ï¼‰é›†æˆæˆæœ¬é«˜ï¼›éœ€è¦è½»é‡ã€å¯æ’æ‹”ä¸”å¯¹æ•ˆç”¨å‹å¥½çš„æ–¹æ¡ˆã€‚<br>â€¢ æŒ‘æˆ˜ï¼šæ”»å‡»å…·æœ‰è„†å¼±æ€§ä½†å¤šæ ·ã€å¯è‡ªé€‚åº”ä¼˜åŒ–ï¼›é˜²å¾¡éœ€å¤šè½®å¤„ç†ã€å¤šç²’åº¦éªŒè¯ï¼Œå¹¶èƒ½åœ¨å¤±è´¥æ—¶å®‰å…¨åœæœºè€Œéå†’é™©æ‰§è¡Œã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºSICï¼ˆSoft Instruction Controlï¼‰ä½œä¸ºä»£ç†å¤–çš„å‰ç½®è¿‡æ»¤å±‚ï¼šå‘ä¸å¯ä¿¡è¾“å…¥æ³¨å…¥é‡‘ä¸é›€æŒ‡ä»¤ï¼Œåå¤è°ƒç”¨LLMå¯¹æŒ‡ä»¤æ€§å†…å®¹è¿›è¡Œæ©è”½/é‡å†™/åˆ é™¤ï¼Œå¹¶å¯¹å…¨é‡ä¸åˆ†å—æ–‡æœ¬è¿›è¡ŒæŒ‡ä»¤æ£€æµ‹ï¼›è‹¥é‡‘ä¸é›€æœªæ¸…é™¤æˆ–ä»å«æŒ‡ä»¤åˆ™HALTï¼Œå¦åˆ™å°†æ¸…æ´—ç»“æœäº¤ç»™ä»£ç†ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ éå‘½ä»¤å¼å·¥ä½œæµæ³¨å…¥çš„æ£€æµ‹ä¸çº¦æŸï¼šè¯†åˆ«JSON/ä¼ªä»£ç /æ¸…å•ç­‰â€œéæŒ‡ä»¤å¼â€å¯æ‰§è¡Œè´Ÿè½½å¹¶é™åˆ¶å…¶è§¦å‘å·¥å…·è°ƒç”¨çš„èƒ½åŠ›ã€‚<br>â€¢ å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¤šç²’åº¦æŒ‡ä»¤å®¡è®¡ï¼šèåˆä¼šè¯å†å²ã€å·¥å…·è°ƒç”¨å›¾ä¸é•¿ç¨‹ä¾èµ–ï¼Œæå‡å¯¹è·¨å¥ä¾èµ–ä¸å¤šæ­¥é“¾æ¡æ”»å‡»çš„æ£€å‡ºç‡ã€‚<br>â€¢ SICä¸ä¿¡æ¯æµæ§åˆ¶çš„ç«¯åˆ°ç«¯ç¨³å¥ä»£ç†ï¼šå°†SICä¸æƒé™/æ²™ç®±/IFCç»“åˆï¼Œå¼ºåˆ¶éš”ç¦»å¤–éƒ¨è´Ÿè½½å¯¹å·¥å…·è°ƒç”¨çš„å½±å“å¹¶æä¾›å¯è¯æ˜çš„å®‰å…¨è¾¹ç•Œã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20708" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20708" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>3D LiDAR sensors are essential for autonomous navigation, environmental monitoring, and precision mapping in remote sensing applications. To efficiently process the massive point clouds generated by these sensors, LiDAR data is often projected into 2D range images that organize points by their angular positions and distances. While these range image representations enable efficient processing, conventional projection methods suffer from fundamental geometric inconsistencies that cause irreversible information loss, compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDAR Intrinsic Calibration Estimation for Lossless Range Images), the first general, sensor-agnostic method that achieves lossless range image generation from spinning LiDAR point clouds without requiring manufacturer metadata or calibration files. Our algorithm automatically reverse-engineers the intrinsic geometry of any spinning LiDAR sensor by inferring critical parameters including laser beam configuration, angular distributions, and per-beam calibration corrections, enabling lossless projection and complete point cloud reconstruction with zero point loss. Comprehensive evaluation across the complete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfect point preservation, with zero points lost across all point clouds. Geometric accuracy is maintained well within sensor precision limits, establishing geometric losslessness with real-time performance. We also present a compression case study that validates substantial downstream benefits, demonstrating significant quality improvements in practical applications. This paradigm shift from approximate to lossless LiDAR projections opens new possibilities for high-precision remote sensing applications requiring complete geometric preservation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ä¼ ç»Ÿå°†LiDARç‚¹äº‘æŠ•å½±ä¸ºäºŒç»´Rangeå›¾åƒå‡è®¾ç†æƒ³çƒé¢æ¨¡å‹ï¼Œä½†çœŸå®ä¼ æ„Ÿå™¨å­˜åœ¨æ¯æŸæ¿€å…‰çš„ç©ºé—´/è§’åº¦åç½®ä¸å·¥å‚æ ‡å®šä¿®æ­£ï¼Œå¯¼è‡´åƒç´ å†²çª/ç©ºæ´ä¸ä¸å¯é€†ä¿¡æ¯ä¸¢å¤±ï¼Œå½±å“é«˜ç²¾åº¦åº”ç”¨ï¼ˆå¦‚åˆ†å‰²/æ£€æµ‹/å‹ç¼©/é‡Œç¨‹è®¡ï¼‰<br>â€¢ å…¬å¼€æ•°æ®å¤šä»…æä¾›â€œå·²æ ¡æ­£ç‚¹äº‘â€è€Œæ— å‚å®¶å…ƒæ•°æ®/æ ‡å®šæ–‡ä»¶ï¼Œç°æœ‰æ›´ç²¾ç¡®æ¨¡å‹ä¾èµ–LUTæˆ–åŸå§‹æ•°æ®åŒ…ï¼Œé€šç”¨æ€§å·®ï¼Œç¼ºä¹â€œåœ¨æ— å…ƒæ•°æ®æ¡ä»¶ä¸‹ä»ç‚¹äº‘ç”Ÿæˆæ— æŸRangeå›¾åƒâ€çš„é€šç”¨æ–¹æ³•<br>â€¢ å¸¸ç”¨PBEAç­‰æŠ•å½±ç­–ç•¥å­˜åœ¨å‡ ä½•å¤±çœŸä¸é‡‡æ ·ä¸¢ç‚¹é—®é¢˜ï¼Œéœ€è¦ä¸€ç§ä¼ æ„Ÿå™¨æ— å…³ã€å¯å®æ—¶ã€å‡ ä½•ä¸€è‡´çš„æ— æŸæŠ•å½±æ¥ä¿éšœä¸‹æ¸¸ä»»åŠ¡ä¸å‹ç¼©è´¨é‡</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>ALICE-LRIç›´æ¥ä»æ ¡æ­£åçš„ç‚¹äº‘è‡ªåŠ¨åæ¼”æ—‹è½¬å¼LiDARå†…å‚ï¼šå…ˆç”¨æ”¹è¿›Houghå˜æ¢+è¯¯å·®æœ‰ç•Œç­›é€‰+åŠ æƒæœ€å°äºŒä¹˜å¹¶é…åˆå†²çªå›æº¯ï¼Œä¼°è®¡æŸæ•°ä¸æ¯æŸçš„ä»°è§’å’Œå‚ç›´åç§»ï¼›å†å¯¹æ¯æŸç©·ä¸¾æ°´å¹³åˆ†è¾¨ç‡å¹¶åŸºäºåˆ†æ®µçº¿æ€§æ¨¡å‹ä¸å›å½’ä¼°è®¡æ°´å¹³åç§»ä¸æ–¹ä½åç§»ï¼Œè¿›è€Œç”¨æ ¡æ­£è§’ç”Ÿæˆæ»¡è¶³ä¸€ä¸€å¯¹åº”çš„Rangeå›¾åƒï¼Œå®ç°ç‚¹äº‘æ— æŸé‡å»ºã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è”åˆå†…å‚ä¸è¿åŠ¨æ•ˆåº”çš„æ— æŸLiDARæŠ•å½±ï¼šåŒæ—¶ä¼°è®¡ä¼ æ„Ÿå™¨å†…å‚ä¸è½¦ä½“è‡ªè¿åŠ¨æ ¡æ­£æ‰€è‡´å¤–å‚/æ—¶åŸŸè¯¯å·®ï¼Œå®ç°å¯¹è¿åŠ¨ä¿®æ­£æ•°æ®é›†çš„æ— æŸæŠ•å½±<br>â€¢ é¢å‘ä¸‹æ¸¸æ„ŸçŸ¥ä»»åŠ¡çš„æ— æŸRangeè§†å›¾å­¦ä¹ ï¼šç³»ç»Ÿè¯„ä¼°å¹¶æ”¹é€ åˆ†å‰²/æ£€æµ‹/é‡Œç¨‹è®¡ç®¡çº¿ï¼Œé‡åŒ–æ— æŸæŠ•å½±å¯¹ç²¾åº¦ä¸é²æ£’æ€§çš„æ”¶ç›Š<br>â€¢ ä¼ æ„Ÿå™¨æ„ŸçŸ¥çš„ç‚¹äº‘è¶…åˆ†ä¸ç‰©ç†ä¸€è‡´é‡å»ºï¼šåˆ©ç”¨åæ¼”å‡ºçš„æŸçº§å‡ ä½•ä¸å™ªå£°ç‰¹æ€§è¿›è¡Œç‚¹äº‘è¶…åˆ†ã€è¡¥å…¨ä¸æ•°æ®å¢å¼ºï¼Œä¿æŒä¸çœŸå®æ‰«æè½¨è¿¹ä¸€è‡´</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.11370" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.11370" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement learning (RL) has emerged as a crucial approach for enhancing the capabilities of large language models. However, in Mixture-of-Experts (MoE) models, the routing mechanism often introduces instability, even leading to catastrophic RL training collapse. We analyze the training-inference consistency of MoE models and identify a notable discrepancy in routing behaviors between the two phases. Moreover, even under identical conditions, the routing framework can yield divergent expert selections across repeated forward passes. To address this foundational inconsistency, we propose Rollout Routing Replay (R3), a method that records routing distributions from the inference engine and replays them during training. R3 significantly reduces training-inference policy KL divergence and mitigates extreme discrepancies without compromising training speed. Extensive experiments on various settings confirm that R3 succeeds in stabilizing RL training, preventing collapse and outperforming methods such as GSPO and TIS. We believe this work can offer a new solution for stabilizing RL in MoE models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šMoE æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å­˜åœ¨è®­ç»ƒ-æ¨ç†ç­–ç•¥ä¸ä¸€è‡´ï¼ˆè·¨å¼•æ“ä¸é‡å¤å‰å‘å¯¼è‡´è·¯ç”±é€‰æ‹©å˜åŒ–ï¼‰ï¼Œå¼•å‘é‡è¦æ€§é‡‡æ ·æ¯”ç‡ä¸ç¨³å®šä¸è®­ç»ƒå´©æºƒã€‚è¯æ®ï¼šMoE çš„è®­ç»ƒ-æ¨ç† KLâ‰ˆ1.535eâˆ’3ï¼Œé«˜äºè‡´å¯†æ¨¡å‹ 6.4eâˆ’4ï¼ˆå›¾2aã€2cï¼Œp.4ï¼‰ï¼›åŒä¸€æ¡†æ¶ä¸¤æ¬¡å‰å‘ KLâ‰ˆ8.4eâˆ’4ï¼ˆå›¾4ï¼Œp.5ï¼‰ã€‚<br>â€¢ æ ¹å› åˆ†æï¼šè·¯ç”±å™¨çš„ä¸è¿ç»­æ€§å¯¼è‡´ä¸“å®¶é€‰æ‹©é«˜åº¦æ•æ„Ÿã€‚ç»Ÿè®¡æ˜¾ç¤ºçº¦10%å±‚çº§è·¯ç”±ä¸åŒã€94% token è‡³å°‘ä¸€å±‚ä¸åŒã€åºåˆ—å¹³å‡æ¯ token çº¦6ä¸ªè·¯ç”±å·®å¼‚ï¼ˆå›¾3a/b/cï¼Œp.5ï¼‰ï¼Œä»è€Œæ”¾å¤§äº† MoE çš„è®­ç»ƒ-æ¨ç†åˆ†å¸ƒåå·®ï¼ˆæç«¯ token æ¯”ä¾‹è§å›¾2dï¼Œp.4ï¼‰ã€‚<br>â€¢ é‡è¦æ€§ï¼šRL æ˜¯ LLM åè®­ç»ƒèƒ½åŠ›æå‡çš„æ ¸å¿ƒï¼Œä½† MoE çš„ä¸ç¨³å®šä¼šå¯¼è‡´æ€§èƒ½é€€åŒ–ç”šè‡³è®­ç»ƒå´©æºƒï¼ˆå•æ­¥ mini_step è®¾ç½®ä¸‹å¤šæ¬¡å´©æºƒï¼›è§å›¾5è®­ç»ƒ-æ¨ç† KL ä¸ F(Ï„=2)æ¿€å¢å¯¹åº”å´©æºƒï¼Œp.9ï¼›è¡¨1æ±‡æ€»äº†æ—  R3 æ—¶çš„å´©æºƒæ­¥æ•°ï¼Œp.8ï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šGSPO/TIS ç­‰ä¸»è¦ä¿®æ­£é‡è¦æ€§é‡‡æ ·æˆ–è£å‰ªï¼Œæœªç›´é¢â€œè·¨å¼•æ“è·¯ç”±ä¸ä¸€è‡´â€çš„æ ¹å› ï¼›ç¡®å®šæ€§å†…æ ¸æ–¹æ³•ä»£ä»·é«˜ä¸”æœªé’ˆå¯¹ MoE RLï¼›ä¸¢å¼ƒå¤§åå·®æ ·æœ¬æ²»æ ‡ä¸æ²»æœ¬ï¼›Recompute Routing Replay ä»…è¦†ç›–é‡ç®—â†’æ›´æ–°é˜¶æ®µï¼Œmini_step=1 æ—¶å¤±æ•ˆï¼Œæ— æ³•è§£å†³ rolloutâ†’è®­ç»ƒå¼•æ“å·®å¼‚ï¼ˆç¬¬6.3èŠ‚ï¼Œp.12ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡º Rollout Routing Replayï¼ˆR3ï¼‰ï¼šåœ¨ rollout æ—¶ä»æ¨ç†å¼•æ“ç¼“å­˜æ¯å±‚æ¯ token çš„ MoE è·¯ç”±æ©ç ï¼Œå¹¶åœ¨è®­ç»ƒçš„â€œæ—§ç­–ç•¥é‡ç®—â€å’Œâ€œç­–ç•¥æ›´æ–°â€é˜¶æ®µå¼ºåˆ¶é‡æ”¾è¯¥æ©ç ï¼›gating ä»ç”¨è®­ç»ƒ logits è¿›è¡Œ softmax ä»¥ä¿ç•™æ¢¯åº¦ï¼Œä»è€Œå¯¹é½ä¸“å®¶é€‰æ‹©ã€æ˜¾è‘—é™ä½è®­ç»ƒ-æ¨ç† KLï¼ˆç”±â‰ˆ1.5eâˆ’3 é™è‡³â‰ˆ7.5eâˆ’4ï¼Œæ¥è¿‘è‡´å¯†æ¨¡å‹ 6.4eâˆ’4ï¼›ç¬¬4.3èŠ‚ï¼Œp.7ï¼‰ã€‚åŒæ—¶ä¸ KVCache è”åˆç¼“å­˜ï¼Œæ”¯æŒå¤šè½®ä»»åŠ¡ï¼Œrollout é¢å¤–å¼€é”€<3%ï¼ˆç¬¬4èŠ‚ï¼Œp.6â€“7ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Routing-Aware Importance Sampling for MoE RLï¼šå¼•å…¥è·¯ç”±åå·®åº¦é‡ï¼ˆå¦‚ F(Ï„)ã€è®­ç»ƒ-æ¨ç† KLï¼‰è¿›è¡Œè‡ªé€‚åº”åŠ æƒ/è£å‰ªï¼Œç»Ÿä¸€å¹¶å¢å¼º R3 ä¸ GSPO/TIS çš„ç¦»ç­–ç•¥ä¿®æ­£ã€‚<br>â€¢ Deterministic Cross-Engine MoE Routing Kernelsï¼šè®¾è®¡æ•°å€¼ç¨³å®šã€è·¨å¼•æ“ä¸€è‡´çš„ Top-K è·¯ç”±ä¸ softmax å†…æ ¸ï¼Œå‡å°‘å¯¹è·¯ç”±é‡æ”¾çš„ä¾èµ–ï¼Œè¿›ä¸€æ­¥é™ä½ KL ä¸æç«¯ token æ¯”ä¾‹ã€‚<br>â€¢ Consistency-Regularized Router Trainingï¼šåŸºäº rollout æ©ç åŠ å…¥ä¸€è‡´æ€§/é²æ£’æ€§æ­£åˆ™æˆ–è’¸é¦ï¼Œä½¿è·¯ç”±å™¨å¯¹å¾®æ‰°ä¸æ¡†æ¶å·®å¼‚ä¸æ•æ„Ÿï¼Œåœ¨æ— éœ€é‡æ”¾æ—¶äº¦ä¿æŒç¨³å®šè®­ç»ƒã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">Redefining Retrieval Evaluation in the Era of LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21440" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21440" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR, assume that human users sequentially examine documents with diminishing attention to lower ranks. This assumption breaks down in Retrieval Augmented Generation (RAG) systems, where search results are consumed by Large Language Models (LLMs), which, unlike humans, process all retrieved documents as a whole rather than sequentially. Additionally, traditional IR metrics do not account for related but irrelevant documents that actively degrade generation quality, rather than merely being ignored. Due to these two major misalignments, namely human vs. machine position discount and human relevance vs. machine utility, classical IR metrics do not accurately predict RAG performance. We introduce a utility-based annotation schema that quantifies both the positive contribution of relevant passages and the negative impact of distracting ones. Building on this foundation, we propose UDCG (Utility and Distraction-aware Cumulative Gain), a metric using an LLM-oriented positional discount to directly optimize the correlation with the end-to-end answer accuracy. Experiments on five datasets and six LLMs demonstrate that UDCG improves correlation by up to 36% compared to traditional metrics. Our work provides a critical step toward aligning IR evaluation with LLM consumers and enables more reliable assessment of RAG components</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ äººæœºæ¶ˆè´¹å·®å¼‚å¯¼è‡´è¯„æµ‹å¤±é…ï¼šä¼ ç»ŸIRæŒ‡æ ‡ï¼ˆnDCG/MAP/MRRï¼‰å‡è®¾ç”¨æˆ·æŒ‰åºæµè§ˆå¹¶å¯¹ä½ä½é€’å‡åŠ æƒï¼Œä½†RAGä¸­çš„LLMä¸€æ¬¡æ€§æ•´ä½“å¤„ç†æ£€ç´¢ç»“æœä¸”å­˜åœ¨ä½ç½®åç½®ï¼ˆå¦‚â€œä¸­é—´ä¸¢å¤±â€ï¼‰ï¼Œè‡´ä½¿æŒ‡æ ‡ä¸çœŸå®ç”Ÿæˆæ•ˆæœä¸ä¸€è‡´ï¼ˆè§å›¾1ï¼‰ã€‚<br>â€¢ å¿½è§†å¹²æ‰°çš„è´Ÿæ•ˆç”¨ï¼šç°æœ‰æŒ‡æ ‡å°†æ‰€æœ‰â€œä¸ç›¸å…³â€ä¸€æ¦‚è€Œè®ºï¼ŒæœªåŒºåˆ†â€œè¯­ä¹‰ç›¸å…³ä½†ä¸å«ç­”æ¡ˆçš„å¼ºå¹²æ‰°â€ä¸â€œæ— å®³æ— å…³â€ï¼Œè€Œå¼ºå¹²æ‰°ä¼šæ˜æ˜¾æ‹‰ä½ç”Ÿæˆè´¨é‡ï¼ˆè¡¨1ï¼‰ï¼Œå¯¼è‡´ä»¥é”™ç›®æ ‡ä¼˜åŒ–æ£€ç´¢å¹¶ä¼¤å®³ç«¯åˆ°ç«¯è¡¨ç°ã€‚<br>â€¢ è¯„æµ‹æˆæœ¬ä¸ç›¸å…³æ€§é—®é¢˜ï¼šç«¯åˆ°ç«¯ç”Ÿæˆè¯„ä¼°å¼€é”€é«˜ã€ä¸å¯æ‰©ï¼›ä¼ ç»ŸæŒ‡æ ‡å¯¹RAGå‡†ç¡®ç‡çš„é¢„æµ‹åŠ›ä¸è¶³ï¼ˆè¡¨3ï¼‰ï¼ŒäºŸéœ€ä¸€ç§ä½æˆæœ¬ä¸”ä¸ä¸‹æ¸¸å‡†ç¡®ç‡å¼ºç›¸å…³çš„ç¦»çº¿åº¦é‡ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºä»¥LLMåœ¨â€œä»…ç»™å•æ®µâ€æ—¶çš„éæ‹’ç­”æ¦‚ç‡ä¸ºåŸºç¡€çš„è¿ç»­æ•ˆç”¨æ ‡æ³¨ï¼ˆç›¸å…³æ®µä¸ºæ­£æ•ˆç”¨ã€å¹²æ‰°æ®µä¸ºè´Ÿæ•ˆç”¨ï¼‰ï¼Œå¹¶æ®æ­¤å®šä¹‰UDCGç³»åˆ—æŒ‡æ ‡ï¼šå°†æ­£/è´Ÿæ•ˆç”¨ï¼ˆå¯é€‰ä½ç½®æƒï¼‰èšåˆåç»Sigmoidå¾—åˆ°ä¸Šä¸‹æ–‡åˆ†æ•°ã€‚åŒ…å«å¯è®­ç»ƒçš„UDCGÎ¸ï¼ˆå­¦ä¹ æ­£/è´Ÿä½ç½®æƒï¼‰ä¸å…è®­ç»ƒçš„UDCGï¼ˆä»…ä¸€è¶…å‚Î³â‰ˆ1/3ï¼‰ï¼Œåœ¨å¤šæ•°æ®é›†ä¸å¤šLLMä¸Šæ˜¾è‘—æå‡ä¸ç«¯åˆ°ç«¯å‡†ç¡®ç‡çš„ç›¸å…³æ€§ï¼ŒåŒæ—¶é™ä½è¯„æµ‹æˆæœ¬ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Logit-Free UDCGï¼šé¢å‘é»‘ç›’APIçš„æ•ˆç”¨ä¼°è®¡â€”â€”ç”¨åˆ¤åˆ«å¼è¯„å®¡ã€åäº‹å®ä¸€è‡´æ€§æˆ–æ ¡å‡†æŠ€å·§è¿‘ä¼¼éæ‹’ç­”æ¦‚ç‡ï¼Œæ— éœ€logitsã€‚<br>â€¢ Multi-hop UDCGï¼šå¤šè·³è¯æ®é“¾çš„æ•ˆç”¨ä¸å¹²æ‰°ä¼ æ’­â€”â€”å®šä¹‰é“¾çº§ï¼ˆæ®µçº§â†’æ­¥çº§â†’é“¾çº§ï¼‰æ•ˆç”¨ï¼Œå»ºæ¨¡å¹²æ‰°çš„è¿é”å½±å“å¹¶ä¼˜åŒ–å¤šè·³RAGã€‚<br>â€¢ UDCG-Rerankï¼šä»¥æ•ˆç”¨-å¹²æ‰°ä¿¡å·ç«¯åˆ°ç«¯è®­ç»ƒæ£€ç´¢ä¸é‡æ’â€”â€”å°†UDCGä½œä¸ºè®­ç»ƒç›®æ ‡/è’¸é¦ä¿¡å·ï¼Œå­¦å¾—å¯¹LLMå‹å¥½çš„æ’åºä¸ä¸Šä¸‹æ–‡æ„å»ºã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21111" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21111" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Visual reasoning in multimodal large language models (MLLMs) has primarily been studied in static, fully observable settings, limiting their effectiveness in real-world environments where information is often incomplete due to occlusion or limited field of view. Humans, in contrast, actively explore and interact with their environment-moving, examining, and manipulating objects-to gather information through a closed-loop process integrating perception, reasoning, and action. Inspired by this human capability, we introduce the Active Visual Reasoning (AVR) task, extending visual reasoning to partially observable, interactive environments. AVR necessitates agents to: (1) actively acquire information via sequential physical actions, (2) integrate observations across multiple steps for coherent reasoning, and (3) dynamically adjust decisions based on evolving visual feedback. To rigorously evaluate AVR, we introduce CLEVR-AVR, a simulation benchmark featuring multi-round interactive environments designed to assess both reasoning correctness and information-gathering efficiency. We present AVR-152k, a large-scale dataset that offers rich Chain-of-Thought (CoT) annotations detailing iterative reasoning for uncertainty identification, action-conditioned information gain prediction, and information-maximizing action selection, crucial for training agents in a higher-order Markov Decision Process. Building on this, we develop PhysVLM-AVR, an MLLM achieving state-of-the-art performance on CLEVR-AVR, embodied reasoning (OpenEQA, RoboVQA), and passive visual reasoning (GeoMath, Geometry30K). Our analysis also reveals that current embodied MLLMs, despite detecting information incompleteness, struggle to actively acquire and integrate new information through interaction, highlighting a fundamental gap in active reasoning capabilities.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°å®ç¯å¢ƒéƒ¨åˆ†å¯è§ï¼ˆé®æŒ¡ã€å®¹å™¨ã€è§†é‡å—é™ï¼‰ä½¿è¢«åŠ¨è§†è§‰æ¨ç†å¤±æ•ˆï¼Œå¿…é¡»é€šè¿‡äº¤äº’ä¸»åŠ¨è·å–ç¼ºå¤±ä¿¡æ¯ï¼›è®ºæ–‡åœ¨å›¾1ï¼ˆç¬¬2é¡µï¼‰ç›´è§‚å±•ç¤ºäº†è¢«åŠ¨æ¨¡å‹åœ¨é®æŒ¡å¢å¤šæ—¶è®¡æ•°ç²¾åº¦æ˜¾è‘—ä¸‹é™ã€‚<br>â€¢ ç°æœ‰èŒƒå¼å‰²è£‚ï¼šVQA/ç©ºé—´æ¨ç†å¤šåŸºäºé™æ€å…¨å¯è§å›¾åƒï¼Œä»…â€œæ„ŸçŸ¥â†’æ¨ç†â€ï¼›EQA/è§†é¢‘æ¨ç†å¤šä¸ºè¢«åŠ¨è§‚çœ‹ï¼›æ¢ç´¢/å¯¼èˆªæ–¹æ³•ä¼˜åŒ–ä»»åŠ¡æˆåŠŸç‡è€Œéä¿¡æ¯å¢ç›Šï¼Œéš¾ä»¥å°†â€œæ¨ç†é©±åŠ¨çš„åŠ¨ä½œé€‰æ‹©â€é—­ç¯èµ·æ¥ï¼ˆç¬¬3-4é¡µï¼‰ã€‚<br>â€¢ ç¼ºä¹ç³»ç»Ÿè¯„æµ‹ä¸ç›‘ç£æ•°æ®ï¼šç¼ºå°‘åœ¨äº¤äº’åœºæ™¯ä¸­åŒæ—¶è¡¡é‡â€œä¿¡æ¯å……è¶³æ€§åˆ¤æ–­ã€ä¿¡æ¯å¢ç›Šç‡ã€æœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§â€çš„åŸºå‡†ä¸æŒ‡æ ‡ï¼Œä»¥åŠèƒ½æ˜¾å¼ç›‘ç£â€œä¸ç¡®å®šæ€§è¯†åˆ«â€”ä¿¡æ¯å¢ç›Šé¢„æµ‹â€”åŠ¨ä½œé€‰æ‹©â€çš„CoTæ•°æ®ï¼ˆç¬¬5é¡µçš„CLEVR-AVRä¸ç¬¬6é¡µçš„AVR-Coreï¼‰ã€‚<br>â€¢ é‡è¦æ€§ï¼šä¸»åŠ¨è§†è§‰æ¨ç†æ˜¯é€šå‘å…·èº«æ™ºèƒ½çš„å…³é”®èƒ½åŠ›ï¼Œèƒ½æ˜¾è‘—æå‡åœ¨çœŸå®æœºå™¨äºº/ç‰©ç†ç¯å¢ƒä¸­çš„é²æ£’æ€§ä¸å¯è§£é‡Šæ€§ï¼Œå¼¥åˆé™æ€æ¨ç†ä¸å…·èº«æ™ºèƒ½ä¹‹é—´çš„é¸¿æ²Ÿï¼ˆç¬¬2-3é¡µï¼‰ã€‚<br>â€¢ å®è¯æ­ç¤ºç°çŠ¶ç“¶é¢ˆï¼šç°æœ‰å…·èº«MLLMè™½èƒ½æ„ŸçŸ¥ä¿¡æ¯ä¸è¶³ï¼Œä½†éš¾ä»¥é€šè¿‡äº¤äº’é«˜æ•ˆè·å–å¹¶æ•´åˆæ–°ä¿¡æ¯ï¼ˆè¡¨1ï¼Œç¬¬8é¡µï¼‰ï¼Œå‡¸æ˜¾ä¸»åŠ¨æ¨ç†èƒ½åŠ›çš„ç¼ºå£ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºä¸»åŠ¨è§†è§‰æ¨ç†ï¼ˆAVRï¼‰èŒƒå¼ä¸CLEVR-AVRåŸºå‡†ï¼Œå°†ä»»åŠ¡å½¢å¼åŒ–ä¸ºé«˜é˜¶MDPä¸­çš„â€œæ„ŸçŸ¥â€”æ¨ç†â€”è¡ŒåŠ¨â€é—­ç¯ï¼Œå¹¶è®¾è®¡ä¸‰æŒ‡æ ‡è¯„æµ‹ï¼ˆä¿¡æ¯å……è¶³æ€§ACCISJã€ä¿¡æ¯å¢ç›Šç‡IGRã€æœ€ç»ˆç­”æ¡ˆACCFAï¼‰ã€‚åŒæ—¶æ„å»ºAVR-152kï¼ˆæ ¸å¿ƒAVR-Coreå«ä¸“å®¶CoTï¼Œç›‘ç£â€œä¸ç¡®å®šæ€§è¯†åˆ«â€”åŠ¨ä½œæ¡ä»¶ä¿¡æ¯å¢ç›Šé¢„æµ‹â€”åŠ¨ä½œé€‰æ‹©â€ï¼‰ï¼Œå¹¶ä»¥SigLIP-400M+Qwen2.5-3Bå®ç°PhysVLM-AVRï¼Œç»“åˆè§†è§‰tokenä¸‹é‡‡æ ·çš„å¤šå›¾åƒé«˜æ•ˆå¤„ç†ä¸åˆ†é˜¶æ®µæ··åˆæ•°æ®å¾®è°ƒï¼Œè¾¾æˆSOTAä¸»åŠ¨æ¨ç†è¡¨ç°ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ å­¦ä¹ åŠ¨ä½œæ¡ä»¶ä¿¡æ¯å¢ç›Šçš„ç­–ç•¥ä¼˜åŒ–ï¼šç”¨å¼ºåŒ–å­¦ä¹ /é€†å‘å­¦ä¹ ç›´æ¥ä¼˜åŒ–ä¿¡æ¯å¢ç›Šä¸æœ€ç»ˆä»»åŠ¡å›æŠ¥ï¼Œç¼©å°é«˜ACCISJä¸ACCFAçš„æ€§èƒ½é¸¿æ²Ÿã€‚<br>â€¢ æ¨¡æ‹Ÿåˆ°ç°å®çš„ä¸»åŠ¨è§†è§‰æ¨ç†è¿ç§»ï¼šè·¨åŸŸè‡ªé€‚åº”ä¸ä¸ç¡®å®šæ€§æ ¡å‡†ï¼Œå®ç°ä»CLEVR-AVRåˆ°å¤æ‚çœŸå®æ¡Œé¢/åœºæ™¯çš„ç¨³å¥è¿ç§»ä¸è¯„æµ‹ã€‚<br>â€¢ å°æ ·æœ¬AVRçš„ç­–ç•¥è’¸é¦ä¸æ•°æ®åˆæˆï¼šç»“åˆç­–ç•¥è’¸é¦ã€åˆæˆäº¤äº’è½¨è¿¹ä¸ç¨‹åºåŒ–åœºæ™¯ç”Ÿæˆï¼Œæ˜¾è‘—æå‡ä¸»åŠ¨æ¨ç†çš„æ ·æœ¬æ•ˆç‡ä¸æ³›åŒ–èƒ½åŠ›ã€‚</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 9</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-10-27 23:11:15 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 9;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>