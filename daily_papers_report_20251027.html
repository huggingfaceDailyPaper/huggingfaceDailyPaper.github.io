<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - October 27, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "SF Pro Display", -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Helvetica Neue", "Arial", "Noto Sans", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", sans-serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">October 27, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">DeepAgent: A General Reasoning Agent with Scalable Toolsets</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21618" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21618" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To address the challenges of long-horizon interactions, particularly the context length explosion from multiple tool calls and the accumulation of interaction history, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. This work takes a step toward more general and capable agents for real-world applications. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ä¼ ç»Ÿå·¥ä½œæµå¼ä»£ç†ï¼ˆå¦‚ ReActã€Plan-and-Solveï¼‰ç¼ºä¹çœŸæ­£çš„è‡ªæ²»ï¼Œæ­¥éª¤ä¸æµç¨‹é¢„å…ˆå›ºå®šï¼Œæ¨ç†ä»…å›´ç»•å±€éƒ¨ä¸‹ä¸€æ­¥å±•å¼€ï¼Œéš¾ä»¥åœ¨æ‰§è¡Œä¸­æŒ‰éœ€åŠ¨æ€å‘ç°ä¸é€‰æ‹©å·¥å…·ï¼Œæ•´ä½“ä»»åŠ¡æ¨ç†ä¸€è‡´æ€§ä¸é€‚é…å¼€æ”¾å·¥å…·ç”Ÿæ€èƒ½åŠ›ä¸è¶³ï¼ˆè§é¡µ1-2ï¼Œå›¾2å¯¹æ¯”ï¼‰ã€‚ â€¢ é•¿æ—¶ç¨‹äº¤äº’å¯¼è‡´ä¸Šä¸‹æ–‡é•¿åº¦çˆ†ç‚¸ä¸è¯¯å·®ç´¯ç§¯ï¼šå¤šæ¬¡å·¥å…·è°ƒç”¨ä¸å†å²å †å ä½¿æ¨¡å‹æ˜“é™·å…¥é”™è¯¯æ¢ç´¢è·¯å¾„ï¼Œç¼ºå°‘èƒ½ç¨³å®šå‹ç¼©ä¸”ä¿æŒå¯ç”¨æ€§çš„è®°å¿†æœºåˆ¶ä»¥ç»´æŒé•¿æœŸä»»åŠ¡çš„é«˜æ•ˆä¸ç¨³å¥ï¼ˆè§é¡µ1ã€3-4ï¼‰ã€‚ â€¢ é€šç”¨å·¥å…·ä½¿ç”¨çš„è®­ç»ƒä¸ç¨³å®šä¸”æˆæœ¬é«˜ï¼šç›´æ¥ä¾èµ–æµ·é‡çœŸå®APIè®­ç»ƒä¼šæ…¢ä¸”è„†å¼±ï¼›ä»…æœ‰ç»ˆå±€ç¨€ç–å¥–åŠ±éš¾ä»¥å­¦åˆ°ç²¾ç¡®çš„å·¥å…·è°ƒç”¨ä¸å‚æ•°å¡«å……ï¼Œéœ€è¦ç»†ç²’åº¦çš„ä¿¡ç”¨åˆ†é…ä¿¡å·ï¼ˆè§é¡µ1ã€5ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡º DeepAgentï¼Œå°†æ€è€ƒã€å·¥å…·æ£€ç´¢ä¸è°ƒç”¨ã€ä»¥åŠè®°å¿†ç®¡ç†ç»Ÿä¸€äºå•ä¸€è¿ç»­æ¨ç†æµï¼Œå¹¶ä»¥è‡ªæ²»è®°å¿†æŠ˜å å°†å†å²å‹ç¼©ä¸ºç»“æ„åŒ–çš„æƒ…æ™¯è®°å¿†ã€å·¥ä½œè®°å¿†ä¸å·¥å…·è®°å¿†ä»¥ç¨³æ€é•¿ç¨‹äº¤äº’ï¼ˆè§å›¾3/é¡µ3-4ï¼‰ã€‚åŒæ—¶æå‡º ToolPO å¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨ LLM å·¥å…·æ¨¡æ‹Ÿå™¨ä¸å·¥å…·è°ƒç”¨ä¼˜åŠ¿å½’å› ï¼Œåœ¨ä¸ä¾èµ–å¤§é‡ä¸ç¨³å®šçœŸå®APIçš„å‰æä¸‹ï¼Œå®ç°ç¨³å®šã€ä½æˆæœ¬ä¸”ç²¾ç»†ç›‘ç£çš„ç«¯åˆ°ç«¯è®­ç»ƒï¼ˆè§é¡µ5ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ æ£€ç´¢å™¨â€”ç­–ç•¥è”åˆä¼˜åŒ–çš„ç«¯åˆ°ç«¯å·¥å…·ä»£ç†ï¼šè”åˆè®­ç»ƒå·¥å…·æ£€ç´¢ä¸å†³ç­–ç­–ç•¥ï¼Œç»“åˆå¯¹æ¯”å­¦ä¹ ä¸è°ƒç”¨ä¼˜åŠ¿å½’å› ï¼Œæå‡å¼€æ”¾é›†å·¥å…·å‘ç°â€”è°ƒç”¨çš„ä¸€è‡´æ€§ä¸æ•ˆç‡ã€‚ â€¢ è‡ªé€‚åº”è®°å¿†æŠ˜å çš„ç†è®ºåŒ–ä¸è°ƒåº¦å­¦ä¹ ï¼šå°†è®°å¿†æŠ˜å å½¢å¼åŒ–ä¸ºæˆæœ¬â€”æ”¶ç›Šä¼˜åŒ–ï¼Œå­¦ä¹ ä½•æ—¶æŠ˜å ä¸æŠ˜å ç²’åº¦ï¼Œå¹¶ç»™å‡ºé•¿æ—¶ç¨‹æ€§èƒ½â€”å¼€é”€æƒè¡¡çš„ç†è®ºç•Œä¸æ§åˆ¶ç­–ç•¥ã€‚ â€¢ ä»æ¨¡æ‹Ÿåˆ°çœŸå®çš„å·¥å…·ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼šåœ¨æ¨¡æ‹Ÿå™¨ä¸çœŸå®APIçš„æ··åˆç¯å¢ƒä¸­å¼•å…¥ä¸ç¡®å®šæ€§æ„ŸçŸ¥ä¸å®‰å…¨çº¦æŸï¼Œç ”ç©¶ä»¿çœŸåˆ°çœŸå®çš„è¿ç§»ã€é²æ£’æ€§ä¸åœ¨çº¿æ ¡å‡†æœºåˆ¶ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Video-As-Prompt: Unified Semantic Control for Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20888" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20888" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Unified, generalizable semantic control in video generation remains a critical open challenge. Existing methods either introduce artifacts by enforcing inappropriate pixel-wise priors from structure-based controls, or rely on non-generalizable, condition-specific finetuning or task-specific architectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes this problem as in-context generation. VAP leverages a reference video as a direct semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via a plug-and-play Mixture-of-Transformers (MoT) expert. This architecture prevents catastrophic forgetting and is guided by a temporally biased position embedding that eliminates spurious mapping priors for robust context retrieval. To power this approach and catalyze future research, we built VAP-Data, the largest dataset for semantic-controlled video generation with over 100K paired videos across 100 semantic conditions. As a single unified model, VAP sets a new state-of-the-art for open-source methods, achieving a 38.7% user preference rate that rivals leading condition-specific commercial models. VAP's strong zero-shot generalization and support for various downstream applications mark a significant advance toward general-purpose, controllable video generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šç¼ºä¹ç»Ÿä¸€ã€å¯æ³›åŒ–çš„â€œè¯­ä¹‰æ§åˆ¶â€è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼›ç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–åƒç´ å¯¹é½å…ˆéªŒã€è¦ä¹ˆä¸ºæ¯ç§è¯­ä¹‰/ä»»åŠ¡å•ç‹¬è®­ç»ƒï¼Œéš¾ä»¥ä¸€ä½“åŒ–ä¸é›¶æ ·æœ¬æ³›åŒ–ï¼ˆè§å›¾2ï¼Œé¡µ3ï¼‰ã€‚ â€¢ é‡è¦æ€§ï¼šè¯­ä¹‰æ§åˆ¶æ”¯æ’‘ç‰¹æ•ˆã€è§†é¢‘é£æ ¼åŒ–ã€åŠ¨ä½œæ¨¡ä»¿ä¸é•œå¤´è°ƒåº¦ç­‰æ ¸å¿ƒåº”ç”¨ï¼ˆé¡µ1-2ï¼‰ï¼Œç»Ÿä¸€æ–¹æ³•èƒ½æ˜¾è‘—æå‡åˆ›ä½œæ•ˆç‡ä¸å¯æ‰©å±•æ€§ã€‚ â€¢ ç»“æ„æ§åˆ¶è¿ç§»çš„å±€é™ï¼šå°†åƒç´ å¯¹é½æ®‹å·®æ³¨å…¥æœºåˆ¶ç¡¬å¥—åˆ°è¯­ä¹‰æ§åˆ¶ä¼šå¼•å…¥copy-and-pasteä¼ªåƒåŠé”™è¯¯çš„åƒç´ æ˜ å°„å…ˆéªŒï¼ˆå›¾2(a)ï¼Œé¡µ3ï¼‰ã€‚ â€¢ è¯­ä¹‰æ§åˆ¶ç°çŠ¶çš„å±€é™ï¼šæ¡ä»¶ç‰¹å®šå¾®è°ƒ/LoRAéœ€é€æ¡ä»¶è®­ç»ƒã€æˆæœ¬é«˜ä¸”æ³›åŒ–å·®ï¼›ä»»åŠ¡ç‰¹å®šæ¨¡å—/æ¨ç†ç­–ç•¥å‰²è£‚å„ä»»åŠ¡ï¼Œéš¾ä»¥ç»Ÿä¸€ä¸é›¶æ ·æœ¬æ‰©å±•ï¼ˆå›¾2(b)(c)ï¼Œé¡µ3ï¼‰ã€‚ â€¢ æ•°æ®ç“¶é¢ˆï¼šç¼ºå°‘è¦†ç›–å¹¿æ³›è¯­ä¹‰æ¡ä»¶çš„æ•°æ®é›†é™åˆ¶äº†ç»Ÿä¸€è®­ç»ƒä¸è¯„æµ‹ï¼Œå› æ­¤ä½œè€…æ„å»ºå«10ä¸‡+é…å¯¹è§†é¢‘ã€100ç§è¯­ä¹‰æ¡ä»¶çš„VAP-Dataï¼ˆå›¾3ï¼Œé¡µ5ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>å°†å‚è€ƒè§†é¢‘ä½œä¸ºâ€œè¯­ä¹‰æç¤ºâ€ï¼Œé€šè¿‡å¯æ’å³ç”¨çš„Mixture-of-Transformersä¸“å®¶å¹¶è¡Œè·¯å¾„ï¼Œå¯¹å†»ç»“çš„Video DiTè¿›è¡Œå±‚é—´å…¨æ³¨æ„åŠ›å¼•å¯¼ï¼Œå¹¶é‡‡ç”¨æ—¶é—´åç½®çš„RoPEå°†å‚è€ƒåºåˆ—ç½®äºç›®æ ‡åºåˆ—ä¹‹å‰ä¸”ä»…å…±äº«ç©ºé—´ä½ç½®ï¼Œä»è€Œæ¶ˆé™¤é”™è¯¯çš„åƒç´ æ˜ å°„å…ˆéªŒå¹¶å®ç°ç¨³å¥çš„ä¸Šä¸‹æ–‡æ£€ç´¢ã€‚é…åˆVAP-Dataç»Ÿä¸€è®­ç»ƒåï¼Œå•ä¸€æ¨¡å‹å³å¯åœ¨æ¦‚å¿µ/é£æ ¼/è¿åŠ¨/ç›¸æœºç­‰å¤šè¯­ä¹‰æ¡ä»¶ä¸‹å®ç°é›¶æ ·æœ¬æ§åˆ¶ï¼ˆå›¾1-2ï¼Œé¡µ2-3ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Compositional Video-As-Prompt: å¤šå‚è€ƒè§†é¢‘çš„è¯­ä¹‰è§£è€¦ä¸å¯ç»„åˆæ§åˆ¶ â€” å°†é£æ ¼/è¿åŠ¨/ç›¸æœº/æ¦‚å¿µåˆ†è§£å¹¶æŒ‰éœ€ç»„åˆï¼Œç ”ç©¶ä¸“å®¶è·¯ç”±ä¸å†²çªæ¶ˆè§£ã€‚ â€¢ 3D-aware Video-As-Prompt: é¢å‘3Dæ—¶ç©ºä¸€è‡´æ€§çš„ç›¸æœºä¸åœºæ™¯è¯­ä¹‰æ§åˆ¶ â€” èåˆNeRF/é«˜æ–¯åœºç­‰3Då…ˆéªŒï¼Œè”åˆæ§åˆ¶ç›¸æœºè½¨è¿¹ä¸åœºæ™¯å‡ ä½•ã€‚ â€¢ Streaming/Retrieval-Augmented VAP: åœ¨çº¿æ£€ç´¢å‚è€ƒä¸ä½æ—¶å»¶å®æ—¶è§†é¢‘ç”Ÿæˆ â€” æ„å»ºæ£€ç´¢å¢å¼ºçš„å‚è€ƒæ± ä¸å¢é‡å¼ä¸“å®¶æ¨ç†ï¼Œå®ç°é•¿è§†é¢‘ä¸å®æ—¶åº”ç”¨ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20286" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20286" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>GUI grounding, which maps natural-language instructions to actionable UI elements, is a core capability of GUI agents. Prior works largely treats instructions as a static proxy for user intent, overlooking the impact of instruction diversity and quality on grounding performance. Through a careful investigation of existing grounding datasets, we find a 23.3% flaw rate in their instructions and show that inference-time exploitation of instruction diversity yields up to a substantial 76% relative performance improvement. In this paper, we introduce the Instruction-as-Reasoning paradigm, treating instructions as dynamic analytical pathways that offer distinct perspectives and enabling the model to select the most effective pathway during reasoning. To achieve this, we propose a two-stage training framework: supervised fine-tuning (SFT) on synthesized, diverse instructions to instill multi-perspective reasoning, followed by reinforcement learning (RL) to optimize pathway selection and composition. Our resulting models, UI-Ins-7B and UI-Ins-32B, achieve state-of-the-art results on five challenging grounding benchmarks and exhibit emergent reasoning, selectively composing and synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model demonstrates strong agentic potential, achieving a 74.1% success rate on AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals additional insights such as how reasoning can be formulated to enhance rather than hinder grounding performance, and how our method mitigates policy collapse in the SFT+RL framework. All code and model checkpoints will be publicly released in https://github.com/alibaba/UI-Ins.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šGUI groundingä¸¥é‡ä¾èµ–è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œä½†ä¸»æµæ–¹æ³•å°†æŒ‡ä»¤è§†ä¸ºé™æ€è¾“å…¥ï¼Œå¿½è§†æŒ‡ä»¤çš„å¤šè§†è§’è¡¨è¾¾ä¸æ¨ç†ä»·å€¼ï¼Œå¯¼è‡´é²æ£’æ€§æ¬ ä½³ï¼ˆå›¾3ä¸å›¾2aæ˜¾ç¤ºä¸åŒè§†è§’å¯å¸¦æ¥æœ€é«˜76%çš„ç›¸å¯¹æå‡ï¼‰ã€‚ â€¢ æ•°æ®è´¨é‡ç—›ç‚¹ï¼šå¼€æºæ•°æ®é›†ä¸­æŒ‡ä»¤å­˜åœ¨æ˜¾è‘—å™ªå£°ä¸æ­§ä¹‰ï¼Œè®ºæ–‡æŠ½æ£€å‘ç°23.3%æ ·æœ¬å­˜åœ¨â€œå¤šå¯¹åº”/ä¸å¯¹åº”â€ç­‰ç¼ºé™·ï¼Œç›´æ¥æ‹‰ä½è®­ç»ƒæ•ˆæœï¼ˆè§å›¾2bã€2cï¼‰ã€‚ â€¢ è®­ç»ƒèŒƒå¼å±€é™ï¼šä»…ç”¨å•é£æ ¼æŒ‡ä»¤çš„SFTæ˜“å½¢æˆâ€œå•ä¸€è·¯å¾„â€ç­–ç•¥ï¼ŒRLé˜¶æ®µæ¢ç´¢ä¸è¶³ä¸”æ˜“ç­–ç•¥åå¡Œï¼›è‡ªç”±å½¢å¼æ€ç»´é“¾ï¼ˆFFRï¼‰åœ¨RLä¸­å¸¸é™æ€§èƒ½ï¼ˆè¡¨8ã€è¡¨9ï¼‰ï¼Œç¼ºå°‘â€œå¯ä¼˜åŒ–çš„æ¨ç†ç»“æ„â€ã€‚ â€¢ ä»»åŠ¡é‡è¦æ€§ï¼šGUIä»£ç†çš„æœ‰æ•ˆæ€§å–å†³äºå°†é«˜å±‚æ„å›¾æ˜ å°„åˆ°å¯æ‰§è¡ŒUIå…ƒç´ çš„èƒ½åŠ›ï¼ŒæŒ‡ä»¤çš„æ¸…æ™°åº¦ã€è§†è§’é€‰æ‹©ä¸è´¨é‡ç›´æ¥å†³å®šè½ç‚¹å‡†ç¡®æ€§ä¸åœ¨çº¿æ‰§è¡Œç¨³å®šæ€§ï¼ˆå¼•è¨€ã€2.èŠ‚ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºâ€œInstruction-as-Reasoningâ€èŒƒå¼ï¼šå°†å¤–è§‚/åŠŸèƒ½/ç©ºé—´/æ„å›¾ç­‰å¤šè§†è§’æŒ‡ä»¤æ˜¾å¼ä½œä¸ºæ¨ç†è·¯å¾„ï¼Œå…ˆç»SFTå­¦ä¹ å¤šè§†è§’æ¨ç†ç”Ÿæˆä¸åæ ‡é¢„æµ‹ï¼Œå†ç”¨GRPOåœ¨RLé˜¶æ®µé€šè¿‡ç‚¹å…¥æ¡†å¥–åŠ±å­¦ä¹ åœ¨ä¸åŒåœºæ™¯ä¸‹é€‰æ‹©æœ€ä¼˜æ¨ç†è§†è§’ï¼›é…å¥—æ•°æ®æµæ°´çº¿ç”¨OmniParseræ ¡æ­£æ ‡æ³¨ã€GPT-4.1ç”Ÿæˆä¸æ ¡éªŒå¤šè§†è§’æ— æ­§ä¹‰æŒ‡ä»¤ï¼ˆè§å›¾5ã€å›¾6ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Know-Ins: çŸ¥è¯†å¢å¼ºçš„å¤šè§†è§’GUIå®šä½â€”â€”å¼•å…¥æ£€ç´¢/çŸ¥è¯†åº“ä¸RAGï¼Œè¡¥é½é¢†åŸŸå®ä½“ä¸å“ç‰Œç­‰å¤–éƒ¨å¸¸è¯†ä»¥åŒ–è§£çŸ¥è¯†ç¼ºå¤±å‹é”™è¯¯ï¼ˆå¯¹åº”å›¾10aï¼‰ã€‚ â€¢ Struct-Ins: åŸºäºç•Œé¢ç»“æ„å›¾çš„ç»„åˆæ¨ç†â€”â€”èåˆUIå±‚çº§/DOM/ç»„ä»¶å›¾ä¸ç©ºé—´å…³ç³»ï¼Œè¿›è¡Œç»“æ„åŒ–å¤šè§†è§’åˆæˆæ¨ç†ï¼Œæå‡å¸ƒå±€ç†è§£ä¸å¯ç‚¹å‡»åŒºåŸŸåˆ¤åˆ«ï¼ˆå¯¹åº”å›¾10bï¼‰ã€‚ â€¢ Ambi-Ins: é¢å‘æ­§ä¹‰æ¶ˆè§£çš„è‡ªé€‚åº”è§†è§’é€‰æ‹©â€”â€”åœ¨æ¨ç†ä¸­å¼•å…¥ä¸ç¡®å®šæ€§ä¼°è®¡ä¸å¤šå€™é€‰å‡è®¾ï¼Œç»“åˆå¯¹æŠ—è´Ÿæ ·æœ¬ä¸ä¸€è‡´æ€§æ ¡éªŒï¼Œç¼“è§£ç›¸ä¼¼å¹²æ‰°å›¾æ ‡å¯¼è‡´çš„æ··æ·†ï¼ˆå¯¹åº”å›¾10cã€10dï¼‰ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.19871" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.19871" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Discrete diffusion models have emerged as a promising direction for vision-language tasks, offering bidirectional context modeling and theoretical parallelization. However, their practical application is severely hindered by a train-inference discrepancy, which leads to catastrophic error cascades: initial token errors during parallel decoding pollute the generation context, triggering a chain reaction of compounding errors and leading to syntactic errors and semantic hallucinations. To address this fundamental challenge, we reframe the generation process from passive denoising to active refining. We introduce ReDiff, a refining-enhanced diffusion framework that teaches the model to identify and correct its own errors. Our approach features a two-stage training process: first, we instill a foundational revision capability by training the model to revise synthetic errors; second, we implement a novel online self-correction loop where the model is explicitly trained to revise its own flawed drafts by learning from an expert's corrections. This mistake-driven learning endows the model with the crucial ability to revisit and refine its already generated output, effectively breaking the error cascade. Extensive experiments demonstrate that ReDiff significantly improves the coherence and factual accuracy of generated content, enabling stable and efficient parallel generation far superior to traditional denoising methods. Our codes and models are available at https://rediff-hku.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç¦»æ•£æ‰©æ•£VLMåœ¨å¹¶è¡Œè§£ç ä¸­å‡ºç°é”™è¯¯çº§è”ï¼šå°‘é‡åˆå§‹tokené”™è¯¯ä¼šç¬æ—¶æ±¡æŸ“å…¨å±€ä¸Šä¸‹æ–‡ï¼Œå¸¦æ¥é‡å¤ã€è¯­æ³•æ··ä¹±ä¸è§†è§‰å¹»è§‰ï¼›æ ¹æºåœ¨äºè®­ç»ƒ-æ¨ç†åˆ†å¸ƒå·®å¼‚ï¼ˆè®­ç»ƒåªè§å¹²å‡€GTï¼Œæ¨ç†éœ€åœ¨è‡ªèº«ä¸­é—´äº§ç‰©ä¸Šç”Ÿæˆï¼‰ã€‚ â€¢ ç°æœ‰â€œæ©ç -é¢„æµ‹â€æ–¹æ³•æ˜¯è¢«åŠ¨å»å™ªï¼Œå·²è§£æ©tokenè¢«å›ºå®šï¼Œç¼ºä¹å¯¹å·²ç”Ÿæˆå†…å®¹çš„å›è®¿ä¸ä¿®è®¢èƒ½åŠ›ï¼Œæ— æ³•è‡ªçº ï¼Œå¯¼è‡´ä¸€æ­¥å¤štokenè§£ç æ—¶è´¨é‡ä¸ç¨³ï¼Œå¸¸é€€åŒ–åˆ°ä¸€token/æ­¥ã€‚ â€¢ æ‰©æ•£æ¨¡å‹å…·å¤‡åŒå‘ä¸Šä¸‹æ–‡ä¸å¹¶è¡Œæ½œåŠ›ï¼Œä½†åœ¨å°‘æ­¥æ•°/é«˜å¹¶è¡Œé€Ÿåº¦ä¸‹è¡¨ç°æ˜¾è‘—åŠ£åŒ–ï¼Œäº‹å®æ€§ä¸è¿è´¯æ€§ä¸è¶³ï¼Œæœªèƒ½å…‘ç°ç†è®ºä¸Šçš„é«˜æ•ˆæ¨ç†ä¼˜åŠ¿ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºReDiffï¼Œå°†ç”ŸæˆèŒƒå¼ä»è¢«åŠ¨å»å™ªè½¬ä¸ºä¸»åŠ¨æ ¡è®¢ï¼šé˜¶æ®µä¸€ä»¥åˆæˆè¯­æ³•/å¹»è§‰é”™è¯¯è®­ç»ƒåŸºç¡€ä¿®è®¢èƒ½åŠ›ï¼Œé˜¶æ®µäºŒåœ¨åœ¨çº¿è‡ªçº ç¯ä¸­è®©æ¨¡å‹ç”Ÿæˆè‰ç¨¿å¹¶ç”±ä¸“å®¶æ¨¡å‹çº é”™ï¼ŒæŒ‰è¢«çº æ­£ç‰‡æ®µè¿›è¡Œç›‘ç£ï¼›æ¨ç†æ—¶åŒæ—¶è§£æ©æ–°tokenå¹¶é‡å†™å·²ç”Ÿæˆtokenï¼Œæ‰“æ–­é”™è¯¯çº§è”å¹¶ç¨³å®šå¹¶è¡Œè§£ç ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ReDiff++: æ— ä¸“å®¶è‡ªä¸¾çš„è‡ªçº æ‰©æ•£æ¨¡å‹ï¼šä»¥è‡ªä¸€è‡´/å¯¹æ¯”å­¦ä¹ ä¸åŒæ¨¡å‹äº’å®¡æ›¿ä»£å¤–éƒ¨ä¸“å®¶ï¼Œé™ä½æ•°æ®ä¸æˆæœ¬ä¾èµ–ã€‚ â€¢ å¯éªŒè¯äº‹å®æ€§çš„è§†è§‰-è¯­è¨€ä¿®è®¢æ‰©æ•£ï¼šå°†æ£€ç´¢ä¸å¯éªŒè¯æ€§æ ¡éªŒå™¨èå…¥ä¿®è®¢å›è·¯ï¼Œæœ€å°åŒ–å¹»è§‰å¹¶æå‡äº‹å®ä¸€è‡´æ€§ã€‚ â€¢ ç»Ÿä¸€ç¼–è¾‘-ç”Ÿæˆçš„å¤šä»»åŠ¡ä¿®è®¢æ‰©æ•£æ¡†æ¶ï¼šæŠŠReDiffæ‰©å±•åˆ°VQAã€é•¿æ–‡æè¿°ä¸å¤šè½®å¯¹è¯ï¼Œå­¦ä¹ é€šç”¨çš„spançº§ç¼–è¾‘ä¸å¹¶è¡Œç”Ÿæˆè¿‡ç¨‹ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20479" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20479" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We unveil that internal representations in large language models (LLMs) serve as reliable proxies of learned knowledge, and propose RECALL, a novel representation-aware model merging framework for continual learning without access to historical data. RECALL computes inter-model similarity from layer-wise hidden representations over clustered typical samples, and performs adaptive, hierarchical parameter fusion to align knowledge across models. This design enables the preservation of domain-general features in shallow layers while allowing task-specific adaptation in deeper layers. Unlike prior methods that require task labels or incur performance trade-offs, RECALL achieves seamless multi-domain integration and strong resistance to catastrophic forgetting. Extensive experiments across five NLP tasks and multiple continual learning scenarios show that RECALL outperforms baselines in both knowledge retention and generalization, providing a scalable and data-free solution for evolving LLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šLLMsåœ¨æŒç»­/å¤šåŸŸåœºæ™¯ä¸­æ˜“å‘ç”Ÿç¾éš¾æ€§é—å¿˜ï¼Œéš¾ä»¥åŒæ—¶ä¿æŒé€šç”¨èƒ½åŠ›ä¸æ–°ä»»åŠ¡ä¸“é•¿ã€‚ â€¢ ç°å®çº¦æŸï¼šå†å²æ•°æ®å¸¸å› éšç§/å­˜å‚¨ä¸å¯ç”¨ã€ä»»åŠ¡è¾¹ç•Œä¸Task IDç¼ºå¤±ï¼Œå›æ”¾ç±»æ–¹æ³•éš¾ä»¥è½åœ°ã€‚ â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šæ­£åˆ™/ç»“æ„åŒ–æ–¹æ³•ä¼˜åŒ–ç©ºé—´å—é™ã€ä¾èµ–Task IDã€æ¨¡å‹å¤æ‚åº¦ç´¯ç§¯ã€è·¨ä»»åŠ¡æ€§èƒ½æŠ˜è¡·æ˜æ˜¾ã€‚ â€¢ åˆå¹¶æ–¹æ³•ç¼ºé™·ï¼šå¤šæ•°æƒé‡å¹³å‡å¿½ç•¥å±‚é—´åŠŸèƒ½å·®å¼‚ä¸è¡¨ç¤ºé”™é…ï¼Œæ·±å±‚è¡¨ç¤ºåœ¨ä»»åŠ¡é—´æ˜¾è‘—åˆ†åŒ–ï¼Œæ˜“å¼•å…¥è¯­ä¹‰å¹²æ‰°ï¼ˆè®ºæ–‡ç¬¬2èŠ‚çš„å±‚å†…/è·¨æ¨¡å‹è¡¨ç¤ºåˆ†æä¸å›¾2ã€å›¾3ï¼‰ã€‚ â€¢ éœ€æ±‚ç¼ºå£ï¼šéœ€è¦ä¸€ç§æ•°æ®æ— å…³ã€ä»»åŠ¡æ— å…³ã€å¯æ‰©å±•çš„çŸ¥è¯†èåˆæ–¹æ¡ˆï¼Œèƒ½åœ¨æµ…å±‚ä¿ç•™é€šç”¨è¡¨å¾ã€æ·±å±‚é€‚é…ä»»åŠ¡ç‰¹æ€§ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºRECALLï¼šä¸€ç§åŸºäºè¡¨ç¤ºå¯¹é½çš„åˆ†å±‚æ¨¡å‹èåˆæ¡†æ¶ã€‚é€šè¿‡å¯¹æ–°ä»»åŠ¡æ•°æ®èšç±»é€‰å–â€œå…¸å‹æ ·æœ¬â€ï¼Œåœ¨å„æ¨¡å‹å„å±‚æå–éšè—è¡¨ç¤ºå¹¶ç”¨RBFæ ¸è®¡ç®—ç›¸ä¼¼åº¦ï¼ŒSoftmaxå¾—åˆ°å±‚çº§è‡ªé€‚åº”èåˆæƒé‡ï¼Œé€å±‚çº¿æ€§æ’å€¼åˆå¹¶å‚æ•°ï¼Œä»è€Œåœ¨ä¸è®¿é—®å†å²æ•°æ®çš„å‰æä¸‹å¯¹é½çŸ¥è¯†ã€ç¼“è§£é—å¿˜ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Cross-Architecture RECALLï¼šè·¨æ¶æ„/åˆ†è¯å™¨çš„è¡¨ç¤ºå¯¹é½æ¨¡å‹åˆå¹¶â€”â€”é€šè¿‡å¯¹é½æ˜ å°„æˆ–è½»é‡Adapterå®ç°å¼‚æ„LLMçš„å±‚é—´å¯¹é½ã€‚ â€¢ Online-RECALLï¼šé¢å‘æµå¼æŒç»­å­¦ä¹ çš„åœ¨çº¿è¡¨ç¤ºå¯¹é½ä¸å¢é‡åˆå¹¶â€”â€”æ— éœ€ç¼“å­˜å†å²æ•°æ®ï¼Œä½¿ç”¨æ»‘åŠ¨/æ°´åº“é‡‡æ ·åŠ¨æ€æ›´æ–°å±‚æƒé‡ã€‚ â€¢ RECALL++ï¼šè®­ç»ƒæœŸæ­£åˆ™ä¸è¡¨ç¤ºå¯¹é½èåˆçš„è”åˆæ¡†æ¶â€”â€”å°†EWC/æ­£äº¤å­ç©ºé—´ç­‰æ­£åˆ™ä¸RECALLè§£è€¦æˆ–è”åˆä¼˜åŒ–ä»¥è¿›ä¸€æ­¥æŠ‘åˆ¶é—å¿˜ã€‚ â€¢ Metric-Learned RECALLï¼šé¢å‘åˆå¹¶çš„å¯å­¦ä¹ ç›¸ä¼¼åº¦åº¦é‡â€”â€”ç”¨å¯¹æ¯”å­¦ä¹ /æ¢æµ‹å™¨æ›¿ä»£å›ºå®šRBFï¼Œå­¦ä¹ æ›´è´´åˆçŸ¥è¯†äº²å’Œåº¦çš„å±‚çº§ç›¸ä¼¼æ€§ã€‚ â€¢ Privacy-Preserving Typical Sample Synthesisï¼šéšç§å‹å¥½çš„å…¸å‹æ ·æœ¬åˆæˆä¸é€‰æ‹©ç¨³å¥æ€§ç ”ç©¶â€”â€”ç”¨ç”Ÿæˆæ›¿ä»£æˆ–ä»£ç†ä»»åŠ¡æŒ‘é€‰å…¸å‹æ ·æœ¬ï¼Œæå‡æ•°æ®å¯ç”¨æ€§ä¸é²æ£’æ€§ã€‚ â€¢ Multilingual/Multimodal RECALLï¼šè·¨è¯­ç§ä¸è·¨æ¨¡æ€çš„è¡¨ç¤ºå¯¹é½åˆå¹¶â€”â€”åœ¨å…±äº«è¯­ä¹‰ç©ºé—´ï¼ˆå¤šè¯­/è§†å¬è¯­ï¼‰ä¸­å®ç°å±‚çº§å¯¹é½ä¸çŸ¥è¯†èåˆã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">A Definition of AGI</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.18212" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.18212" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The lack of a concrete definition for Artificial General Intelligence (AGI) obscures the gap between today's specialized AI and human-level cognition. This paper introduces a quantifiable framework to address this, defining AGI as matching the cognitive versatility and proficiency of a well-educated adult. To operationalize this, we ground our methodology in Cattell-Horn-Carroll theory, the most empirically validated model of human cognition. The framework dissects general intelligence into ten core cognitive domains-including reasoning, memory, and perception-and adapts established human psychometric batteries to evaluate AI systems. Application of this framework reveals a highly "jagged" cognitive profile in contemporary models. While proficient in knowledge-intensive domains, current AI systems have critical deficits in foundational cognitive machinery, particularly long-term memory storage. The resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify both rapid progress and the substantial gap remaining before AGI.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç¼ºå°‘å¯æ“ä½œçš„AGIå®šä¹‰ä¸ç»Ÿä¸€å°ºåº¦ï¼Œå¯¼è‡´â€œç§»åŠ¨é—¨æ§›â€å’Œæ— æ•ˆäº‰è®ºï¼›éœ€è¦èƒ½é‡åŒ–åŒºåˆ†å½“ä»£ä¸“ç”¨AIä¸äººç±»çº§é€šç”¨æ™ºèƒ½å·®è·çš„æ¡†æ¶ï¼ˆè§è®ºæ–‡ç¬¬1â€“2é¡µå¯¹å®šä¹‰ä¸ç›®æ ‡çš„é˜è¿°ï¼‰ã€‚ â€¢ ç°æœ‰è¯„æµ‹å¤šé›†ä¸­äºçª„åŸŸä»»åŠ¡ä¸è‡ªåŠ¨åŒ–æ•°æ®é›†ï¼Œæ˜“å—æ•°æ®æ±¡æŸ“ä¸æç¤ºå·¥ç¨‹å½±å“ï¼Œéš¾ä»¥æµ‹åˆ°åº•å±‚è®¤çŸ¥æœºåˆ¶ä¸å¤šæ¨¡æ€èƒ½åŠ›ï¼ˆç¬¬13â€“15é¡µâ€œContaminationâ€â€œSolving the Dataset vs. Solving the Taskâ€ï¼‰ã€‚ â€¢ å•ä¸€æ€»åˆ†æˆ–ä»»åŠ¡æˆåŠŸç‡æ©ç›–â€œé”¯é½¿çŠ¶â€èƒ½åŠ›ç»“æ„ä¸å…³é”®ç“¶é¢ˆï¼Œè¯¯å¯¼AGIè¿›å±•åˆ¤æ–­ï¼›å°¤å…¶é•¿æœŸè®°å¿†å­˜å‚¨ï¼ˆMSï¼‰â‰ˆ0%ã€è§†è§‰æ¨ç†ä¸é€Ÿåº¦ç­‰çŸ­æ¿ä¸»å¯¼ç³»ç»Ÿâ€œé©¬åŠ›â€ï¼ˆç¬¬13â€“14é¡µä¸å›¾3ï¼‰ã€‚ â€¢ å·¥ç¨‹ä¸Šé€šè¿‡è¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆWMï¼‰å’ŒRAGå¤–éƒ¨æ£€ç´¢â€œèƒ½åŠ›è¡¥å¿â€æ©ç›–å†…éƒ¨é•¿æœŸå­˜å‚¨ä¸é«˜ç²¾åº¦æ£€ç´¢ç¼ºé™·ï¼Œåˆ¶é€ é€šç”¨æ€§é”™è§‰ï¼ˆç¬¬13é¡µâ€œCapability Contortionsâ€ï¼‰ã€‚ â€¢ ä»¥ç»æµäº§å‡ºæˆ–å•ä¸€è¡Œä¸ºæµ‹è¯•å®šä¹‰AGIï¼ˆå¦‚â€œç›ˆåˆ©å®šä¹‰â€â€œå›¾çµå¼â€ï¼‰ä¸èƒ½ç›´æ¥æ˜ å°„åˆ°äººç±»è®¤çŸ¥å¹¿åº¦ä¸ç†Ÿç»ƒåº¦ï¼ŒäºŸéœ€ä¸å¿ƒç†æµ‹é‡å­¦å¯¹é½çš„è®¤çŸ¥ç»´åº¦åŒ–æµ‹åº¦ï¼ˆç¬¬15â€“16é¡µâ€œDefinitions of Related Conceptsâ€ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>åŸºäºCHCç†è®ºæ„å»ºæ¶µç›–10å¤§è®¤çŸ¥åŸŸã€æ•°åé¡¹çª„èƒ½åŠ›çš„å¤šæ¨¡æ€è¯„æµ‹è“å›¾ï¼Œç­‰æƒèšåˆä¸º0â€“100%çš„â€œAGI Scoreâ€ï¼Œå¹¶è¾“å‡ºæŒ‰åŸŸåˆ†è§£çš„â€œè®¤çŸ¥å‰–é¢â€ã€‚æ–¹æ³•ä»¥äººç±»å¿ƒç†æµ‹é‡ç”µæ± æ”¹é€ ä¸ºä»»åŠ¡é›†åˆï¼Œç»“åˆç°æœ‰åŸºå‡†ä¸äººå·¥åˆ¤åˆ†ã€è·¨åˆ†å¸ƒå¤è¿°å’Œç¦ç”¨å¤–éƒ¨å·¥å…·ç­‰è®¾ç½®æ¥æå‡ç¨³å¥æ€§ä¸æŠ—æ•°æ®æ±¡æŸ“ï¼ˆç¬¬2â€“12ã€22â€“57é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ é¢å‘AGIçš„æŒç»­åŒ–é•¿æœŸè®°å¿†ï¼šä»å‚æ•°åŒ–æ›´æ–°åˆ°å¤–æ¥æ¨¡å—çš„ç»Ÿä¸€æ¡†æ¶ï¼šè®¾è®¡å¯æŒä¹…å†™å…¥ä¸å·©å›ºçš„è®°å¿†ç³»ç»Ÿï¼ˆå…³è”/è¯­ä¹‰/é€å­—ï¼‰ï¼Œå¹¶ç»™å‡ºè·¨ä¼šè¯ã€å»¶æ—¶48å°æ—¶çš„æ ‡å‡†åŒ–è¯„æµ‹åè®®ï¼ˆå¯¹æ ‡MSçš„0%ç“¶é¢ˆï¼‰ã€‚ â€¢ ä»èƒ½åŠ›è¡¥å¿åˆ°çœŸå®èƒ½åŠ›ï¼šæ‹†è§£RAGä¸è¶…é•¿ä¸Šä¸‹æ–‡å¯¹è¯„æµ‹çš„åç½®ï¼šåŒºåˆ†å†…éƒ¨æ£€ç´¢ç²¾åº¦ä¸å¤–éƒ¨æ£€ç´¢ä¾èµ–ï¼Œç³»ç»Ÿé‡åŒ–å¹»è§‰ç‡ã€ä¸ªæ€§åŒ–ä¿æŒä¸ä¼šè¯è·¨æ—¶ä¸€è‡´æ€§ï¼Œé¿å…â€œèƒ½åŠ›æ‰­æ›²â€è¯¯åˆ¤ï¼ˆå¯¹æ ‡MRä¸WMï¼‰ã€‚ â€¢ ç»Ÿä¸€å¤šæ¨¡æ€ç©ºé—´è®¤çŸ¥ä¸è§†è§‰æ¨ç†è¯„æµ‹ï¼šæ„å»ºæ¶µç›–é•¿è§†é¢‘ç†è§£ã€ç©ºé—´å¯¼èˆªã€ç›´è§‰ç‰©ç†ä¸å…·èº«æ¨ç†çš„ä¸€ä½“åŒ–åŸºå‡†ä¸è®­ç»ƒæ–¹æ¡ˆï¼Œåˆ»ç”»Vã€WMã€Rçš„è€¦åˆå¹¶æå‡è§†è§‰æ¨ç†ä¸å¯¼èˆªèƒ½åŠ›ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20206" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20206" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Prompt design plays a crucial role in text-to-video (T2V) generation, yet user-provided prompts are often short, unstructured, and misaligned with training data, limiting the generative potential of diffusion-based T2V models. We present RAPO++, a cross-stage prompt optimization framework that unifies training-data--aligned refinement, test-time iterative scaling, and large language model (LLM) fine-tuning to substantially improve T2V generation without modifying the underlying generative backbone. In Stage 1, Retrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with semantically relevant modifiers retrieved from a relation graph and refactors them to match training distributions, enhancing compositionality and multi-object fidelity. Stage 2 introduces Sample-Specific Prompt Optimization (SSPO), a closed-loop mechanism that iteratively refines prompts using multi-source feedback -- including semantic alignment, spatial fidelity, temporal coherence, and task-specific signals such as optical flow -- yielding progressively improved video generation quality. Stage 3 leverages optimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing task-specific optimization patterns and enabling efficient, high-quality prompt generation even before inference. Extensive experiments across five state-of-the-art T2V models and five benchmarks demonstrate that RAPO++ achieves significant gains in semantic alignment, compositional reasoning, temporal stability, and physical plausibility, outperforming existing methods by large margins. Our results highlight RAPO++ as a model-agnostic, cost-efficient, and scalable solution that sets a new standard for prompt optimization in T2V generation. The code is available at https://github.com/Vchitect/RAPO.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç”¨æˆ·æç¤ºå¾€å¾€çŸ­ã€éç»“æ„åŒ–ä¸”ä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸ä¸€è‡´ï¼Œé™åˆ¶äº†æ‰©æ•£å¼T2Væ¨¡å‹åœ¨è¯­ä¹‰å¯¹é½ã€ç»„åˆæ€§ä¸å¤šç›®æ ‡ä¿çœŸæ–¹é¢çš„æ½œåŠ›ã€‚ â€¢ ç°æœ‰T2Iæç¤ºä¼˜åŒ–æˆ–LLMé‡å†™ä¾§é‡ç©ºé—´ç¾å­¦ï¼Œéš¾ä»¥æå‡è§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§ã€è¿åŠ¨å¹³æ»‘ä¸ç‰©ç†å¯è¡Œæ€§ï¼›T2Vä¸­çš„æç¤ºå·¥ç¨‹å¤šä¸ºæ¨¡å‹ç‰¹å®šï¼Œæ³›åŒ–æ€§å¼±ã€‚ â€¢ RLHFå¼æç¤ºä¼˜åŒ–åœ¨T2Vä¸Šæ¨ç†æˆæœ¬é«˜ã€éœ€è¦å¤§é‡ç”Ÿæˆå›æ»šä»¥ä¼°è®¡å¥–åŠ±ï¼Œéš¾ä»¥æ‰©å±•åˆ°è§†é¢‘ï¼›ç¼ºä¹åœ¨ä¸æ”¹åŠ¨ç”Ÿæˆå™¨æƒé‡ä¸‹æå‡è´¨é‡çš„é€šç”¨æ–¹æ¡ˆã€‚ â€¢ æµ‹è¯•æ—¶æ‰©å±•ç”¨äºâ€œè¿­ä»£æç¤ºä¼˜åŒ–â€çš„ç³»ç»ŸåŒ–æ–¹æ³•ä¸è¶³ï¼Œå¤šæºåé¦ˆï¼ˆè¯­ä¹‰ã€ç©ºé—´ã€æ—¶é—´ã€ç‰©ç†ï¼‰æœªè¢«ç»Ÿä¸€è€Œé«˜æ•ˆåœ°åˆ©ç”¨ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºRAPO++ä¸‰é˜¶æ®µæ¡†æ¶ï¼šé˜¶æ®µ1ä»¥æ£€ç´¢å¢å¼ºï¼ˆå…³ç³»å›¾ï¼‰+å¥å¼é‡æ„å¯¹é½è®­ç»ƒåˆ†å¸ƒï¼Œå¹¶ç”±åˆ¤åˆ«å™¨åœ¨å€™é€‰ä¸­é€‰ä¼˜ï¼›é˜¶æ®µ2åœ¨æ¨ç†æ—¶åˆ©ç”¨VLMæ ¡éªŒã€ç©ºé—´/æ—¶é—´/ç‰©ç†æŒ‡æ ‡çš„å¤šæºåé¦ˆé—­ç¯è¿­ä»£é‡å†™æç¤ºï¼›é˜¶æ®µ3å°†è¿­ä»£å¾—åˆ°çš„ä¼˜é€‰æç¤ºå¯¹ç”¨äºæŒ‡ä»¤å¾®è°ƒé‡å†™LLMï¼Œå†…åŒ–ä¼˜åŒ–è§„å¾‹ï¼Œå®ç°æ¨¡å‹æ— å…³ã€ä½æˆæœ¬çš„T2Vè´¨é‡æå‡ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è½»é‡åŒ–SSPOï¼šé¢å‘å®æ—¶è§†é¢‘ç”Ÿæˆçš„ä½å»¶è¿Ÿè¯„ä¼°å™¨ä¸è‡ªé€‚åº”è®¡ç®—é¢„ç®—ï¼šå­¦ä¹ æ›¿ä»£è¯„ä¼°å™¨ä¸åŠ¨æ€åœæ­¢å‡†åˆ™ï¼Œé™ä½è¿­ä»£é‡å†™ä¸éªŒè¯çš„æ¨ç†å¼€é”€ã€‚ â€¢ ç‰©ç†å…ˆéªŒé©±åŠ¨çš„æç¤ºä¼˜åŒ–ï¼šå°†å¯å¾®ç‰©ç†ä¸å…‰æµ/ç¢°æ’çº¦æŸèå…¥æµ‹è¯•æ—¶é—­ç¯ï¼šæŠŠç‰©ç†å¼•æ“æˆ–å¯å­¦ä¹ ç‰©ç†ä»£ç†çº³å…¥åé¦ˆï¼Œå¼ºåŒ–è¿åŠ¨è§„å¾‹ä¸å› æœä¸€è‡´æ€§ã€‚ â€¢ è·¨æ¨¡å‹å…ƒé‡å†™å™¨ï¼šé¢å‘å¤šT2Véª¨å¹²çš„å°‘æ ·æœ¬è‡ªé€‚åº”æç¤ºä¼˜åŒ–ï¼šé€šè¿‡å…ƒå­¦ä¹ /å‚æ•°é«˜æ•ˆè°ƒä¼˜ï¼Œä½¿é‡å†™å™¨å¿«é€Ÿè¿ç§»åˆ°æ–°è§†é¢‘ç”Ÿæˆå™¨ä¸æ•°æ®åŸŸã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">WorldGrow: Generating Infinite 3D World</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21682" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21682" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We tackle the challenge of generating the infinitely extendable 3D world -- large, continuous environments with coherent geometry and realistic appearance. Existing methods face key challenges: 2D-lifting approaches suffer from geometric and appearance inconsistencies across views, 3D implicit representations are hard to scale up, and current 3D foundation models are mostly object-centric, limiting their applicability to scene-level generation. Our key insight is leveraging strong generation priors from pre-trained 3D models for structured scene block generation. To this end, we propose WorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our method features three core components: (1) a data curation pipeline that extracts high-quality scene blocks for training, making the 3D structured latent representations suitable for scene generation; (2) a 3D block inpainting mechanism that enables context-aware scene extension; and (3) a coarse-to-fine generation strategy that ensures both global layout plausibility and local geometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset, WorldGrow achieves SOTA performance in geometry reconstruction, while uniquely supporting infinite scene generation with photorealistic and structurally consistent outputs. These results highlight its capability for constructing large-scale virtual environments and potential for building future world models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç›®æ ‡é—®é¢˜ï¼šå¦‚ä½•ç”Ÿæˆå¯æ— é™æ‰©å±•ã€å…¨å±€è¿è´¯ä¸”å¤–è§‚é€¼çœŸçš„3Dä¸–ç•Œï¼ˆå¤§å°ºåº¦è¿ç»­ç¯å¢ƒï¼‰ â€¢ é‡è¦æ€§ï¼šä¸ºæ¸¸æˆã€VR/ARã€CADã€å½±è§†æä¾›å¤§è§„æ¨¡è™šæ‹Ÿç¯å¢ƒåŸºåº§ï¼Œæ›´å…³é”®åœ°ä¸ºWorld Modelsä¸å…·èº«æ™ºèƒ½æä¾›å¯æŒç»­æ‰©å±•çš„å¼€æ”¾å¼å­¦ä¹ åœºæ™¯ â€¢ ç°æœ‰2Dâ†’3Dæå‡æ–¹æ³•å±€é™ï¼šåŸºäº2Dæ‰©æ•£å›¾åƒå†â€œæŠ¬å‡â€åˆ°3Dï¼Œè·¨è§†è§’å‡ ä½•ä¸å¤–è§‚ä¸ä¸€è‡´ã€æ˜“å¤±çœŸï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤§åœºæ™¯ â€¢ ç°æœ‰3Dç”Ÿæˆæ–¹æ³•å±€é™ï¼šéšå¼è¡¨ç¤º/ä¸‰å¹³é¢ç­‰å—é™äºåœºæ™¯çº§æ•°æ®è§„æ¨¡ä¸å¤šæ ·æ€§ã€éš¾ä»¥æ³›åŒ–ï¼Œä¸”å¤šç¼ºä¹æ˜¾å¼çº¹ç†å»ºæ¨¡ï¼ˆå¸¸éœ€é¢å¤–è´´å›¾ç®¡çº¿ï¼‰ â€¢ 3DåŸºç¡€æ¨¡å‹å±€é™ï¼šå¤šä¸ºç‰©ä½“çº§å…ˆéªŒï¼Œç›´æ¥ç”¨äºåœºæ™¯çº§ç”Ÿæˆç¼ºä¹ä¸Šä¸‹æ–‡ä¸å—é—´è¡”æ¥èƒ½åŠ› â€¢ å—çº§ç”Ÿé•¿æŒ‘æˆ˜ï¼šå¦‚ä½•è¿ç§»ç‰©ä½“å…ˆéªŒåˆ°â€œåœºæ™¯å—â€ã€ä¿è¯ç›¸é‚»å—å‡ ä½•/é£æ ¼/çº¹ç†æ— ç¼è¡”æ¥ï¼Œå¹¶åœ¨å…¨å±€å¸ƒå±€åˆç†æ€§ä¸å±€éƒ¨ç»†èŠ‚ä¿çœŸä¹‹é—´å–å¾—å¹³è¡¡</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºWorldGrowï¼šä»¥å—ä¸ºå•ä½çš„å±‚çº§å¼æ— é™åœºæ™¯ç”Ÿæˆæ¡†æ¶ï¼ŒåŒ…å«æ•°æ®ç­›é€‰æ„å»ºé«˜è´¨é‡åœºæ™¯å—ã€å°†SLATæ”¹é€ ä¸ºâ€œåœºæ™¯å‹å¥½â€ï¼ˆé®æŒ¡æ„ŸçŸ¥ç‰¹å¾èšåˆ+åŸºäºåœºæ™¯å—é‡è®­è§£ç å™¨ï¼‰ã€åŸºäºæµåŒ¹é…çš„ç»“æ„/æ½œå˜é‡åŒé˜¶æ®µ3Då—ä¿®å¤ï¼ˆinpaintingï¼‰ï¼Œå¹¶é‡‡ç”¨ç²—åˆ°ç»†ç­–ç•¥å…ˆé“ºè®¾å…¨å±€å¸ƒå±€åç»†åŒ–å‡ ä½•ä¸å¤–è§‚ï¼Œæœ€ç»ˆè§£ç ä¸ºå¯æ¸²æŸ“çš„3Dä¸–ç•Œã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ WorldGrow-Zï¼šé¢å‘å¤šæ¥¼å±‚çš„å‚ç›´å¯æ‰©å±•æ— é™3Dä¸–ç•Œç”Ÿæˆâ€”â€”å°†å—çº§ç”Ÿé•¿ä»XYå¹³é¢æ‰©å±•åˆ°Zè½´ï¼Œæ”¯æŒå¤šå±‚å»ºç­‘ä¸ç«‹ä½“è¿é€š â€¢ SemWorldGrowï¼šåŸºäºLLMè¯­ä¹‰çº¦æŸçš„å¯æ§æ— é™åœºæ™¯ç”Ÿæˆâ€”â€”ç”¨æ–‡æœ¬/è§„åˆ™æ§åˆ¶æˆ¿é—´ç±»å‹ã€ç›¸é‚»å…³ç³»ä¸é£æ ¼ï¼Œå®ç°è¯­ä¹‰ä¸€è‡´çš„å¯ç¼–æ’ä¸–ç•Œ â€¢ UniWorldGrowï¼šå‡ ä½•-çº¹ç†ç»Ÿä¸€æ½œå˜é‡çš„å•é˜¶æ®µæ— é™åœºæ™¯ç”Ÿæˆâ€”â€”èåˆå‡ ä½•ä¸å¤–è§‚æ½œç©ºé—´ï¼ˆå¦‚ç»Ÿä¸€SLATï¼‰ä»¥æå‡æ•ˆç‡ä¸ä¸€è‡´æ€§</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Model Merging with Functional Dual Anchors</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21223" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21223" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Model merging is an efficient post-training strategy for integrating knowledge from multiple finetuned checkpoints of a shared foundation model. Existing methods operate in the parameter space, combining task vectors to mitigate conflicts, but remain constrained by parameter inconsistencies. We propose Functional Dual Anchors (FDAs), a framework that instead models the input-representation space. FDAs are synthetic inputs whose induced gradients align with task vectors, capturing task-specific functional shifts relative to the pretrained model. This perspective bridges joint multi-task training and post-hoc merging, offering both robustness and flexibility. We further introduce a principled initialization scheme and show that FDAs are complementary to parameter-space model merging. Comprehensive experiments demonstrate the effectiveness of FDAs in model merging.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šå¦‚ä½•å°†åŒä¸€é¢„è®­ç»ƒéª¨å¹²å¾®è°ƒå¾—åˆ°çš„å¤šä»»åŠ¡/å¤šé¢†åŸŸæ£€æŸ¥ç‚¹é«˜æ•ˆã€æ•°æ®æ— å…³åœ°åˆå¹¶ä¸ºä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ï¼ŒåŒæ—¶ç¼“è§£ä»»åŠ¡é—´çŸ¥è¯†å†²çªï¼ˆç¬¬1é¡µï¼‰ã€‚ â€¢ é‡è¦æ€§ï¼šç›¸æ¯”å¤šä»»åŠ¡è”åˆè®­ç»ƒä¸æŒç»­å­¦ä¹ ï¼Œæ¨¡å‹åˆå¹¶æ˜¯ä¸€ç§åè®­ç»ƒã€æ— éœ€åŸå§‹æ•°æ®çš„çŸ¥è¯†æ•´åˆæ–¹å¼ï¼Œé€‚ç”¨äºæ•°æ®éšç§ä¸å¤§æ¨¡å‹è®­ç»ƒæˆæœ¬å—é™åœºæ™¯ï¼ˆç¬¬1é¡µï¼‰ã€‚ â€¢ ç°æœ‰å±€é™ï¼šä¸»æµæ–¹æ³•åœ¨å‚æ•°ç©ºé—´å¯¹ä»»åŠ¡å‘é‡åšç¼©æ”¾/æ­£äº¤/å­ç©ºé—´ç­‰æ“ä½œï¼Œå—å‚æ•°ä¸ä¸€è‡´ä¸åˆå§‹åŒ–æ•æ„Ÿå½±å“ï¼Œæä¾›çš„æ˜¯ä»é¢„è®­ç»ƒç‚¹å‡ºå‘çš„å›ºå®šçº¿æ€§è·¯å¾„ï¼Œæ˜“åç¦»çœŸå®æŸå¤±ç›†åœ°ï¼ˆå›¾2ï¼Œé¡µ2ï¼›ç¬¬1-2é¡µï¼‰ï¼›éƒ¨åˆ†æ–¹æ³•è¿˜éœ€ä¾èµ–ä»»åŠ¡æ•°æ®æˆ–å¯å‘å¼å…ˆéªŒï¼Œé²æ£’æ€§ä¸å¯æ‰©å±•æ€§å—é™ï¼ˆç¬¬2-3é¡µï¼‰ã€‚ â€¢ å…³é”®æ´è§ï¼šè¾“å…¥-è¡¨å¾ç©ºé—´æ¯”å‚æ•°ç©ºé—´æ›´ç»“æ„åŒ–ã€æ˜“å»ºæ¨¡ï¼ˆä¸æ•°æ®è’¸é¦ç­‰å·¥ä½œä¸€è‡´ï¼‰ï¼Œè‹¥èƒ½åœ¨è¾“å…¥ç©ºé—´â€œæŠ•å½±â€ä»»åŠ¡å‘é‡æ‰€ä»£è¡¨çš„åŠŸèƒ½è¿ç§»ï¼Œå¯æ›´ç¨³å¥åœ°å¤ç°è”åˆè®­ç»ƒæ•ˆæœï¼ˆç¬¬2é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºåŠŸèƒ½åŒé”šï¼ˆFDAsï¼‰ï¼šä¸ºæ¯ä¸ªä¸‹æ¸¸æ¨¡å‹åœ¨è¾“å…¥-è¡¨å¾ç©ºé—´æ„é€ ä¸€ç»„åˆæˆé”šç‚¹ï¼Œä½¿å…¶åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸Šè¯±å¯¼çš„æ¢¯åº¦æ–¹å‘ä¸å¯¹åº”ä»»åŠ¡å‘é‡å¯¹é½ï¼ˆå¼1ï¼‰ï¼›éšåç”¨è¿™äº›é”šç‚¹æœ€å°åŒ–é¢„è®­ç»ƒæ¨¡å‹ä¸å„ä¸‹æ¸¸æ¨¡å‹åœ¨é”šç‚¹å¤„çš„è¡¨ç¤ºå·®å¼‚æ¥ä¼˜åŒ–å‚æ•°ï¼ˆå¼5ï¼‰ï¼Œæˆ–ä½œä¸ºç²¾ç‚¼å™¨æ”¹è¿›å·²æœ‰çš„å‚æ•°ç©ºé—´åˆå¹¶ç»“æœï¼ˆå¼6ï¼‰ï¼Œé‡‡ç”¨å±‚çº§å®ç°ä¸ä¸¤ç§åˆå§‹åŒ–ï¼ˆæƒé‡é‡‡æ ·/ç¼©æ”¾é«˜æ–¯ï¼‰ä»¥æå‡æ”¶æ•›ä¸é²æ£’æ€§ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è‡ªé€‚åº”å±‚ä¸é”šç‚¹é¢„ç®—æœç´¢çš„FDAåˆå¹¶ï¼šè‡ªåŠ¨ç¡®å®šå„å±‚é”šç‚¹æ•°é‡ã€tokené•¿åº¦ä¸ç¼©æ”¾ç³»æ•°Ïƒï¼Œå…¼é¡¾æ”¶æ•›é€Ÿåº¦ä¸è®¡ç®—å¼€é”€ã€‚ â€¢ å‚æ•°-è¾“å…¥åŒç©ºé—´ååŒçš„ç«¯åˆ°ç«¯åˆå¹¶ï¼šè”åˆä¼˜åŒ–ä»»åŠ¡å‘é‡è°ƒæ•´ä¸FDAæ„é€ /é€‚é…ï¼Œåœ¨åŒä¸€ç›®æ ‡ä¸‹å­¦ä¹ ä»»åŠ¡æƒé‡ä¸è¡¨ç¤ºå¯¹é½ã€‚ â€¢ å¤šæ¨¡æ€ä¸è”é‚¦åœºæ™¯ä¸‹çš„éšç§å‹å¥½FDAåˆå¹¶ï¼šå°†FDAæ‰©å±•è‡³å›¾æ–‡/è¯­éŸ³ç­‰å¤šæ¨¡æ€ä¸è”é‚¦å­¦ä¹ ç¯å¢ƒï¼Œåœ¨æ— æºæ•°æ®æ¡ä»¶ä¸‹å®ç°ç¨³å¥åˆå¹¶ä¸éšç§ä¿éšœã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Visual Diffusion Models are Geometric Solvers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21697" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21697" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In this paper we show that visual diffusion models can serve as effective geometric solvers: they can directly reason about geometric problems by working in pixel space. We first demonstrate this on the Inscribed Square Problem, a long-standing problem in geometry that asks whether every Jordan curve contains four points forming a square. We then extend the approach to two other well-known hard geometric problems: the Steiner Tree Problem and the Simple Polygon Problem. Our method treats each problem instance as an image and trains a standard visual diffusion model that transforms Gaussian noise into an image representing a valid approximate solution that closely matches the exact one. The model learns to transform noisy geometric structures into correct configurations, effectively recasting geometric reasoning as image generation. Unlike prior work that necessitates specialized architectures and domain-specific adaptations when applying diffusion to parametric geometric representations, we employ a standard visual diffusion model that operates on the visual representation of the problem. This simplicity highlights a surprising bridge between generative modeling and geometric problem solving. Beyond the specific problems studied here, our results point toward a broader paradigm: operating in image space provides a general and practical framework for approximating notoriously hard problems, and opens the door to tackling a far wider class of challenging geometric tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ éœ€è¦ä¸€ä¸ªè·¨ä»»åŠ¡ã€ç»Ÿä¸€ä¸”ç®€å•çš„æ¡†æ¶æ¥è¿‘ä¼¼æ±‚è§£éš¾å‡ ä½•é—®é¢˜ï¼ˆå¦‚å†…æ¥æ­£æ–¹å½¢ã€æ¬§æ°Steineræ ‘ã€æœ€å¤§é¢ç§¯å¤šè¾¹å½¢ï¼‰ï¼Œè€Œä¸æ˜¯ä¸ºæ¯ä¸ªé—®é¢˜å•ç‹¬è®¾è®¡å¤æ‚çš„ç¬¦å·/å›¾è¡¨ç¤ºä¸ä¸“ç”¨ç®—æ³•ã€‚ â€¢ ä¼ ç»Ÿç®—æ³•å¯¹è¿™äº›é—®é¢˜è¦ä¹ˆæœ¬è´¨NP-hardæˆ–å°šæ— æ™®é€‚è§£ï¼ˆå¦‚å†…æ¥æ­£æ–¹å½¢çŒœæƒ³ï¼‰ï¼Œå®ç°å¤æ‚ã€å¯¹è§„æ¨¡æ•æ„Ÿï¼Œä¸”éš¾ä»¥é«˜æ•ˆäº§ç”Ÿå¤šæ ·è§£ï¼›ç°æœ‰æ‰©æ•£/å­¦ä¹ æ–¹æ³•å¤šåœ¨å‚æ•°æˆ–å›¾ç©ºé—´å»ºæ¨¡ï¼Œä¾èµ–ç¦»æ•£åŒ–å’Œä¸“ç”¨æ¶æ„ï¼Œè¿ç§»æ€§ä¸é€šç”¨æ€§ä¸è¶³ã€‚ â€¢ äºŸéœ€éªŒè¯ï¼šèƒ½å¦å°†â€œå‡ ä½•çº¦æŸæ»¡è¶³â€è§†ä¸ºå›¾åƒåˆ†å¸ƒï¼Œç”¨æ ‡å‡†è§†è§‰æ‰©æ•£æ¨¡å‹åœ¨åƒç´ åŸŸç›´æ¥è¿›è¡Œå‡ ä½•æ¨ç†ï¼Œä»¥æ›´ä½å·¥ç¨‹å¤æ‚åº¦è·å¾—é«˜è´¨é‡è¿‘ä¼¼è§£å¹¶è‡ªç„¶æ”¯æŒä¸€é¢˜å¤šè§£ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>å°†æ¯ä¸ªå‡ ä½•å®ä¾‹æ¸²æŸ“ä¸ºæ¡ä»¶å›¾åƒï¼ˆå¦‚æ›²çº¿æˆ–ç‚¹é›†é€šé“ï¼‰ï¼Œç”¨æ ‡å‡†æ¡ä»¶è§†è§‰æ‰©æ•£æ¨¡å‹ï¼ˆU-Netï¼‰ä»å™ªå£°ç”Ÿæˆè¡¨ç¤ºå¯è¡Œè§£çš„å›¾åƒï¼›è®­ç»ƒç”¨ç¨‹åºåŒ–åˆæˆçš„å¤§è§„æ¨¡â€œå®ä¾‹-æœ‰æ•ˆè§£â€å¯¹ï¼Œæ¨ç†é€šè¿‡å¤šç§å­é‡‡æ ·å¹¶é…åˆè½»é‡åå¤„ç†ï¼ˆå¦‚é¡¶ç‚¹snappingã€èŠ‚ç‚¹/è¾¹æå–ä¸åˆæ³•æ€§æ ¡éªŒï¼‰å¾—åˆ°ç»“æ„åŒ–è§£ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ä»åƒç´ åˆ°å‚æ•°ï¼šè§†è§‰æ‰©æ•£è§£çš„å¯å¾®å‡ ä½•ç²¾åŒ–ï¼šåœ¨é‡‡æ ·åå åŠ å¯å¾®å‡ ä½•ä¼˜åŒ–/è¿ç»­å‚æ•°åŒ–ï¼ˆå¦‚è§’åº¦ã€é¡¶ç‚¹ä½ç½®ï¼‰ä»¥çªç ´åƒç´ ç¦»æ•£è¯¯å·®å¹¶æå‡å¯è¡Œæ€§ä¸æœ€ä¼˜æ€§ã€‚ â€¢ èƒ½é‡å¼•å¯¼çš„æ— ç›‘ç£å‡ ä½•æ‰©æ•£æ±‚è§£ï¼šç»“åˆT2T/èƒ½é‡å¼•å¯¼ï¼Œåœ¨æ— GTæœ€ä¼˜è§£æˆ–ä»…æœ‰å¯è¡Œæ€§åˆ¤å®šçš„åœºæ™¯ä¸­ï¼Œä»¥ç›®æ ‡èƒ½é‡ï¼ˆé•¿åº¦/é¢ç§¯/çº¦æŸï¼‰åœ¨æµ‹è¯•æ—¶å¼•å¯¼æ‰©æ•£æœç´¢ã€‚ â€¢ è·¨ä»»åŠ¡ä¸å°ºåº¦æ³›åŒ–çš„ç»Ÿä¸€å‡ ä½•æ‰©æ•£æ¨¡å‹ï¼šå•æ¨¡å‹å¤šä»»åŠ¡è”åˆè®­ç»ƒä¸æ¡ä»¶ç»„åˆï¼ˆæ›²çº¿+ç‚¹/å¤šçº¦æŸï¼‰ï¼Œé…åˆåˆ†å±‚é‡‡æ ·ä¸ç¨€ç–æ³¨æ„åŠ›ï¼Œå®ç°ä»å°è§„æ¨¡åˆ°å¤§è§„æ¨¡å®ä¾‹çš„ç¨³å¥å¤–æ¨ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21652" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21652" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>AI agents hold the potential to revolutionize scientific productivity by automating literature reviews, replicating experiments, analyzing data, and even proposing new directions of inquiry; indeed, there are now many such agents, ranging from general-purpose "deep research" systems to specialized science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of these agents is critical for progress. Yet existing benchmarks fall short on several fronts: they (1) fail to provide holistic, product-informed measures of real-world use cases such as science research; (2) lack reproducible agent tools necessary for a controlled comparison of core agentic capabilities; (3) do not account for confounding variables such as model cost and tool access; (4) do not provide standardized interfaces for quick agent prototyping and evaluation; and (5) lack comprehensive baseline agents necessary to identify true advances. In response, we define principles and tooling for more rigorously benchmarking agents. Using these, we present AstaBench, a suite that provides the first holistic measure of agentic ability to perform scientific research, comprising 2400+ problems spanning the entire scientific discovery process and multiple scientific domains, and including many problems inspired by actual user requests to deployed Asta agents. Our suite comes with the first scientific research environment with production-grade search tools that enable controlled, reproducible evaluation, better accounting for confounders. Alongside, we provide a comprehensive suite of nine science-optimized classes of Asta agents and numerous baselines. Our extensive evaluation of 57 agents across 22 agent classes reveals several interesting findings, most importantly that despite meaningful progress on certain individual aspects, AI remains far from solving the challenge of science research assistance.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç§‘ç ”åŠ©ç†ç±»AIä»£ç†å¿«é€Ÿæ¶Œç°ä½†ç¼ºä¹ç»Ÿä¸€ã€å¯å¤ç°ã€å¯æ¯”çš„è¯„æµ‹æ¡†æ¶ï¼Œç”¨æˆ·ä¸å¼€å‘è€…éš¾ä»¥åˆ¤æ–­ä½•è€…æ›´ä¼˜ â€¢ ç°æœ‰åŸºå‡†ç¼ºå°‘è¦†ç›–â€œæ–‡çŒ®ç†è§£â€”ä»£ç ä¸æ‰§è¡Œâ€”æ•°æ®åˆ†æâ€”ç«¯åˆ°ç«¯å‘ç°â€çš„å…¨æµç¨‹ç§‘å­¦ä»»åŠ¡ï¼Œä¸”å¾ˆå°‘åŸºäºçœŸå®äº§å“ä½¿ç”¨éœ€æ±‚ â€¢ ç¼ºä¹æ ‡å‡†åŒ–ã€å—æ§ä¸”å¯å›æ”¾çš„å·¥å…·ç¯å¢ƒï¼ˆå°¤å…¶å¤§è§„æ¨¡æ–‡çŒ®æ£€ç´¢ä¸ä»£ç æ²™ç®±ï¼‰ï¼Œéš¾ä»¥å°†ä¿¡æ¯å¯å¾—æ€§ä¸ä»£ç†èƒ½åŠ›è§£è€¦ â€¢ è¯„åˆ†æœªç³»ç»Ÿæ§åˆ¶æ··æ‚å› ç´ ï¼ˆè®¡ç®—æˆæœ¬ã€å·¥å…·/æ¨¡å‹å¼€æ”¾æ€§ç­‰ï¼‰ï¼Œé«˜åˆ†å¯èƒ½åªæ˜¯â€œçƒ§ç®—åŠ›â€ä¸ç‰¹æƒå·¥å…·å¸¦æ¥çš„å‡è±¡ â€¢ ä»»åŠ¡æ¥å£ä¸ç»Ÿä¸€ã€éš¾ä»¥å³æ’å³ç”¨åœ°è¯„æµ‹é€šç”¨å‹ä»£ç†ï¼ŒåŒæ—¶ç¼ºå°‘å…¨é¢å¼ºåŸºçº¿ï¼Œå¯¼è‡´â€œè¿›æ­¥â€éš¾ä»¥è¢«å®¢è§‚è¯†åˆ«</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºAstaBenchç§‘å­¦ç ”ç©¶åŸºå‡†å¥—ä»¶ï¼Œé…å¥—Asta Environmentï¼ˆå«æ—¥æœŸæˆªæ–­çš„Asta Scientific Corpusæ£€ç´¢ä¸çŠ¶æ€åŒ–è®¡ç®—Notebookï¼‰ï¼Œåœ¨ç»Ÿä¸€å·¥å…·ä¸æ—¶é—´ä¸å˜æˆæœ¬æ ¸ç®—ï¼ˆagent-evalLeaderboardï¼‰ä¸‹ï¼ŒæŒ‰Inspect/MCPæ ‡å‡†è¯„æµ‹ä»£ç†å¹¶ç”¨LLM-as-judgeä¸Paretoæ›²çº¿æŠ¥å‘Šè´¨-æˆæœ¬æƒè¡¡ã€‚åŒæ­¥å‘å¸ƒagent-baselinesæ ‡å‡†åŒ–ä»£ç†ï¼Œè¦†ç›–9ç±»Astaç§‘ç ”ä»£ç†ä¸å¤šç§é€šç”¨/å•†ç”¨åŸºçº¿ï¼Œåˆè®¡è¯„æµ‹57ä¸ªä»£ç†ï¼ˆ22ç±»æ¶æ„ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ é¢å‘è·¨è¡Œä¸šçš„å¯æ§é€šç”¨ä»£ç†åŸºå‡†ï¼šå°†å—æ§å·¥å…·ä¸æˆæœ¬è®°è´¦æ–¹æ³•ä»ç§‘å­¦ç ”ç©¶æ‰©å±•åˆ°åŒ»ç–—ã€æ³•å¾‹ä¸ä¼ä¸šçŸ¥è¯†åœºæ™¯ â€¢ å¯ä¿¡LLMè¯„å®¡æ¡†æ¶ï¼šå¤šè£åˆ¤é›†æˆã€è¯æ®å¼ºåˆ¶ä¸äººæœºæ··åˆå¤æ ¸ä»¥æå‡é•¿æ–‡ä¸ç«¯åˆ°ç«¯ä»»åŠ¡çš„åˆ¤åˆ†ä¸€è‡´æ€§ä¸å¯è¿½æº¯æ€§ â€¢ å¼€æ”¾æƒé‡ç§‘ç ”ä»£ç†çš„èƒ½åŠ›è¡¥é½ï¼šé¢å‘ä»£ç æ‰§è¡Œä¸æ•°æ®é©±åŠ¨å‘ç°çš„ä¸“ç”¨è®­ç»ƒã€å·¥å…·ç¼–æ’ä¸è’¸é¦ï¼Œç¼©å°ä¸é—­æºç³»ç»Ÿå·®è·</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21447" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21447" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Interactive world models that simulate object dynamics are crucial for robotics, VR, and AR. However, it remains a significant challenge to learn physics-consistent dynamics models from limited real-world video data, especially for deformable objects with spatially-varying physical properties. To overcome the challenge of data scarcity, we propose PhysWorld, a novel framework that utilizes a simulator to synthesize physically plausible and diverse demonstrations to learn efficient world models. Specifically, we first construct a physics-consistent digital twin within MPM simulator via constitutive model selection and global-to-local optimization of physical properties. Subsequently, we apply part-aware perturbations to the physical properties and generate various motion patterns for the digital twin, synthesizing extensive and diverse demonstrations. Finally, using these demonstrations, we train a lightweight GNN-based world model that is embedded with physical properties. The real video can be used to further refine the physical properties. PhysWorld achieves accurate and fast future predictions for various deformable objects, and also generalizes well to novel interactions. Experiments show that PhysWorld has competitive performance while enabling inference speeds 47 times faster than the recent state-of-the-art method, i.e., PhysTwin.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ éœ€è¦ä»çŸ­æ—¶é•¿çœŸå®è§†é¢‘å­¦ä¹ å¯äº¤äº’çš„å¯å˜å½¢ä½“ä¸–ç•Œæ¨¡å‹ï¼Œä½†æ•°æ®ç¨€ç¼ºã€è§‚æµ‹å•ä¸€ï¼Œéš¾ä»¥è¦†ç›–å¤šæ ·åŠ¨ä½œä¸ç‰©æ€§åˆ†å¸ƒ â€¢ çº¯å­¦ä¹ æ–¹æ³•ä¾èµ–æµ·é‡æ•°æ®ï¼Œå®¹æ˜“ä¸çœŸå®ç‰©ç†ä¸ä¸€è‡´ï¼Œä¸”å¤šé‡‡ç”¨å…¨å±€ç‰©æ€§ï¼Œéš¾ä»¥åˆ»ç”»ç©ºé—´å¼‚è´¨ææ–™ï¼ˆå¦‚å±€éƒ¨åˆšåº¦/å¯†åº¦å˜åŒ–ï¼‰ â€¢ ç‰©ç†ä»¿çœŸæ–¹æ³•ï¼ˆå¦‚MPM/MSSï¼‰ç‰©ç†é€¼çœŸä½†æ¨ç†å¼€é”€å¤§ã€éš¾ä»¥å®æ—¶ï¼Œæ— æ³•ç›´æ¥æ”¯æ’‘è§„åˆ’ä¸äº¤äº’ï¼ˆè®ºæ–‡è¡¨1æ˜¾ç¤ºç°æœ‰SOTAé€Ÿåº¦å—é™ï¼‰ â€¢ å‚æ•°è¯†åˆ«ä¸æ¨¡å‹é€‰æ‹©å›°éš¾ï¼šä»çŸ­è§†é¢‘é²æ£’ä¼°è®¡æœ¬æ„æ¨¡å‹ä¸ç‰©æ€§å®¹æ˜“é™·å…¥ä¸ç¨³å®šæˆ–åˆå€¼æ•æ„Ÿï¼Œå¯¼è‡´ä»¿çœŸ-çœŸå®åŸŸå·® â€¢ ç°æœ‰æ–¹æ³•åœ¨å¯¹æœªè§äº¤äº’çš„æ³›åŒ–ã€é•¿æ—¶é¢„æµ‹ç¨³å®šæ€§ä¸å›¾åƒé‡å»ºä¸€è‡´æ€§ä¸Šå­˜åœ¨æ˜æ˜¾ä¸è¶³</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>å…ˆä»¥VLMè¾…åŠ©é€‰æ‹©æœ¬æ„æ¨¡å‹å¹¶åœ¨MPMä¸­è¿›è¡Œâ€œå…¨å±€åˆ°å±€éƒ¨â€çš„ç‰©æ€§ä¼˜åŒ–ï¼Œæ„å»ºç‰©ç†ä¸€è‡´çš„æ•°å­—å­ªç”Ÿï¼›å†é€šè¿‡æ›²ç‡ä¸é€Ÿåº¦çº¦æŸçš„è½¨è¿¹ç”Ÿæˆå’Œéƒ¨ä»¶æ„ŸçŸ¥çš„ç‰©æ€§æ‰°åŠ¨åˆæˆå¤šæ ·æ¼”ç¤ºï¼Œç”¨å…¶è®­ç»ƒåµŒå…¥ç©ºé—´å¼‚è´¨ç‰©æ€§çš„è½»é‡GNNï¼Œå¹¶ç”¨çœŸå®è§†é¢‘å¾®è°ƒç‰©æ€§ä»¥ç¼©å°ä»¿çœŸ-çœŸå®åŸŸå·®ï¼Œå®ç°åŠ¨ä½œæ¡ä»¶çš„å¿«é€Ÿå‡†ç¡®é¢„æµ‹ä¸æ¸²æŸ“ï¼ˆç»“åˆ3DGS+LBSï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ç«¯åˆ°ç«¯è§†é¢‘-ç‰©æ€§è”åˆä¼°è®¡ä¸ä¸ç¡®å®šæ€§å»ºæ¨¡ï¼šå°†VLMé€‰æ‹©ã€æœ¬æ„è¯†åˆ«ã€ç‰©æ€§ä¼˜åŒ–ä¸GNNå­¦ä¹ ç»Ÿä¸€åˆ°å¯å¾®æ¡†æ¶ä¸­ï¼Œå¹¶æ˜¾å¼é‡åŒ–ç‰©æ€§ä¸åŠ¨åŠ›å­¦ä¸ç¡®å®šæ€§ â€¢ é¢å‘å¤šå¯¹è±¡ä¸å¤šç›¸ææ–™çš„ç»Ÿä¸€ä¸–ç•Œæ¨¡å‹ï¼šæ‰©å±•è‡³å¤šæ¥è§¦/è‡ªæ¥è§¦ã€åˆšè½¯è€¦åˆä¸æµå›ºè€¦åˆåœºæ™¯ï¼Œå­¦ä¹ è·¨ææ–™è¾¹ç•Œçš„äº¤äº’ä¸ä¼ è¾“æœºåˆ¶ â€¢ å¯å¾®ä¸–ç•Œæ¨¡å‹é©±åŠ¨çš„é—­ç¯æœºå™¨äººè§„åˆ’ï¼šå°†ç‰©æ€§æ¡ä»¶GNNåµŒå…¥MPC/MPPIï¼Œæ”¯æŒæ¢¯åº¦/é‡‡æ ·æ··åˆçš„å®‰å…¨çº¦æŸä¼˜åŒ–ï¼Œå®ç°å®æ—¶æŠ“å–ã€æŠ˜å ä¸æ•´å½¢ç­‰ä»»åŠ¡</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">ARC-Encoder: learning compressed text representations for large language models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20535" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20535" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent techniques such as retrieval-augmented generation or chain-of-thought reasoning have led to longer contexts and increased inference costs. Context compression techniques can reduce these costs, but the most effective approaches require fine-tuning the target model or even modifying its architecture. This can degrade its general abilities when not used for this specific purpose. Here we explore an alternative approach: an encoder that compresses the context into continuous representations which replace token embeddings in decoder LLMs. First, we perform a systematic study of training strategies and architecture choices for the encoder. Our findings led to the design of an Adaptable text Representations Compressor, named ARC-Encoder, which outputs x-times fewer continuous representations (typically x!in!{4,8}) than text tokens. We evaluate ARC-Encoder across a variety of LLM usage scenarios, ranging from in-context learning to context window extension, on both instruct and base decoders. Results show that ARC-Encoder achieves state-of-the-art performance on several benchmarks while improving computational efficiency at inference. Finally, we demonstrate that our models can be adapted to multiple decoders simultaneously, allowing a single encoder to generalize across different decoder LLMs. This makes ARC-Encoder a flexible and efficient solution for portable encoders that work seamlessly with multiple LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder , fine-tuning dataset and pretrained models are available at https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šRAGä¸CoTè®©LLMä¸Šä¸‹æ–‡æ€¥å‰§å˜é•¿ï¼Œæ³¨æ„åŠ›è®¡ç®—å¯¹é•¿åº¦å‘ˆäºŒæ¬¡å¤æ‚åº¦ï¼Œæ¨ç†æˆæœ¬ä¸æ—¶å»¶é£™å‡ï¼Œä¸”æ˜“è§¦è¾¾ä¸Šä¸‹æ–‡çª—å£ä¸Šé™å¹¶ç¨€é‡Šå…³é”®ä¿¡æ¯ã€‚ â€¢ ç°æœ‰ç¡¬å‹ç¼©å±€é™ï¼šåŸºäºåˆ é™¤/æ‘˜è¦çš„ç¡¬å‹ç¼©è™½æ¨¡å‹æ— å…³ã€å¯è§£é‡Šï¼Œä½†å‹ç¼©ç‡æœ‰é™ã€æ˜“ä¸¢å¤±è¯­ä¹‰è¦ç‚¹ï¼Œéš¾ä»¥åœ¨å¤šä»»åŠ¡ä¸Šç¨³å®šä¿æŒç²¾åº¦ã€‚ â€¢ ç°æœ‰è½¯å‹ç¼©å±€é™ï¼šè®°å¿†/æç¤ºå‘é‡ç­‰è½¯å‹ç¼©é€šå¸¸éœ€ä¸“ç”¨ç¼–ç å™¨å¹¶è”åŠ¨å¾®è°ƒæˆ–æ”¹é€ è§£ç å™¨ï¼Œç‰ºç‰²é€šç”¨æ€§ä¸å¯ç§»æ¤æ€§ï¼›ä¸”å¤šä¸ºå›ºå®šæ•°é‡çš„å‹ç¼©tokenï¼Œéš¾éšè¾“å…¥é•¿åº¦çµæ´»ç¼©æ”¾ï¼›ä»…åšé‡å»ºè®­ç»ƒæ˜“å¯¼è‡´â€œå¤è¿°ä¸Šä¸‹æ–‡â€è€Œéæç‚¼ä¿¡æ¯ã€‚ â€¢ è¯„æµ‹åå·®ä¸æ³›åŒ–ä¸è¶³ï¼šä¸å°‘å·¥ä½œåœ¨æŒ‡ä»¤æ¨¡å‹ä¸Šé›¶æ ·æœ¬è¯„æµ‹ï¼Œå¯èƒ½æ©ç›–çœŸå®çš„EMèƒ½åŠ›ï¼›è·¨è§£ç å™¨è¿ç§»å¼±ï¼Œéš¾åšåˆ°â€œä¸€æ¬¡è®­ç»ƒï¼Œå¤šæ¨¡å‹å¯ç”¨â€ã€‚ â€¢ è¿«åˆ‡éœ€æ±‚ï¼šä¸€ç§å¯¹è§£ç å™¨é›¶æ”¹åŠ¨ã€ä¿æŒFew-shot ICLèƒ½åŠ›ã€å¯è°ƒå‹ç¼©æ¯”ã€èƒ½æ‰©å±•é•¿ä¸Šä¸‹æ–‡å¹¶å¯è·¨è§£ç å™¨è¿ç§»çš„æ–¹æ³•ï¼ˆè®ºæ–‡åœ¨è¡¨1ç¬¬6é¡µæ˜¾ç¤º4Ã—å‹ç¼©æ¥è¿‘open-bookï¼›è¡¨3ç¬¬8é¡µå±•ç¤ºæ— æ”¹åŠ¨è§£ç å™¨å³å¯æ‰©å±•æœ‰æ•ˆä¸Šä¸‹æ–‡ï¼›å›¾1ç¬¬3é¡µã€å›¾2ç¬¬4é¡µç»™å‡ºå…³é”®æœºåˆ¶ï¼›å›¾4ç¬¬9é¡µæ˜¾ç¤ºå¯åœ¨ä¸åŸæ–‡ç›¸è¿‘çš„å­˜å‚¨æˆæœ¬ä¸‹é¢„å­˜å‹ç¼©è¡¨ç¤ºï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºARC-Encoderï¼šä»¥LLMç¼–ç å™¨+å°å‹MLPæŠ•å½±å™¨ç»„æˆçš„å¯æ’æ‹”å‹ç¼©å™¨ï¼Œåœ¨ç¼–ç å™¨æœ€åä¸€å±‚è‡ªæ³¨æ„åŠ›ä¸­å¯¹ç›¸é‚»queryåšå¹³å‡æ± åŒ–ï¼Œå°†nä¸ªtokenå‹ç¼©ä¸ºn/xä¸ªè¿ç»­å‘é‡ï¼Œç›´æ¥æ›¿æ¢é€å…¥å†»ç»“è§£ç å™¨çš„åµŒå…¥ï¼›é‡‡ç”¨äº¤æ›¿çš„é‡å»ºä¸ç»­å†™é¢„è®­ç»ƒï¼ˆé…<Rec>/<Cont>ï¼‰ï¼Œå†åšå°‘æ ·ä¾‹æ ¼å¼å¾®è°ƒï¼Œå¯ä¸ºä¸åŒè§£ç å™¨é…ç½®æå°çš„ä¸“ç”¨MLPå¹¶è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡åˆ†å—å¹¶è¡Œå‹ç¼©å®ç°é•¿ä¸Šä¸‹æ–‡æ‰©å±•ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ARC-Encoder++ï¼šè‡ªé€‚åº”å‹ç¼©æ¯”çš„å†…å®¹æ„ŸçŸ¥æ± åŒ–ä¸è°ƒåº¦â€”â€”ä¾æ®ä¿¡æ¯å¯†åº¦åŠ¨æ€å†³å®šæ± åŒ–ç²’åº¦ä¸å‹ç¼©æ¯”ï¼Œå…¼é¡¾ç²¾åº¦ä¸æ•ˆç‡ã€‚ â€¢ UniARCï¼šè·¨æ¨¡å‹æ—çš„ç»Ÿä¸€å‹ç¼©è¡¨ç¤ºç©ºé—´â€”â€”ä»¥å¤šè§£ç å™¨å¯¹æ¯”å¯¹é½ä¸å°æŠ•å½±é€‚é…ï¼Œæ¢ç´¢é›¶/å°‘å‚æ•°è¿ç§»åˆ°æ–°è§£ç å™¨ã€‚ â€¢ RAG-ARCï¼šæ£€ç´¢æ„ŸçŸ¥çš„è”åˆæ£€ç´¢â€”å‹ç¼©â€”ç”Ÿæˆä¼˜åŒ–â€”â€”ç”¨å¯å¾®æ£€ç´¢ä¿¡å·æŒ‡å¯¼å‹ç¼©ä¸è§£ç ï¼Œç«¯åˆ°ç«¯æå‡çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡EMã€‚ â€¢ MoE-ARCï¼šé¢å‘é•¿ä¸Šä¸‹æ–‡çš„ä¸“å®¶åŒ–å¹¶è¡Œå‹ç¼©â€”â€”å¼•å…¥Mixture-of-Expertså¯¹ä¸åŒæ–‡æœ¬å—åˆ†è·¯å‹ç¼©ï¼Œè¿›ä¸€æ­¥æé«˜ååä¸é²æ£’æ€§ã€‚ â€¢ FaithARCï¼šå‹ç¼©è¡¨ç¤ºçš„å¿ å®æ€§ä¸ä¿¡æ¯ä¸‹ç•Œå­¦ä¹ â€”â€”å»ºç«‹ä¿¡æ¯è®ºåº¦é‡ä¸å¯éªŒè¯çº¦æŸï¼ŒæŠ‘åˆ¶å¤è¿°/å¹»è§‰ï¼Œä¿è¯ç­”æ¡ˆå¯è¾¾æ€§ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Document Understanding, Measurement, and Manipulation Using Category Theory</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21553" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21553" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We apply category theory to extract multimodal document structure which leads us to develop information theoretic measures, content summarization and extension, and self-supervised improvement of large pretrained models. We first develop a mathematical representation of a document as a category of question-answer pairs. Second, we develop an orthogonalization procedure to divide the information contained in one or more documents into non-overlapping pieces. The structures extracted in the first and second steps lead us to develop methods to measure and enumerate the information contained in a document. We also build on those steps to develop new summarization techniques, as well as to develop a solution to a new problem viz. exegesis resulting in an extension of the original document. Our question-answer pair methodology enables a novel rate distortion analysis of summarization techniques. We implement our techniques using large pretrained models, and we propose a multimodal extension of our overall mathematical framework. Finally, we develop a novel self-supervised method using RLVR to improve large pretrained models using consistency constraints such as composability and closure under certain operations that stem naturally from our category theoretic framework.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç¼ºä¹ç»Ÿä¸€ã€å¯è®¡ç®—çš„è¯­ä¹‰ç»“æ„è¡¨ç¤ºï¼šç°æœ‰æ–¹æ³•å¤šåœç•™åœ¨å¥æ³•/ç»Ÿè®¡å±‚é¢ï¼Œéš¾ä»¥åœ¨è¯­ä¹‰ç©ºé—´ä¸­å¯¹æ–‡æ¡£è¿›è¡Œåº¦é‡ã€æ¯”è¾ƒã€åˆå¹¶ä¸å¯æ“ä½œåŒ–å¤„ç†ï¼ˆå¦‚æ‘˜è¦ä¸æ‰©å±•ï¼‰ â€¢ æ‘˜è¦ä¸æ‰©å±•ç¼ºå°‘å¯ç»„åˆã€å¯éªŒè¯çš„ç»“æ„çº¦æŸä¸æŒ‡æ ‡ï¼šç¼ºä¹åœ¨æ•°å­¦ä¸Šå°é—­çš„ç»“æ„ï¼ˆå¦‚æ ¼ä¸èŒƒç•´ï¼‰ä»¥åŠå¯é‡åŒ–è¯„ä¼°ï¼ˆå¦‚ç‡å¤±çœŸåˆ†æï¼Œè§ç¬¬14é¡µå›¾3ç¤ºä¾‹ï¼‰ â€¢ è·¨æ–‡æ¡£ä¸å¤šæ¨¡æ€æ•´åˆå›°éš¾ï¼šç¼ºå°‘å°†ä¸åŒæ¥æº/æ¨¡æ€çš„è¯­ä¹‰å¯¹é½ã€åˆå¹¶å¹¶è®¡ç®—ç›¸ä¼¼åº¦ä¸ä¿¡æ¯å¢ç›Šçš„ç»Ÿä¸€æœºåˆ¶ â€¢ å¤§æ¨¡å‹ç¼ºä¹è‡ªç›‘ç£çš„å¯éªŒè¯åé¦ˆï¼šéš¾ä»¥ç”¨å¯è®¡ç®—çš„ä¸€è‡´æ€§ï¼ˆå¯ç»„åˆæ€§ã€é—­åŒ…æ€§ï¼‰æ¥è‡ªåŠ¨æå‡æ¨¡å‹è¾“å‡ºè´¨é‡ â€¢ è¯­ä¹‰ä¿¡æ¯åº¦é‡ä½“ç³»è–„å¼±ï¼šç¼ºå°‘å¯¹ä¿¡æ¯å«é‡ã€ä¿¡æ¯å¯†åº¦ã€äº’ä¿¡æ¯ã€å†…å®¹ç†µä¸å¤šæ ·æ€§-æ·±åº¦ç†µç­‰è¯­ä¹‰å±‚é¢çš„ç³»ç»ŸåŒ–å®šä¹‰ä¸è®¡ç®—</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>å°†æ–‡æ¡£ç¦»æ•£åŒ–ä¸ºé—®ç­”å¯¹çš„èŒƒç•´ï¼Œåœ¨æŠ½è±¡å¼ä¿®è¾DAGä¸Šå®šä¹‰åŸºäºå¯ç­”é—®é¢˜é›†åˆçš„Jaccardè·ç¦»å¹¶æ‰§è¡Œæ­£äº¤åŒ–ï¼Œå¾—åˆ°åŸå­QAä¸å…¶æ ¼ç»“æ„ï¼›æ®æ­¤æå‡ºä¸€ç»„è¯­ä¹‰ä¿¡æ¯è®ºåº¦é‡ï¼Œå¹¶æŠŠæ‘˜è¦/é‡Šä¹‰å»ºæ¨¡ä¸ºæ ¼ä¸Šçš„æŠ‘åˆ¶/æ‰©å¼ ï¼ŒåŒæ—¶åˆ©ç”¨LLMæ‰§è¡Œåˆ†è§£ä¸åˆ¤å®šï¼Œå¹¶ä»¥RLVRç­‰å¯éªŒè¯çº¦æŸè¿›è¡Œè‡ªç›‘ç£æ”¹è¿›ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ å¤šæ¨¡æ€Sheafä¸€è‡´æ€§æ‰©å±•ä¸è¯æ®å¯¹é½ï¼šç”¨Sheafä¸€è‡´æ€§ä¿è¯è·¨æ¨¡æ€æ‰©å±•çš„å…¨å±€ä¸€è‡´ä¸æ— å†²çªï¼ˆç»“åˆç¬¬15é¡µè®¨è®ºï¼‰ â€¢ èŒƒç•´åŒ–æ‘˜è¦çš„æœ€ä¼˜ç‡å¤±çœŸç•Œï¼šå»ºç«‹æ‘˜è¦R(D)ç†è®ºä¸‹ç•Œä¸è¿‘ä¼¼ç®—æ³•ï¼Œå¹¶å¯¹æ¯”æ“ä½œæ›²çº¿ï¼ˆå‚è€ƒç¬¬14é¡µå›¾3ï¼‰ â€¢ è¯­ä¹‰ä¿¡æ¯åº¦é‡çš„å¤§è§„æ¨¡å®è¯ï¼šç³»ç»Ÿè¯„ä¼°å†…å®¹ç†µã€ä¿¡æ¯å¯†åº¦ã€äº’ä¿¡æ¯ä¸E/E0/E1æŒ‡æ ‡çš„åŒºåˆ†åŠ›ä¸ç¨³å¥æ€§ â€¢ åŸºäºRLVRçš„èŒƒç•´ä¸€è‡´æ€§è‡ªç›‘ç£è®­ç»ƒï¼šä»¥å¯ç»„åˆæ€§ä¸é—­åŒ…æ€§ç”Ÿæˆå¯éªŒè¯è®­ç»ƒä¿¡å·ï¼Œæå‡LLMä¸€è‡´æ€§ä¸å¯è§£é‡Šæ€§ â€¢ æ–‡æ¡£èŒƒç•´é—´çš„å…³ç³»å­¦ä¹ ä¸å¯å¾®å¯¹é½ï¼šå°†äºŒå€¼å…³ç³»çŸ©é˜µæ€å°„æ¨å¹¿ä¸ºå¯å­¦ä¹ æ¨¡å—ï¼Œæ”¯æŒè·¨æ–‡æ¡£åˆæˆã€è¿ç§»ä¸å¯¹é½</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Taming Modality Entanglement in Continual Audio-Visual Segmentation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.17234" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.17234" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recently, significant progress has been made in multi-modal continual learning, aiming to learn new tasks sequentially in multi-modal settings while preserving performance on previously learned ones. However, existing methods mainly focus on coarse-grained tasks, with limitations in addressing modality entanglement in fine-grained continual learning settings. To bridge this gap, we introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to continuously segment new classes guided by audio. Through comprehensive analysis, two critical challenges are identified: 1) multi-modal semantic drift, where a sounding objects is labeled as background in sequential tasks; 2) co-occurrence confusion, where frequent co-occurring classes tend to be confused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework is designed to address these challenges. Specifically, for multi-modal semantic drift, a Multi-modal Sample Selection (MSS) strategy is proposed to select samples with high modal consistency for rehearsal. Meanwhile, for co-occurence confusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed, allowing for the increase of rehearsal sample frequency of those confusable classes during training process. Moreover, we construct three audio-visual incremental scenarios to verify effectiveness of our method. Comprehensive experiments demonstrate that our method significantly outperforms single-modal continual learning methods.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç ”ç©¶ç©ºç™½ï¼šé¦–æ¬¡ç³»ç»ŸåŒ–æå‡ºç»†ç²’åº¦çš„æŒç»­éŸ³è§†é¢‘åˆ†å‰²ï¼ˆCAVSï¼‰ä»»åŠ¡ï¼Œè¦æ±‚åœ¨åºåˆ—ä»»åŠ¡ä¸­è¿›è¡Œåƒç´ çº§å£°æºå®šä½å¹¶ä¿æŒæ—§ç±»æ€§èƒ½ï¼›ç°æœ‰å¤šæ¨¡æ€æŒç»­å­¦ä¹ å¤šé›†ä¸­äºåˆ†ç±»/åˆ†ç¦»ç­‰ç²—ç²’åº¦ä»»åŠ¡ï¼Œæ— æ³•ç›´æ¥å¤„ç†åƒç´ çº§è·¨æ¨¡æ€å¯¹é½ä¸é—å¿˜é—®é¢˜ã€‚ â€¢ å…³é”®æŒ‘æˆ˜ï¼šæå‡ºå¹¶å®è¯ä¸¤å¤§æ¨¡æ€çº ç¼ éš¾é¢˜â€”â€”(1) å¤šæ¨¡æ€è¯­ä¹‰æ¼‚ç§»ï¼šæ—§ç±»åœ¨æ–°ä»»åŠ¡è¢«æ ‡ä¸ºèƒŒæ™¯ï¼Œå¯¼è‡´éŸ³-è§†å¯¹é½è¢«é”™è¯¯å¼ºåŒ–ä¸ºâ€œèƒŒæ™¯â€ï¼›(2) å…±ç°æ··æ·†ï¼šé«˜å…±ç°ç±»åˆ«åœ¨å­¦ä¹ æ–°ç±»åæ›´æ˜“è¢«æ··æ·†ï¼Œè·¨æ¨¡æ€è¡¨å¾å½¼æ­¤ç‰µå¼•ã€‚ â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šå•æ¨¡æ€CISSæ–¹æ³•è¿ç§»åˆ°éŸ³è§†é¢‘åœºæ™¯è¡¨ç°æ¬ ä½³ï¼›å¤šæ¨¡æ€CLæ–¹æ³•ç¼ºå°‘ç»†ç²’åº¦å¯¹é½ä¸è®°å¿†é€‰æ‹©æœºåˆ¶ï¼›å¸¸è§„é‡æ”¾æœªè€ƒè™‘è·¨æ¨¡æ€ä¸€è‡´æ€§ä¸ç±»é—´å…±ç°ç»“æ„ï¼Œéš¾ä»¥æŠ‘åˆ¶ç¾éš¾æ€§é—å¿˜ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºç¢°æ’é©±åŠ¨çš„å¤šæ¨¡æ€é‡æ”¾æ¡†æ¶CMRï¼ŒåŒ…å«å¤šæ¨¡æ€æ ·æœ¬é€‰æ‹©MSSä¸åŸºäºç¢°æ’çš„æ ·æœ¬é‡æ”¾CSRï¼šMSSä»¥è§†è§‰/éŸ³è§†ä¸¤æ¨¡å‹mIoUå·®å€¼è¯„ä¼°éŸ³é¢‘è´¡çŒ®ï¼Œé€‰å–é«˜æ¨¡æ€ä¸€è‡´æ€§æ ·æœ¬å…¥åº“ï¼›CSRç»Ÿè®¡æ—§æ¨¡å‹é¢„æµ‹ä¸å½“å‰çœŸå€¼çš„â€œç¢°æ’å¯¹â€ï¼ŒæŒ‰ç¢°æ’é¢‘ç‡é‡é‡‡æ ·æ˜“æ··ç±»åˆ«ä»¥æ‰“ç ´å…±ç°çº ç¼ å¹¶å‡å¿˜ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„å¤šæ¨¡æ€æ ·æœ¬ä¼°ä»·ä¸è®°å¿†ä¼˜åŒ–ï¼šç»“åˆä¸ç¡®å®šæ€§/ä¿¡æ¯å¢ç›Š/Shapley è¯„ä¼°ï¼ŒåŠ¨æ€é…é¢ä¸è‡ªé€‚åº”é‡æ”¾ä»¥æœ€å°åŒ–é—å¿˜ â€¢ å¼€æ”¾è¯æ±‡çš„æŒç»­éŸ³è§†é¢‘åˆ†å‰²ï¼šå¼•å…¥æ–‡æœ¬å…ˆéªŒä¸å¯¹æ¯”è¯­è¨€ç›‘ç£ï¼Œå®ç°é›¶æ ·æœ¬/å°‘æ ·æœ¬æ–°ç±»çš„æ— ç¼å¢é‡ä¸è·¨æ¨¡æ€å¯¹é½ â€¢ å¤šç›®æ ‡è§£è€¦ä¸å› æœå…±ç°æŠ‘åˆ¶çš„åçº ç¼ å­¦ä¹ ï¼šåŸºäºç›®æ ‡è½¨è¿¹é‡æ”¾ä¸å…±ç°å›¾/å› æœæ¨æ–­ï¼Œæ˜¾å¼çº¦æŸå…±ç°åå·®å¹¶æå‡å¤šç›®æ ‡åœºæ™¯é²æ£’æ€§</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 5</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-10-27 03:42:03 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 5;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>