<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - October 27, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "SF Pro Display", -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Helvetica Neue", "Arial", "Noto Sans", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", sans-serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">October 27, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">DeepAgent: A General Reasoning Agent with Scalable Toolsets</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21618" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21618" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To address the challenges of long-horizon interactions, particularly the context length explosion from multiple tool calls and the accumulation of interaction history, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. This work takes a step toward more general and capable agents for real-world applications. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 传统工作流式代理（如 ReAct、Plan-and-Solve）缺乏真正的自治，步骤与流程预先固定，推理仅围绕局部下一步展开，难以在执行中按需动态发现与选择工具，整体任务推理一致性与适配开放工具生态能力不足（见页1-2，图2对比）。 • 长时程交互导致上下文长度爆炸与误差累积：多次工具调用与历史堆叠使模型易陷入错误探索路径，缺少能稳定压缩且保持可用性的记忆机制以维持长期任务的高效与稳健（见页1、3-4）。 • 通用工具使用的训练不稳定且成本高：直接依赖海量真实API训练会慢且脆弱；仅有终局稀疏奖励难以学到精确的工具调用与参数填充，需要细粒度的信用分配信号（见页1、5）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出 DeepAgent，将思考、工具检索与调用、以及记忆管理统一于单一连续推理流，并以自治记忆折叠将历史压缩为结构化的情景记忆、工作记忆与工具记忆以稳态长程交互（见图3/页3-4）。同时提出 ToolPO 强化学习，利用 LLM 工具模拟器与工具调用优势归因，在不依赖大量不稳定真实API的前提下，实现稳定、低成本且精细监督的端到端训练（见页5）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 检索器—策略联合优化的端到端工具代理：联合训练工具检索与决策策略，结合对比学习与调用优势归因，提升开放集工具发现—调用的一致性与效率。 • 自适应记忆折叠的理论化与调度学习：将记忆折叠形式化为成本—收益优化，学习何时折叠与折叠粒度，并给出长时程性能—开销权衡的理论界与控制策略。 • 从模拟到真实的工具使用强化学习：在模拟器与真实API的混合环境中引入不确定性感知与安全约束，研究仿真到真实的迁移、鲁棒性与在线校准机制。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Video-As-Prompt: Unified Semantic Control for Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20888" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20888" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Unified, generalizable semantic control in video generation remains a critical open challenge. Existing methods either introduce artifacts by enforcing inappropriate pixel-wise priors from structure-based controls, or rely on non-generalizable, condition-specific finetuning or task-specific architectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes this problem as in-context generation. VAP leverages a reference video as a direct semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via a plug-and-play Mixture-of-Transformers (MoT) expert. This architecture prevents catastrophic forgetting and is guided by a temporally biased position embedding that eliminates spurious mapping priors for robust context retrieval. To power this approach and catalyze future research, we built VAP-Data, the largest dataset for semantic-controlled video generation with over 100K paired videos across 100 semantic conditions. As a single unified model, VAP sets a new state-of-the-art for open-source methods, achieving a 38.7% user preference rate that rivals leading condition-specific commercial models. VAP's strong zero-shot generalization and support for various downstream applications mark a significant advance toward general-purpose, controllable video generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：缺乏统一、可泛化的“语义控制”视频生成框架；现有方法要么依赖像素对齐先验、要么为每种语义/任务单独训练，难以一体化与零样本泛化（见图2，页3）。 • 重要性：语义控制支撑特效、视频风格化、动作模仿与镜头调度等核心应用（页1-2），统一方法能显著提升创作效率与可扩展性。 • 结构控制迁移的局限：将像素对齐残差注入机制硬套到语义控制会引入copy-and-paste伪像及错误的像素映射先验（图2(a)，页3）。 • 语义控制现状的局限：条件特定微调/LoRA需逐条件训练、成本高且泛化差；任务特定模块/推理策略割裂各任务，难以统一与零样本扩展（图2(b)(c)，页3）。 • 数据瓶颈：缺少覆盖广泛语义条件的数据集限制了统一训练与评测，因此作者构建含10万+配对视频、100种语义条件的VAP-Data（图3，页5）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>将参考视频作为“语义提示”，通过可插即用的Mixture-of-Transformers专家并行路径，对冻结的Video DiT进行层间全注意力引导，并采用时间偏置的RoPE将参考序列置于目标序列之前且仅共享空间位置，从而消除错误的像素映射先验并实现稳健的上下文检索。配合VAP-Data统一训练后，单一模型即可在概念/风格/运动/相机等多语义条件下实现零样本控制（图1-2，页2-3）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Compositional Video-As-Prompt: 多参考视频的语义解耦与可组合控制 — 将风格/运动/相机/概念分解并按需组合，研究专家路由与冲突消解。 • 3D-aware Video-As-Prompt: 面向3D时空一致性的相机与场景语义控制 — 融合NeRF/高斯场等3D先验，联合控制相机轨迹与场景几何。 • Streaming/Retrieval-Augmented VAP: 在线检索参考与低时延实时视频生成 — 构建检索增强的参考池与增量式专家推理，实现长视频与实时应用。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20286" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20286" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>GUI grounding, which maps natural-language instructions to actionable UI elements, is a core capability of GUI agents. Prior works largely treats instructions as a static proxy for user intent, overlooking the impact of instruction diversity and quality on grounding performance. Through a careful investigation of existing grounding datasets, we find a 23.3% flaw rate in their instructions and show that inference-time exploitation of instruction diversity yields up to a substantial 76% relative performance improvement. In this paper, we introduce the Instruction-as-Reasoning paradigm, treating instructions as dynamic analytical pathways that offer distinct perspectives and enabling the model to select the most effective pathway during reasoning. To achieve this, we propose a two-stage training framework: supervised fine-tuning (SFT) on synthesized, diverse instructions to instill multi-perspective reasoning, followed by reinforcement learning (RL) to optimize pathway selection and composition. Our resulting models, UI-Ins-7B and UI-Ins-32B, achieve state-of-the-art results on five challenging grounding benchmarks and exhibit emergent reasoning, selectively composing and synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model demonstrates strong agentic potential, achieving a 74.1% success rate on AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals additional insights such as how reasoning can be formulated to enhance rather than hinder grounding performance, and how our method mitigates policy collapse in the SFT+RL framework. All code and model checkpoints will be publicly released in https://github.com/alibaba/UI-Ins.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：GUI grounding严重依赖自然语言指令，但主流方法将指令视为静态输入，忽视指令的多视角表达与推理价值，导致鲁棒性欠佳（图3与图2a显示不同视角可带来最高76%的相对提升）。 • 数据质量痛点：开源数据集中指令存在显著噪声与歧义，论文抽检发现23.3%样本存在“多对应/不对应”等缺陷，直接拉低训练效果（见图2b、2c）。 • 训练范式局限：仅用单风格指令的SFT易形成“单一路径”策略，RL阶段探索不足且易策略坍塌；自由形式思维链（FFR）在RL中常降性能（表8、表9），缺少“可优化的推理结构”。 • 任务重要性：GUI代理的有效性取决于将高层意图映射到可执行UI元素的能力，指令的清晰度、视角选择与质量直接决定落点准确性与在线执行稳定性（引言、2.节）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出“Instruction-as-Reasoning”范式：将外观/功能/空间/意图等多视角指令显式作为推理路径，先经SFT学习多视角推理生成与坐标预测，再用GRPO在RL阶段通过点入框奖励学习在不同场景下选择最优推理视角；配套数据流水线用OmniParser校正标注、GPT-4.1生成与校验多视角无歧义指令（见图5、图6）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Know-Ins: 知识增强的多视角GUI定位——引入检索/知识库与RAG，补齐领域实体与品牌等外部常识以化解知识缺失型错误（对应图10a）。 • Struct-Ins: 基于界面结构图的组合推理——融合UI层级/DOM/组件图与空间关系，进行结构化多视角合成推理，提升布局理解与可点击区域判别（对应图10b）。 • Ambi-Ins: 面向歧义消解的自适应视角选择——在推理中引入不确定性估计与多候选假设，结合对抗负样本与一致性校验，缓解相似干扰图标导致的混淆（对应图10c、10d）。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.19871" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.19871" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Discrete diffusion models have emerged as a promising direction for vision-language tasks, offering bidirectional context modeling and theoretical parallelization. However, their practical application is severely hindered by a train-inference discrepancy, which leads to catastrophic error cascades: initial token errors during parallel decoding pollute the generation context, triggering a chain reaction of compounding errors and leading to syntactic errors and semantic hallucinations. To address this fundamental challenge, we reframe the generation process from passive denoising to active refining. We introduce ReDiff, a refining-enhanced diffusion framework that teaches the model to identify and correct its own errors. Our approach features a two-stage training process: first, we instill a foundational revision capability by training the model to revise synthetic errors; second, we implement a novel online self-correction loop where the model is explicitly trained to revise its own flawed drafts by learning from an expert's corrections. This mistake-driven learning endows the model with the crucial ability to revisit and refine its already generated output, effectively breaking the error cascade. Extensive experiments demonstrate that ReDiff significantly improves the coherence and factual accuracy of generated content, enabling stable and efficient parallel generation far superior to traditional denoising methods. Our codes and models are available at https://rediff-hku.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 离散扩散VLM在并行解码中出现错误级联：少量初始token错误会瞬时污染全局上下文，带来重复、语法混乱与视觉幻觉；根源在于训练-推理分布差异（训练只见干净GT，推理需在自身中间产物上生成）。 • 现有“掩码-预测”方法是被动去噪，已解掩token被固定，缺乏对已生成内容的回访与修订能力，无法自纠，导致一步多token解码时质量不稳，常退化到一token/步。 • 扩散模型具备双向上下文与并行潜力，但在少步数/高并行速度下表现显著劣化，事实性与连贯性不足，未能兑现理论上的高效推理优势。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出ReDiff，将生成范式从被动去噪转为主动校订：阶段一以合成语法/幻觉错误训练基础修订能力，阶段二在在线自纠环中让模型生成草稿并由专家模型纠错，按被纠正片段进行监督；推理时同时解掩新token并重写已生成token，打断错误级联并稳定并行解码。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• ReDiff++: 无专家自举的自纠扩散模型：以自一致/对比学习与双模型互审替代外部专家，降低数据与成本依赖。 • 可验证事实性的视觉-语言修订扩散：将检索与可验证性校验器融入修订回路，最小化幻觉并提升事实一致性。 • 统一编辑-生成的多任务修订扩散框架：把ReDiff扩展到VQA、长文描述与多轮对话，学习通用的span级编辑与并行生成过程。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20479" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20479" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We unveil that internal representations in large language models (LLMs) serve as reliable proxies of learned knowledge, and propose RECALL, a novel representation-aware model merging framework for continual learning without access to historical data. RECALL computes inter-model similarity from layer-wise hidden representations over clustered typical samples, and performs adaptive, hierarchical parameter fusion to align knowledge across models. This design enables the preservation of domain-general features in shallow layers while allowing task-specific adaptation in deeper layers. Unlike prior methods that require task labels or incur performance trade-offs, RECALL achieves seamless multi-domain integration and strong resistance to catastrophic forgetting. Extensive experiments across five NLP tasks and multiple continual learning scenarios show that RECALL outperforms baselines in both knowledge retention and generalization, providing a scalable and data-free solution for evolving LLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：LLMs在持续/多域场景中易发生灾难性遗忘，难以同时保持通用能力与新任务专长。 • 现实约束：历史数据常因隐私/存储不可用、任务边界与Task ID缺失，回放类方法难以落地。 • 现有方法局限：正则/结构化方法优化空间受限、依赖Task ID、模型复杂度累积、跨任务性能折衷明显。 • 合并方法缺陷：多数权重平均忽略层间功能差异与表示错配，深层表示在任务间显著分化，易引入语义干扰（论文第2节的层内/跨模型表示分析与图2、图3）。 • 需求缺口：需要一种数据无关、任务无关、可扩展的知识融合方案，能在浅层保留通用表征、深层适配任务特性。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出RECALL：一种基于表示对齐的分层模型融合框架。通过对新任务数据聚类选取“典型样本”，在各模型各层提取隐藏表示并用RBF核计算相似度，Softmax得到层级自适应融合权重，逐层线性插值合并参数，从而在不访问历史数据的前提下对齐知识、缓解遗忘。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• Cross-Architecture RECALL：跨架构/分词器的表示对齐模型合并——通过对齐映射或轻量Adapter实现异构LLM的层间对齐。 • Online-RECALL：面向流式持续学习的在线表示对齐与增量合并——无需缓存历史数据，使用滑动/水库采样动态更新层权重。 • RECALL++：训练期正则与表示对齐融合的联合框架——将EWC/正交子空间等正则与RECALL解耦或联合优化以进一步抑制遗忘。 • Metric-Learned RECALL：面向合并的可学习相似度度量——用对比学习/探测器替代固定RBF，学习更贴合知识亲和度的层级相似性。 • Privacy-Preserving Typical Sample Synthesis：隐私友好的典型样本合成与选择稳健性研究——用生成替代或代理任务挑选典型样本，提升数据可用性与鲁棒性。 • Multilingual/Multimodal RECALL：跨语种与跨模态的表示对齐合并——在共享语义空间（多语/视听语）中实现层级对齐与知识融合。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">A Definition of AGI</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.18212" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.18212" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The lack of a concrete definition for Artificial General Intelligence (AGI) obscures the gap between today's specialized AI and human-level cognition. This paper introduces a quantifiable framework to address this, defining AGI as matching the cognitive versatility and proficiency of a well-educated adult. To operationalize this, we ground our methodology in Cattell-Horn-Carroll theory, the most empirically validated model of human cognition. The framework dissects general intelligence into ten core cognitive domains-including reasoning, memory, and perception-and adapts established human psychometric batteries to evaluate AI systems. Application of this framework reveals a highly "jagged" cognitive profile in contemporary models. While proficient in knowledge-intensive domains, current AI systems have critical deficits in foundational cognitive machinery, particularly long-term memory storage. The resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify both rapid progress and the substantial gap remaining before AGI.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 缺少可操作的AGI定义与统一尺度，导致“移动门槛”和无效争论；需要能量化区分当代专用AI与人类级通用智能差距的框架（见论文第1–2页对定义与目标的阐述）。 • 现有评测多集中于窄域任务与自动化数据集，易受数据污染与提示工程影响，难以测到底层认知机制与多模态能力（第13–15页“Contamination”“Solving the Dataset vs. Solving the Task”）。 • 单一总分或任务成功率掩盖“锯齿状”能力结构与关键瓶颈，误导AGI进展判断；尤其长期记忆存储（MS）≈0%、视觉推理与速度等短板主导系统“马力”（第13–14页与图3）。 • 工程上通过超长上下文（WM）和RAG外部检索“能力补偿”掩盖内部长期存储与高精度检索缺陷，制造通用性错觉（第13页“Capability Contortions”）。 • 以经济产出或单一行为测试定义AGI（如“盈利定义”“图灵式”）不能直接映射到人类认知广度与熟练度，亟需与心理测量学对齐的认知维度化测度（第15–16页“Definitions of Related Concepts”）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>基于CHC理论构建涵盖10大认知域、数十项窄能力的多模态评测蓝图，等权聚合为0–100%的“AGI Score”，并输出按域分解的“认知剖面”。方法以人类心理测量电池改造为任务集合，结合现有基准与人工判分、跨分布复述和禁用外部工具等设置来提升稳健性与抗数据污染（第2–12、22–57页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 面向AGI的持续化长期记忆：从参数化更新到外接模块的统一框架：设计可持久写入与巩固的记忆系统（关联/语义/逐字），并给出跨会话、延时48小时的标准化评测协议（对标MS的0%瓶颈）。 • 从能力补偿到真实能力：拆解RAG与超长上下文对评测的偏置：区分内部检索精度与外部检索依赖，系统量化幻觉率、个性化保持与会话跨时一致性，避免“能力扭曲”误判（对标MR与WM）。 • 统一多模态空间认知与视觉推理评测：构建涵盖长视频理解、空间导航、直觉物理与具身推理的一体化基准与训练方案，刻画V、WM、R的耦合并提升视觉推理与导航能力。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20206" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20206" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Prompt design plays a crucial role in text-to-video (T2V) generation, yet user-provided prompts are often short, unstructured, and misaligned with training data, limiting the generative potential of diffusion-based T2V models. We present RAPO++, a cross-stage prompt optimization framework that unifies training-data--aligned refinement, test-time iterative scaling, and large language model (LLM) fine-tuning to substantially improve T2V generation without modifying the underlying generative backbone. In Stage 1, Retrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with semantically relevant modifiers retrieved from a relation graph and refactors them to match training distributions, enhancing compositionality and multi-object fidelity. Stage 2 introduces Sample-Specific Prompt Optimization (SSPO), a closed-loop mechanism that iteratively refines prompts using multi-source feedback -- including semantic alignment, spatial fidelity, temporal coherence, and task-specific signals such as optical flow -- yielding progressively improved video generation quality. Stage 3 leverages optimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing task-specific optimization patterns and enabling efficient, high-quality prompt generation even before inference. Extensive experiments across five state-of-the-art T2V models and five benchmarks demonstrate that RAPO++ achieves significant gains in semantic alignment, compositional reasoning, temporal stability, and physical plausibility, outperforming existing methods by large margins. Our results highlight RAPO++ as a model-agnostic, cost-efficient, and scalable solution that sets a new standard for prompt optimization in T2V generation. The code is available at https://github.com/Vchitect/RAPO.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 用户提示往往短、非结构化且与训练数据分布不一致，限制了扩散式T2V模型在语义对齐、组合性与多目标保真方面的潜力。 • 现有T2I提示优化或LLM重写侧重空间美学，难以提升视频的时间一致性、运动平滑与物理可行性；T2V中的提示工程多为模型特定，泛化性弱。 • RLHF式提示优化在T2V上推理成本高、需要大量生成回滚以估计奖励，难以扩展到视频；缺乏在不改动生成器权重下提升质量的通用方案。 • 测试时扩展用于“迭代提示优化”的系统化方法不足，多源反馈（语义、空间、时间、物理）未被统一而高效地利用。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出RAPO++三阶段框架：阶段1以检索增强（关系图）+句式重构对齐训练分布，并由判别器在候选中选优；阶段2在推理时利用VLM校验、空间/时间/物理指标的多源反馈闭环迭代重写提示；阶段3将迭代得到的优选提示对用于指令微调重写LLM，内化优化规律，实现模型无关、低成本的T2V质量提升。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 轻量化SSPO：面向实时视频生成的低延迟评估器与自适应计算预算：学习替代评估器与动态停止准则，降低迭代重写与验证的推理开销。 • 物理先验驱动的提示优化：将可微物理与光流/碰撞约束融入测试时闭环：把物理引擎或可学习物理代理纳入反馈，强化运动规律与因果一致性。 • 跨模型元重写器：面向多T2V骨干的少样本自适应提示优化：通过元学习/参数高效调优，使重写器快速迁移到新视频生成器与数据域。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">WorldGrow: Generating Infinite 3D World</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21682" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21682" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We tackle the challenge of generating the infinitely extendable 3D world -- large, continuous environments with coherent geometry and realistic appearance. Existing methods face key challenges: 2D-lifting approaches suffer from geometric and appearance inconsistencies across views, 3D implicit representations are hard to scale up, and current 3D foundation models are mostly object-centric, limiting their applicability to scene-level generation. Our key insight is leveraging strong generation priors from pre-trained 3D models for structured scene block generation. To this end, we propose WorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our method features three core components: (1) a data curation pipeline that extracts high-quality scene blocks for training, making the 3D structured latent representations suitable for scene generation; (2) a 3D block inpainting mechanism that enables context-aware scene extension; and (3) a coarse-to-fine generation strategy that ensures both global layout plausibility and local geometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset, WorldGrow achieves SOTA performance in geometry reconstruction, while uniquely supporting infinite scene generation with photorealistic and structurally consistent outputs. These results highlight its capability for constructing large-scale virtual environments and potential for building future world models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 目标问题：如何生成可无限扩展、全局连贯且外观逼真的3D世界（大尺度连续环境） • 重要性：为游戏、VR/AR、CAD、影视提供大规模虚拟环境基座，更关键地为World Models与具身智能提供可持续扩展的开放式学习场景 • 现有2D→3D提升方法局限：基于2D扩散图像再“抬升”到3D，跨视角几何与外观不一致、易失真，难以扩展到大场景 • 现有3D生成方法局限：隐式表示/三平面等受限于场景级数据规模与多样性、难以泛化，且多缺乏显式纹理建模（常需额外贴图管线） • 3D基础模型局限：多为物体级先验，直接用于场景级生成缺乏上下文与块间衔接能力 • 块级生长挑战：如何迁移物体先验到“场景块”、保证相邻块几何/风格/纹理无缝衔接，并在全局布局合理性与局部细节保真之间取得平衡</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出WorldGrow：以块为单位的层级式无限场景生成框架，包含数据筛选构建高质量场景块、将SLAT改造为“场景友好”（遮挡感知特征聚合+基于场景块重训解码器）、基于流匹配的结构/潜变量双阶段3D块修复（inpainting），并采用粗到细策略先铺设全局布局后细化几何与外观，最终解码为可渲染的3D世界。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• WorldGrow-Z：面向多楼层的垂直可扩展无限3D世界生成——将块级生长从XY平面扩展到Z轴，支持多层建筑与立体连通 • SemWorldGrow：基于LLM语义约束的可控无限场景生成——用文本/规则控制房间类型、相邻关系与风格，实现语义一致的可编排世界 • UniWorldGrow：几何-纹理统一潜变量的单阶段无限场景生成——融合几何与外观潜空间（如统一SLAT）以提升效率与一致性</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Model Merging with Functional Dual Anchors</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21223" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21223" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Model merging is an efficient post-training strategy for integrating knowledge from multiple finetuned checkpoints of a shared foundation model. Existing methods operate in the parameter space, combining task vectors to mitigate conflicts, but remain constrained by parameter inconsistencies. We propose Functional Dual Anchors (FDAs), a framework that instead models the input-representation space. FDAs are synthetic inputs whose induced gradients align with task vectors, capturing task-specific functional shifts relative to the pretrained model. This perspective bridges joint multi-task training and post-hoc merging, offering both robustness and flexibility. We further introduce a principled initialization scheme and show that FDAs are complementary to parameter-space model merging. Comprehensive experiments demonstrate the effectiveness of FDAs in model merging.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：如何将同一预训练骨干微调得到的多任务/多领域检查点高效、数据无关地合并为一个统一模型，同时缓解任务间知识冲突（第1页）。 • 重要性：相比多任务联合训练与持续学习，模型合并是一种后训练、无需原始数据的知识整合方式，适用于数据隐私与大模型训练成本受限场景（第1页）。 • 现有局限：主流方法在参数空间对任务向量做缩放/正交/子空间等操作，受参数不一致与初始化敏感影响，提供的是从预训练点出发的固定线性路径，易偏离真实损失盆地（图2，页2；第1-2页）；部分方法还需依赖任务数据或启发式先验，鲁棒性与可扩展性受限（第2-3页）。 • 关键洞见：输入-表征空间比参数空间更结构化、易建模（与数据蒸馏等工作一致），若能在输入空间“投影”任务向量所代表的功能迁移，可更稳健地复现联合训练效果（第2页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出功能双锚（FDAs）：为每个下游模型在输入-表征空间构造一组合成锚点，使其在预训练模型上诱导的梯度方向与对应任务向量对齐（式1）；随后用这些锚点最小化预训练模型与各下游模型在锚点处的表示差异来优化参数（式5），或作为精炼器改进已有的参数空间合并结果（式6），采用层级实现与两种初始化（权重采样/缩放高斯）以提升收敛与鲁棒性。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 自适应层与锚点预算搜索的FDA合并：自动确定各层锚点数量、token长度与缩放系数σ，兼顾收敛速度与计算开销。 • 参数-输入双空间协同的端到端合并：联合优化任务向量调整与FDA构造/适配，在同一目标下学习任务权重与表示对齐。 • 多模态与联邦场景下的隐私友好FDA合并：将FDA扩展至图文/语音等多模态与联邦学习环境，在无源数据条件下实现稳健合并与隐私保障。</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Visual Diffusion Models are Geometric Solvers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21697" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21697" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In this paper we show that visual diffusion models can serve as effective geometric solvers: they can directly reason about geometric problems by working in pixel space. We first demonstrate this on the Inscribed Square Problem, a long-standing problem in geometry that asks whether every Jordan curve contains four points forming a square. We then extend the approach to two other well-known hard geometric problems: the Steiner Tree Problem and the Simple Polygon Problem. Our method treats each problem instance as an image and trains a standard visual diffusion model that transforms Gaussian noise into an image representing a valid approximate solution that closely matches the exact one. The model learns to transform noisy geometric structures into correct configurations, effectively recasting geometric reasoning as image generation. Unlike prior work that necessitates specialized architectures and domain-specific adaptations when applying diffusion to parametric geometric representations, we employ a standard visual diffusion model that operates on the visual representation of the problem. This simplicity highlights a surprising bridge between generative modeling and geometric problem solving. Beyond the specific problems studied here, our results point toward a broader paradigm: operating in image space provides a general and practical framework for approximating notoriously hard problems, and opens the door to tackling a far wider class of challenging geometric tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 需要一个跨任务、统一且简单的框架来近似求解难几何问题（如内接正方形、欧氏Steiner树、最大面积多边形），而不是为每个问题单独设计复杂的符号/图表示与专用算法。 • 传统算法对这些问题要么本质NP-hard或尚无普适解（如内接正方形猜想），实现复杂、对规模敏感，且难以高效产生多样解；现有扩散/学习方法多在参数或图空间建模，依赖离散化和专用架构，迁移性与通用性不足。 • 亟需验证：能否将“几何约束满足”视为图像分布，用标准视觉扩散模型在像素域直接进行几何推理，以更低工程复杂度获得高质量近似解并自然支持一题多解。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>将每个几何实例渲染为条件图像（如曲线或点集通道），用标准条件视觉扩散模型（U-Net）从噪声生成表示可行解的图像；训练用程序化合成的大规模“实例-有效解”对，推理通过多种子采样并配合轻量后处理（如顶点snapping、节点/边提取与合法性校验）得到结构化解。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 从像素到参数：视觉扩散解的可微几何精化：在采样后叠加可微几何优化/连续参数化（如角度、顶点位置）以突破像素离散误差并提升可行性与最优性。 • 能量引导的无监督几何扩散求解：结合T2T/能量引导，在无GT最优解或仅有可行性判定的场景中，以目标能量（长度/面积/约束）在测试时引导扩散搜索。 • 跨任务与尺度泛化的统一几何扩散模型：单模型多任务联合训练与条件组合（曲线+点/多约束），配合分层采样与稀疏注意力，实现从小规模到大规模实例的稳健外推。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21652" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21652" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>AI agents hold the potential to revolutionize scientific productivity by automating literature reviews, replicating experiments, analyzing data, and even proposing new directions of inquiry; indeed, there are now many such agents, ranging from general-purpose "deep research" systems to specialized science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of these agents is critical for progress. Yet existing benchmarks fall short on several fronts: they (1) fail to provide holistic, product-informed measures of real-world use cases such as science research; (2) lack reproducible agent tools necessary for a controlled comparison of core agentic capabilities; (3) do not account for confounding variables such as model cost and tool access; (4) do not provide standardized interfaces for quick agent prototyping and evaluation; and (5) lack comprehensive baseline agents necessary to identify true advances. In response, we define principles and tooling for more rigorously benchmarking agents. Using these, we present AstaBench, a suite that provides the first holistic measure of agentic ability to perform scientific research, comprising 2400+ problems spanning the entire scientific discovery process and multiple scientific domains, and including many problems inspired by actual user requests to deployed Asta agents. Our suite comes with the first scientific research environment with production-grade search tools that enable controlled, reproducible evaluation, better accounting for confounders. Alongside, we provide a comprehensive suite of nine science-optimized classes of Asta agents and numerous baselines. Our extensive evaluation of 57 agents across 22 agent classes reveals several interesting findings, most importantly that despite meaningful progress on certain individual aspects, AI remains far from solving the challenge of science research assistance.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 科研助理类AI代理快速涌现但缺乏统一、可复现、可比的评测框架，用户与开发者难以判断何者更优 • 现有基准缺少覆盖“文献理解—代码与执行—数据分析—端到端发现”的全流程科学任务，且很少基于真实产品使用需求 • 缺乏标准化、受控且可回放的工具环境（尤其大规模文献检索与代码沙箱），难以将信息可得性与代理能力解耦 • 评分未系统控制混杂因素（计算成本、工具/模型开放性等），高分可能只是“烧算力”与特权工具带来的假象 • 任务接口不统一、难以即插即用地评测通用型代理，同时缺少全面强基线，导致“进步”难以被客观识别</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出AstaBench科学研究基准套件，配套Asta Environment（含日期截断的Asta Scientific Corpus检索与状态化计算Notebook），在统一工具与时间不变成本核算（agent-evalLeaderboard）下，按Inspect/MCP标准评测代理并用LLM-as-judge与Pareto曲线报告质-成本权衡。同步发布agent-baselines标准化代理，覆盖9类Asta科研代理与多种通用/商用基线，合计评测57个代理（22类架构）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 面向跨行业的可控通用代理基准：将受控工具与成本记账方法从科学研究扩展到医疗、法律与企业知识场景 • 可信LLM评审框架：多裁判集成、证据强制与人机混合复核以提升长文与端到端任务的判分一致性与可追溯性 • 开放权重科研代理的能力补齐：面向代码执行与数据驱动发现的专用训练、工具编排与蒸馏，缩小与闭源系统差距</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21447" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21447" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Interactive world models that simulate object dynamics are crucial for robotics, VR, and AR. However, it remains a significant challenge to learn physics-consistent dynamics models from limited real-world video data, especially for deformable objects with spatially-varying physical properties. To overcome the challenge of data scarcity, we propose PhysWorld, a novel framework that utilizes a simulator to synthesize physically plausible and diverse demonstrations to learn efficient world models. Specifically, we first construct a physics-consistent digital twin within MPM simulator via constitutive model selection and global-to-local optimization of physical properties. Subsequently, we apply part-aware perturbations to the physical properties and generate various motion patterns for the digital twin, synthesizing extensive and diverse demonstrations. Finally, using these demonstrations, we train a lightweight GNN-based world model that is embedded with physical properties. The real video can be used to further refine the physical properties. PhysWorld achieves accurate and fast future predictions for various deformable objects, and also generalizes well to novel interactions. Experiments show that PhysWorld has competitive performance while enabling inference speeds 47 times faster than the recent state-of-the-art method, i.e., PhysTwin.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 需要从短时长真实视频学习可交互的可变形体世界模型，但数据稀缺、观测单一，难以覆盖多样动作与物性分布 • 纯学习方法依赖海量数据，容易与真实物理不一致，且多采用全局物性，难以刻画空间异质材料（如局部刚度/密度变化） • 物理仿真方法（如MPM/MSS）物理逼真但推理开销大、难以实时，无法直接支撑规划与交互（论文表1显示现有SOTA速度受限） • 参数识别与模型选择困难：从短视频鲁棒估计本构模型与物性容易陷入不稳定或初值敏感，导致仿真-真实域差 • 现有方法在对未见交互的泛化、长时预测稳定性与图像重建一致性上存在明显不足</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>先以VLM辅助选择本构模型并在MPM中进行“全局到局部”的物性优化，构建物理一致的数字孪生；再通过曲率与速度约束的轨迹生成和部件感知的物性扰动合成多样演示，用其训练嵌入空间异质物性的轻量GNN，并用真实视频微调物性以缩小仿真-真实域差，实现动作条件的快速准确预测与渲染（结合3DGS+LBS）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 端到端视频-物性联合估计与不确定性建模：将VLM选择、本构识别、物性优化与GNN学习统一到可微框架中，并显式量化物性与动力学不确定性 • 面向多对象与多相材料的统一世界模型：扩展至多接触/自接触、刚软耦合与流固耦合场景，学习跨材料边界的交互与传输机制 • 可微世界模型驱动的闭环机器人规划：将物性条件GNN嵌入MPC/MPPI，支持梯度/采样混合的安全约束优化，实现实时抓取、折叠与整形等任务</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">ARC-Encoder: learning compressed text representations for large language models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20535" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20535" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent techniques such as retrieval-augmented generation or chain-of-thought reasoning have led to longer contexts and increased inference costs. Context compression techniques can reduce these costs, but the most effective approaches require fine-tuning the target model or even modifying its architecture. This can degrade its general abilities when not used for this specific purpose. Here we explore an alternative approach: an encoder that compresses the context into continuous representations which replace token embeddings in decoder LLMs. First, we perform a systematic study of training strategies and architecture choices for the encoder. Our findings led to the design of an Adaptable text Representations Compressor, named ARC-Encoder, which outputs x-times fewer continuous representations (typically x!in!{4,8}) than text tokens. We evaluate ARC-Encoder across a variety of LLM usage scenarios, ranging from in-context learning to context window extension, on both instruct and base decoders. Results show that ARC-Encoder achieves state-of-the-art performance on several benchmarks while improving computational efficiency at inference. Finally, we demonstrate that our models can be adapted to multiple decoders simultaneously, allowing a single encoder to generalize across different decoder LLMs. This makes ARC-Encoder a flexible and efficient solution for portable encoders that work seamlessly with multiple LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder , fine-tuning dataset and pretrained models are available at https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 关键问题：RAG与CoT让LLM上下文急剧变长，注意力计算对长度呈二次复杂度，推理成本与时延飙升，且易触达上下文窗口上限并稀释关键信息。 • 现有硬压缩局限：基于删除/摘要的硬压缩虽模型无关、可解释，但压缩率有限、易丢失语义要点，难以在多任务上稳定保持精度。 • 现有软压缩局限：记忆/提示向量等软压缩通常需专用编码器并联动微调或改造解码器，牺牲通用性与可移植性；且多为固定数量的压缩token，难随输入长度灵活缩放；仅做重建训练易导致“复述上下文”而非提炼信息。 • 评测偏差与泛化不足：不少工作在指令模型上零样本评测，可能掩盖真实的EM能力；跨解码器迁移弱，难做到“一次训练，多模型可用”。 • 迫切需求：一种对解码器零改动、保持Few-shot ICL能力、可调压缩比、能扩展长上下文并可跨解码器迁移的方法（论文在表1第6页显示4×压缩接近open-book；表3第8页展示无改动解码器即可扩展有效上下文；图1第3页、图2第4页给出关键机制；图4第9页显示可在与原文相近的存储成本下预存压缩表示）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出ARC-Encoder：以LLM编码器+小型MLP投影器组成的可插拔压缩器，在编码器最后一层自注意力中对相邻query做平均池化，将n个token压缩为n/x个连续向量，直接替换送入冻结解码器的嵌入；采用交替的重建与续写预训练（配<Rec>/<Cont>），再做少样例格式微调，可为不同解码器配置极小的专用MLP并行训练，并通过分块并行压缩实现长上下文扩展。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• ARC-Encoder++：自适应压缩比的内容感知池化与调度——依据信息密度动态决定池化粒度与压缩比，兼顾精度与效率。 • UniARC：跨模型族的统一压缩表示空间——以多解码器对比对齐与小投影适配，探索零/少参数迁移到新解码器。 • RAG-ARC：检索感知的联合检索—压缩—生成优化——用可微检索信号指导压缩与解码，端到端提升知识密集型任务EM。 • MoE-ARC：面向长上下文的专家化并行压缩——引入Mixture-of-Experts对不同文本块分路压缩，进一步提高吞吐与鲁棒性。 • FaithARC：压缩表示的忠实性与信息下界学习——建立信息论度量与可验证约束，抑制复述/幻觉，保证答案可达性。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Document Understanding, Measurement, and Manipulation Using Category Theory</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21553" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21553" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We apply category theory to extract multimodal document structure which leads us to develop information theoretic measures, content summarization and extension, and self-supervised improvement of large pretrained models. We first develop a mathematical representation of a document as a category of question-answer pairs. Second, we develop an orthogonalization procedure to divide the information contained in one or more documents into non-overlapping pieces. The structures extracted in the first and second steps lead us to develop methods to measure and enumerate the information contained in a document. We also build on those steps to develop new summarization techniques, as well as to develop a solution to a new problem viz. exegesis resulting in an extension of the original document. Our question-answer pair methodology enables a novel rate distortion analysis of summarization techniques. We implement our techniques using large pretrained models, and we propose a multimodal extension of our overall mathematical framework. Finally, we develop a novel self-supervised method using RLVR to improve large pretrained models using consistency constraints such as composability and closure under certain operations that stem naturally from our category theoretic framework.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 缺乏统一、可计算的语义结构表示：现有方法多停留在句法/统计层面，难以在语义空间中对文档进行度量、比较、合并与可操作化处理（如摘要与扩展） • 摘要与扩展缺少可组合、可验证的结构约束与指标：缺乏在数学上封闭的结构（如格与范畴）以及可量化评估（如率失真分析，见第14页图3示例） • 跨文档与多模态整合困难：缺少将不同来源/模态的语义对齐、合并并计算相似度与信息增益的统一机制 • 大模型缺乏自监督的可验证反馈：难以用可计算的一致性（可组合性、闭包性）来自动提升模型输出质量 • 语义信息度量体系薄弱：缺少对信息含量、信息密度、互信息、内容熵与多样性-深度熵等语义层面的系统化定义与计算</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>将文档离散化为问答对的范畴，在抽象式修辞DAG上定义基于可答问题集合的Jaccard距离并执行正交化，得到原子QA与其格结构；据此提出一组语义信息论度量，并把摘要/释义建模为格上的抑制/扩张，同时利用LLM执行分解与判定，并以RLVR等可验证约束进行自监督改进。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 多模态Sheaf一致性扩展与证据对齐：用Sheaf一致性保证跨模态扩展的全局一致与无冲突（结合第15页讨论） • 范畴化摘要的最优率失真界：建立摘要R(D)理论下界与近似算法，并对比操作曲线（参考第14页图3） • 语义信息度量的大规模实证：系统评估内容熵、信息密度、互信息与E/E0/E1指标的区分力与稳健性 • 基于RLVR的范畴一致性自监督训练：以可组合性与闭包性生成可验证训练信号，提升LLM一致性与可解释性 • 文档范畴间的关系学习与可微对齐：将二值关系矩阵态射推广为可学习模块，支持跨文档合成、迁移与对齐</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Taming Modality Entanglement in Continual Audio-Visual Segmentation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.17234" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.17234" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recently, significant progress has been made in multi-modal continual learning, aiming to learn new tasks sequentially in multi-modal settings while preserving performance on previously learned ones. However, existing methods mainly focus on coarse-grained tasks, with limitations in addressing modality entanglement in fine-grained continual learning settings. To bridge this gap, we introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to continuously segment new classes guided by audio. Through comprehensive analysis, two critical challenges are identified: 1) multi-modal semantic drift, where a sounding objects is labeled as background in sequential tasks; 2) co-occurrence confusion, where frequent co-occurring classes tend to be confused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework is designed to address these challenges. Specifically, for multi-modal semantic drift, a Multi-modal Sample Selection (MSS) strategy is proposed to select samples with high modal consistency for rehearsal. Meanwhile, for co-occurence confusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed, allowing for the increase of rehearsal sample frequency of those confusable classes during training process. Moreover, we construct three audio-visual incremental scenarios to verify effectiveness of our method. Comprehensive experiments demonstrate that our method significantly outperforms single-modal continual learning methods.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>• 研究空白：首次系统化提出细粒度的持续音视频分割（CAVS）任务，要求在序列任务中进行像素级声源定位并保持旧类性能；现有多模态持续学习多集中于分类/分离等粗粒度任务，无法直接处理像素级跨模态对齐与遗忘问题。 • 关键挑战：提出并实证两大模态纠缠难题——(1) 多模态语义漂移：旧类在新任务被标为背景，导致音-视对齐被错误强化为“背景”；(2) 共现混淆：高共现类别在学习新类后更易被混淆，跨模态表征彼此牵引。 • 现有方法局限：单模态CISS方法迁移到音视频场景表现欠佳；多模态CL方法缺少细粒度对齐与记忆选择机制；常规重放未考虑跨模态一致性与类间共现结构，难以抑制灾难性遗忘。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出碰撞驱动的多模态重放框架CMR，包含多模态样本选择MSS与基于碰撞的样本重放CSR：MSS以视觉/音视两模型mIoU差值评估音频贡献，选取高模态一致性样本入库；CSR统计旧模型预测与当前真值的“碰撞对”，按碰撞频率重采样易混类别以打破共现纠缠并减忘。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>• 不确定性感知的多模态样本估价与记忆优化：结合不确定性/信息增益/Shapley 评估，动态配额与自适应重放以最小化遗忘 • 开放词汇的持续音视频分割：引入文本先验与对比语言监督，实现零样本/少样本新类的无缝增量与跨模态对齐 • 多目标解耦与因果共现抑制的反纠缠学习：基于目标轨迹重放与共现图/因果推断，显式约束共现偏差并提升多目标场景鲁棒性</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 5</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>← 上一页</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button></span>
            <button id="next-btn" onclick="changePage(1)">下一页 →</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-10-27 03:42:03 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 5;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>