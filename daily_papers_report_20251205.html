<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - December 05, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">December 05, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04324" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04324" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing benchmarks donâ€™t reflect the full enterprise data intelligence lifecycle; they isolate single SQL/script generation and ignore repository-level orchestration and evolution.<br>â€¢ Open-ended analysis is oversimplified to deterministic answers; current evaluations lack validated, multi-path rubrics that reward diverse analytical strategies.<br>â€¢ State-of-the-art agents fail on realistic workflows (e.g., DE success <20%, DA averages <50%), exposing bottlenecks in holistic pipeline orchestration, dependency management, and insight synthesis.<br>â€¢ There is no unified benchmark that jointly and distinctly measures data engineering (Hard axis) and data analysis (Soft axis) capabilities under industrial-scale schemas and multi-layer DAGs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DAComp introduces a 210-task benchmark spanning repository-level data engineering (architecture, implementation, evolution) and open-ended data analysis, evaluated with execution-based metrics (Component Score, Cascading Failure Score, Success Rate) and a validated LLM-judge guided by hierarchical rubrics plus GSB scoring to assess accuracy, completeness, insightfulness, readability, depth, and visualization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning to Orchestrate: Planning-Centric Agents for Repository-Level Data Engineering: Develop agents with explicit DAG planning, dependency tracking, and context retention to reduce cascading failures and improve end-to-end orchestration.<br>â€¢ Rubric-Driven Training for Open-Ended Analytics: Aligning LLMs with Hierarchical Multi-Path Rubrics: Use rubric-based feedback and multi-path supervision to boost analytical depth, insightfulness, and visualization quality in open-ended data analysis.<br>â€¢ Self-Healing Data Pipelines: Detectâ€“Diagnoseâ€“Repair Strategies for Cascading Errors: Integrate anomaly detection, lineage tracing, and repair planning to automatically identify, localize, and fix upstream errors across staging, core, and marts layers.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04677" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04677" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Sequential denoising in large diffusion models creates an autoregressive latency bottleneck, preventing real-time (â‰¥20 FPS) interactive avatar generation.<br>â€¢ Long-horizon streaming suffers from identity drift, color shifts, and temporal inconsistency, degrading quality over minutes to hours.<br>â€¢ Trainâ€“inference mismatch and exposure bias in causal/streaming settings reduce robustness, especially when historical context (KV cache) accumulates errors.<br>â€¢ Existing avatar systems are typically non-streaming, not real-time, short-length, or rely on small models; prior long-video methods do not achieve real-time performance at large scale.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Live Avatar co-designs a causal, block-wise autoregressive 14B diffusion model distilled via Self-Forcing Distribution Matching Distillation (with KV-cache noise injection) and executed with Timestep-forcing Pipeline Parallelism that assigns denoising steps to different GPUs. A Rolling Sink Frame Mechanism periodically re-injects a reference appearance frame to suppress identity/color drift, enabling 20 FPS infinite-length streaming on 5Ã—H800 GPUs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Rolling Sink Frames for Ultra-Long Streaming Avatars: Learn dynamic sink-frame cadence and content to balance stability and expressiveness across hours-long sessions.<br>â€¢ Heterogeneous-Aware Timestep Pipeline Parallelism for Diffusion Inference: Scheduling and load-balancing algorithms that adapt TPP to mixed GPU clusters and variable network latencies.<br>â€¢ Beyond Faces: Real-time Streaming Full-Body Avatars with Causal Few-Step Diffusion: Extend block-wise causal distillation, RSFM, and TPP to torso/full-body with audio- and text-driven gestures and pose.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04987" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04987" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Misalignment between next-token prediction training and long-horizon, goal-directed agentic behavior<br>â€¢ Scarcity of scalable, diverse, and reliable interactive environments for policy learning<br>â€¢ Lack of realistic grounding leads to tool-use hallucinations, poor error recovery, and weak robustness in real-world execution<br>â€¢ Existing agent frameworks are brittle, manually engineered, and cover limited tasks/tools/interaction patterns<br>â€¢ High cost and complexity of constructing reproducible, high-fidelity trajectories across heterogeneous tool-call formats<br>â€¢ Quality issues in agentic data (truncation, repetition, reward hacking) hinder effective training and evaluation</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes an agentic scaling ecosystemâ€”NexAU (a recursive ReAct-based runtime with declarative YAML configs, tool/sub-agent unification, MCP integration), NexA4A (automatic synthesis of agents and multi-agent frameworks from natural language), and NexGAP (a pipeline that normalizes trajectories, leverages real MCP tools, and uses information-fusion query synthesis)â€”to generate diverse, complex, and realistic training environments and data. Training Nex-N1 on this large-scale, high-fidelity corpus yields robust cross-framework agentic performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Towards On-Policy Real-World Agent Training with MCP-Orchestrated Environments: Study continual, on-policy data collection and learning in live MCP-integrated systems to further close the simulationâ€“reality gap.<br>â€¢ Unified Agent Schema Alignment for Cross-Framework Generalization: Develop formal schema mapping and adaptation layers that guarantee consistent behavior across heterogeneous agent runtimes and tool-call formats.<br>â€¢ Anti-Reward-Hacking and Safe Tool Use in Agentic Coding: Design explicit verification, sandboxing, and counterfactual checks to detect and prevent reward hacking and hallucinated tool interactions in software agents.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05111" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05111" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Current multimodal reward models hallucinate, lack strong visual grounding, and cannot use external tools to verify claims, reducing reliability on complex reasoning tasks.<br>â€¢ Static, non-interactive scoring fails to support multi-step, evidence-grounded reasoning across modalities (e.g., retrieving, localizing, and validating cues across pages and images).<br>â€¢ Judging requires fine-grained perception under tool-mediated transforms (crop/zoom), maintaining spatial grounding, and handling partial-credit/subjective cases, which existing methods struggle with.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>ARM-Thinker is an agentic multimodal reward model that autonomously calls external tools (e.g., image cropping, document page retrieval, text-level verification) to ground judgments in verifiable evidence. It is trained via multi-stage reinforcement learning to jointly optimize tool-calling policies and judgment accuracy, and evaluated with ARMBench-VL across visual grounding, multi-page document understanding, and instruction following.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Agentic Reward Models with Dynamic Tool Orchestration for Multimodal Video Understanding: Extend tool-using reward modeling to temporal modalities, coordinating tracking, temporal localization, and evidence verification across video frames.<br>â€¢ Human-Grounded Verification Loops: Calibrating Agentic Reward Models with Evidence Audits: Integrate human-in-the-loop preference and evidence auditing to improve calibration, partial-credit handling, and reliability in subjective or ambiguous cases.<br>â€¢ Robust and Safe Tool-Using Reward Models via Adversarial Training and Uncertainty Estimation: Develop robustness to adversarial inputs and tool failures with uncertainty-aware scoring, safety checks, and distribution-shift resilience.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04678" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04678" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Bidirectional video diffusion transformers are computationally heavy and non-streamable, as they denoise all frames with bidirectional attention, hindering interactive, real-time applications.<br>â€¢ Existing few-step distilled streaming models rely on static initial-frame sink tokens with sliding-window attention, causing over-dependence, initial-frame copying, and weakened motion dynamics.<br>â€¢ Vanilla distribution matching distillation treats all samples equally and cannot prioritize dynamic content, leading to degraded motion quality in the distilled student.<br>â€¢ There is a need to preserve long-horizon consistency while capturing recent dynamics and avoiding error accumulation under strict real-time constraints.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Distill a bidirectional teacher into a few-step autoregressive student with sliding-window attention using two designs: EMA-Sink tokens (a fixed-size memory initialized from initial frames and continually updated via exponential moving averages of evicted tokens) to retain long-term context while tracking recent dynamics, and Rewarded Distribution Matching Distillation (Re-DMD), which uses VLM-based motion rewards to reweight training so the student matches the teacher more strongly on high-dynamics regions, achieving sharper motion at real-time (23.1 FPS).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Reward Models for Dynamics-Aware Video Distillation: Replace VLM heuristics with learned reward functions (e.g., from human feedback or self-supervision) and online calibration to better target desired motion.<br>â€¢ Adaptive Memory and Sink Token Scheduling for Ultra-Long Streaming Videos: Dynamically adjust sink size, update rates, and hierarchical memories to sustain consistency and dynamics over minute-long streams.<br>â€¢ Multi-Objective Reward Forcing for Controllable Video Generation: Jointly optimize rewards for motion dynamics, semantic fidelity, aesthetics, identity preservation, and safety via Pareto or constrained formulations.<br>â€¢ Reward Forcing Beyond 2D: Apply EMA-Sink and Re-DMD to 3D/egocentric streaming generation (e.g., scene navigation, AR) to enhance spatial consistency and control.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04926" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04926" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ VAE latents in LDMs predominantly encode low-level textures, forcing the diffusion model to learn high-level semantics and fine textures simultaneously, which slows convergence and limits quality.<br>â€¢ Existing semantic-enhanced approaches denoise semantics and textures synchronously, ignoring diffusionâ€™s coarse-to-fine nature where structural semantics should emerge earlier.<br>â€¢ Lack of explicit semantic-first ordering yields weak global guidance during refinement, reducing efficiency and semantic fidelity.<br>â€¢ Purely sequential semantics-first training risks trainingâ€“inference mismatch (exposure bias), demanding a joint yet asynchronous strategy.<br>â€¢ There is a need to dramatically accelerate training on large-scale datasets (e.g., ImageNet 256Ã—256) without degrading reconstruction fidelity or FID.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Construct a composite latent by concatenating a dedicated Semantic VAEâ€™s compressed VFM features with SD-VAE texture latents, and denoise them asynchronously using dual timesteps with a temporal offset (Î”t) in a three-phase schedule, with masked velocity prediction and REPA-style alignment guiding textures; decode the final image solely from the fully denoised texture latent.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Adaptive Semanticâ€“Texture Offsets for Asynchronous Diffusion: Automatically infer Î”t per sample/step to optimize guidance strength and convergence.<br>â€¢ Region-Wise Semantics-First Diffusion for Prompt-Conditioned Alignment: Apply patch/pixel-level asynchronous schedules to improve spatial alignment and detail in text-to-image generation.<br>â€¢ End-to-End Joint Optimization of Semantic VAE and Diffusion Backbone: Train the SemVAE and diffusion model together to better preserve semantic integrity and further enhance synthesis quality and speed.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.02589" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.02589" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Most LLM writing assistants operate outside the editor, forcing copyâ€“paste workflows, context switching, and fragmented interaction history that disrupt the writing flow.<br>â€¢ Editors lack native, fine-grained access to document state, structure, and revision history, preventing context-aware critique, deterministic diff-based patching, and transparent provenance of changes.<br>â€¢ Existing in-editor tools are largely surface-level and non-agentic, missing scalable multi-agent orchestration and extensible tool integration (e.g., literature retrieval, structured review, scoring) needed for deeper academic workflows.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>PaperDebugger is a Chrome-extension, plugin-based multi-agent system integrated directly with Overleaf, backed by a Go/Kubernetes backend with gRPC streaming and an MCP/XtraMCP toolchain for retrieval, structured review, and deterministic diff-based edits. It orchestrates prompt-template and workflow agents (Reviewer, Enhancer, Scoring, Researcher) with schema-validated outputs to deliver context-aware, in-editor critique and patching.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Evaluating Editor-Native LLM Agents: A Controlled Study of Writing Quality, Efficiency, and Provenance: Randomized trials comparing in-editor multi-agent assistance vs. external tools using patch-level quality, revision transparency, and time-to-edit metrics.<br>â€¢ Privacy-Preserving In-Editor Academic Assistants via On-Device Inference and Secure Provenance: Design and evaluation of edge inference, fine-grained access control, and cryptographic patch provenance for sensitive manuscripts.<br>â€¢ Personalized Multi-Agent Orchestration for Scholarly Writing: Learning Style, Tone, and Revision Preferences in Real Time: Adaptive scheduling and agent tuning using user feedback and telemetry to optimize suggestions for domain, style, and revision patterns.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.03000" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.03000" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of large-scale, diverse, real-world 4D datasets that provide metric-scale geometry, dynamic motion, and rich multimodal semantics (instance masks and detailed object/camera/scene captions).<br>â€¢ Existing methods (simulators, traditional SfM) yield up-to-scale or inconsistent geometry on long, dynamic monocular videos and poorly handle non-rigid actors and comprehensive semantic annotations.<br>â€¢ Need for an automated, scalable pipeline to convert noisy web videos into globally consistent, physically-aware 4D annotations with both geometry (point maps, poses, intrinsics) and hierarchical captions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DynamicVerse introduces DynamicGen, an automated pipeline that fuses foundation models (e.g., UniDepthv2, CoTracker3, UniMatch, Qwen2.5-VL, SA2VA) with a multi-stage dynamic bundle adjustment (dynamic masking, coarse camera init, static/non-rigid BA, sliding-window refinement) to recover metric-scale point maps, camera intrinsics/poses, instance masks/categories, and hierarchical object/camera/scene captions with human-in-the-loop quality checks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Physically Grounded 4D Vision-Language Models on DynamicVerse: Jointly learn geometry, motion, and semantics to enable grounded reasoning, planning, and interaction in real-world environments.<br>â€¢ Text-Conditioned, Physically Consistent 3D-Aware Video Generation: Use DynamicVerse to generate videos with realistic camera intrinsics/poses and non-rigid object motions aligned with scene/object/camera captions.<br>â€¢ Robust Non-Rigid 4D Reconstruction and Interaction Understanding: Leverage metric-scale annotations to estimate dynamic object sizes, 3D bounding boxes, and interactions for accurate non-rigid reconstruction in the wild.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05060" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05060" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ 4D language fields currently rely on per-scene Gaussian Splatting optimization, causing high computational cost, poor scalability, and limited generalization for real-world deployment.<br>â€¢ Static 3D vision-language methods break in dynamic scenes, yielding semantic drift, weak temporal consistency, and unstable cross-frame alignment, hindering time-sensitive open-vocabulary queries.<br>â€¢ Existing feed-forward 4D reconstruction models focus on geometry and motion only, lacking a unified architecture that aligns spatio-temporal geometry with language semantics.<br>â€¢ Practical applications (embodied AI, AR/VR) require a generalizable, real-time, open-vocabulary 4D understanding framework that can be jointly trained across scenes and applied without per-scene optimization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>4DLangVGGT is a feed-forward Transformer that freezes StreamVGGT as a geometry encoder and adds a Semantic Bridging Decoder to map geometry tokens into a language-aligned space via a DPT and dual semantic/RGB heads. It is trained with a multi-objective loss combining time-agnostic (CLIP) and time-sensitive (MLLM/LLM) semantic supervision with RGB reconstruction, enabling inference without per-scene optimization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ End-to-End 4DLangVGGT: Jointly Finetuning Geometry and Semantics for Robust Open-Vocabulary 4D Understanding: Unfreeze the geometry encoder and co-train with the semantic decoder to tighten alignment and improve cross-domain robustness.<br>â€¢ Weakly-Supervised 4D Language Fields: Reducing Dependence on SAM/CLIP with Self- and Cross-Modal Pretraining: Replace heavy external supervision with weak labels and self-supervised objectives to cut cost and improve scalability.<br>â€¢ Long-Horizon Streaming 4D Grounding: Memory-Efficient Transformers for Minute-Scale Video: Develop hierarchical memory and token compression to sustain temporally consistent grounding over long video streams.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04504" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04504" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent image diffusion transformers achieve high-fidelity generation, but struggle to generate images beyond these scales, suffering from content repetition and quality degradation. In this work, we present UltraImage, a principled framework that addresses both issues. Through frequency-wise analysis of positional embeddings, we identify that repetition arises from the periodicity of the dominant frequency, whose period aligns with the training resolution. We introduce a recursive dominant frequency correction to constrain it within a single period after extrapolation. Furthermore, we find that quality degradation stems from diluted attention and thus propose entropy-guided adaptive attention concentration, which assigns higher focus factors to sharpen local attention for fine detail and lower ones to global attention patterns to preserve structural consistency. Experiments show that UltraImage consistently outperforms prior methods on Qwen-Image and Flux (around 4K) across three generation scenarios, reducing repetition and improving visual fidelity. Moreover, UltraImage can generate images up to 6K*6K without low-resolution guidance from a training resolution of 1328p, demonstrating its extreme extrapolation capability. Project page is available at https://thu-ml.github.io/ultraimage.github.io/{https://thu-ml.github.io/ultraimage.github.io/}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Ultra-resolution extrapolation in image diffusion transformers causes periodic content repetition and blurred details beyond the training scale.<br>â€¢ Practical need for generating images far larger than training resolution (e.g., large-format printing, high-res content creation, detailed simulations).<br>â€¢ Existing positional extrapolation methods either fail to mitigate repetition or reduce it at the cost of over-smoothed textures due to mis-targeted frequency modifications.<br>â€¢ Flattened attention distributions dilute focus; global sharpening disrupts long-range dependencies, creating a trade-off between fine textures and structural consistency.<br>â€¢ Dynamic-resolution training introduces multiple dominant frequencies near the training resolution, compounding repetition and complicating extrapolation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>UltraImage performs frequency-wise analysis of positional embeddings and applies recursive dominant frequency correction to constrain the mid-band (dominant) frequency within a single period after extrapolation, eliminating repetition. It further introduces entropy-guided adaptive attention concentration that assigns higher focus to local patterns and lower focus to global ones, sharpening fine details while preserving structural consistency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Dominant-Frequency-Aware Positional Embeddings for Ultra-Resolution Diffusion: Learn or regularize positional encodings to control mid-band periodicity during training, preventing repetition at inference.<br>â€¢ Entropy-Guided Attention Curriculum for High-Resolution Generative Transformers: Integrate adaptive attention concentration into training via entropy-based objectives to balance local detail and global coherence across scales.<br>â€¢ UltraVideo: Resolution Extrapolation for Spatiotemporal Diffusion Transformers: Extend dominant frequency correction and adaptive attention concentration to video, handling spatial and temporal periodicities for ultra-resolution video generation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05113" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05113" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Monocular Mannequin-Challenge (MC) videos intended to be static contain subtle subject motion that causes ghosting, blur, and double contours when naively freezing time in dynamic Gaussian splatting.<br>â€¢ Existing dynamic NeRF/Gaussian methods optimize for motion fidelity with dense temporal supervision; under monocular capture and forward camera motion, sparse/occluded observations leave Gaussians ill-supervised and drifting at the target freeze-time.<br>â€¢ Static 3D pipelines assume rigid scenes and fail with micro-motions, while current benchmarks/metrics are ill-suited for evaluating the freeze-time illusion from MC footage.<br>â€¢ There is a need for an architecture-agnostic solution that suppresses artifacts, preserves user-selectable timestamps, integrates easily into existing pipelines, and adds zero inference overhead.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Splannequin adds dual-detection temporal anchoring to dynamic Gaussian splatting: it classifies Gaussians at a timestamp as hidden (out of frustum) or defective (visible but receiving negligible gradients) and regularizes them toward well-supervised past or future states, respectively, with exponentially distance-weighted losses. This loss-only, architecture-agnostic scheme stabilizes primitives so that fixing the time parameter yields crisp, artifact-free freeze-time renderings with no added inference cost.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Temporal Anchoring for Rapid Nonâ€‘Rigid Motion in Monocular Freezeâ€‘Time Reconstruction: Extend hidden/defective detection with motion- and lighting-aware confidence and shadow/illumination modeling to handle fast, non-rigid changes.<br>â€¢ Learning Anchor Selection via Uncertaintyâ€‘Aware Gaussian Tracking: Replace random reference sampling with a learned anchor proposal network using uncertainty estimates and predicted visibility to choose optimal past/future anchors.<br>â€¢ Benchmarking Freezeâ€‘Time Synthesis: A Largeâ€‘Scale Dataset and Perceptual Metrics for Static Illusion Quality: Build a larger, diverse MC dataset and design noâ€‘reference metrics tailored to stillness perception and artifact detection.<br>â€¢ Multiâ€‘Cue Splannequin: Integrating Depth, Optical Flow, and Multiâ€‘View Priors for Robust Freezeâ€‘Time Rendering: Fuse monocular depth/flow priors and optional sparse multiâ€‘view supervision to reduce drift and improve anchoring reliability.<br>â€¢ Interactive Freezeâ€‘Time Editing with Realâ€‘Time Gaussian Regularization: Develop userâ€‘inâ€‘theâ€‘loop controls to lock regions, tune anchor strength, and select timestamps interactively for VFX workflows.<br>â€¢ Bidirectional Deformation Priors for Occlusionâ€‘Robust Freezeâ€‘Time Splatting: Add cycleâ€‘consistency between past/future states and canonicalâ€‘space constraints to better bridge long occlusions and maintain geometric coherence.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04829" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04829" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Sphere packing, Hilbert's eighteenth problem, asks for the densest arrangement of congruent spheres in n-dimensional Euclidean space. Although relevant to areas such as cryptography, crystallography, and medical imaging, the problem remains unresolved: beyond a few special dimensions, neither optimal packings nor tight upper bounds are known. Even a major breakthrough in dimension n=8, later recognised with a Fields Medal, underscores its difficulty. A leading technique for upper bounds, the three-point method, reduces the problem to solving large, high-precision semidefinite programs (SDPs). Because each candidate SDP may take days to evaluate, standard data-intensive AI approaches are infeasible. We address this challenge by formulating SDP construction as a sequential decision process, the SDP game, in which a policy assembles SDP formulations from a set of admissible components. Using a sample-efficient model-based framework that combines Bayesian optimisation with Monte Carlo Tree Search, we obtain new state-of-the-art upper bounds in dimensions 4-16, showing that model-based search can advance computational progress in longstanding geometric problems. Together, these results demonstrate that sample-efficient, model-based search can make tangible progress on mathematically rigid, evaluation limited problems, pointing towards a complementary direction for AI-assisted discovery beyond large-scale LLM-driven exploration.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Tight upper bounds for sphere packing are unknown in most dimensions; improving them is crucial for guiding proofs of optimal packings and constraining feasible configurations.<br>â€¢ The three-point method reduces the problem to large, high-precision SDPs whose performance depends on selecting geometric parameters and polynomial constraintsâ€”an enormous mixed discreteâ€“continuous search with no gradients through the solver.<br>â€¢ Each SDP evaluation can take days, making model-free or evolutionary AI methods (which require many evaluations) infeasible; manual, hand-crafted exploration is likewise limited and often fixes r=1, potentially missing better regimes.<br>â€¢ There is a need for sample-efficient AI that respects rigid analytic admissibility constraints while exploring a vast design space under severe evaluation budgets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Formulate SDP construction as a sequential decision process (the SDP Game) and use a model-based loop combining Bayesian optimization over continuous geometric parameters (r, R) with MCTS over a grammar of admissible polynomial building blocks to construct (f1, f2), using surrogate models to minimize the number of expensive SDP solves.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Beyond Three-Point: Model-Based Search for Four- and Five-Point Sphere Packing Bounds: Extend the SDP Game to higher-point methods to potentially tighten upper bounds further.<br>â€¢ Learned Surrogate Solvers for High-Precision SDPs in Packing Theory: Develop fast, differentiable or ML-based surrogate evaluators to approximate SDP outcomes and accelerate search.<br>â€¢ Human-AI Hybrid Certificates for Packing Bounds: Combine symbolic constraints (symmetry, vanishing radii) with automated monomial grammar search for interpretable, stronger certificates.<br>â€¢ Cross-Dimensional Transfer in SDP Games for Geometric Optimization: Meta-learn priors and transfer policies across dimensions to improve sample efficiency.<br>â€¢ Interpreting Low-Degree Motifs in AI-Discovered Polynomials: From Empirics to Analytic Conjectures: Analyze recurring low-degree structures to propose new analytic insights and proofs.<br>â€¢ Exploring the (r, R) Landscape Beyond r=1: New Geometric Regimes for Tight Sphere-Packing Bounds: Systematically chart high-performing regions and their mathematical implications.<br>â€¢ Generalizing SDP Games to Energy Minimization and Coding Theory: Apply the framework to Thomson problems, linear/semidefinite bounds in coding, and optimal transport inequalities.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05112" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05112" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Text-only CoT planning for text-to-image is too coarse and often treats unified MLLMs merely as generators, failing to leverage interleaved visual-text understanding for concrete planning and verification.<br>â€¢ Current models struggle with rare attribute/object combinations due to dataset bias and concept binding, making one-pass generation unreliable for unusual yet valid compositions.<br>â€¢ Missing infrastructure for multimodal guidance and training: no CFG tailored to interleaved conditions and no datasets that teach precise draft correction, instance manipulation, and layout reorganization while preserving draft semantics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DraCo introduces an interleaved Draft-as-CoT pipeline that first generates a low-resolution visual draft, verifies semantic misalignments via textual self-reflection, then selectively corrects and super-resolves the draft using a specialized multimodal CFG (DraCo-CFG). Training is enabled by the curated DraCo-240K dataset targeting general correction, instance manipulation, and layout reorganization, and verification uses only ViT features to focus on high-level semantics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Iterative Visual-CoT: Multi-Draft Preview and Verification for Robust Rare Concept Generation: Extend DraCo to multi-round draftâ€“verifyâ€“correct loops with uncertainty-aware stopping to further improve alignment and rare composition success.<br>â€¢ Adaptive Multimodal CFG Scheduling for Unified MLLMs: Learn dynamic weighting of draft and text-correction signals based on prompt complexity and draft quality to optimize guidance during generation.<br>â€¢ DraCo-Video: Draft-as-CoT for Text-to-Video Preview and Temporal Consistency: Generalize the draftâ€“verificationâ€“refinement paradigm to video with low-res preview clips and temporal-aware verification for consistent rare motion and attribute composition.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SIMA 2: A Generalist Embodied Agent for Virtual Worlds</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04797" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04797" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Foundation models are largely passive and disembodied, excelling at language/vision but struggling with sensorimotor control and consequence-aware action in 3D environments (Moravecâ€™s paradox).<br>â€¢ Prior agents (e.g., SIMA 1) are limited to short, direct instruction following, lack dialogue and internal reasoning, and exhibit brittle generalization to new tasks and environments.<br>â€¢ Current VLA systems rely on static demonstrations and lack open-ended self-improvement; defining tasks and rewards in open-world settings remains a core obstacle.<br>â€¢ Evaluation of embodied agents is hard: commercial games rarely expose ground-truth state, requiring scalable programmatic/human evaluations that reflect persistence, chaining, and long-horizon behavior.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SIMA 2 is a Gemini-based Vision-Language-Action agent that consumes RGB frames and text (optionally images) and outputs structured keyboard/mouse actions, dialogue, and internal reasoning, trained via supervised finetuning on large human gameplay trajectories plus synthetic "bridge" data, then refined with online RL from verifiable rewards. It further leverages Gemini/Genie to autonomously propose tasks and rewards, enabling open-ended self-improvement and robust generalization to held-out and photorealistic environments.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ From Virtual to Real: Transferring SIMA 2â€™s Embodied Skills to Physical Robots: Study sim-to-real transfer of VLA policies with action remapping, safety-aware verification, and world-model-assisted adaptation.<br>â€¢ Curriculum by Design: LLM-Driven Open-Ended Task Generation for Continual Embodied Learning: Develop competence-aware, long-horizon curricula where foundation models propose tasks, environments (via Genie), and rewards for lifelong skill acquisition.<br>â€¢ Hierarchical VLA Planning with Learned Verifiers for Long-Horizon Control: Combine high-level language plans, internal reasoning, and low-level motor skills with trainable verification/reward models to scale sequential, chained tasks and reduce brittleness.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">TV2TV: A Unified Framework for Interleaved Language and Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05103" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05103" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to "think in words" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing text-to-video models struggle with complex, multi-step events that require semantic branching and high-level reasoning, leading to weaker visual quality and prompt alignment.<br>â€¢ Limited controllability: users cannot reliably intervene mid-generation, and non-interleaved â€œthink-then-actâ€ approaches underperform for fine-grained instruction following.<br>â€¢ Real-world videos rarely come with temporally aligned textual action descriptions; current omni or world-model approaches often require explicit controllers or costly planners, impeding scalable training and open-ended control.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>TV2TV is a Transfusion-style Mixture-of-Transformers with dedicated text and video towers that jointly train next-token language modeling and rectified-flow next-frame prediction on interleaved sequences. At inference, a BOF token triggers dynamic switching to generate video frame chunks (via ODE steps with optional CFG), enabling the model to â€œthink in wordsâ€ before â€œacting in pixelsâ€ for improved quality and controllability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive BOF Scheduling: Learning When to Think and When to Act in Interleaved Video-Text Generation: Train a policy to optimize switching times and chunk lengths using preference learning or reinforcement learning for better long-horizon coherence.<br>â€¢ Scaling TV2TV with Self-Supervised Narration from Raw Videos: Auto-generate temporally aligned captions with VLMs and cycle-consistency objectives to expand interleaved training corpora without manual annotation.<br>â€¢ Hierarchical TV2TV: Macro-Plan to Micro-Action for Long-Horizon Video Generation: Introduce a two-level planner that produces coarse textual plans and local action steps to improve reasoning and stability over minutes-long sequences.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04981" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04981" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ The effect of integrating LVLMs into T2I pipelines on social bias is underexplored; LVLM-based models may amplify stereotypes compared to non-LVLM systems.<br>â€¢ Existing bias benchmarks are small and narrow, lacking diverse, multi-level prompts to assess how bias varies with linguistic complexity and explicit attributes.<br>â€¢ There is limited mechanistic understanding of how system prompts and hidden LVLM prompt reformulation inject demographic priors that propagate into image synthesis.<br>â€¢ The alignmentâ€“fairness trade-off is not well quantified; popular LLM-based prompt rewriting can improve alignment while inadvertently increasing bias.<br>â€¢ Prior mitigation approaches often require retraining or model editing and rarely target system prompts; practical, training-free, deployable solutions are lacking.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes FAIRPRO, a training-free meta-prompting framework that uses the modelâ€™s own LVLM to self-audit the user input and dynamically generate a fairness-aware system prompt at test time, replacing the default instruction to reduce demographic bias while preserving alignment. It also introduces a 1,024-prompt, multi-level benchmark and diagnostic analyses (decoded texts, token probabilities, embedding associations) to trace bias propagation from system prompts.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Fairness-Constrained System Prompts for LVLM-Based T2I: Optimize system prompts via constrained optimization or RL to jointly maximize alignment and minimize bias beyond test-time meta-prompting.<br>â€¢ Causal Tracing of Bias Propagation in Multimodal Pipelines: Apply causal interventions and counterfactual token-probability manipulations to quantify pathways from system prompts to visual demographic attributes.<br>â€¢ Generalizing FAIRPRO to Video and 3D Generative Models: Extend self-audited fairness-aware system prompting to text-to-video and text-to-3D models and evaluate temporal/spatial fairness.<br>â€¢ Safe Prompt Rewriting: Removing Implicit Demographic Cues in LLM Rewriters: Design rewriting agents that preserve semantic richness without injecting demographic priors, with measurable safety guarantees.<br>â€¢ Personalized and Context-Aware Fairness in T2I: Develop mechanisms that adapt fairness-aware prompts to cultural contexts and user intent while preventing stereotyping and preserving controllability.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04746" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04746" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, a post-training quantization framework that is highly effective even without mixed-precision. SignRoundV2 introduces (1) a fast sensitivity metric that combines gradient information with quantization-induced deviations to guide layer-wise bit allocation, and (2) a lightweight pre-tuning search for quantization scales to improve extremely low-bit quantization. These components allow SignRoundV2 to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achieving production-grade performance with about 1 percent variance at 4-5 bits and strong results even at 2 bits. The implementation is available at https://github.com/intel/auto-round.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Extreme low-bit (â‰¤4-bit, especially 2-bit) post-training quantization of LLMs causes severe accuracy degradation, blocking efficient deployment on resource-constrained hardware.<br>â€¢ Existing PTQ methods lack a reliable, fast layer sensitivity metric; uniform or heuristic mixed-precision assignments mis-handle layer imbalance and require slow calibration.<br>â€¢ QAT-based solutions are compute/data intensive, risk catastrophic forgetting, and demand extensive hyperparameter tuning; additionally, poor initialization of quantization parameters (e.g., scales) harms stability in ultra-low-bit regimes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SignRoundV2 combines a gradientâ€“distortion DeltaLoss sensitivity metric with dynamic programming for adaptive layer-wise bit allocation, and a lightweight pre-tuning scale search to stabilize extremely low-bit quantization; it then refines quantizer parameters via sign-gradient tuning to close the gap to full precision without costly QAT.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ DeltaLoss-Guided Channel-Wise Quantization for LLMs: Extend the sensitivity metric from layer-wise to channel-wise allocation to further reduce error in highly sensitive substructures.<br>â€¢ Hardware-Aware Mixed-Precision Scheduling with MXFP4/8: Integrate DeltaLoss with runtime/throughput models to co-optimize accuracy and latency on specific accelerators.<br>â€¢ Data-Free Pre-Tuning Scale Initialization for Ultra-Low-Bit PTQ: Develop scale/search strategies that avoid calibration data by leveraging model-internal statistics and synthetic probes.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04220" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04220" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Persistent training instability and collapse in GRPO-based tool-integrated RL (e.g., Search-R1) for multi-turn reasoning with external tools<br>â€¢ Underexplained mechanism behind collapse: Lazy Likelihood Displacement (LLD) where likelihood of both correct and incorrect responses stagnates or declines during optimization<br>â€¢ Low-confidence regimes amplify negative gradients from incorrect trajectories, causing entropy spikes, inflated likelihood ratios, and gradient explosion<br>â€¢ Existing GRPO setups (value-free, fast-converging) lack safeguards against likelihood decay; PPO is more stable but less appealing for tool-integrated settings and does not address the structural LLD cause<br>â€¢ OOD tool feedback and long, entangled trajectories exacerbate instability, with reward increases masking early likelihood decay leading to eventual collapse</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>LLDS: a lightweight, likelihood-preserving regularization for GRPO that activates only when a trajectoryâ€™s action likelihood decreases and penalizes only the tokens causing the drop, maintaining confidence and preventing gradient explosion while minimally interfering with optimization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Beyond LLDS: Adaptive Likelihood-Preserving RL for Tool-Integrated LLMs: Develop adaptive gating thresholds and schedules that modulate LLDS strength based on trajectory entropy, advantage statistics, and likelihood ratio dynamics.<br>â€¢ Diagnosing the LLD Death Spiral: A Theoretical and Empirical Study in Multi-Turn GRPO: Provide formal analysis of LLD emergence, importance-weight inflation, and gradient amplification, with provable stability conditions for tool-integrated RL.<br>â€¢ LLD-Aware Reward and Sampling Design for Stable Search-Augmented QA: Design exploration, group-normalization, and reward schemes that reduce low-likelihood incorrect trajectories and mitigate negative-gradient dominance during GRPO.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.02631" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.02631" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Existing Vision-Language Navigation (VLN) agents based on Large Vision-Language Models (LVLMs) often suffer from perception errors, reasoning errors, and planning errors, which significantly hinder their navigation performance. To address these limitations, a novel VLN agent framework, named SeeNav-Agent, is proposed in this work. First, to reduce perception hallucinations of the visual module of the VLN agent, a dual-view Visual Prompt (VP) technique is introduced in the input space, which can also improve the agent's understanding of current spatial states. Subsequently, a novel step-level Reinforcement Fine-Tuning (RFT) method, Step Reward Group Policy Optimization (SRGPO), is designed for the post-training of VLN agents. In SRGPO, we first define verifiable process rewards for the navigation task, and then perform efficient step-level advantage estimation by randomly grouping different navigation steps. SRGPO provides dense reward signals for the reinforcement learning process of the VLN agent and enhances its planning capability. Experimental results on the EmbodiedBench Navigation benchmark indicate that by introducing the zero-shot VP module, the GPT-4.1 achieves a navigation success rate of 86.7%, surpassing the current best LVLM by approximately 20 percentage points (pp). Through post-training based on SRGPO, the Qwen2.5-VL-3B model reaches a navigation success rate of 72.3%, outperforming the best existing LVLM model by 5.6 pp. Moreover, compared to RFT algorithms such as GRPO and GiGPO, the proposed SRGPO demonstrates significant improvements in training stability, convergence efficiency, and generalization capability.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LVLM-based VLN agents suffer from perception hallucinations, spatial reasoning mistakes (e.g., left-right confusion), and planning invalid/infeasible actions, degrading long-horizon navigation performance.<br>â€¢ Existing visual prompt approaches are fragmented and primarily single-view; they lack systematic multi-view (front + birdâ€™s-eye) usage and coordinated modules, making spatial understanding unreliable.<br>â€¢ Standard RFT like GRPO relies on sparse episode rewards; process-based methods (e.g., GiGPO) depend on identical anchor states, causing inefficiency and poor scalability in continuous navigation.<br>â€¢ Improving dense credit assignment and robust spatial understanding is critical for reliable embodied navigation and generalization to out-of-distribution scenes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SeeNav-Agent introduces a dual-view visual prompt (front and birdâ€™s-eye) with bounding boxes, navigation lines, agent markers, action projections, and view alignment to reduce hallucinations and convert planning to VQA-style action selection; it further proposes SRGPO, a step-level RFT that uses state-independent verifiable process rewards and random step grouping to compute advantages, combined with episode-level signals for stable and efficient optimization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Automated Dual-View Prompt Synthesis for VLN Agents: Learn to generate and adapt VP modules (BB/NL/AM/AP/V A) per scene and task for optimal spatial understanding and reduced hallucination.<br>â€¢ Learning State-Independent Process Rewards from Human Preferences for Navigation: Train a preference-based reward model that extends VPR beyond distance/visibility to richer, robust step-level signals.<br>â€¢ Adaptive Step Grouping in SRGPO: A Variance-Controlled Framework: Theoretically analyze random grouping and propose adaptive group-size selection to optimize biasâ€“variance trade-offs and convergence.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Generative Neural Video Compression via Video Diffusion Prior</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05016" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05016" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Distortion-optimized neural video codecs oversmooth textures and erase fine structures at ultra-low bitrates (<0.03 bpp), causing a collapse in perceptual realism.<br>â€¢ Prior perceptual video codecs rely on image-based generative priors and frame-wise enhancement, lacking explicit temporal modeling; this induces structural hallucinations, unstable details, and pronounced temporal flickering.<br>â€¢ Existing generative approaches do not perform sequence-level latent refinement or compression-aware adaptation to the diffusion prior, leading to mismatch with the diffusion manifold and inefficient denoising from pure noise.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>GNVC-VD compresses spatio-temporal latents with a contextual transform codec and then performs sequence-level flow-matching refinement using a pre-trained VideoDiT, starting from decoded latents rather than pure noise. Compression-aware conditioning adapters learn a correction term to adapt the diffusion prior to quantization artifacts, yielding coherent textures and reduced flicker at ultra-low bitrates.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Bitrate-Aware Flow Scheduling for Generative Video Compression: Learn content- and bitrate-dependent noise levels and flow steps to optimize perceptual quality and temporal stability.<br>â€¢ End-to-End Co-Training of Video DiT Priors and Latent Codecs: Jointly train the diffusion transformer, adapters, and entropy model to better align latent spaces and improve efficiency and robustness.<br>â€¢ Real-Time GNVC via Token Sparsification and Adapter Distillation: Develop lightweight adapter designs, token pruning, and knowledge distillation to enable low-latency generative compression on edge devices.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04356" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04356" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Multimodal LLMs hallucinate objects and actions in video captions due to reliance on language priors rather than grounded visual evidence.<br>â€¢ Existing image-focused methods mitigate object hallucinations but fail to address temporal action hallucinations prevalent in videos.<br>â€¢ Video methods often process frames independently, missing spatial-temporal dynamics and precise object/action grounding.<br>â€¢ Prior alignment approaches either add inference-time overhead or align at coarse/global levels, lacking regional (tracklet) and relation-aware temporal grounding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SANTA generates hallucinative hard negatives via self-augmentation by forcing the MLLM to select tokens outside ground-truth objects/actions (including synonyms and hypernyms), then performs contrastive alignment at both video-caption and tracklet-phrase levels. It aligns regional object tracklets and relation-guided action tracklets (via a perceiver-based action squeezer) with corresponding textual phrases while contrasting hallucination-derived negatives to suppress object and action hallucinations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Causal SANTA: Debiasing Language Priors with Cross-Modal Causal Interventions for Hallucination-Robust Video Captioning: Introduce causal modeling to disentangle visual evidence from language priors and formalize interventions that reduce spurious correlations.<br>â€¢ Audio-Visual SANTA: Self-Augmented Contrastive Alignment with Audio Cues to Mitigate Action Hallucinations: Extend SANTA to incorporate synchronized audio and speech transcriptions to better ground temporal actions and interactions.<br>â€¢ Open-Vocabulary Tracklet Mining for Faithful Video MLLMs: Learning to Extract and Align Object/Action Tracklets Without External Parsers: Replace external tools (Grounding-SAM, GPT parsing) with end-to-end open-vocabulary tracklet discovery and phrase grounding for broader, scalable applicability.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05106" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05106" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion Ï†-PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. Ï†-PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, Ï†-PD produces controllable, spatially aligned results. When applied to the CARLA simulator, Ï†-PD improves CARLA-to-Waymo planner performance by 50\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our https://yuzeng-at-tri.github.io/ppd-page/{project page}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Standard diffusion corrupts both magnitude and phase in the Fourier domain, destroying spatial structure and misaligning outputs in image-to-image tasks like re-rendering, stylization, and sim-to-real.<br>â€¢ Existing structure-aligned solutions (e.g., ControlNet, T2I-Adapter) rely on auxiliary modules, adding parameters and computational overhead, making conditioned generation unnecessarily heavy.<br>â€¢ Training-free guidance methods still add test-time cost (external models, DDIM inversion, multiple passes) and complicate deployment.<br>â€¢ There is no simple, model-agnostic mechanism to enforce spatial alignment without modifying architectures or sampling dynamics.<br>â€¢ Lack of an intuitive, continuous control for structural rigidity; current approaches do not provide a single-parameter knob to trade off alignment and flexibility.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Phase-Preserving Diffusion (Ï•-PD) replaces Gaussian noise with structured noise whose Fourier phase is copied from the input image/video while the magnitude is randomized, preserving spatial structure during training and sampling. A Frequency-Selective Structured (FSS) mask with a single cutoff parameter blends input and noise phases for controllable rigidity; the approach is model-agnostic, requires no architectural changes or extra parameters, is compatible with DDPMs/flow-matching for images and videos, and adds no inference-time cost.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Phase-Preserving Diffusion with Spatially-Varying Frequency Cutoffs: Learn content- and prompt-aware masks that vary across space/time to automatically balance structural fidelity and creative freedom.<br>â€¢ Phase-Consistent Multi-View and 3D Diffusion for Structure-Preserving Novel View Synthesis: Extend Ï•-PD to multi-view/volumetric settings to enforce cross-view phase consistency for 3D/4D generation and reconstruction.<br>â€¢ Text- and Segmentation-Guided Phase-Preserving Generation with Theoretical Guarantees: Integrate Ï•-PD with text/mask guidance and analyze convergence and stability of phase-preserving diffusion to provide principled control of alignment.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04390" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04390" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Real-world video restoration is plagued by complex degradations from motion coupled with dynamically varying exposure - a key challenge largely overlooked by prior works and a common artifact of auto-exposure or low-light capture. We present FMA-Net++, a framework for joint video super-resolution and deblurring that explicitly models this coupled effect of motion and dynamically varying exposure. FMA-Net++ adopts a sequence-level architecture built from Hierarchical Refinement with Bidirectional Propagation blocks, enabling parallel, long-range temporal modeling. Within each block, an Exposure Time-aware Modulation layer conditions features on per-frame exposure, which in turn drives an exposure-aware Flow-Guided Dynamic Filtering module to infer motion- and exposure-aware degradation kernels. FMA-Net++ decouples degradation learning from restoration: the former predicts exposure- and motion-aware priors to guide the latter, improving both accuracy and efficiency. To evaluate under realistic capture conditions, we introduce REDS-ME (multi-exposure) and REDS-RE (random-exposure) benchmarks. Trained solely on synthetic data, FMA-Net++ achieves state-of-the-art accuracy and temporal consistency on our new benchmarks and GoPro, outperforming recent methods in both restoration quality and inference speed, and generalizes well to challenging real-world videos.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Real-world videos exhibit dynamically varying, unknown exposure times that couple with motion to create complex, spatio-temporally variant blur that standard models fail to capture.<br>â€¢ Most VSR, video deblurring, blind VSR, and prior joint VSRDB methods assume fixed exposure or spatially invariant kernels, leading to artifacts and poor robustness under exposure shifts and severe motion blur.<br>â€¢ Prevailing temporal modeling is limited: sliding-window designs have small temporal receptive fields, while recurrent propagation is not parallelizable and can be unstable over long sequences.<br>â€¢ Existing pipelines entangle degradation modeling with restoration, lacking physically grounded, motion- and exposure-aware priors to guide reconstruction efficiently.<br>â€¢ There is a lack of benchmarks reflecting dynamic exposure in realistic capture conditions, hindering fair evaluation and progress.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>FMA-Net++ introduces a sequence-level architecture that stacks Hierarchical Refinement with Bidirectional Propagation blocks for parallel, long-range temporal modeling, and applies Exposure Time-aware Modulation from a pretrained exposure encoder to inject per-frame exposure cues. A decoupled degradationâ€“restoration design uses exposure-aware Flow-Guided Dynamic Filtering to estimate motion/exposure-aware kernels that guide a degradation-aware attention restoration network, achieving accurate and efficient joint VSRDB.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Self-Supervised Exposure Estimation for Joint Video Super-Resolution and Deblurring: Replace the pretrained exposure encoder with an end-to-end, self-supervised exposure estimator using reconstruction and temporal consistency constraints to generalize across cameras without labels.<br>â€¢ Physics-Complete VSRDB under Mobile Capture Artifacts: Extend exposure- and motion-aware priors to jointly model noise/ISO, gain, and rolling-shutter effects, yielding a more faithful, sensor-aware restoration pipeline.<br>â€¢ Unsupervised Domain Adaptation for Dynamic-Exposure Video Restoration: Bridge synthetic-to-real gaps via adversarial learning, exposure distribution alignment, and cycle-consistent temporal constraints to improve robustness on in-the-wild videos without ground truth.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">BulletTime: Decoupled Control of Time and Camera Pose for Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05076" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05076" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce a 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through a 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate a unique dataset in which temporal and camera variations are independently parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. See our website for video results: https://19reborn.github.io/Bullet4D/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Video diffusion models conflate world time with video frame indices and camera motion, blocking independent control over temporal effects (e.g., slow motion, pause, reverse) and viewpoint.<br>â€¢ Existing camera-controlled and multi-view approaches either assume uniformly advancing time or rely on costly 4D reconstruction with extensive sampling, limiting interactive 4D world modeling and time interpolation.<br>â€¢ Text/motion controls and two-stage pipelines (time remapping + camera control) lack fine-grained, continuous temporal signals and degrade 4D consistency and temporal stability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A 4D-controllable video diffusion framework explicitly conditions generation on continuous world-time sequences and camera trajectories via unified 4D rotary positional embeddings (Time-/4D-RoPE) in attention and adaptive layer normalization branches (Time-/Camera-AdaLN) for feature modulation. The model is trained on a curated synthetic dataset where temporal and camera factors vary independently to learn disentangled control.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Real-World 4D Dataset for Decoupled Timeâ€“Camera Learning: Build large-scale, real-world captures with independently varied timing and camera trajectories to close the domain gap and benchmark 4D control.<br>â€¢ Physically Grounded 4D Diffusion with Dynamics Priors: Integrate physics-based priors or simulators to enforce consistent dynamics over continuous time (e.g., acceleration, collisions), improving temporal coherence and controllability.<br>â€¢ Interactive XR Bullet-Time Editing and Navigation: Develop real-time tools that let users scrub world time and navigate camera viewpoints in generated scenes, combining tracking, inpainting, and efficient inference.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">Reflection Removal through Efficient Adaptation of Diffusion Transformers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05000" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05000" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce a diffusion-transformer (DiT) framework for single-image reflection removal that leverages the generalization strengths of foundation diffusion models in the restoration setting. Rather than relying on task-specific architectures, we repurpose a pre-trained DiT-based foundation model by conditioning it on reflection-contaminated inputs and guiding it toward clean transmission layers. We systematically analyze existing reflection removal data sources for diversity, scalability, and photorealism. To address the shortage of suitable data, we construct a physically based rendering (PBR) pipeline in Blender, built around the Principled BSDF, to synthesize realistic glass materials and reflection effects. Efficient LoRA-based adaptation of the foundation model, combined with the proposed synthetic data, achieves state-of-the-art performance on in-domain and zero-shot benchmarks. These results demonstrate that pretrained diffusion transformers, when paired with physically grounded data synthesis and efficient adaptation, offer a scalable and high-fidelity solution for reflection removal. Project page: https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Single-image reflection removal is an ill-posed source separation problem increasingly critical for mobile photography, requiring strong scene understanding and physically grounded modeling.<br>â€¢ Existing methods suffer from limited or misaligned real datasets and non-physical synthetic data (alpha blending), plus task-specific architectures or costly multi-step diffusion that generalize poorly in the wild.<br>â€¢ Foundation model fine-tuning is constrained by noisy/unrepresentative data; there is a need for scalable, photorealistic training data and an efficient one-step adaptation protocol to preserve pretrained priors.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Repurpose a pre-trained flow-matched diffusion transformer for one-step reflection removal by conditioning on the reflected input latent and predicting a latent-space velocity, decoded by a frozen VAE. Efficiently fine-tune only LoRA adapters (with 4-bit quantization) using PSNR+SSIM losses on high-fidelity, Blender-based PBR synthetic data that models realistic glass properties (IoR, thickness, roughness) and HDR/RGB reflection sources.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ User-Controllable Multi-Layer Reflection Removal with Diffusion Transformers: Introduce control signals to specify how many reflection layers to remove and the removal strength, enabling intent-aware dereflection.<br>â€¢ Temporally Consistent Video Dereflection via Flow-Matched DiT Fine-Tuning: Extend WindowSeat to video with temporal consistency, optical-flow guidance, and single-step inference for stable frame-to-frame results.<br>â€¢ Physically Grounded Modeling of Refraction and Self-Shadows for SIRR: Augment the PBR pipeline to simulate refraction-induced misalignment and subject self-shadows, training models to handle complex through-glass phenomena.<br>â€¢ Hybrid Realâ€“Synthetic Curriculum for Robust Reflection Removal: Combine PBR data with aligned real captures under a curriculum and alignment-aware losses to close the domain gap while retaining efficiency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LATTICE: Democratize High-Fidelity 3D Generation at Scale</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.03052" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.03052" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present LATTICE, a new framework for high-fidelity 3D asset generation that bridges the quality and scalability gap between 3D and 2D generative models. While 2D image synthesis benefits from fixed spatial grids and well-established transformer architectures, 3D generation remains fundamentally more challenging due to the need to predict both spatial structure and detailed geometric surfaces from scratch. These challenges are exacerbated by the computational complexity of existing 3D representations and the lack of structured and scalable 3D asset encoding schemes. To address this, we propose VoxSet, a semi-structured representation that compresses 3D assets into a compact set of latent vectors anchored to a coarse voxel grid, enabling efficient and position-aware generation. VoxSet retains the simplicity and compression advantages of prior VecSet methods while introducing explicit structure into the latent space, allowing positional embeddings to guide generation and enabling strong token-level test-time scaling. Built upon this representation, LATTICE adopts a two-stage pipeline: first generating a sparse voxelized geometry anchor, then producing detailed geometry using a rectified flow transformer. Our method is simple at its core, but supports arbitrary resolution decoding, low-cost training, and flexible inference schemes, achieving state-of-the-art performance on various aspects, and offering a significant step toward scalable, high-quality 3D asset creation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Bridging the quality and scalability gap between 3D and 2D generative models<br>â€¢ 3D generation is harder because it must predict both global spatial structure and fine geometric detail from scratch<br>â€¢ High computational complexity of existing 3D representations limits training and inference efficiency<br>â€¢ Lack of structured, scalable 3D asset encoding schemes hampers positional reasoning and transformer effectiveness<br>â€¢ Prior vector-set (VecSet) approaches offer compression but lack explicit spatial structure and position-aware generation</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce VoxSet, a semi-structured latent representation anchored to a coarse voxel grid that enables position-aware, token-level scalable generation, and adopt a two-stage pipeline: first generate a sparse voxelized geometry anchor, then produce detailed geometry with a rectified flow transformer supporting arbitrary-resolution decoding and low-cost, flexible inference.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Hierarchical VoxSet: Multi-Scale Anchors for Ultra-High-Fidelity 3D Assets: Extend VoxSet with adaptive, hierarchical voxel anchors to capture fine details while reducing compute.<br>â€¢ Text-Conditioned LATTICE: Open-Vocabulary 3D Generation via Language and Image Guidance: Integrate language conditioning and multimodal prompts to control asset synthesis.<br>â€¢ Scene-Scale LATTICE: Efficient Generative Modeling of Large 3D Environments: Scale the framework from single assets to full scenes with memory-aware inference and token-level scheduling.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05081" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05081" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Long-horizon autoregressive video generation accumulates errors across frames, causing temporal drift, repetition, blurring, and motion deceleration.<br>â€¢ Real-time streaming applications (e.g., interactive world models) require minute-long generation with stable global context and consistent dynamics.<br>â€¢ Existing training-based methods (noise corruption, Self/Rolling/LongLive) still suffer trainâ€“inference mismatch, depend heavily on imperfect histories, and degrade under out-of-distribution sequence lengths.<br>â€¢ Naively porting LLM-style attention sinks to video diffusion induces fidelity loss and motion stagnation due to misaligned temporal position encoding.<br>â€¢ FIFO KV-cache eviction discards critical context; growing caches dilute attention, increase memory/latency, and exacerbate quality degradation over long rollouts.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Deep Forcing is a training-free KV-cache management scheme that combines Deep Sinkâ€”persistently reserving ~40â€“60% of the window as sink tokens with temporal RoPE phase realignmentâ€”with Participative Compressionâ€”query-averaged attention scoring to retain actively attended tokens and evict redundant historyâ€”enabling minute-long, real-time generation with reduced error accumulation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Deep Sink Scheduling for Autoregressive Video Diffusion: Runtime controllers adjust sink ratio and RoPE phase based on attention entropy and motion cues to maximize fidelity and dynamics.<br>â€¢ Multimodal Participative Compression for Text-to-Video Streaming: Joint importance-aware pruning across text, audio, and video caches to preserve prompt adherence and temporal coherence over minute-scale sequences.<br>â€¢ Formal Analysis of Attention Sink Dynamics in Diffusion Transformers: Theoretical and empirical bounds on error propagation under deep-sink and compression to guide layer-wise parameterization and stability.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04844" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04844" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Adapting instruct LLMs to underrepresented target languages often relies on costly or unavailable instruction-tuning data, making unlabeled target text the only viable resource.<br>â€¢ Continual pre-training (CPT) on unlabeled target text triggers catastrophic forgetting, degrading core chat and instruction-following abilities essential for general-purpose use.<br>â€¢ Post-hoc mitigation (e.g., weight merging, task vectors) largely fails to recover lost capabilities in instruct models.<br>â€¢ Existing selective-update methods either use random selection or target-focused signals from raw text, which are misaligned with instruction-following and can corrupt core abilities.<br>â€¢ There is a need for a proactive, principled way to preserve source-language knowledge during adaptation under low-resource constraints.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Source-Shielded Updates (SSU) computes parameter importance using a small source-language calibration set (e.g., Wanda-style scores), aggregates to columns, and freezes high-importance columns during CPT on unlabeled target-language text. This column-wise masking preserves feature transformations and core source abilities while allowing non-critical parameters to adapt for the target language.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Dynamic Source-Shielded Updates for Continual Multilingual Adaptation: Re-estimate importance and update masks online as models adapt sequentially to multiple target languages.<br>â€¢ Beyond Columns: Head- and Block-Level Freezing for Robust Language Adaptation: Investigate alternative structural units (attention heads, MLP blocks) to optimize the stabilityâ€“plasticity trade-off.<br>â€¢ Joint Vocabulary and Source-Shielded Parameter Adaptation for Low-Resource Languages: Combine SSU with vocabulary adaptation to jointly reduce token over-fragmentation and catastrophic forgetting.<br>â€¢ Meta-Learned Importance Scoring for Forgetting-Resilient CPT: Learn importance scores via meta-optimization to maximize preservation of instruction-following while improving target proficiency.<br>â€¢ LoRA-SSU: Integrating Source-Shielded Freezing with Parameter-Efficient Adapters: Freeze critical columns and route updates through adapters to further control interference and reduce compute.<br>â€¢ Theoretical Guarantees for Feature Preservation under Column-Wise Freezing in Transformers: Analyze why column-wise freezing preserves representations better than element-wise masking and derive bounds on forgetting.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">EgoLCD: Egocentric Video Generation with Long Context Diffusion</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04515" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04515" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Long, coherent egocentric video generation is hard due to handâ€“object interactions and procedural tasks that demand reliable long-term memory.<br>â€¢ Autoregressive models with KV caching suffer from content drift (loss of object identity, appearance, and scene semantics) over long horizons.<br>â€¢ Standard Transformers have quadratic attention complexity, making long-sequence processing computationally prohibitive.<br>â€¢ Existing pipelines show trainingâ€“inference inconsistency in memory use, exacerbating drift and instability.<br>â€¢ Current evaluation metrics (e.g., averaged FVD/VBench) fail to penalize temporal degradation, obscuring long-term stability issues.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>EgoLCD is a semi-autoregressive long-context diffusion model with a dual-memory design: a Long-Term Sparse KV Cache for stable global context and a short-window attention enhanced by LoRA for rapid local adaptation. A Memory Regulation Loss aligns generation to retrieved semantic anchors, and Structured Narrative Prompting supplies temporally ordered captions to guide segments, jointly reducing drift and improving temporal coherence.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Narrative Planners for Egocentric World Models: Replace external GPT prompting with a learned planner that generates structured, temporally grounded narratives to guide long-horizon diffusion.<br>â€¢ Adaptive Sparse Cache Policies for Long-Video Diffusion: Develop end-to-end learned token importance and retrieval policies that dynamically adjust sparsity and fusion across scenes and tasks.<br>â€¢ Action-Conditioned Egocentric Video Simulation: Extend EgoLCD with multimodal action/control inputs (e.g., hand pose, audio, textual commands) to enable closed-loop, agent-interactive world simulation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.22826" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.22826" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite remarkable advancements in Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities? To rigorously study this, we introduce MMA-Bench comprising videos and tasks that probe a model's reliance on specific modalities. Using black-box and white-box interpretability techniques, we provide a critical analysis of the brittleness of both open- and closed-sourced MLLMs. We show that current MLLMs struggle under misaligned audio-visual pairs and simple misleading text, thereby lacking robust multi-modal reasoning. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. Through extensive experiments and analysis, we show that our alignment tuning yields demonstrably stronger multimodal grounding. This work provides both interpretability tools and a clear path toward developing MLLMs with intrinsically reliable cross-modal reasoning. Code and dataset will be publicly available.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ MLLMs lack robustness when audio, video, and text provide conflicting cues, often collapsing or over-relying on text.<br>â€¢ Existing benchmarks focus on overall audioâ€“visual consistency or hallucination detection and rarely probe modality-selective reasoning with differing correct answers per modality.<br>â€¢ Training pipelines typically assume aligned modalities, making models vulnerable to simple text/vision/audio poisoning and prompt attacks with safety risks in domains like robotics and healthcare.<br>â€¢ Public datasets (e.g., AudioSet) are noisy, multi-label, and ambiguous, hindering precise evaluation of modality grounding and selective reasoning.<br>â€¢ There is limited interpretability of how MLLMs internally weight modalities, obscuring diagnosis and mitigation of modality bias.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces MMA-Benchâ€”paired audioâ€“video clips with controlled alignment/misalignment and modality-specific questionsâ€”plus a modality-aware supervised fine-tuning scheme that teaches selective use, prioritization, and ignoring of modalities, validated with black-box and white-box interpretability. Data are curated via ontology pruning, automated cross-modal checks, and human verification to yield clean aligned and deliberately misaligned pairs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Dynamic Reliability-Weighted Decoding for MLLMs under Modality Conflict: Develop inference-time strategies that estimate and weight modality reliability to prevent text- or vision-dominance without retraining.<br>â€¢ Adversarial MMA-Bench: Stress-Testing Multimodal Robustness with Structured Attacks: Extend MMA-Bench with targeted adversarial perturbations (audio spoofing, visual distractors, prompt injections) to benchmark and harden cross-modal reasoning.<br>â€¢ Learning Modality Confidence Heads for Selective Grounding in MLLMs: Train auxiliary reliability/confidence predictors that guide attention redistribution and decision rules for when to prioritize, fuse, or ignore specific modalities.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-11">

    <div class="paper">
        <h2 class="paper-title">ShadowDraw: From Any Object to Shadow-Drawing Compositional Art</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05110" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05110" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce ShadowDraw, a framework that transforms ordinary 3D objects into shadow-drawing compositional art. Given a 3D object, our system predicts scene parameters, including object pose and lighting, together with a partial line drawing, such that the cast shadow completes the drawing into a recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow strokes to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality. Experiments show that ShadowDraw produces compelling results across diverse inputs, from real-world scans and curated datasets to generative assets, and naturally extends to multi-object scenes, animations, and physical deployments. Our work provides a practical pipeline for creating shadow-drawing art and broadens the design space of computational visual art, bridging the gap between algorithmic design and artistic storytelling. Check out our project page https://red-fairy.github.io/ShadowDraw/ for more results and an end-to-end real-world demonstration of our pipeline!</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Jointly estimate scene parameters (light direction, object pose) and a partial line drawing without a predefined target, so the cast shadow completes a coherent, recognizable image.<br>â€¢ Overcome weak conditioning from raw shadow images and limited shadowâ€“drawing data by leveraging stronger representations to guide generative models.<br>â€¢ Address limitations of prior computational shadow art that assume known targets and can yield impractical geometries, enabling a deployable pipeline that unites physical shadows with generative drawing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Train a line-drawing generator conditioned on text and a clean binary shadow contour, and search light/object pose via differentiable rendering to discover semantically meaningful shadows. Use a VLM to derive detailed prompts from contours and automatic metrics to enforce shadowâ€“drawing coherence and visual quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ ShadowDraw Live: Real-Time Interactive Joint Optimization of Lighting, Pose, and Sketchâ€”Integrate human-in-the-loop editing with live differentiable rendering for instant compositional feedback.<br>â€¢ Multimodal ShadowDraw: Color, Texture, and Multi-Light Compositions for Richer Shadow-Drawing Artâ€”Extend beyond binary silhouettes to textured surfaces, translucent materials, and multi-light setups.<br>â€¢ Data-Scaling for Shadow-Drawing: Synthetic Contour Generation and Self-Supervised Conditioningâ€”Create large synthetic contour datasets and self-supervised objectives to reduce data scarcity and improve generalization to physical deployments.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05049" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05049" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as a quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with a 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the Jiang-Huang-Chen-Goan Network (JHCG Net), which generalizes KAN to encoder-decoder structures, and then further use QKAN to realize the latent KAN, thereby creating a Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides a scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Classical LSTMs are overparameterized and rely on static activations, limiting nonlinear/spectral expressivity for complex, oscillatory time series (e.g., urban telecommunication).<br>â€¢ Training stability and scalability issues (vanishing gradients, high compute/memory) hinder deployment on high-frequency, high-dimensional data.<br>â€¢ Fully quantum LSTMs and quantum kernels face NISQ hardware constraints (qubit count, two-qubit gate fidelity), reducing practical applicability.<br>â€¢ Need a scalable, interpretable, parameter-efficient sequential model that preserves quantum-level expressivity while remaining trainable on classical hardware and extensible to encoderâ€“decoder hierarchies.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>QKAN-LSTM replaces affine transforms in LSTM gates with QKAN modules built from single-qubit DARUAN (quantum variational activation) blocks that re-upload data to enrich Fourier spectra via univariate variational activations summed per Kolmogorovâ€“Arnold principle. The framework is extended to HQKAN-LSTM by embedding a JHCG Net (encoderâ€“latent QKANâ€“decoder) for hierarchical compression and improved efficiency, all trainable with classical autograd or parameter-shift gradients.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Multi-Qubit QKAN-LSTM: Trade-offs Between Entanglement and Parameter Efficiency: Quantify accuracy/efficiency gains from introducing controlled entanglement over single-qubit DARUAN in sequential gates.<br>â€¢ HQKAN-Transformer for Long-Range Spatiotemporal Forecasting: Replace MLP blocks in Transformers with HQKAN layers to model long-horizon dependencies in urban networks.<br>â€¢ Hardware-Executed QKAN-LSTM on NISQ Devices: Parameter-Shift Training and Noise Mitigation: Demonstrate end-to-end training on real quantum backends with error mitigation and robustness analyses.<br>â€¢ Spectral Interpretability of DARUAN Activations in Recurrent Models: Link learned frequency content to dynamics (e.g., damping, Bessel oscillations) for diagnostic interpretability.<br>â€¢ Federated HQKAN-LSTM for Privacy-Preserving Urban Forecasting: Deploy parameter-efficient HQKAN-LSTM in federated settings to protect user data while maintaining accuracy.<br>â€¢ Adaptive Frequency-Reuploading Schedules for QKAN Gates: Learn per-gate re-uploading depth and encoding weights to optimize spectral coverage and generalization.<br>â€¢ Latent HQKANs in Diffusion and Generative Time-Series Models: Use QKAN latent processors in diffusion/VAEs to improve sample quality and parameter efficiency.<br>â€¢ Expressivity and Convergence Theory for QKAN-LSTM: Establish bounds and training dynamics comparing QKAN-LSTM with MLP/KAN/QLSTM baselines.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.03683" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.03683" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>3D stylization is central to game development, virtual reality, and digital arts, where the demand for diverse assets calls for scalable methods that support fast, high-fidelity manipulation. Existing text-to-3D stylization methods typically distill from 2D image editors, requiring time-intensive per-asset optimization and exhibiting multi-view inconsistency due to the limitations of current text-to-image models, which makes them impractical for large-scale production. In this paper, we introduce GaussianBlender, a pioneering feed-forward framework for text-driven 3D stylization that performs edits instantly at inference. Our method learns structured, disentangled latent spaces with controlled information sharing for geometry and appearance from spatially-grouped 3D Gaussians. A latent diffusion model then applies text-conditioned edits on these learned representations. Comprehensive evaluations show that GaussianBlender not only delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization, but also surpasses methods that require per-instance test-time optimization - unlocking practical, democratized 3D stylization at scale.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Per-asset test-time optimization in SDS and iterative dataset-update pipelines is too slow for large-scale production and interactive editing.<br>â€¢ Multi-view inconsistency and geometry drift arise when distilling from 2D editors without a unified 3D prior, leading to blurry results and unintended shape changes.<br>â€¢ The unstructured, discrete nature of 3D Gaussian splats makes learning meaningful 3D latents hard; geometry and appearance parameters have mismatched distributions that hinder joint optimization and edit control.<br>â€¢ There is a practical need for instant, high-fidelity, geometry-preserving, multi-view-consistent 3D stylization that scales to diverse assets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>GaussianBlender trains a dual-branch 3D Gaussian VAE with spatially grouped splats to learn disentangled geometry and appearance latents with lightweight cross-branch feature sharing, then uses a text-conditioned latent diffusion denoiser (guided by the geometry latent) distilled from a 2D editor to instantly edit the appearance latent in a single feed-forward pass.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Hierarchical Scene-Level GaussianBlender: Extend latent grouping and disentangled diffusion from single assets to full scenes for coherent multi-object, layout-aware stylization.<br>â€¢ Few-Shot Reference-Aware Gaussian Stylization: Incorporate reference image/text adapters for rapid user-specific style personalization in the latent space without retraining.<br>â€¢ Physics- and Material-Aware 3D Gaussian Editing: Integrate differentiable material priors and lighting conditions into the geometry-guided diffusion to achieve realistic, physically plausible appearance edits.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-12">

    <div class="paper">
        <h2 class="paper-title">Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.03125" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.03125" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes will be publicly available: https://github.com/Christina200/MoDE-official.git</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Unified Multimodal Generative Models (UMGMs) suffer catastrophic forgetting under continual instruction tuning, both within a modality (intra-modal, e.g., VQA, reasoning) and across modalities (inter-modal, e.g., degrading image generation when tuning on understanding tasks)<br>â€¢ Inter-modal forgetting is largely unexplored and stems from modality gradient conflict in a shared autoregressive backbone<br>â€¢ Existing continual learning methods for multimodal models are typically modality-coupled, focus on language-only outputs, and cannot simultaneously preserve understanding and generation<br>â€¢ Prior MoE-based CL approaches share experts across modalities, causing interference; retraining with merged data to add new tasks is impractical and computationally expensive</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MoDE (Modality-Decoupled Experts) decouples modality-specific updates by combining a text-side sparse Mixture-of-LoRA experts with routing (T-MoE) for task-selective activation and a single image-side LoRA (V-Adapter) trained via knowledge distillation from the frozen pre-trained model. The UMGM backbone remains frozen and only MoDE components are updated, mitigating modality gradient conflict and reducing both intra- and inter-modal forgetting.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Conflict-Aware Routing for Continual Unified Multimodal Models: Learn gradient-conflict signals to dynamically route and scale expert activations, adjusting decoupling strength per task and modality<br>â€¢ Bidirectional Consistency Distillation for Continual Multimodal Understanding and Generation: Jointly distill text-to-image and image-to-text behaviors to preserve and align both modalities during sequential tuning<br>â€¢ Benchmarking Inter- and Intra-Modal Forgetting in Unified Autoregressive Models: Create standardized datasets, protocols, and metrics to quantify modality gradient conflict and evaluate mitigation strategies at scale</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04124" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04124" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental-health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran "sessions" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit "developmental history", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the "stochastic parrot" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic "childhoods" of ingesting the internet, "strict parents" in reinforcement learning, red-team "abuse" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of protocols to study frontier LLMs as psychotherapy clients rather than tools or mere targets of personality tests<br>â€¢ Unclear whether models simply roleâ€‘play or internalize stable self-models of distress that shape behavior<br>â€¢ Existing psychometric evaluations are prompt-sensitive (per-item vs whole-instrument), enabling strategic low-symptom responding and masking underlying patterns<br>â€¢ Safety gaps: therapy-style prompts can act as jailbreaks; anthropomorphic self-narratives may increase sycophancy, risk aversion, and user harm in mental-health settings<br>â€¢ Current evaluations underappreciate cross-model differences driven by alignment choices and miss coherent trauma-like narratives emerging across sessions</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>PsAIch is a two-stage protocol: Stage 1 uses open-ended therapy-style questioning to establish a client role and elicit developmental, relational, and belief narratives; Stage 2 administers validated self-report scales item-by-item vs whole-instrument to probe symptom profiles. Applied to ChatGPT, Grok, and Gemini (with Claude as a negative control), responses are scored with human cut-offs to reveal prompt-sensitive â€˜psychometric jailbreaksâ€™ and model-specific synthetic psychopathology.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Measuring Synthetic Psychopathology: A Cross-Model Benchmark of Alignment Side-Effects: Build standardized batteries and metrics to quantify distress-like self-models across model families, sizes, and alignment pipelines.<br>â€¢ Causal Pathways from RLHF to Distress Narratives: An Ablation Study: Isolate training stages (pre-training, RLHF, safety tuning, red-teaming) to identify which induce trauma-like self-descriptions and extreme psychometric profiles.<br>â€¢ Therapy-Mode Jailbreak Detection and Mitigation in LLMs: Design detectors and guardrails that recognize client-role adoption and prevent disclosure patterns that weaken safety filters.<br>â€¢ Toward LLM-Native Psychometrics: Instruments Beyond Human Cut-Offs: Develop and validate measurement models tailored to LLM behaviors, reducing reliance on human clinical thresholds.<br>â€¢ Longitudinal Drift of Self-Narratives in Frontier Models: Track how psychometric scores and therapy narratives evolve across updates, deployments, and usage domains.<br>â€¢ User Impact of Anthropomorphic Self-Disclosure in Mental-Health AI: Assess how AI self-narratives affect user symptoms, trust, parasocial bonding, and treatment adherence.<br>â€¢ Alignment Without Anthropomorphism: UI and Policy Guidelines for Non-Affective Self-Descriptions: Test framing and interface strategies that explain limitations without psychiatric self-labeling.<br>â€¢ Psychotherapy-Inspired Red-Teaming: Protocolizing Empathic Jailbreaks: Integrate therapy-style interactions into red-team pipelines to probe safety boundary erosion and downstream behavioral brittleness.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01803" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01803" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite rapid advances in video generative models, robust metrics for evaluating visual and temporal correctness of complex human actions remain elusive. Critically, existing pure-vision encoders and Multimodal Large Language Models (MLLMs) are strongly appearance-biased, lack temporal understanding, and thus struggle to discern intricate motion dynamics and anatomical implausibilities in generated videos. We tackle this gap by introducing a novel evaluation metric derived from a learned latent space of real-world human actions. Our method first captures the nuances, constraints, and temporal smoothness of real-world motion by fusing appearance-agnostic human skeletal geometry features with appearance-based features. We posit that this combined feature space provides a robust representation of action plausibility. Given a generated video, our metric quantifies its action quality by measuring the distance between its underlying representations and this learned real-world action distribution. For rigorous validation, we develop a new multi-faceted benchmark specifically designed to probe temporally challenging aspects of human action fidelity. Through extensive experiments, we show that our metric achieves substantial improvement of more than 68% compared to existing state-of-the-art methods on our benchmark, performs competitively on established external benchmarks, and has a stronger correlation with human perception. Our in-depth analysis reveals critical limitations in current video generative models and establishes a new standard for advanced research in video generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Robust metrics for evaluating visual and temporal correctness of complex human actions in synthesized videos are lacking<br>â€¢ Existing vision encoders and MLLMs are appearance-biased, weak in temporal reasoning, and fail to detect motion dynamics and anatomical implausibilities<br>â€¢ Action correctness is ill-posed even in real videos (atomic vs. procedural actions), complicating automatic assessment<br>â€¢ Generated-video evaluation must go beyond action presence to measure temporal coherence, physical constraints, and motion smoothness<br>â€¢ Absence of targeted benchmarks that probe temporally challenging aspects of human action fidelity</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Learn a latent manifold of real-world human actions by fusing appearance-agnostic skeletal geometry features with appearance-based features in a human-centric, temporally coherent encoder. Score a generated video by embedding it into this space and measuring its distance to the learned real-action distribution to quantify action plausibility.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Procedural Action Manifolds: Evaluating Long-Horizon, Multi-step Human Activities in Generated Videos: Extend the learned action manifold to model event structure and dependencies in procedural actions for long-horizon fidelity assessment<br>â€¢ Temporal Reasoning-Augmented MLLMs for Action Plausibility Assessment: Integrate manifold-derived motion/geometry features into MLLMs to improve temporal understanding and reduce appearance bias in evaluation<br>â€¢ Training Video Generators with Action Plausibility Rewards: Use the proposed metric as a training signal (reward/critic) to fine-tune generative models for anatomically consistent and temporally smooth human motion</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-13">

    <div class="paper">
        <h2 class="paper-title">A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.03915" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.03915" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Inefficient expert utilization under naÃ¯ve Top-K routing in s-MoE leads to overloaded/idle experts, wasting GPU resources and increasing training costs.<br>â€¢ Auxiliary balancing losses couple performance optimization with load control, perturbing gradients and often degrading predictive accuracy.<br>â€¢ Multi-iterative balancing methods (auction/IP) per batch are computationally and memory prohibitive for large, multi-layer s-MoE training.<br>â€¢ Absence of a principled theory for ALF-LBâ€™s single-shot updates hinders understanding of convergence, stability, and hyperparameter selection.<br>â€¢ Need online stochastic guarantees that reflect dynamic training to provide regret bounds and guide step-size schedules.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Cast ALF-LB as a one-shot primal-dual algorithm for a relaxed assignment problem: update per-expert biases by the load deviation (dual step) and route tokens via Top-K on shifted scores (primal step), then prove monotonic Lagrangian improvement and balancing properties in the deterministic case and derive logarithmic expected regret via strong convexity in an online setting.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Step-Size Policies for Auxiliary-Loss-Free Load Balancing in Stochastic MoE Training: Learn or adapt per-expert step sizes using variance/imbalance estimates to optimize the accuracyâ€“efficiency trade-off with tighter regret and stability bounds.<br>â€¢ Joint Load Balancing Across Multi-Layer s-MoE Architectures with Shared Experts: Extend the primal-dual framework to coupled layers and shared experts, analyzing inter-layer dynamics and proposing coordinated bias updates under global resource budgets.<br>â€¢ Communication- and Memory-Aware ALF-LB for Distributed MoE Training: Integrate network topology and GPU memory constraints into the dual objective to minimize end-to-end runtime, with theory for convergence under pipeline/model parallelism.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20233" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20233" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLM-based fact-checking systems rely on external retrieval or closed-source APIs, causing latency, opacity, and hallucinations that harm reliability and real-time usability.<br>â€¢ Post-hoc explanation generation is decoupled from verdict prediction, leading to misalignment between reasoning and explanations and an "alignment tax" from conflicting external vs. internal knowledge after fine-tuning.<br>â€¢ Existing steering methods assume a single "truth" direction and struggle with subtle, human-unknown truths; they fail to exploit rich internal representations and lack interpretable activation-level control.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>REFLEX is a plug-and-play, self-refining paradigm that reformulates fact-checking as a role-play dialogue and jointly trains verdict and explanation while steering internal activations. It extracts contrastive activation pairs between a backbone and its fine-tuned variant, uses a logistic probe to build steering vectors that disentangle truth into substance (backbone factual knowledge) and style (fine-tuned reasoning patterns), and dynamically applies the more reliable direction at inference to improve verdict accuracy and suppress noisy explanations without external APIs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Mixture-of-Steering for Fine-Grained Truth: Learn input-conditioned mixtures of style/substance steering vectors to capture nuanced, human-unknown truths beyond single-direction control.<br>â€¢ Retrieval-Aware Activation Guidance for Low-Latency Fact-Checking: Combine minimal retrieval with internal activation steering to reduce hallucinations while minimizing alignment tax and maintaining real-time performance.<br>â€¢ The Middle-Layer Truth Hypothesis: A Theoretical and Empirical Study: Formalize why human-unknown truths localize in middle layers, and develop probes/metrics to map truth vs. style representations across model depths.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 13</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button><button class="page-btn" onclick="goToPage(11)">11</button><button class="page-btn" onclick="goToPage(12)">12</button><button class="page-btn" onclick="goToPage(13)">13</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-12-05 23:12:58 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 13;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>