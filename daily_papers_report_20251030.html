<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - October 30, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">October 30, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23538" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23538" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The scope of neural code intelligence is rapidly expanding beyond text-based source code to encompass the rich visual outputs that programs generate. This visual dimension is critical for advanced applications like flexible content generation and precise, program-driven editing of visualizations. However, progress has been impeded by the scarcity of high-quality multimodal code data, a bottleneck stemming from challenges in synthesis and quality assessment. To address these challenges, we make contributions from both a data and modeling perspective. We first introduce a complete synthesis toolkit that leverages reciprocal synergies between data modalities to efficiently produce a large-scale, high-quality corpus spanning from standard charts to complex interactive web UIs and code-driven animations. Leveraging this toolkit, we construct JanusCode-800K, the largest multimodal code corpus to date. This powers the training of our models, JanusCoder and JanusCoderV, which establish a visual-programmatic interface for generating code from textual instructions, visual inputs, or a combination of both. Our unified model is a departure from existing approaches that build specialized models for isolated tasks. Extensive experiments on both text-centric and vision-centric coding tasks demonstrate the superior performance of the JanusCoder series, with our 7B to 14B scale models approaching or even exceeding the performance of commercial models. Furthermore, extensive analysis provides key insights into harmonizing programmatic logic with its visual expression. Our code and checkpoints will are available at https://github.com/InternLM/JanusCoder.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Lack of high-quality, diverse multimodal code data (charts, WebUIs, SVG, animations) and difficulty in synthesis, execution, and quality assessment across heterogeneous PLs and visual outputs (pages 1–4, Table 1)<br>• Fragmented modeling landscape with task-specific systems; absence of a unified interface that links program logic to visual expression and generalizes across scenarios (pages 1–3)<br>• Underdeveloped fine-grained perception and generative capability for vision-centric coding (e.g., chart-to-code, UI editing) despite progress in program-aided reasoning (pages 2–3)<br>• Executability is an insufficient proxy for visual fidelity; missing scalable reward/assessment for instruction–code–visual alignment (page 5, Section 3.4)<br>• Long, monolithic code (e.g., Manim animations) is hard to learn from without decomposition and structuring (page 4, Section 3.1)</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>They propose a unified visual–programmatic pipeline: a multi-strategy data toolkit (guided evolution, re-contextualization, reverse-instruction, bidirectional translation, AST-based decomposition), execution sandboxing, and VLM/LLM reward modeling to build JANUSCODE-800K. On this corpus, they train unified models (JanusCoder for text-centric and JanusCoderV for multimodal inputs) and introduce DTVBench to evaluate dynamic theorem visualizations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• AutoReward: Learning Human-Preference Visual–Code Reward Models for Better Alignment: Augment the current VLM/LLM judges with human preference data and RL to optimize instruction–code–visual fidelity beyond executability.<br>• UniRender: Differentiable Rendering for End-to-End Training of Visual-Programmatic Models: Integrate differentiable renderers and perceptual losses to fine-tune code generation directly against visual targets and layout/style constraints.<br>• Agentic Janus: Interactive Execute–Diagnose–Edit Agents for WebUI and Scientific Demonstrations: Build agents that iteratively render pages/figures, localize mismatches, and apply code edits using the paper’s synthesis toolkit as a self-correction loop.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Video-Thinker: Sparking "Thinking with Videos" via Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.23473" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.23473" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in image reasoning methods, particularly "Thinking with Images", have demonstrated remarkable success in Multimodal Large Language Models (MLLMs); however, this dynamic reasoning paradigm has not yet been extended to video reasoning tasks. In this paper, we propose Video-Thinker, which empowers MLLMs to think with videos by autonomously leveraging their intrinsic "grounding" and "captioning" capabilities to generate reasoning clues throughout the inference process. To spark this capability, we construct Video-Thinker-10K, a curated dataset featuring autonomous tool usage within chain-of-thought reasoning sequences. Our training strategy begins with Supervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group Relative Policy Optimization (GRPO) to strengthen this reasoning capability. Through this approach, Video-Thinker enables MLLMs to autonomously navigate grounding and captioning tasks for video reasoning, eliminating the need for constructing and calling external tools. Extensive experiments demonstrate that Video-Thinker achieves significant performance gains on both in-domain tasks and challenging out-of-domain video reasoning benchmarks, including Video-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B substantially outperforms existing baselines such as Video-R1 and establishes state-of-the-art performance among 7B-sized MLLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• MLLMs treat videos as monolithic inputs and struggle with dynamic temporal reasoning (localizing, revisiting, and synthesizing across time).<br>• Existing methods rely on pre-designed Chain-of-Thought scripts or external tool calls, rather than intrinsically integrating temporal operations into reasoning.<br>• Lack of built-in temporal grounding and segment-level captioning within the CoT limits autonomous navigation of key segments.<br>• Data inefficiency: prior state-of-the-art video reasoning often requires very large datasets (e.g., >160K samples), hindering scalability.<br>• Supervised fine-tuning alone generalizes poorly out of domain; models need RL-based post-training to robustly reason across diverse benchmarks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Video-Thinker enables MLLMs to “think with videos” by interleaving two intrinsic operators—temporal grounding (<time>) and segment captioning (<caption>)—within chain-of-thought traces (<think>), learned via SFT on a curated 10K dataset and reinforced with GRPO using outcome (answer correctness) and format rewards. This end-to-end approach removes external tools and lets the model autonomously navigate, localize, and synthesize key temporal evidence for reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Thinker-XL: Scaling Video-Thinker with Larger Models and Longer Contexts: Scale parameters and temporal context windows to improve reasoning over hour-long videos and dense event chains.<br>• Audio-Thinker: Integrating Speech and Environmental Audio into Temporal CoT: Add audio grounding/captioning operators to fuse speech, music, and sound events with visual reasoning.<br>• Beyond Grounding & Captioning: Intrinsic Operators for Tracking, Counting, and Causality: Introduce native actions (e.g., track, compare, predict) to handle object/actor persistence and causal inference.<br>• Process-Aware RL for Video CoT: Rewarding Temporal Fidelity and Evidence Use: Design RL rewards that jointly assess answer correctness, temporal localization accuracy, and evidence faithfulness.<br>• Streaming Video Agents: Online, Real-Time Video Reasoning without External Tools: Extend to low-latency, incremental inference for live feeds with continual grounding and self-correction.<br>• Hindsight-Curation++: Large-Scale, Weakly-Supervised Trace Synthesis with Minimal LLM Calls: Improve data efficiency and trace quality via iterative auto-curation and cheaper teacher models.<br>• Benchmarking the Process: New Metrics and Datasets for Evidence Alignment in Video CoT: Create evaluations that score both final answers and the correctness/coverage of temporal evidence chains.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Scaling Latent Reasoning via Looped Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25741" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25741" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Modern LLMs are trained to "think" primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to post-training and under-leverages pre-training data. We present and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped Language Models (LoopLM) that instead build reasoning into the pre-training phase through (i) iterative computation in latent space, (ii) an entropy-regularized objective for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and 2.6B models enjoy superior performance that match the results of up to 12B SOTA LLMs across a wide range of benchmarks. Through controlled experiments, we show this advantage stems not from increased knowledge capacity, but from superior knowledge manipulation capabilities. We also show that LoopLM yields reasoning traces more aligned with final outputs than explicit CoT. We hope our results show the potential of LoopLM as a novel scaling direction in the reasoning era. Our model could be found in: http://ouro-llm.github.io.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Parameter efficiency and deployment cost: Large dense LLMs require huge compute/memory, causing latency and accessibility issues; need more capability per parameter.<br>• Limitations of Chain-of-Thought and inference-time compute: Extends output sequences, inflates context/latency, and can produce post‑hoc rationalizations rather than faithful reasoning; under‑leverages pretraining for reasoning.<br>• Fixed-depth transformers lack adaptive compute: Standard models spend the same compute on easy/hard inputs; need learned depth allocation without increasing parameters.<br>• Missing large-scale evidence for looped architectures: Prior looped/latent reasoning works are small scale; unclear scaling behavior (loss/efficiency/safety/capability) at multi‑trillion‑token regimes.<br>• Early-exit training bias: Common geometric/Poisson priors push shallow halting and under-explore deeper steps; need an unbiased objective for adaptive computation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Ouro is a looped language model that reuses a shared stack of transformer blocks across recurrent steps and learns an adaptive Q-exit halting policy via an entropy-regularized objective with a uniform prior, followed by a focused gate loss that aligns halting with step-wise loss improvements. Trained on 7.7T tokens, it delivers adaptive depth and 2–3× parameter efficiency, with practical 4× decoding memory savings via last-step KV-cache reuse.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Deeper-Than-Trained: Improving Recurrent-Depth Extrapolation in Looped Language Models: Techniques (auxiliary losses, curriculum, step adapters/distillation) to maintain or improve task performance when running more loops than seen in training.<br>• Mixture-of-Recursions: Token-Level Routing for Adaptive Latent Compute: Combine looped blocks with token/position-level routing (e.g., MoR/MoE) to allocate variable recurrence across tokens for better accuracy–efficiency trade-offs.<br>• Reinforcement Learning for Early-Exit Policies and Safety in LoopLMs: Build RLVR-compatible infrastructure for dynamic halting graphs to jointly optimize accuracy, latency, and safety, leveraging verifiable rewards and step-aware policies.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24592" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24592" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Autoformalization, which translates natural language mathematics into machine-verifiable formal statements, is critical for using formal mathematical reasoning to solve math problems stated in natural language. While Large Language Models can generate syntactically correct formal statements, they often fail to preserve the original problem's semantic intent. This limitation arises from the LLM approaches' treating autoformalization as a simplistic translation task which lacks mechanisms for self-reflection and iterative refinement that human experts naturally employ. To address these issues, we propose ReForm, a Reflective Autoformalization method that tightly integrates semantic consistency evaluation into the autoformalization process. This enables the model to iteratively generate formal statements, assess its semantic fidelity, and self-correct identified errors through progressive refinement. To effectively train this reflective model, we introduce Prospective Bounded Sequence Optimization (PBSO), which employs different rewards at different sequence positions to ensure that the model develops both accurate autoformalization and correct semantic validations, preventing superficial critiques that would undermine the purpose of reflection. Extensive experiments across four autoformalization benchmarks demonstrate that ReForm achieves an average improvement of 17.2 percentage points over the strongest baselines. To further ensure evaluation reliability, we introduce ConsistencyCheck, a benchmark of 859 expert-annotated items that not only validates LLMs as judges but also reveals that autoformalization is inherently difficult: even human experts produce semantic errors in up to 38.5% of cases.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Autoformalization suffers from poor semantic fidelity: models often pass the Lean compiler yet fail to preserve the problem’s intent (e.g., quantifier scope, hidden constraints, edge cases)<br>• Prevailing one-pass translation paradigm lacks self-reflection and iterative refinement, preventing models from identifying and correcting their own semantic errors during generation<br>• Data-centric fixes help but do not address the process-level limitation; errors persist on subtle semantics even with better datasets<br>• Existing RL methods rely on terminal-only rewards and cannot supervise intermediate critiques, leading to superficial or hallucinated self-validation that degrades correction quality<br>• Evaluation reliability is under-examined: LLM judges’ accuracy needs validation and even human-written formalizations contain non-trivial semantic errors, complicating assessment</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>ReForm reframes autoformalization as a single-sequence, iterative loop that interleaves statement generation with semantic self-validation and refinement, enabling progressive error detection and correction. It is trained with Prospective Bounded Sequence Optimization (PBSO), which integrates heterogeneous rewards for critique quality (intermediate) and final correctness (terminal) via prospective bounded returns with clipping for stable, position-specific credit assignment; a new ConsistencyCheck benchmark validates judge reliability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Adaptive Reflective Depth for Autoformalization: Confidence-calibrated policies that learn when to continue or stop reflection, optimizing the number of critique–revision rounds per problem<br>• Cross-System Reflective Formalization Beyond Lean: System-agnostic reflective pipelines that target Coq/Isabelle/Lean with shared semantics, transfer learning, and cross-system consistency checks<br>• Co-Training the Formalizer and the Judge with PBSO: Jointly optimize a formalizer and a semantic critic under heterogeneous rewards to reduce critique hallucinations and improve end-to-end semantic fidelity</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Reasoning-Aware GRPO using Process Mining</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25065" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25065" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement learning (RL)-based post-training has been crucial for enabling multi-step reasoning in large reasoning models (LRMs), yet current reward schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware Group Relative Policy Optimization (GRPO) that augments standard answer/format rewards with signals over the reasoning procedure. To this end, process mining techniques are utilized to compute a scalar conformance reward that measures how closely a policy model's reasoning aligns with the pretrained teacher model. The empirical results on five benchmarks demonstrate that PM4GRPO significantly outperforms existing methodologies for GRPO-based post-training. These results highlight that leveraging process mining for reasoning-aware GRPO effectively enhances the reasoning capabilities of policy models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Outcome-centric GRPO rewards ignore the reasoning process, leading to verbosity, speculative leaps, and accidental correctness instead of genuine understanding (pp. 1–2).<br>• Existing reward signals rely on superficial textual attributes (e.g., length, formatting, keyword matches) and fail to evaluate process quality (p. 1).<br>• There is a mismatch between token-level off-policy correction and sequence-level rewards in GRPO; optimization should match the unit of reward (pp. 2–3).<br>• Improving multi-step reasoning is crucial for LRMs; aligning the process (not just answers) can enhance robustness and generalization across benchmarks (pp. 1, 4).<br>• Need a way to encourage alignment with strong teacher reasoning while preserving student model diversity and freedom of thought (p. 2).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>PM4GRPO augments GRPO with a process-aware conformance reward: it discovers a process model from the student’s chain-of-thought via an Inductive Miner and measures alignment to a teacher’s reasoning using alignment-based conformance checking, using the F1 of fitness and precision as a reward. The total reward combines format, answer, and conformance terms and is optimized at the sequence level with Group Sequence Policy Optimization to match sequence-level rewards.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Differentiable Conformance Rewards for End-to-End Reasoning RL: Learn neural surrogates for process discovery and conformance to provide low-latency, differentiable process rewards during training.<br>• Teacher-Free Process-Constrained RL for LRMs: Mine high-quality self-generated reasoning traces and enforce self-conformance to bootstrap process-aware alignment without external teachers.<br>• Multi-Teacher Process Ensembles for Robust Reasoning Alignment: Aggregate conformance across multiple teacher processes with uncertainty-aware weighting to avoid overfitting to a single reasoning style.<br>• Step-Level Process Shaping with Online Conformance Feedback: Provide incremental conformance signals during generation to guide intermediate steps, not just sequence-level rewards.<br>• Process Mining for Tool-Use and Code Reasoning in LRMs: Extend conformance rewards to structured tool-call and program synthesis traces, aligning procedural plans and executions.<br>• Safety-Aware Reasoning via Conformance to Risk-Constrained Process Templates: Use safety-verified process templates to penalize unsafe or speculative reasoning trajectories while preserving performance.<br>• Cross-Domain Process Alignment for Multimodal Scientific Reasoning: Apply process-aware GRPO to multimodal benchmarks (e.g., Olympiad-style tasks) to align visual-textual reasoning processes.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25726" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25726" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Real-world language agents must handle complex, multi-step workflows across diverse Apps. For instance, an agent may manage emails by coordinating with calendars and file systems, or monitor a production database to detect anomalies and generate reports following an operating manual. However, existing language agent benchmarks often focus on narrow domains or simplified tasks that lack the diversity, realism, and long-horizon complexity required to evaluate agents' real-world performance. To address this gap, we introduce the Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering diverse Apps and tools, realistic environment setup, and reliable execution-based evaluation. Toolathlon spans 32 software applications and 604 tools, ranging from everyday platforms such as Google Calendar and Notion to professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools are based on a high-quality set of Model Context Protocol (MCP) servers that we may have revised or implemented ourselves. Unlike prior works, which primarily ensure functional realism but offer limited environment state diversity, we provide realistic initial environment states from real software, such as Canvas courses with dozens of students or real financial spreadsheets. This benchmark includes 108 manually sourced or crafted tasks in total, requiring interacting with multiple Apps over around 20 turns on average to complete. Each task is strictly verifiable through dedicated evaluation scripts. Comprehensive evaluation of SOTA models highlights their significant shortcomings: the best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate with 20.2 tool calling turns on average, while the top open-weights model DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development of more capable language agents for real-world, long-horizon task execution.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Existing tool-use benchmarks are narrow and simplified, lacking diverse applications and realistic workflows, so they fail to reflect real-world agent needs.<br>• Prior environments start from empty or synthetic states (often read-only), missing the complexity and variability of authentic software data and configurations.<br>• Many evaluations rely on LLM judges instead of deterministic, execution-based verification, hurting reliability and reproducibility.<br>• Tasks are often short and over-specified, not testing long-horizon planning, cross-application orchestration, or fuzzy, real user prompts.<br>• Benchmarks rarely provide scalable, safe isolation and reset of complex states, making robust, parallel evaluation difficult.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>TOOLATHLON is a fully execution-based benchmark that pairs 32 real MCP-connected and containerized applications (604 tools) with realistic initial states, fuzzy prompts, and deterministic evaluation scripts across 108 multi-app tasks. A safe, containerized framework enables parallel runs and equips agents with robust tool error handling, long-output paging, and context management to fairly stress-test long-horizon behavior.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Beyond Short Context: Efficient Long-Context Memory for Real-World Language Agents: Design retrieval, paging, and summarization mechanisms to handle overlong tool outputs and 20+ turn trajectories without loss of critical information.<br>• Self-Healing Tool Use: Grounded Error Recovery and Tool Name Disambiguation in MCP Agents: Learn to detect, correct, and recover from tool-call errors and hallucinated tool names via schema grounding and interaction-aware repair.<br>• Plan to Explore: Preventing Premature Termination in Long-Horizon Agent Tasks: Introduce progress tracking, goal checks, and exploration incentives to avoid giving up early under fuzzy instructions.<br>• Cross-App Orchestration at Scale: Learning to Route and Compose Tools Under Distractors: Train policies for selecting and composing tools across dozens of apps while ignoring irrelevant tools and noisy outputs.<br>• Stateful Benchmarks at Scale: Automatic Synthesis and Reset of Realistic Initial Environment States: Build generators and snapshot/restore systems to create, vary, and reset complex multi-app states reproducibly.<br>• Cost-Aware Agenting: Optimizing Performance–Budget Trade-offs in Realistic Tool Use: Develop agents that adapt token usage and tool-calling strategies to meet accuracy targets under cost constraints.<br>• Verifiable-by-Design Agents: Aligning Execution Logs with Deterministic State Checks: Train agents to produce actions and artifacts that directly satisfy script-based verifiers and generate audit-ready traces.<br>• Multi-Agent MCP Systems: Coordinated Specialist Agents for Enterprise-Scale Workflows: Explore collaboration and delegation across specialized sub-agents to handle large, cross-application pipelines.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25772" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25772" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Visual effects (VFX) are crucial to the expressive power of digital media, yet their creation remains a major challenge for generative AI. Prevailing methods often rely on the one-LoRA-per-effect paradigm, which is resource-intensive and fundamentally incapable of generalizing to unseen effects, thus limiting scalability and creation. To address this challenge, we introduce VFXMaster, the first unified, reference-based framework for VFX video generation. It recasts effect generation as an in-context learning task, enabling it to reproduce diverse dynamic effects from a reference video onto target content. In addition, it demonstrates remarkable generalization to unseen effect categories. Specifically, we design an in-context conditioning strategy that prompts the model with a reference example. An in-context attention mask is designed to precisely decouple and inject the essential effect attributes, allowing a single unified model to master the effect imitation without information leakage. In addition, we propose an efficient one-shot effect adaptation mechanism to boost generalization capability on tough unseen effects from a single user-provided video rapidly. Extensive experiments demonstrate that our method effectively imitates various categories of effect information and exhibits outstanding generalization to out-of-domain effects. To foster future research, we will release our code, models, and a comprehensive dataset to the community.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Generative video models struggle to produce anti-physical, surreal VFX (e.g., energy beams, magical particles) from text alone, due to data scarcity and complex, unstructured dynamics.<br>• The “one-LoRA-per-effect” paradigm is resource-intensive, scales poorly, and fundamentally fails to generalize to unseen (out-of-domain) effects.<br>• Existing controllable methods (keypoints, depth, edges) provide spatially aligned control but cannot capture intricate temporal dynamics, textures, and transformations of VFX.<br>• Uncontrolled reference conditioning risks content leakage (subjects/backgrounds from reference bleeding into target), harming fidelity and editability.<br>• There is a lack of unified, scalable frameworks and evaluation metrics tailored to reference-based VFX generation and OOD generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>VFXMaster reframes VFX generation as in-context learning: it concatenates reference prompt–video and target prompt–first-frame latents, and applies a customized in-context attention mask to inject only effect attributes while preventing content leakage. For tough unseen effects, it performs efficient one-shot adaptation by learning a small set of concept-enhancing tokens, while fine-tuning only spatial–temporal attention layers in a DiT-based I2V backbone.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Physics-Guided VFX Diffusion: Integrating Differentiable Dynamics into In-Context Effect Transfer: Combine physical priors/simulation losses with the in-context framework to improve stability and plausibility of complex particle/fluid/pyro effects.<br>• Multi-Reference and Compositional VFX Imitation via Cross-Video Consistency: Learn to fuse multiple reference clips, decompose and recombine effect attributes, and enforce cross-video consistency for controllable, compositional VFX transfer.<br>• 3D-Aware, Camera-Controllable VFX Transfer with Geometry and Pose Priors: Extend to 3D-aware diffusion with camera trajectories and scene geometry to maintain view-consistent effects under camera motion and multi-view generation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24821" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24821" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables highly efficient scaling (dramatically improving computational efficiency while significantly expanding model capacity) and empowers stronger unified multimodal intelligence across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in contextual ASR and highly competitive results in dialect-aware ASR. In image generation, Ming-Flash-Omni introduces high-fidelity text rendering and demonstrates marked gains in scene consistency and identity preservation during image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Building a truly unified omni-MLLM that integrates understanding and generation across vision, speech, and language is hard due to representational disparities and modality imbalance (Sec. 1, p. 1–2).<br>• Scaling capacity without exploding compute/latency remains a bottleneck; dense LLMs and unstable/inefficient MoE routing limit practical AGI-scale models (Sec. 2, Fig. 2, p. 3–4).<br>• Conventional ASR fails in context-dependent scenarios (proper nouns, homophones, accents), hurting real-world dialogue and hotword use; the paper targets ContextASR specifically (Table 6, p. 11; Sec. 2.2, p. 3–4).<br>• Discrete speech tokens introduce quantization artifacts, degrading TTS naturalness and expressiveness (Sec. 2.2, p. 3–4).<br>• Image generation/editing lacks fine-grained spatial-semantic control, consistent identity preservation, and high-fidelity in-image text; prior pipelines separate segmentation from generation and Ming-Omni’s frozen language path causes objective mismatch (Sec. 2.3, p. 4–5).<br>• Video understanding needs stronger temporal modeling over long sequences; standard positional encodings are insufficient (VideoRoPE, Sec. 2.1, p. 3–4).<br>• Multimodal training systems suffer from data/model heterogeneity causing load imbalance and memory fragmentation; existing frameworks lack packing/sharding tailored for multimodal workloads (Sec. 2.5, p. 6–7).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Ming-Flash-Omni builds a unified omni-MLLM on a sparser Ling-Flash-2.0 MoE (100B total; 6.1B active/token) with per-modality routing and VideoRoPE for temporal modeling, adds context-aware ASR and continuous-latent TTS, and reframes “generative segmentation as editing” to achieve fine-grained, controllable image generation with improved ID/text rendering. Training uses a two-stage perception→generation pipeline with coherent RL (D-GRPO, U-DPO) and system optimizations (sequence packing, flexible encoder sharding) for efficient multimodal scaling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• End-to-End Co-Optimization of Perception and Generation in Sparse Omni-MLLMs: Jointly train understanding and generation (removing frozen components) with multi-task spatio-semantic objectives and stability-aware MoE routing.<br>• Generative Segmentation for Video and 3D Scenes in Unified Omni Models: Extend segmentation-as-editing to videos and 3D/NeRF, enforcing spatio-temporal consistency for controllable video/scene editing and generation.<br>• Context-Aware Multilingual Speech with Continuous Latents: Unify streaming ASR+TTS for code-switching and dialects using continuous acoustic representations, on-the-fly hotword/context injection, and low-latency decoding.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RegionE: Adaptive Region-Aware Generation for Efficient Image Editing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25590" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25590" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recently, instruction-based image editing (IIE) has received widespread attention. In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computational redundancy, existing IIE models do not account for this distinction, instead applying a uniform generation process across the entire image. This motivates us to propose RegionE, an adaptive, region-aware generation framework that accelerates IIE tasks without additional training. Specifically, the RegionE framework consists of three main components: 1) Adaptive Region Partition. We observed that the trajectory of unedited regions is straight, allowing for multi-step denoised predictions to be inferred in a single step. Therefore, in the early denoising stages, we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image. 2) Region-Aware Generation. After distinguishing the regions, we replace multi-step denoising with one-step prediction for unedited areas. For edited regions, the trajectory is curved, requiring local iterative denoising. To improve the efficiency and quality of local iterative generation, we propose the Region-Instruction KV Cache, which reduces computational cost while incorporating global information. 3) Adaptive Velocity Decay Cache. Observing that adjacent timesteps in edited regions exhibit strong velocity similarity, we further propose an adaptive velocity decay cache to accelerate the local denoising process. We applied RegionE to state-of-the-art IIE base models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o confirmed that semantic and perceptual fidelity were well preserved.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• High latency in instruction-based image editing (IIE) limits real-time use because current models denoise the whole image uniformly even when edits are local.<br>• Spatial redundancy: most pixels are unedited, yet existing IIE pipelines expend equal computation on edited and unedited regions.<br>• Temporal redundancy: attention K/V states and predicted velocities are highly similar across adjacent timesteps, but standard sampling recomputes them every step.<br>• Curvature mismatch: unedited regions follow near-linear denoising trajectories while edited regions are curved, but current methods ignore this distinction.<br>• Existing diffusion accelerators (pruning, quantization, distillation, generic caching/sparsity) are not tailored to IIE’s region-specific dynamics or the denoising-only paradigm.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>RegionE is a training-free, adaptive, region-aware framework that (1) partitions edited vs. unedited areas via early one-step prediction and similarity to the instruction image, (2) replaces multi-step denoising with one-step estimation for unedited regions while iteratively denoising only edited tokens using a Region‑Instruction KV Cache for global context, and (3) accelerates time steps with an Adaptive Velocity Decay Cache that scales cached velocities by a timestep-aware decay until an error threshold. A three-stage schedule (stabilization, region-aware generation with periodic full refresh, and smoothing) preserves quality while delivering ~2.06–2.57× speedups.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Learned Region Partition for Instruction-Based Image Editing: Train a lightweight predictor to infer edit masks from early-step features and uncertainty, replacing heuristic similarity thresholds to boost robustness and reduce boundary artifacts.<br>• Region-Aware Video Editing with Spatio-Temporal Velocity Caching: Extend RegionE to videos by coupling spatial region-aware generation with cross-frame KV/velocity caches and motion-aware masks for temporal consistency.<br>• Curvature-Aware Schedulers for Local Diffusion Edits: Design second-order or curvature-aware samplers that adapt step sizes and caching to local trajectory geometry, improving quality and efficiency in strongly edited regions.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">The Principles of Diffusion Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21890" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21890" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This monograph presents the core principles that have guided the development of diffusion models, tracing their origins and showing how diverse formulations arise from shared mathematical ideas. Diffusion modeling starts by defining a forward process that gradually corrupts data into noise, linking the data distribution to a simple prior through a continuum of intermediate distributions. The goal is to learn a reverse process that transforms noise back into data while recovering the same intermediates. We describe three complementary views. The variational view, inspired by variational autoencoders, sees diffusion as learning to remove noise step by step. The score-based view, rooted in energy-based modeling, learns the gradient of the evolving data distribution, indicating how to nudge samples toward more likely regions. The flow-based view, related to normalizing flows, treats generation as following a smooth path that moves samples from noise to data under a learned velocity field. These perspectives share a common backbone: a time-dependent velocity field whose flow transports a simple prior to the data. Sampling then amounts to solving a differential equation that evolves noise into data along a continuous trajectory. On this foundation, the monograph discusses guidance for controllable generation, efficient numerical solvers, and diffusion-motivated flow-map models that learn direct mappings between arbitrary times. It provides a conceptual and mathematically grounded understanding of diffusion models for readers with basic deep-learning knowledge.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Fragmented theory and duplicated practice: diffusion research is split across variational (DDPM), score/SDE, and flow-based (flow matching) lines with ad‑hoc links; parameterizations and schedulers are often conflated, obscuring when methods are truly equivalent or different.<br>• Slow, iterative sampling: standard samplers need hundreds–thousands of NFEs; there is a need for principled, training‑free accelerations (better ODE/SDE solvers) and training‑based accelerations (distillation/one‑step generators) without quality loss.<br>• Controllability and alignment gaps: guidance is mostly heuristic; a principled decomposition of the conditional score is needed to unify classifier/CFG/training‑free guidance, and to align outputs with human preferences without unstable RLHF.<br>• Hand‑wavy derivations and unclear theory: reverse‑time SDEs and density evolution are often justified informally; a precise Fokker–Planck/continuity‑equation view is needed to guarantee marginal consistency and unify ODE/SDE samplers.<br>• Misconceptions about OT/SB links: diffusion PF‑ODEs are not generally optimal transport maps and Reflow doesn’t guarantee convergence to OT; a rigorous mapping to entropic OT/Schrödinger bridges clarifies what is (and isn’t) optimal.<br>• Beyond two‑stage pipelines: distillation relies on slow teachers; there is a need for standalone, few‑step flow‑map/consistency models that train from scratch, keep fidelity, and preserve guidance/likelihood tooling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>The paper presents a unified, continuous‑time framework that casts diffusion as learning a time‑dependent score/velocity field whose flow transports a prior to data, with sampling equal to solving a reverse‑time SDE or its probability‑flow ODE. It systematizes guidance via the conditional‑score decomposition, accelerates sampling with principled ODE solvers (DDIM/DEIS/DPM‑Solver), and develops two families of fast generators—distillation and teacher‑free flow‑map/consistency models—under a common Fokker–Planck/semigroup lens.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Learning OT‑Consistent Probability Flows for Diffusion Models: Add variational/regularization constraints so PF‑ODE trajectories approximate OT/SB solutions, closing the gap that current PF‑ODEs are generally not optimal transport maps.<br>• Time‑Parallel and Physics‑Guided Training of Flow‑Map Generators: Train few‑step flow maps with Picard fixed‑point objectives and Fokker–Planck residuals to achieve O(1) NFEs while guaranteeing marginal consistency.<br>• Preference‑Aligned Guidance via Direct Tilt Learning: Treat guidance as density tilting and learn the tilt (e.g., DPO‑style) directly from preferences to replace ad‑hoc surrogates and avoid explicit reward models while preserving controllability.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ODesign: A World Model for Biomolecular Interaction Design</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22304" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22304" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Biomolecular interactions underpin almost all biological processes, and their rational design is central to programming new biological functions. Generative AI models have emerged as powerful tools for molecular design, yet most remain specialized for individual molecular types and lack fine-grained control over interaction details. Here we present ODesign, an all-atom generative world model for all-to-all biomolecular interaction design. ODesign allows scientists to specify epitopes on arbitrary targets and generate diverse classes of binding partners with fine-grained control. Across entity-, token-, and atom-level benchmarks in the protein modality, ODesign demonstrates superior controllability and performance to modality-specific baselines. Extending beyond proteins, it generalizes to nucleic acid and small-molecule design, enabling interaction types such as protein-binding RNA/DNA and RNA/DNA-binding ligands that were previously inaccessible. By unifying multimodal biomolecular interactions within a single generative framework, ODesign moves toward a general-purpose molecular world model capable of programmable design. ODesign is available at https://odesign.lglab.ac.cn ,</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Lack of a general-purpose, cross-modality generative model: existing methods are modality-specific (protein-only, RNA-only, or small-molecule-only) and cannot design across proteins, nucleic acids, and ligands within one framework.<br>• Insufficient controllability: prior models offer limited control over where and how binding occurs, lacking fine-grained conditioning from entity → motif → atom levels, epitope targeting, and rigid vs. flexible receptor modes.<br>• Predictive vs. generative gap: AlphaFold3/RF-AA learn cross-modal interactions but are predictive, not generative; current generators struggle to create new interacting partners that satisfy specific interfacial constraints.<br>• Representation mismatch: purely atomic generation often fails to respect molecular modality/hierarchy (residues, nucleotides, functional groups), making cross-modality generation unreliable.<br>• Throughput bottlenecks: practical design pipelines yield too few filter-passing candidates per GPU-day, limiting wet-lab screening and iteration.<br>• Sparse tools/benchmarks for nucleic acid interactions: few methods can design protein-binding RNA/DNA or nucleic-acid-binding ligands; standardized aptamer benchmarks are lacking.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>ODesign unifies proteins, DNA/RNA, and ligands in a shared generative token space and decodes full-atom 3D structures via a conditional diffusion model built atop an AlphaFold3-like Pairformer. It uses a two-stage pipeline (backbone generation → OInvFold for sequence/atom types), multi-level masking (all/entity/token/atom) for controllable conditioning, and supports rigid/flexible target conformations with epitope (hotspot) guidance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• ODesign-XL: A Self-Evolving Molecular World Model via Closed-Loop In Silico Evolution—Integrate physics-based constraints, docking/MD feedback, and automated design–evaluate–update cycles to continually improve cross-modality design quality and robustness.<br>• Physics- and Drug-Likeness-Constrained ODesign for Therapeutic-Grade Generation—Embed differentiable physical priors and medicinal chemistry objectives (e.g., stability, ADMET, aromaticity, Lipinski/QED) to steer generation toward synthesisable, developable candidates across modalities.<br>• Semantic-Control ODesign: Fine-Grained Control of Secondary Structure, Pocket Geometry, and Atomic Motifs—Add explicit semantic knobs (e.g., helix/strand content, pocket volume/shape, catalytic triad geometry) to enable precise property- and motif-aware conditional generation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.18455" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.18455" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Retrieval Augmented Generation (RAG) systems are increasingly vital in dynamic domains like online gaming, yet the lack of a dedicated benchmark has impeded standardized evaluation in this area. The core difficulty lies in Dual Dynamics: the constant interplay between game content updates and the shifting focus of the player community. Furthermore, the necessity of automating such a benchmark introduces a critical requirement for player-centric authenticity to ensure generated questions are realistic. To address this integrated challenge, we introduce ChronoPlay, a novel framework for the automated and continuous generation of game RAG benchmarks. ChronoPlay utilizes a dual-dynamic update mechanism to track both forms of change, and a dual-source synthesis engine that draws from official sources and player community to ensure both factual correctness and authentic query patterns. We instantiate our framework on three distinct games to create the first dynamic RAG benchmark for the gaming domain, offering new insights into model performance under these complex and realistic conditions. Code is avaliable at: https://github.com/hly1998/ChronoPlay.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Lack of a standardized, dynamic RAG benchmark for video games, a domain where both game knowledge (patches, balance, DLC) and community focus change rapidly<br>• Core unmet need to model Dual Dynamics: Knowledge Evolution and User Interest Drift; static or snapshot-only benchmarks become obsolete and misaligned with what players actually ask<br>• Existing dynamic benchmarks largely track only factual updates, neglecting player-centric authenticity; synthesized questions often don’t reflect real player phrasing/intents, causing biased evaluation<br>• Manual curation cannot keep pace across diverse games and update cadences; an automated, continuously refreshed, player-authentic benchmark is required<br>• Ignoring user interest drift optimizes models on the wrong problem distribution, degrading real-world relevance and user experience</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>ChronoPlay automatically builds dynamic, authentic game RAG benchmarks via a dual-source synthesis engine (authoritative wikis/patch notes + community-mined question templates and user personas) and a dual-dynamic update mechanism. It refreshes stale items by entity-aware monitoring of official updates and rebalances topic mix using weighted-JSD drift detection over community posts, with LLM-based quality control and retrieval aided by hypothetical Q&A.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Dual-Dynamic-Aware Retriever: Learning retrievers that jointly adapt to entity-level knowledge updates and shifting topic distributions in real time<br>• Forecasting User Interest Drift for Proactive RAG: Time-series models that predict future topic shifts to preemptively rebalance retrieval and evaluation<br>• Authenticity-Guided Synthesis with Debiasing: Techniques to mine and denoise community templates/personas while preserving realism and reducing demographic or platform biases<br>• Multimodal ChronoPlay: Extending the framework to ingest videos, images, and streams (e.g., clips, minimaps, UI) for richer retrieval and grounding<br>• Persona-Conditioned RAG Evaluation: Measuring model performance under diverse, emergent player personas and playstyles, not just average-case queries<br>• Continual RAG Learning with Stale-Knowledge Correction: Training strategies that detect and unlearn outdated facts while preserving stable knowledge<br>• Cross-Domain ChronoBench: Porting the dual-dynamics + authenticity paradigm to e-commerce and social media support to test generality<br>• Reliable LLM-as-Judge Calibration: Methods to align automated judges with human experts on nuanced criteria (correctness vs. faithfulness vs. clarity)<br>• Counterfactual Dual-Dynamics Analysis: Isolating the separate causal impacts of knowledge updates and interest drift on RAG failures<br>• Live A/B Validation of Benchmark Predictivity: Linking phase-wise benchmark scores to real assistant metrics (helpfulness, resolution rate) in production</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25760" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25760" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Humans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However, systematic reviews and publicly available benchmarks for these models remain limited. In this survey, we provide a comprehensive review of multimodal spatial reasoning tasks with large models, categorizing recent progress in multimodal large language models (MLLMs) and introducing open benchmarks for evaluation. We begin by outlining general spatial reasoning, focusing on post-training techniques, explainability, and architecture. Beyond classical 2D tasks, we examine spatial relationship reasoning, scene and layout understanding, as well as visual question answering and grounding in 3D space. We also review advances in embodied AI, including vision-language navigation and action models. Additionally, we consider emerging modalities such as audio and egocentric video, which contribute to novel spatial understanding through new sensors. We believe this survey establishes a solid foundation and offers insights into the growing field of multimodal spatial reasoning. Updated information about this survey, codes and implementation of the open benchmarks can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Fragmented landscape: Lack of a unified taxonomy, standardized evaluation protocols, and open benchmarks for multimodal spatial reasoning makes fair comparison and progress difficult.<br>• Weak spatial grounding in MLLMs: Models over-rely on semantics/co-occurrence, struggle with 3D reasoning, perspective shifts, occlusions, and temporal consistency due to missing geometric priors and representational imbalance.<br>• Evaluation gaps: Limited tests for multimodal integration (2D/3D/video/audio), interactive/embodied settings, and process transparency, hindering diagnosis of failure modes and generalization.<br>• Data limitations: Scarcity of large, diverse, well-annotated 3D/temporal/audio datasets and overreliance on synthetic or narrow domains constrain robustness and open-world applicability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes a comprehensive taxonomy, problem setup, and standardized evaluation dimensions for multimodal spatial reasoning, and releases open benchmarks spanning 2D, 3D, embodied, video, and audio tasks. It synthesizes methods across test-time scaling (prompting, tools), post-training (SFT/RL), architectural designs, and explainability to provide a unified, comparable assessment and guidelines for future work.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• EgoSpace-R1: Egocentric Multimodal Spatial Reasoning with Persistent World Memory: Build models that disentangle agent- and object-motion with long-horizon memory and process-level rewards for robust 4D (space–time) reasoning from first-person video.<br>• Uni3D-LLM: Unified, Efficient, and Interpretable 3D Representations for Spatial Reasoning: Develop uncertainty-aware spatial tokenizers and compressive 2D↔3D projectors with self-/synthetic supervision and symbolic constraints to scale 3D reasoning without dense labels.<br>• EmbodiedWorldLab: Physically-Grounded Benchmarks and Agentic Tool APIs for Interactive Spatial Reasoning: Create multi-sensor (RGB-D/point cloud/audio/IMU) benchmarks with standardized tool schemas, causal metrics, and verify–reflect loops to evaluate agentic planning and manipulation under real-time constraints.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Parallel Loop Transformer for Efficient Test-Time Computation Scaling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24824" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24824" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) are powerful but often too slow and costly for real-world use during inference. Looped transformers save on parameters by reusing the same weights for multiple computational steps, or "loops." However, this approach has a major flaw: the loops run one after another, causing inference latency and memory requirements to increase with each added loop. This makes them impractical for fast applications. To solve this problem, we introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that delivers the performance benefits of a deep, looped model but with the low latency of a standard, non-looped model. PLT works using two key techniques. First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by computing different loops for different tokens at the same time, all within a single pass. Second, to prevent memory costs from growing, we use an Efficient Representation Enhancement strategy. This method shares the memory (KV cache) from the first loop with all other loops. It then uses a Gated Sliding-Window Attention (G-SWA) to combine this shared global information with local information, maintaining high accuracy. Our experiments show that PLT achieves the high accuracy of a traditional looped model but with almost no extra latency or memory cost compared to a standard transformer.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• LLM inference is memory-bound; vanilla looped transformers execute loops sequentially, causing per-token latency and KV cache to grow linearly with loop count L, making them impractical for latency-sensitive and long-context serving.<br>• Practitioners need to scale test-time compute (effective depth) to gain accuracy without paying proportional latency and memory costs.<br>• Vanilla looped transformers require L serial passes per token and O(Lnd) KV cache, increasing memory-bandwidth pressure and reducing throughput.<br>• Prior parallelization attempts (e.g., token/sequence repetition or layer staggering) either add large KV-cache overheads, suffer incomplete parallelism, or hurt throughput, limiting real-world efficiency gains.<br>• Naively sharing KV across loops reduces memory but degrades accuracy unless local information is reintroduced effectively.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>PLT introduces Cross-Loop Parallelism (CLP) that computes different loops for different tokens concurrently in one forward pass, enabled by training with right-shifted states to remove same-step dependencies. It avoids KV blow-up by sharing the first loop’s KV as a global cache and restores accuracy via a gated sliding-window attention in non-first loops that fuses local (windowed) and global (shared) information, yielding near-baseline latency and O(nd + (L−1)wd) KV.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Adaptive Cross-Loop Depth Allocation for PLT: Learn per-token, per-step loop counts under CLP to allocate extra compute only where beneficial while preserving parallelism.<br>• Hierarchical KV Sharing and Compression in PLT: Build multi-granular shared KV (segment- and loop-level) with compression to extend context length under tight memory budgets.<br>• Compiler and Kernel Co-Design for CLP on GPUs: Develop scheduling, fused kernels, and memory layouts specialized for CLP to further cut latency in memory-bound regimes.<br>• Multimodal PLT: Parallel Loops for Vision–Language Models: Extend CLP and gated local–global fusion to multimodal transformers and study cross-modal context sharing.<br>• Theoretical Foundations of Parallel Loop Transformers: Analyze expressivity, stability, and convergence of CLP vs. sequential looping and standard depth, with roofline-based latency models.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PairUni: Pairwise Training for Unified Multimodal Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25682" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25682" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Unified vision-language models (UVLMs) must perform both understanding and generation within a single architecture, but these tasks rely on heterogeneous data and supervision, making it difficult to balance them during reinforcement learning (RL). We propose PairUni, a unified framework that reorganizes data into understanding-generation (UG) pairs and aligns optimization accordingly. We first use GPT-o3 to augment single-task data, generating captions for understanding samples and question-answer (QA) pairs for generation samples, forming aligned pairs from the same instance. Additionally, for each generation sample, we retrieve a semantically related understanding example to form a retrieved pair, linking different but related data points. These paired structures expose cross-task semantic correspondences and support consistent policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware variant based on Group Relative Policy Optimization. It assigns a similarity score to each pair to modulate the advantage, strengthening learning from well-aligned examples and reducing task interference. We curate a high-quality dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on various UVLMs, outperforming strong UVLM RL baselines. Code: https://github.com/Haochen-Wang409/PairUni{github.com/Haochen-Wang409/PairUni}</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Unified RL for UVLMs suffers from task interference: understanding and generation use heterogeneous objectives and data formats, leading to competing gradients, unstable updates, and regressions when trained jointly.<br>• Lack of semantic alignment between data used for understanding and generation creates a data–optimization mismatch; without aligned supervision, shared policies receive conflicting signals, hurting both tasks.<br>• Existing GRPO-based schemes often focus on a single capability or alternate tasks in multi-stage schedules, which is fragile and does not resolve cross-task credit assignment or gradient conflict.<br>• Reward design and data selection at scale are hard: understanding spans diverse sub-tasks (math, OCR, charts) that resist a single reward; there is limited guidance on how to select and align data for unified RL.<br>• Empirical evidence (Figure 1, page 2) shows that semantically aligned pairs increase gradient cosine similarity between tasks and correlate with better scores on MMMU, MMStar, and GenEval, underscoring the importance of data-level alignment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>PairUni reorganizes training data into understanding–generation (UG) pairs—combining aligned quadruples completed via LLM augmentation with retrieval-based pairs—and optimizes with Pair-GRPO, a pair-aware GRPO that weights advantages by pair similarity to amplify well-aligned updates and reduce cross-task interference. The curated 16K PairUG dataset and this pair-weighted RL yield balanced gains in both understanding and generation (see the data pipeline in Figure 2, page 3, and framework in Figure 3, page 5).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Learning Pair Similarity via Gradient Agreement for Unified Multimodal RL: Train a neural similarity predictor supervised by online gradient cosine agreement to replace fixed embedding-based scores for more effective pair-weighted advantages.<br>• Online Pair Mining and Curriculum for Unified Vision–Language RL: Develop an on-policy retrieval and self-augmentation system that discovers UG pairs and schedules training by pair quality, reducing reliance on external LLM augmentation while improving stability.<br>• Pair-Aware RL for Video–Language Understanding and Generation: Extend PairUni to video by constructing temporal UG pairs and applying pair-weighted GRPO to jointly improve video reasoning and video-conditioned generation.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">MASPRM: Multi-Agent System Process Reward Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24803" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24803" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Practical deployment of Multi-Agent Systems (MAS) demands strong test-time performance, motivating methods that guide inference-time search and selectively spend compute to improve quality. We present the Multi-Agent System Process Reward Model (MASPRM). It assigns per-action, per-agent values to partial inter-agent transcripts and acts as an inference-time controller. MASPRM is trained from multi-agent Monte Carlo Tree Search (MCTS) rollouts without requiring step-level human annotations, by propagating returns to local targets. At inference, MASPRM guides step-level beam search and MCTS, focusing computation on promising branches and pruning early. On GSM8K and MATH, MASPRM-guided decoding with an outcome reward model (ORM) applied to the final answer, improves exact match (EM) over a single straight-through MAS pass by +30.7 and +22.9 points, respectively. A MASPRM trained on GSM8K transfers zero-shot to MATH without retraining, adding 8.4 EM points at the same budget. MASPRM is a plug-in value model that estimates per-agent progress and complements verifier-style decoders, enabling more reliable, compute-aware multi-agent reasoning. Code: https://github.com/milad1378yz/MASPRM</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• MAS inference is unreliable without intermediate guidance: outcome-only evaluation (e.g., majority vote or final-answer verifiers) is too sparse to steer long inter-agent exchanges and cannot attribute which agent/action advanced progress.<br>• Policy-likelihood guidance extends likely but unhelpful branches, causing error propagation and wasted compute; systems lack a mechanism to score and prune partial inter-agent states early.<br>• Effective deployment requires compute-aware test-time search that allocates budget to promising branches and stops early on poor ones.<br>• Existing PRMs focus on single-agent chains and don’t transfer directly to MAS due to MAS-specific challenges: granular multi-substep turns, schedule/topology dependence, heterogeneous agents/tools, and partial observability across a communication graph.<br>• Collecting step-level human annotations is costly; a training recipe without manual step labels is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>MASPRM is a per-action, per-agent process reward model that predicts the value of intermediate inter-agent states from the acting agent’s local view, trained via search-generated supervision by regressing MCTS Monte Carlo Q-values onto child states without step-level human labels. At inference, MASPRM initializes and guides step-level beam search and MCTS (optionally mixed with an outcome reward model at terminal leaves), focusing compute on promising branches and pruning unproductive ones early.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• MASPRM-RL: Safe and Stable Multi-Agent Reinforcement Learning with Dense Process Rewards: Use MASPRM as a fine-grained progress signal for online/offline multi-agent RL with safeguards against reward hacking.<br>• Learning to Route: MASPRM-Guided Optimization of Communication Topologies and Schedules Under Compute Budgets: Search for or learn instance-adaptive MAS graphs and turn schedules guided by MASPRM values.<br>• Beyond Pointwise Regression: Advantage-, Ranking-, and Uncertainty-Aware Process Reward Models for MAS: Replace pointwise regression with pairwise/ranking, advantage-style, or distributional losses to improve calibration and robustness.<br>• Cross-Domain MASPRM: Process-Guided Agents for Software Engineering, Retrieval-Augmented QA, Robotics, and Multimodal Teams: Apply MASPRM to diverse domains where intermediate progress signals can drive efficient search.<br>• Process–Outcome Fusion: Joint Training of MASPRMs and Outcome Reward Models for Calibrated Multi-Agent Decoding: Combine process guidance in search with outcome scoring at leaves to maximize accuracy under fixed compute.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Gaperon: A Peppered English-French Generative Language Model Suite</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25771" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25771" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We release Gaperon, a fully open suite of French-English-coding language models designed to advance transparency and reproducibility in large-scale model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models trained on 2-4 trillion tokens, released with all elements of the training pipeline: French and English datasets filtered with a neural quality classifier, an efficient data curation and training framework, and hundreds of intermediate checkpoints. Through this work, we study how data filtering and contamination interact to shape both benchmark and generative performance. We find that filtering for linguistic quality enhances text fluency and coherence but yields subpar benchmark results, and that late deliberate contamination -- continuing training on data mixes that include test sets -- recovers competitive scores while only reasonably harming generation quality. We discuss how usual neural filtering can unintentionally amplify benchmark leakage. To support further research, we also introduce harmless data poisoning during pretraining, providing a realistic testbed for safety studies. By openly releasing all models, datasets, code, and checkpoints, Gaperon establishes a reproducible foundation for exploring the trade-offs between data curation, evaluation, safety, and openness in multilingual language model development.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Reproducibility and openness gap: state‑of‑the‑art LLMs (data, code, checkpoints) are largely closed, and there is no fully open, strong bilingual French–English suite with a transparent pipeline.<br>• Evaluation unreliability: benchmark contamination/leakage confounds scores; common neural quality filters can implicitly amplify leakage; educationally biased corpora (e.g., FineWeb‑Edu) overfit benchmarks rather than general linguistic quality.<br>• Unclear data–performance trade‑offs: the field lacks rigorous, open studies on how linguistic‑quality filtering, data mixing, and mid‑training (instruction/reasoning) affect both benchmark and generative quality.<br>• Safety blind spots: realistic, trillion‑token scale pretraining poisoning/backdoor testbeds are missing; prior poisoning work often targets smaller scales or instruction‑tuning only.<br>• Practical/engineering limitations: need for efficient, hardware‑agnostic training (AMD/NVIDIA), stable pure bf16 training, scalable data curation and deduplication, and availability of intermediate checkpoints for scientific analysis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>They build and release GAPERON (1.5B/8B/24B) trained on 2–4T tokens with a bilingual web corpus filtered by a multilingual quality classifier and a progressive data‑mix schedule (Naive→Drop‑in‑the‑Ocean→High‑Quality→Pepper→Garlic), plus open datasets (Penicillin/Plus), code, and checkpoints. They introduce pure‑bf16 training with scaled RMSNorm, study Headless‑LM vs cross‑entropy at scale, inject harmless pretraining poison (triggers/fictional facts), and analyze contamination (including deliberate late contamination) with both benchmarks and LLM‑as‑a‑judge generation evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Incentive-Compatible LLM Evaluation: A Game-Theoretic Framework to Deter Benchmark Contamination: Model incentives, detection, and penalties to design evaluation regimes that make contamination suboptimal.<br>• When Quality Filters Leak: Quantifying and Mitigating Contamination Amplification in Neural Web Filtering: Systematically measure how different quality classifiers uplift leaked items and propose leakage‑aware filtering.<br>• Headless LMs at Scale: Understanding Performance Plateaus of Contrastive Weight Tying Beyond One Trillion Tokens: Diagnose why CWT plateaus vs cross‑entropy and develop modifications that sustain long‑horizon learning.<br>• Pure BF16 at Scale: Scaled RMSNorm and Other Techniques for Stable 16‑bit Pretraining: Generalize and validate stability tricks for pure bf16 across model sizes, tasks, and hardware.<br>• Trillion-Token Poisoning Benchmarks: Detecting, Stress-Testing, and Mitigating Backdoors in Pretraining: Build standardized, large‑scale poisoned corpora and detection/defense baselines for realistic pretraining regimes.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Automating Benchmark Design</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25039" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25039" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapid progress and widespread deployment of LLMs and LLM-powered agents has outpaced our ability to evaluate them. Hand-crafted, static benchmarks are the primary tool for assessing model capabilities, but these quickly become saturated. In contrast, dynamic benchmarks evolve alongside the models they evaluate, but are expensive to create and continuously update. To address these challenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a framework that leverages environment design principles to automate the process of dynamic benchmark design. BeTaL works by parameterizing key design choices in base benchmark templates and uses LLMs to reason through the resulting parameter space to obtain target properties (such as difficulty and realism) in a cost-efficient manner. We validate this approach on its ability to create benchmarks with desired difficulty levels. Using BeTaL, we create two new benchmarks and extend a popular agentic benchmark tau-bench. Extensive evaluation on these three tasks and multiple target difficulty levels shows that BeTaL produces benchmarks much closer to the desired difficulty, with average deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the baselines.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Static, human-curated benchmarks saturate quickly as models improve, making it hard to differentiate SOTA systems.<br>• Dynamic/live benchmarks still rely on expensive, unscalable human authoring and manual updates.<br>• Agentic evaluations require carefully crafted simulated environments; repeatedly designing them is labor-intensive.<br>• Parameter spaces of environment templates are large/intractable; naive random sampling yields trivial or unsolvable instances.<br>• Single-shot prompting/CoT is unreliable for controlling dataset difficulty; feedback-free methods waste rollouts.<br>• Many existing approaches operate at the task level (not the environment design space), or require RL-style training loops, limiting generality and scalability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>BeTaL (Benchmark Tuning with an LLM-in-the-loop) formulates benchmark creation as optimization over a parameterized environment template: a designer LLM iteratively proposes parameters, a simulator instantiates tasks with verifiable ground truth, the target model is evaluated to measure the performance gap to a desired target (e.g., difficulty), and feedback is fed back to refine parameters, returning the best configuration after few iterations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• BeTaL-MO: Multi-Objective LLM-in-the-Loop Benchmark Design for Difficulty, Realism, and Diversity: Extend BeTaL to jointly optimize multiple goals (e.g., difficulty calibration, domain realism, content diversity) with constraint handling and trade-off control.<br>• Co-Evolutionary BenchBench: Multi-Agent Designer–Solver Loops for Adaptive Evaluation: Pair designer(s) and solver(s) in adversarial/cooperative loops to continually generate, calibrate, and harden benchmarks without training the solver, enabling curriculum-like evaluation.<br>• AutoParam: Learning Controllable Parameter Spaces from LLMs and Data for Dynamic Benchmarks: Move beyond fixed templates by having LLMs propose, validate, and refine the parameter space itself, improving difficulty smoothness and controllability across the full performance range.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.19195" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.19195" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advancements in driving world models enable controllable generation of high-quality RGB videos or multimodal videos. Existing methods primarily focus on metrics related to generation quality and controllability. However, they often overlook the evaluation of downstream perception tasks, which are really crucial for the performance of autonomous driving. Existing methods usually leverage a training strategy that first pretrains on synthetic data and finetunes on real data, resulting in twice the epochs compared to the baseline (real data only). When we double the epochs in the baseline, the benefit of synthetic data becomes negligible. To thoroughly demonstrate the benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data generation framework designed for enhancing the downstream perception tasks. Dream4Drive first decomposes the input video into several 3D-aware guidance maps and subsequently renders the 3D assets onto these guidance maps. Finally, the driving world model is fine-tuned to produce the edited, multi-view photorealistic videos, which can be used to train the downstream perception models. Dream4Drive enables unprecedented flexibility in generating multi-view corner cases at scale, significantly boosting corner case perception in autonomous driving. To facilitate future research, we also contribute a large-scale 3D asset dataset named DriveObj3D, covering the typical categories in driving scenarios and enabling diverse 3D-aware video editing. We conduct comprehensive experiments to show that Dream4Drive can effectively boost the performance of downstream perception models under various training epochs. Page: https://wm-research.github.io/Dream4Drive/ GitHub Link: https://github.com/wm-research/Dream4Drive</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Scarcity of long-tail, safety-critical driving data and the high cost of collecting/annotating large-scale multi-view datasets hinder perception performance.<br>• Prior driving world models emphasize visual quality/controllability but rarely measure impact on downstream detection/tracking; their synthetic-data gains are often overstated due to unfair protocols that double training epochs versus real-only baselines.<br>• Existing generators offer limited instance-level control over object pose/appearance, constraining diversity and corner-case creation.<br>• Single-view editing breaks cross-view consistency needed for BEV perception; NeRF/3DGS insertion suffers from sparse-view artifacts and lighting/appearance mismatches with backgrounds.<br>• When training epochs are matched, large quantities of generic synthetic data can underperform real-only training, exposing the need for task-aligned, geometry-faithful synthesis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Dream4Drive performs 3D-aware video editing by decomposing real videos into dense guidance maps (depth, normal, edge, cutout, mask), rendering high-quality 3D assets into scene geometry, and using a DiT-based multi-condition fusion adapter for multi-view, photorealistic inpainting. It is paired with DriveObj3D, a large-scale asset library built via segmentation → multiview image synthesis → mesh generation, enabling precise, cross-view-consistent corner-case generation that measurably boosts downstream perception with <2% synthetic samples.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Trajectory-Safe Generative Editing for Driving World Models: Constrain inserted assets with map priors and motion forecasting to ensure drivable-area adherence and collision avoidance during synthesis.<br>• Cross-Sensor 3D-Aware Editing for Camera–LiDAR Perception: Extend dense-guidance editing to jointly render synchronized LiDAR point clouds for sensor-fusion training and evaluation.<br>• Domain-Gap Minimization via Relighting and Style Harmonization: Integrate inverse rendering/relighting and learned appearance matching to align shadows, reflections, and materials with scene illumination.<br>• Active Corner-Case Synthesis with Uncertainty-Guided Placement: Use detector uncertainty and error analysis to adaptively select views, distances, and poses that maximize marginal utility of each synthetic sample.<br>• Fair Benchmarking of Synthetic Data Under Fixed Compute Budgets: Standardize evaluation protocols that control epochs/compute to quantify true gains from synthetic data across tasks and resolutions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Evolving Diagnostic Agents in a Virtual Clinical Environment</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24654" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24654" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In this paper, we present a framework for training large language models (LLMs) as diagnostic agents with reinforcement learning, enabling them to manage multi-turn diagnostic processes, adaptively select examinations, and commit to final diagnoses. Unlike instruction-tuned models trained on static case summaries, our method acquires diagnostic strategies through interactive exploration and outcome-based feedback. Our contributions are fourfold: (i) We present DiagGym, a diagnostics world model trained with electronic health records that emits examination outcomes conditioned on patient history and recommended examination, serving as a virtual clinical environment for realistic diagnosis training and evaluation; (ii) We train DiagAgent via end-to-end, multi-turn reinforcement learning to learn diagnostic policies that optimize both information yield and diagnostic accuracy; (iii) We introduce DiagBench, a diagnostic benchmark comprising 750 cases with physician-validated examination recommendations and 99 cases annotated with 973 physician-written rubrics on diagnosis process; (iv) we demonstrate superior performance across diverse diagnostic settings. DiagAgent significantly outperforms 10 state-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two prompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34% higher diagnostic accuracy and 44.03% improvement in examination recommendation hit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic accuracy and 23.09% boost in examination recommendation F1 score. In rubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by 7.1% in weighted rubric score. These findings indicate that learning policies in interactive clinical environments confers dynamic and clinically meaningful diagnostic management abilities unattainable through passive training alone.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Real-world diagnosis is a long-horizon, multi-turn decision process; most LLMs trained on static instruction data cannot plan which exams to order, when to stop, or when to commit to a diagnosis.<br>• Static EHRs lack results for unperformed tests, making end-to-end interactive training/evaluation impossible without a simulator; this prevents learning adaptive policies under uncertainty.<br>• Prompted multi-agent systems add orchestration but do not learn policies, often causing premature closure, redundant steps, and hallucinated findings.<br>• Existing evaluations emphasize single-shot accuracy and overlook process quality; there is a need for rubric-based assessment of diagnostic trajectories.<br>• High-quality interactive trajectories are scarce and historical records are guideline-heavy and conservative, limiting generalization to rare or atypical cases.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Train a diagnostics world model (DiagGym) on EHRs to conditionally generate exam outcomes given a patient profile, prior exams, and a queried test, then optimize an LLM policy (DiagAgent) with end-to-end multi-turn reinforcement learning to select exams or finalize diagnoses under a composite reward (diagnosis correctness, exam relevance, and turn efficiency). A new benchmark (DiagBench) evaluates trajectory quality with automatic metrics and physician-authored rubrics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Scaling Diagnostic RL Agents with Frontier Foundation Models: Train DiagAgent on larger backbones to map scaling effects on trajectory quality, data efficiency, and out-of-distribution robustness.<br>• From Diagnosis to Management: Joint RL for Test Ordering and Treatment Planning in a Virtual Hospital: Extend DiagGym to simulate interventions, costs, and safety events, and learn policies that interleave stabilization/therapy with diagnostics.<br>• Sim2Real Clinical Validation of RL-Trained Diagnostic Agents: Integrate with live EHRs and clinician oversight to run prospective studies assessing utility, safety, and workflow impact with human-in-the-loop guardrails.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25092" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25092" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in text-only large language models (LLMs), such as DeepSeek-R1, demonstrate remarkable reasoning ability. However, these models remain fragile or entirely incapable when extended to multi-modal tasks. Existing approaches largely rely on single-form captions, which lack diversity and often fail to adapt across different types of Visual Question Answering (VQA) benchmarks. As a result, they provide no principled or efficient channel for transmitting fine-grained visual information. We introduce Seeing Eye, a modular framework that unlocks multimodal reasoning in text-only LLMs through an agent-based small VLM translator. This translator acts as a perception agent: it can invoke specialized tools (e.g., OCR and crop) and iteratively distill multimodal inputs into structured intermediate representations (SIRs) tailored to the question. These SIRs are then passed to the text-only LLM, which serves as a reasoning agent. Crucially, the translator and reasoner engage in multi-round feedback and interaction, enabling the extraction of targeted visual details and yielding more confident answers. Experiments on knowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate that Seeing Eye not only reduces inference cost but also surpasses much larger end-to-end VLMs. For example, an instantiation combining a 3B-parameter vision translator with an 8B-parameter language reasoner outperforms a monolithic 32B VLM on challenging knowledge-based questions. Our results highlight that decoupling perception from reasoning via agent information flow offers a scalable and plug-and-play pathway to multimodal reasoning, allowing strong text-only LLMs to fully leverage their reasoning capabilities. Code is available at: https://github.com/ulab-uiuc/SeeingEye</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Strong text-only LLMs (e.g., DeepSeek-R1) excel at reasoning but are fragile or incapable on multimodal tasks, leaving a gap in applying their abilities to vision problems.<br>• Converting images to single, static captions creates a fixed information bottleneck that loses fine-grained visual/spatial details and fails to adapt across diverse VQA benchmarks (e.g., MMMU, MIA-Bench).<br>• Existing dynamic approaches either rely on unstructured conversational/tool-call histories or tightly coupled monolithic VLMs, offering no principled, efficient, and scalable channel to transmit targeted visual evidence to a text-only reasoner.<br>• Monolithic end-to-end VLMs are costly to scale and cannot easily leverage rapidly improving text-only reasoners; a cost-efficient, plug-and-play alternative is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>SeeingEye is a modular two-agent framework where a lightweight vision Translator uses tools (e.g., OCR, crop/SmartGridCaption, read_table) with a Visual Chain-of-Thought to iteratively distill images into a task-tailored Structured Intermediate Representation (SIR). A powerful text-only Reasoner consumes the SIR, reasons with CoT, and either answers or emits targeted feedback to refine the SIR via multi-round, confidence-thresholded interaction.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Learning Structured Intermediate Representations by Program Synthesis: Automatically induce task-specific SIR schemas and typing to maximize downstream reasoning fidelity and compression across domains.<br>• End-to-End RL for Translator–Reasoner Coordination: Jointly optimize tool selection, feedback policies, and halting criteria to improve accuracy–cost trade-offs under computational budgets.<br>• Streaming SIRs for Video and Interactive Multimodal Reasoning: Extend the agentic information flow to temporal settings with incremental SIR updates, memory, and online tool use for video, UI, or embodied tasks.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.22543" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.22543" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models (LLMs). In this context, models explore reasoning trajectories and exploit rollouts with correct answers as positive signals for policy optimization. However, these rollouts might involve flawed patterns such as answer-guessing and jump-in-reasoning. Such flawed-positive rollouts are rewarded identically to fully correct ones, causing policy models to internalize these unreliable reasoning patterns. In this work, we first conduct a systematic study of flawed-positive rollouts in RL and find that they enable rapid capability gains during the early optimization stage, while constraining reasoning capability later by reinforcing unreliable patterns. Building on these insights, we propose Flawed-Aware Policy Optimization (FAPO), which presents a parameter-free reward penalty for flawed-positive rollouts, enabling the policy to leverage them as useful shortcuts in the warm-up stage, securing stable early gains, while gradually shifting optimization toward reliable reasoning in the later refinement stage. To accurately and comprehensively detect flawed-positive rollouts, we introduce a generative reward model (GenRM) with a process-level reward that precisely localizes reasoning errors. Experiments show that FAPO is effective in broad domains, improving outcome correctness, process reliability, and training stability without increasing the token budget.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Outcome-only rewards in RLVR treat flawed-positive rollouts (e.g., answer-guessing, jump-in-reasoning, overlong self-correction) the same as fully correct solutions, reinforcing unreliable reasoning and capping performance.<br>• Flawed positives are prevalent and persistent in training (notably high early and ~30% later), acting as early shortcuts but hindering later refinement; existing RL methods lack a mechanism to shift optimization away from these shortcuts at the right time.<br>• Existing detectors (LLM-as-a-judge or PRMs) are either too heavy/slow for online RL or over-criticize minor issues, leading to low precision; process-level, accurate, and efficient detection is missing.<br>• Dense/process rewards are vulnerable to reward hacking and can destabilize training; current infrastructures also struggle with efficiency (long-tail latency, token budgets).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>FAPO penalizes flawed-positive rollouts using a lightweight generative reward model (GenRM) trained via step-wise RL with outcome + process (error-localization) signals, then integrates a group-relative, parameter-free penalty (λ=1) into GRPO to naturally shift from leveraging shortcuts in warm-up to prioritizing reliable reasoning later. This improves correctness, process reliability, and stability without increasing token budgets, supported by an asynchronous GenRM–actor setup.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Beyond Math: Flawed-Aware RL for Multi-Turn and Agentic Reasoning: Extend FAPO to multi-choice tasks, multi-turn dialogues, tool-use, and agent settings where flawed processes are more pronounced, and evaluate end-to-end process reliability.<br>• Adaptive Flawed-Aware Optimization with Learned Penalties: Learn or schedule the penalty strength (λ) based on learning progress and critic confidence to optimize the timing of the warm-up-to-refinement shift across tasks and model scales.<br>• Scaling Generative Reward Models for Robust, Hack-Resistant Feedback: Develop larger or MoE GenRMs with anti-reward-hacking training, distillation, and fully asynchronous pipelines to deliver precise, efficient, and stable process-level rewards at scale.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Reasoning Language Model Inference Serving Unveiled: An Empirical Study</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.18672" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.18672" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The reasoning large language model (RLLM) has been proven competitive in solving complex reasoning tasks such as mathematics, coding, compared to general LLM. However, the serving performance and behavior of RLLM remains unexplored, which may undermine the deployment and utilization of RLLM in real-world scenario. To close this gap, in this paper, we conduct a comprehensive study of RLLM service. We first perform a pilot study on comparing the serving performance between RLLM and traditional LLM and reveal that there are several distinct differences regarding serving behavior: (1) significant memory usage and fluctuations; (2) straggler requests; (3) adaptive running time; (4) domain preference. Then we further investigate whether existing inference optimization techniques are valid for RLLM. Our main takeaways are that model quantization methods and speculative decoding can improve service system efficiency with small compromise to RLLM accuracy, while prefix caching, KV cache quantization may even degrade accuracy or serving performance for small RLLM. Lastly, we conduct evaluation under real world workload modeled by Gamma distribution to verify our findings. Empirical results of real world workload evaluation across different dataset are aligned with our main findings regarding RLLM serving. We hope our work can provide the research community and industry with insights to advance RLLM inference serving.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Lack of systematic study on serving behavior/performance of Reasoning LLMs (RLLMs) vs. standard LLMs; existing engines (e.g., vLLM, TensorRT, LMDeploy) are tuned for non-reasoning LLMs and can be suboptimal for long-CoT workloads (Abstract; §1; Fig. 2, p.5).<br>• Need for a holistic assessment that couples accuracy with service-provider throughput and user-visible latency; TTFT is insufficient for RLLMs that hide reasoning tokens, necessitating TTFVT and E2E (ASU framework, §3.2).<br>• Unclear validity of popular LLM optimizations for RLLMs; some improve efficiency (model quantization, speculative decoding) while others can degrade accuracy/efficiency, especially on small RLLMs (prefix cache, KV-cache quantization) (§5; Fig. 3–5, pp.6–8).<br>• Real-world bursty workloads (Gamma arrivals) trigger high KV-cache utilization (often near 100%), long stragglers, and queueing delays—phenomena not captured by batch-only benchmarks (Fig. 6–7, p.9).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes ASU (Accuracy, Service-end, User-end), a trinity assessment framework, and ASU-Perf, a benchmarking suite, to empirically characterize RLLM serving. It runs controlled and real-world (Gamma-modeled) experiments across models/engines/datasets, analyzes serving traces, and evaluates optimization techniques (model/KV quantization, prefix cache, speculative decoding) to derive actionable takeaways.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• RLLM-Aware Serving Engine with Straggler-Resilient Scheduling: Design dynamic batching, preemption, and TTFVT-aware schedulers that detect and mitigate long-tail requests, balancing throughput and user latency under bursty traffic.<br>• Memory-Centric KV Cache Management for Long-CoT Reasoning: Develop adaptive KV cache compression/quantization and eviction policies tailored to RLLMs (and safe for small models), reducing saturation while preserving accuracy.<br>• Overthinking-Aware Dynamic Token Budgeting for RLLM Serving: Create online controllers that detect overthinking and adapt token budgets/test-time compute per request to cut cost and latency without harming accuracy.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25409" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25409" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapid advancement of large language models(LLMs) has intensified the need for domain and culture specific evaluation. Existing benchmarks are largely Anglocentric and domain-agnostic, limiting their applicability to India-centric contexts. To address this gap, we introduce BhashaBench V1, the first domain-specific, multi-task, bilingual benchmark focusing on critical Indic knowledge systems. BhashaBench V1 contains 74,166 meticulously curated question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from authentic government and domain-specific exams. It spans four major domains: Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs reveals significant domain and language specific performance gaps, with especially large disparities in low-resource domains. For instance, GPT-4o achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models consistently perform better on English content compared to Hindi across all domains. Subdomain-level analysis shows that areas such as Cyber Law, International Finance perform relatively well, while Panchakarma, Seed Science, and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive dataset for evaluating large language models across India's diverse knowledge domains. It enables assessment of models' ability to integrate domain-specific knowledge with bilingual understanding. All code, benchmarks, and resources are publicly available to support open research.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Existing benchmarks are largely Anglocentric and domain-agnostic, failing to capture India-specific knowledge and cultural nuances across Agriculture, Legal, Finance, and Ayurveda.<br>• Large performance gaps persist in Indic languages (especially Hindi) and low-resource domains; models consistently perform better in English than Hindi.<br>• Lack of fine-grained, subdomain-level evaluation (90+ subdomains, 500+ topics needed) and diverse task formats reflective of real Indian exams.<br>• Inconsistent evaluation methodologies (language coverage, task scope, scoring) hinder fair cross-model comparison in India-centric contexts.<br>• Real-world stakes are high: millions rely on accurate advice and decisions in farming, justice, healthcare (Ayurveda), and finance; underperformance can cause harm.<br>• Scarcity of authentic, exam-grade, culturally faithful evaluation data that preserves regional practices and terminology.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Introduce BhashaBench V1, a bilingual (English/Hindi), domain-specific benchmark of 74,166 exam-sourced QA items across 4 Indic domains and 90+ subdomains, curated via high-accuracy OCR (Surya), LLM-assisted extraction/classification (GPT-OSS-120B), language verification, rigorous cleaning, and expert validation. Standardize zero-shot evaluation for 29+ LLMs (open-/closed-source) using lm-eval log-likelihood and API prompting, reporting domain-, subdomain-, difficulty-, and question-type-wise accuracy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• BhashaBench V2: Scaling to 22 Indic Languages and Multimodal Evaluation: Extend coverage beyond English/Hindi and add image/table/document questions to assess real exam artifacts.<br>• AyurvedaLLM: Domain-Continual Pretraining and Retrieval-Augmented Generation for Traditional Indian Medicine: Boost weak subdomains (e.g., Panchakarma, Dravyaguna) with curated corpora and retrieval over classical texts.<br>• Subdomain-Aware Mixture-of-Experts for Indic Legal, Finance, and Agriculture: Train adaptive MoE/adapters routed by subdomain to improve specialization without sacrificing generality.<br>• Reasoning Beyond MCQs: Tool-Use, Program-of-Thought, and Free-Form Evaluation for Indic Domains: Introduce calculator/statute lookup/code/data-table tools and step-graded scoring to assess procedural reasoning.<br>• Measuring and Mitigating Bias in Indic Domain QA: Quantifying Regional, Linguistic, and Institutional Biases: Build fairness diagnostics across states/dialects and propose debiasing for exam-derived content.<br>• Data Synthesis for Low-Resource Indic Subdomains with Human-in-the-Loop Validation: Generate and validate high-quality items to upsample scarce areas (e.g., Human Rights, Seed Science) while preserving cultural accuracy.<br>• Agentic Workflows for Finance and Legal in India: Multi-Step Casework with Tools and Compliance Constraints: Benchmark agents that use RBI/SEBI circulars, statutes, and calculators to complete end-to-end tasks.<br>• Explanatory Benchmarks for Indic Domains: Rationales, Error Taxonomies, and Calibration: Add human-written rationales, categorize failure modes, and evaluate confidence calibration across languages.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">Generative View Stitching</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24718" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24718" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Autoregressive video diffusion models are capable of long rollouts that are stable and consistent with history, but they are unable to guide the current generation with conditioning from the future. In camera-guided video generation with a predefined camera trajectory, this limitation leads to collisions with the generated scene, after which autoregression quickly collapses. To address this, we propose Generative View Stitching (GVS), which samples the entire sequence in parallel such that the generated scene is faithful to every part of the predefined camera trajectory. Our main contribution is a sampling algorithm that extends prior work on diffusion stitching for robot planning to video generation. While such stitching methods usually require a specially trained model, GVS is compatible with any off-the-shelf video model trained with Diffusion Forcing, a prevalent sequence diffusion framework that we show already provides the affordances necessary for stitching. We then introduce Omni Guidance, a technique that enhances the temporal consistency in stitching by conditioning on both the past and future, and that enables our proposed loop-closing mechanism for delivering long-range coherence. Overall, GVS achieves camera-guided video generation that is stable, collision-free, frame-to-frame consistent, and closes loops for a variety of predefined camera paths, including Oscar Reutersv\"ard's Impossible Staircase. Results are best viewed as videos at https://andrewsonga.github.io/gvs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Autoregressive (AR) rollouts cannot condition on the future camera path, leading to collisions with the generated scene and eventual collapse under predefined trajectories.<br>• Retrieval-augmented AR improves recall of the distant past but still cannot enforce consistency with future constraints or reliably plan for loop closure.<br>• Existing stitching methods are not video-ready: StochSync lacks temporal consistency for videos; CompDiffuser requires specially trained models with custom conditioning, making it costly for video.<br>• Current video models have short context (≈5–10s), so generating long, camera-guided videos that are faithful to the entire trajectory, collision-free, consistent, and able to close loops remains unmet.<br>• Training larger-context models is expensive; a training-free method compatible with off-the-shelf Diffusion Forcing (DF) backbones is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Generative View Stitching (GVS) is a training-free diffusion stitching method for camera-guided video that partitions a sequence into chunks and jointly denoises each target chunk with past and future neighbors using an off-the-shelf Diffusion Forcing model. It adds Omni Guidance to steer the joint score toward conditional consistency and uses cyclic conditioning over temporal and spatial windows for explicit loop closure, enabling stable, collision-free long videos.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Omni-Guided Multi-Modal View Stitching: Incorporating image and text prompts into GVS to propagate external conditioning reliably across the full video.<br>• Adaptive Noise Scheduling for Information Flow in Stitching: Per-frame noise control to regulate how context frames influence distant targets, reducing oversmoothing and improving propagation from external inputs.<br>• Wide-Baseline Loop Closure with Multi-View-Trained DF Backbones: Training DF backbones on wide-baseline datasets (e.g., DL3DV/ScanNet++) to enable robust loop closure under large viewpoint changes.<br>• Memory-Augmented GVS for Global Consistency: Combining GVS with retrieval/memory to reinforce long-range correspondences and reduce late or inconsistent loop closure.<br>• Hierarchical and Goal-Conditioned Robot Planning via View Stitching: Extending GVS to plan long-horizon, goal-driven trajectories with spatial-temporal stitching and Omni Guidance for robotics.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24211" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24211" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While autoregressive (AR) modeling has recently emerged as a new paradigm in visual generation, its practical adoption is severely constrained by the slow inference speed of per-token generation, which often requires thousands of steps to produce a single sample. To address this challenge, we propose MC-SJD, a training-free, lossless parallel decoding framework designed to accelerate AR visual generation by extending the recently introduced Speculative Jacobi Decoding (SJD). Although SJD shows strong potential for accelerating AR generation, we demonstrate that token instability across iterations significantly reduces the acceptance rate, a limitation that primarily arises from the independent sampling process used during draft token generation. To overcome this, we introduce MC-SJD, an information-theoretic approach based on coupling, which substantially accelerates standard SJD by maximizing the probability of sampling identical draft tokens across consecutive iterations, all while preserving its lossless property. Remarkably, this method requires only a single-line modification to the existing algorithm, yet achieves substantial performance gains, delivering up to a ~4.2x acceleration in image generation and ~13.3x acceleration in video generation compared to standard AR decoding, without any degradation in output quality.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Autoregressive (AR) visual generation is slow due to per-token sequential decoding, requiring thousands of steps for images/videos, leading to high latency and poor deployability.<br>• Standard speculative decoding (SD) needs a separate draft model, creating training/maintenance overhead and draft–target communication bottlenecks, and shows limited gains in vision tasks.<br>• Speculative Jacobi Decoding (SJD) is training-free but achieves only modest speedups because independent draft sampling causes low token collision across iterations, unstable contexts, and low acceptance rates, especially under flat visual token distributions.<br>• Acceptance in SJD hinges on similarity between consecutive contexts (1 − TV distance), but independent sampling fails to transfer distributional similarity to realized tokens, preventing convergence and limiting acceleration.<br>• There is a need for a simple, lossless, training-free method that stabilizes iterations, boosts acceptance, and scales with larger verification windows without degrading quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>MC-SJD replaces independent draft sampling in SJD with coupling between consecutive draft distributions—using either maximal coupling implemented via modified rejection sampling or a Gumbel-noise-sharing coupling—to maximize re-sampling the same token across iterations, stabilize contexts, and increase acceptance while preserving exactness. This one-line, training-free change yields up to ~4.2× image and ~13× video speedups without quality degradation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Blockwise Coupled Speculative Jacobi Decoding: Combine coupling with block-level verification to jointly optimize multi-token acceptance and further reduce sequential depth.<br>• Adaptive Coupling Schedules for AR Vision: Learn or estimate when to use maximal vs. Gumbel coupling per position/iteration based on TV distance or entropy to balance latency and stability.<br>• Long-Range Coupled Iterations for Video AR: Share coupling signals beyond consecutive iterations (e.g., across frames or multiple steps) to exploit temporal redundancy and analyze convergence guarantees.<br>• Hybrid Draft Coupling: Unify MC-SJD with lightweight external draft heads (e.g., multi-head/Medusa-style) via joint couplings to further raise acceptance while maintaining lossless decoding.<br>• Hardware-Aware Coupled Decoding: Design fused kernels for MRS and Gumbel coupling, analyze memory/compute trade-offs, and target real-time AR image/video generation on commodity GPUs.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.25758" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.25758" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) in psychological counseling have attracted increasing attention. However, existing approaches often lack emotional understanding, adaptive strategies, and the use of therapeutic methods across multiple sessions with long-term memory, leaving them far from real clinical practice. To address these critical gaps, we introduce TheraMind, a strategic and adaptive agent for longitudinal psychological counseling. The cornerstone of TheraMind is a novel dual-loop architecture that decouples the complex counseling process into an Intra-Session Loop for tactical dialogue management and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session Loop perceives the patient's emotional state to dynamically select response strategies while leveraging cross-session memory to ensure continuity. Crucially, the Cross-Session Loop empowers the agent with long-term adaptability by evaluating the efficacy of the applied therapy after each session and adjusting the method for subsequent interactions. We validate our approach in a high-fidelity simulation environment grounded in real clinical cases. Extensive evaluations show that TheraMind outperforms other methods, especially on multi-session metrics like Coherence, Flexibility, and Therapeutic Attunement, validating the effectiveness of its dual-loop design in emulating strategic, adaptive, and longitudinal therapeutic behavior. The code is publicly available at https://0mwwm0.github.io/TheraMind/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Existing LLM counselors suffer from 'clinical amnesia'—they lack cross-session memory retrieval, breaking continuity and weakening the therapeutic alliance (p.1–2; Fig. 1).<br>• Strategic rigidity: most systems are hard‑coded to a single modality (e.g., CBT) and cannot adapt therapy methods across sessions as patient needs evolve (p.2).<br>• Limited intra-session clinical reasoning: weak perception of emotion/attitude and poor, generic empathetic replies without dynamic strategy selection or treatment-phase awareness (p.3).<br>• Fixed context windows cause catastrophic forgetting, harming longitudinal coherence in multi-session counseling (p.4).<br>• Evaluation and datasets focus on single-turn interactions, under-assessing long-term coherence, flexibility, empathy, and therapeutic attunement (p.6; Table 2).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>TheraMind proposes a dual-loop agent: an Intra-Session Loop that perceives patient state (primary emotion, intensity, attitude), retrieves cross-session memory, infers treatment stage, and selects supportive/challenging strategies to guide response generation; and a Cross-Session Loop that evaluates per‑session therapeutic efficacy and adaptively maintains or switches the therapy method for subsequent sessions (Fig. 2, p.4).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Multi-Modal TheraMind: Extending dual-loop counseling with speech prosody, facial affect, and physiology to improve emotion/attitude perception and attunement.<br>• Human-in-the-Loop Therapy Selection: Incorporating clinician feedback and preference learning to calibrate cross-session therapy switching with safety guardrails.<br>• Real-World Longitudinal Trials of LLM Therapists: Prospective, supervised clinical studies measuring outcomes, alliance, and safety across multi-session care.<br>• Cultural and Cross-Lingual Attunement in Longitudinal AI Counseling: Domain adaptation and value alignment for diverse languages and cultural norms.<br>• Privacy-Preserving Long-Horizon Memory for Therapy Agents: Structured, encrypted, and differentially private memory architectures for secure cross-session recall.<br>• Safety-Constrained Therapeutic Control: Formal risk modeling and fail-safe intervention policies for high-intensity emotions and crisis scenarios.<br>• Benchmarking Longitudinal Counseling: A standardized multi-session benchmark and metrics suite for coherence, flexibility, empathy, and attunement with human ratings.<br>• Personalization via Patient Digital Twins: Dynamic patient models that simulate trajectories and personalize strategy/therapy selection over time.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">GraphNet: A Large-Scale Computational Graph Dataset for Tensor Compiler Research</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24035" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24035" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce GraphNet, a dataset of 2.7K real-world deep learning computational graphs with rich metadata, spanning six major task categories across multiple deep learning frameworks. To evaluate tensor compiler performance on these samples, we propose the benchmark metric Speedup Score S(t), which jointly considers runtime speedup and execution correctness under tunable tolerance levels, offering a reliable measure of general optimization capability. Furthermore, we extend S(t) to the Error-aware Speedup Score ES(t), which incorporates error information and helps compiler developers identify key performance bottlenecks. In this report, we benchmark the default tensor compilers, CINN for PaddlePaddle and TorchInductor for PyTorch, on computer vision (CV) and natural language processing (NLP) samples to demonstrate the practicality of GraphNet. The full construction pipeline with graph extraction and compiler evaluation tools is available at https://github.com/PaddlePaddle/GraphNet .</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Lack of a large, diverse, real-world computational-graph dataset to systematically evaluate tensor compilers; existing benchmarks are ad hoc, small, and not cross-framework.<br>• Absence of a unified performance metric that jointly accounts for speedup, numerical correctness under tunable tolerances, slowdowns, and failures (compilation/runtime), making comparisons unreliable.<br>• Vendor operator libraries lag behind emerging precisions (e.g., BF16/FP8) and new memory patterns, pushing workloads toward custom ops and tensor compilers that still require heavy manual tuning.<br>• Need for reproducible, cross-backend evaluation tooling with standardized graph formats and validation to ensure runnable, serializable, and statically analyzable samples.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>GraphNet introduces a 2.7K-sample, real-world computational-graph dataset in a standardized, cross-framework format with an automated trace–serialize–reconstruct–retrace validation pipeline. It proposes correctness-aware metrics (St, ESt) that combine geometric-mean speedup with explicit penalties for slowdowns and error types under tunable numerical/error tolerances, enabling fair, reproducible benchmarking across compilers.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• GraphNet-X: Extending Cross-Framework Benchmarks to TensorFlow/JAX and TPUs/NPUs<br>• Fine-Grained Task Taxonomy for Compiler Evaluation on GraphNet<br>• Decomposable Graphs: Subgraph-Level Fusion and Scheduling Benchmarking with GraphNet<br>• GraphNet-Distributed: Compiler Evaluation for Communication-Aware Distributed Workloads<br>• GraphNet-IR: Unified High-Level IR Translation for Cross-Ecosystem Compiler Studies<br>• Beyond Defaults: Systematic Multi-Compiler Benchmarking (TVM, XLA, BladeDISC, TensorRT) with ESt<br>• Learning Compiler Passes: Training AI Agents on GraphNet with Error-Aware Speedup Supervision<br>• Error-Guided Autotuning: Using ESt Decomposition to Prioritize Hotspots and Failure Modes</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Fortytwo: Swarm Inference with Peer-Ranked Consensus</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.24801" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.24801" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As centralized AI hits compute ceilings and diminishing returns from ever-larger training runs, meeting demand requires an inference layer that scales horizontally in both capacity and capability. We present Fortytwo, a novel protocol that leverages swarm intelligence principles and distributed pairwise ranking consensus to achieve superior performance in AI inference. Our approach reimagines collaboration among AI nodes using swarm inference: a peer-ranked, reputation-weighted consensus across heterogeneous models that surfaces the highest-quality responses. Using pairwise ranking with a custom Bradley-Terry-style aggregation model, we demonstrate that swarm inference substantially outperforms majority voting, achieving 85.90% on GPQA Diamond versus 68.69% for majority voting with the same model set - an improvement of +17.21 percentage points (approximately +25.1% relative). The protocol incorporates on-chain reputation so node influence adapts to demonstrated accuracy over time, yielding a meritocratic consensus that filters low-quality or malicious participants. To resist Sybil attacks, Fortytwo employs proof-of-capability in its consensus: nodes must successfully complete calibration/test requests and stake reputation to enter ranking rounds, making multi-identity attacks economically unattractive while preserving openness. Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and AIME, our evaluation indicates higher accuracy and strong resilience to adversarial and noisy free-form prompting (e.g., prompt-injection degradation of only 0.12% versus 6.20% for a monolithic single-model baseline), while retaining practical deployability. Together, these results establish a foundation for decentralized AI systems - democratizing access to high-quality inference through collective intelligence without sacrificing reliability or security.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>Research Motivation</h3>
            <div class="motivation">
                <p>• Centralized AI faces compute ceilings and diminishing returns; demand requires horizontally scalable inference in both capacity and capability<br>• Oligopolistic, opaque AI infrastructure creates bottlenecks, inequity, and weak auditability/transparency<br>• Existing trustless verification is impractical at scale: ZKML/FHE incur massive overhead; OPML delays finality; Proof-of-Quality validators are noisy and <~70% accurate<br>• Naive majority voting across models wastes preference information and underperforms principled aggregation<br>• Single-model systems are brittle to free-form noise and prompt injection; need robustness under extraneous or adversarial context<br>• Open participation invites Sybil/collusion attacks; need permissionless yet economically secure, merit-based influence<br>• WAN-scale coordination needs efficient routing and BFT-style fault tolerance without prohibitive latency</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>Research Method</h3>
            <div class="method">
                <p>Fortytwo coordinates heterogeneous, dual-role nodes that both generate answers and judge peers via cryptographically randomized pairwise comparisons with concise multi-token rationales; a reputation-weighted Bradley–Terry aggregation produces the consensus. On-chain reputation and compute-anchored proof-of-capability (compute stake) resist Sybils and adapt influence, while a semantic topology with gossip/blockchain coordination enables scalable routing and robust operation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>Research Ideas</h3>
            <div class="idea">
                <p>• Provable Swarm Inference: Sample Complexity, Robustness, and Diversity Bounds: Derive accuracy and Byzantine-tolerance guarantees; quantify how swarm size and heterogeneity drive convergence and resilience<br>• Beyond Bradley–Terry: Neural and Plackett–Luce Consensus for Decentralized LLM Judging: Compare alternative rank-aggregation models and commit–reveal/crypto schemes to reduce latency and manipulation<br>• Multimodal Swarm Inference: Extending Peer-Ranked Consensus to Vision, Audio, and VLM Tasks: Design cross-modal ranking templates, bandwidth-aware routing, and evaluation protocols<br>• Human-in-the-Loop Swarms: Calibrating Reputation and Ground Truth with Expert Oversight: Integrate expert nodes to seed, audit, and recalibrate reputations for subjective or high-stakes domains</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 10</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>← 上一页</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button></span>
            <button id="next-btn" onclick="changePage(1)">下一页 →</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-10-30 23:14:05 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 10;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>