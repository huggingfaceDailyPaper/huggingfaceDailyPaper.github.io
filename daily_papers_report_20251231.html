<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - December 31, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">December 31, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21185" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21185" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In this report, we introduce UltraShape 1.0, a scalable 3D diffusion framework for high-fidelity 3D geometry generation. The proposed approach adopts a two-stage generation pipeline: a coarse global structure is first synthesized and then refined to produce detailed, high-quality geometry. To support reliable 3D generation, we develop a comprehensive data processing pipeline that includes a novel watertight processing method and high-quality data filtering. This pipeline improves the geometric quality of publicly available 3D datasets by removing low-quality samples, filling holes, and thickening thin structures, while preserving fine-grained geometric details. To enable fine-grained geometry refinement, we decouple spatial localization from geometric detail synthesis in the diffusion process. We achieve this by performing voxel-based refinement at fixed spatial locations, where voxel queries derived from coarse geometry provide explicit positional anchors encoded via RoPE, allowing the diffusion model to focus on synthesizing local geometric details within a reduced, structured solution space. Our model is trained exclusively on publicly available 3D datasets, achieving strong geometric quality despite limited training resources. Extensive evaluations demonstrate that UltraShape 1.0 performs competitively with existing open-source methods in both data processing quality and geometry generation. All code and trained models will be released to support future research.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Scarcity and low-quality 3D training assets (non-watertight meshes, thin shells, fragmented scans, and misposed models) degrade SDF semantics and hinder robust generative training.<br>‚Ä¢ Scalability bottlenecks in high-resolution 3D geometry (cubic memory/compute scaling and token explosion) limit fine-detail synthesis and practical deployment.<br>‚Ä¢ Existing watertight remeshing is brittle: UDF-based sign ambiguity causes double layers; visibility-based methods introduce noisy SDFs; flood-fill leaks on open/self-intersecting meshes.<br>‚Ä¢ Vector-set diffusion models struggle with fine details due to an unstructured latent space and coupling of spatial localization with local geometry synthesis; sparse voxel methods reduce ambiguity but are computationally heavy.<br>‚Ä¢ Inconsistent object poses and intricate internal assemblies impede learning coherent shape priors and destabilize training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>UltraShape 1.0 couples a robust CUDA-parallel sparse voxel watertightening and data-filtering pipeline with a two-stage diffusion generator: a DiT-based vector-set model produces coarse global structure, then a RoPE-encoded voxel-conditioned DiT+VAE decouples spatial localization from detail synthesis to refine geometry by decoding SDFs and extracting surfaces via marching cubes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ UltraShape-Tex: Joint Geometry and PBR Material Generation via Voxel-Conditioned Diffusion: Extend the voxel-refinement stage to simultaneously synthesize textures/materials with cross-modal conditioning and segmentation-aware token masking.<br>‚Ä¢ RoPE-Grid Diffusion at 4096¬≥: Memory-Efficient Sparse Voxel Refinement for Ultra-High Fidelity 3D: Scale the refinement resolution using dynamic token pruning, hierarchical grids, and optimized RoPE to preserve fine details under constrained compute.<br>‚Ä¢ Self-Correcting Watertightness: Learning-Based Volumetric Repair for Real-World Scans Using Diffusion Priors: Train a repair model that infers SDF signs and closes holes in noisy scans, integrating the proposed sparse voxel pipeline with data-driven priors for robust remeshing.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DreamOmni3: Scribble-based Editing and Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22525" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22525" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recently unified generation and editing models have achieved remarkable success with their impressive performance. These models rely mainly on text prompts for instruction-based editing and generation, but language often fails to capture users intended edit locations and fine-grained visual details. To this end, we propose two tasks: scribble-based editing and generation, that enables more flexible creation on graphical user interface (GUI) combining user textual, images, and freehand sketches. We introduce DreamOmni3, tackling two challenges: data creation and framework design. Our data synthesis pipeline includes two parts: scribble-based editing and generation. For scribble-based editing, we define four tasks: scribble and instruction-based editing, scribble and multimodal instruction-based editing, image fusion, and doodle editing. Based on DreamOmni2 dataset, we extract editable regions and overlay hand-drawn boxes, circles, doodles or cropped image to construct training data. For scribble-based generation, we define three tasks: scribble and instruction-based generation, scribble and multimodal instruction-based generation, and doodle generation, following similar data creation pipelines. For the framework, instead of using binary masks, which struggle with complex edits involving multiple scribbles, images, and instructions, we propose a joint input scheme that feeds both the original and scribbled source images into the model, using different colors to distinguish regions and simplify processing. By applying the same index and position encodings to both images, the model can precisely localize scribbled regions while maintaining accurate editing. Finally, we establish comprehensive benchmarks for these tasks to promote further research. Experimental results demonstrate that DreamOmni3 achieves outstanding performance, and models and code will be publicly released.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Text-only and image-only instruction paradigms struggle to unambiguously specify edit locations and fine-grained visual details, especially with multiple similar objects or hard-to-name items, hindering practical interactive editing.<br>‚Ä¢ Binary mask-based editing/generation requires precise masks, scales poorly to multi-region edits, and often fails to preserve structure/context or align with natural language instructions, limiting usability.<br>‚Ä¢ Lack of training data combining text, images, and freehand scribbles prevents models from learning to interpret user intent from sketches and doodles.<br>‚Ä¢ Existing unified frameworks use inconsistent inputs and are not optimized for scribble interactions, making integration and user workflows cumbersome.<br>‚Ä¢ There is no dedicated benchmark to evaluate scribble-based editing/generation, impeding fair assessment and progress.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DreamOmni3 introduces scribble-guided editing and generation via a synthetic data pipeline (built on DreamOmni2 with Referseg and GPT-Image-1) and a joint-input MM-DiT framework that feeds both the original and scribbled source images with shared index/position encodings to localize edits while preserving pixels. The model is trained with LoRA on Kontext, supports text/image/scribble instructions, and is evaluated on a new real-image benchmark with VLM- and human-based criteria.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Scribble2Text: Learning Bidirectional Alignment Between Freehand Scribbles and Natural Language for Explainable Fine-Grained Editing: Train models to translate scribbles into textual referential expressions and back, improving disambiguation and edit controllability.<br>‚Ä¢ Scribe-Video: Scribble-Guided Unified Video Editing and Generation with Temporal Consistency: Extend the joint-input scheme and encodings to video, enforcing temporal coherence for multi-region, multi-object scribble edits.<br>‚Ä¢ Human-in-the-Loop DreamOmni: A Large-Scale, Real-World Tablet Interaction Dataset and Adaptive Encoding for Robust Scribble-Based Editing: Collect human-scribbled sessions and learn adaptive (learned) index/position encodings to handle noisy inputs and diverse devices.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">End-to-End Test-Time Training for Long Context</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23675" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23675" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Efficiently leveraging very long contexts (e.g., 128K tokens) without quadratic attention cost; full attention has O(T^2) prefill and O(T) decode latency, which becomes prohibitive.<br>‚Ä¢ RNN/SSM alternatives (e.g., Mamba 2, Gated DeltaNet) offer constant per-token cost but degrade as context length grows, failing to capitalize on longer context.<br>‚Ä¢ Sliding-window and hybrid attention reduce cost but underperform full attention in utilizing long context for language modeling.<br>‚Ä¢ Training‚Äìinference mismatch: prior dynamic evaluation/test-time training methods are not meta-optimized for post-adaptation performance and often optimize proxies (e.g., KV buffers) rather than end-to-end next-token loss.<br>‚Ä¢ Need to reframe long-context modeling as continual learning with compression‚Äîstoring essential information in weights rather than near-lossless recall over the entire context.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TTT-E2E performs test-time training via sequential next-token prediction updates during prefill on a standard Transformer with sliding-window attention, compressing the observed context into the model‚Äôs weights (O(T) prefill, O(1) decode). It meta-learns the initialization via an outer-loop optimization that directly minimizes the post-TTT loss, yielding full-attention-like scaling in loss with constant-latency inference.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Test-Time Optimizers for Long-Context TTT-E2E: Design token- and layer-aware optimizers and schedules that improve stability, speed, and data efficiency of test-time updates.<br>‚Ä¢ Retrieval-Augmented End-to-End TTT for Ultra-Long Contexts: Combine retrieval with weight-based adaptation to seed and guide updates, enhancing utilization of rare or distant information while controlling forgetting.<br>‚Ä¢ Information-Theoretic and Optimization Bounds for Weight-Based Context Compression: Theoretically characterize how much contextual information can be stored via limited test-time gradient steps and how this scales with model size, steps, and context length.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">Evaluating Parameter Efficient Methods for RLVR</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23165" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23165" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (e.g., PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (e.g., VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ The community default to standard LoRA for RLVR lacks evidence that it is optimal for RL‚Äôs distinct optimization dynamics.<br>‚Ä¢ RLVR is resource-intensive and driven by sparse, binary rewards, motivating parameter-efficient alternatives to reduce cost.<br>‚Ä¢ No systematic, controlled comparison exists across diverse PEFT families (structural, initialization, efficiency) under RLVR and across model scales.<br>‚Ä¢ The suitability of SVD-informed initializations (e.g., PiSSA, MiLoRA) for RLVR is unclear and may misalign with RL‚Äôs off-principal update dynamics.<br>‚Ä¢ The expressivity limits of extreme parameter reduction (e.g., VeRA, Rank-1, IA3, LN tuning) for complex reasoning are unknown.<br>‚Ä¢ Practitioners lack guidance on robust adapter configurations (rank, batch size, learning rate) and robustness across RLVR algorithms.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>This paper conducts a large-scale, controlled benchmark of 12+ PEFT methods for RLVR on DeepSeek-R1-Distill (1.5B and 7B) across math reasoning tasks, using unified hyperparameters and ablations (batch size, learning rate, rank, RL algorithms) to isolate method effects. It adds spectral analyses of update directions to diagnose SVD-based initialization failures and identifies an expressivity floor for parameter-efficient RLVR.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Scaling Parameter-Efficient RLVR with VeRL: High-Throughput Training and Long-Horizon Schedules: Build a high-performance RLVR pipeline to test PEFT at larger scales, longer training, and across architectures.<br>‚Ä¢ Spectral Mechanics of PEFT under RLVR: Why DoRA Works and SVD Inits Collapse: Formalize and validate the spectral evolution of adapter updates to explain structural success and failure modes.<br>‚Ä¢ Beyond Text: Parameter-Efficient RLVR for Multimodal and Multi-Turn Reasoning: Extend PEFT-RLVR to multimodal inputs, dialogues, and asynchronous RL settings.<br>‚Ä¢ Expressivity Floors in Parameter-Efficient RL: Theoretical and Empirical Bounds for Reasoning: Quantify minimal adapter capacity and rank needed to sustain complex policy shifts in RLVR.<br>‚Ä¢ Deployment-Robust PEFT-RL: Stable Weight Merging and Train‚ÄìInference Consistency: Develop numerically stable merging and inference procedures for PEFT-trained RLVR models.<br>‚Ä¢ Algorithm-Invariant PEFT: Transferability Across GRPO, DAPO, and Dr. GRPO: Study why PEFT rankings persist across RLVR objectives and design algorithm-agnostic adapters.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">GraphLocator: Graph-guided Causal Reasoning for Issue Localization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22469" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22469" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code implementation. This gap manifests as two mismatches:(1) symptom-to-cause mismatches, where descriptions do not explicitly reveal underlying root causes; (2) one-to-many mismatches, where a single issue corresponds to multiple interdependent code entities. To address these two mismatches, we propose GraphLocator, an approach that mitigates symptom-to-cause mismatches through causal structure discovering and resolves one-to-many mismatches via dynamic issue disentangling. The key artifact is the causal issue graph (CIG), in which vertices represent discovered sub-issues along with their associated code entities, and edges encode the causal dependencies between them. The workflow of GraphLocator consists of two phases: symptom vertices locating and dynamic CIG discovering; it first identifies symptom locations on the repository graph, then dynamically expands the CIG by iteratively reasoning over neighboring vertices. Experiments on three real-world datasets demonstrates the effectiveness of GraphLocator: (1) Compared with baselines, GraphLocator achieves more accurate localization with average improvements of +19.49% in function-level recall and +11.89% in precision. (2) GraphLocator outperforms baselines on both symptom-to-cause and one-to-many mismatch scenarios, achieving recall improvement of +16.44% and +19.18%, precision improvement of +7.78% and +13.23%, respectively. (3) The CIG generated by GraphLocator yields the highest relative improvement, resulting in a 28.74% increase in performance on downstream resolving task.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Semantic gap between natural-language issue descriptions and concrete code, causing failures even in strong LLM systems to reliably find files/functions to change<br>‚Ä¢ Symptom-to-cause mismatch: issues describe observable symptoms while true root causes lie multiple hops away in dependency chains<br>‚Ä¢ One-to-many mismatch: many issues require coordinated edits across multiple interdependent entities, not a single location<br>‚Ä¢ Limitations of embedding-based methods: rely on similarity, retrieve broad but non-causal code, high recall yet low precision<br>‚Ä¢ Limitations of procedural LLM workflows: fixed top-down traversal misses cross-hierarchy dependencies, leading to low recall on complex cases<br>‚Ä¢ Limitations of agentic LLM workflows: unconstrained exploration favors superficial relevance, loses causal coherence, and over-expands search space</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>GraphLocator builds a repository dependency fractal structure (RDFS) and incrementally constructs a causal issue graph (CIG) where sub-issues are grounded to code and edges encode causal dependencies; it first locates symptom vertices via structure-aware search tools, then performs priority-driven, graph-guided abductive reasoning over neighbors to discover and disentangle multi-hop causal chains.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning-Calibrated Causal Issue Graphs for Issue Localization: Train models to estimate and calibrate edge probabilities in CIGs from supervision, improving robustness over LLM-only judgments<br>‚Ä¢ Polyglot GraphLocator: A Unified RDFS for Multi-Language and Cross-Boundary Repositories: Extend RDFS/CIG to polyglot repos and service boundaries with language-agnostic dependency extraction<br>‚Ä¢ CIG-Guided Patch Synthesis and Verification: Integrate causal graphs with automated repair to generate, prioritize, and verify patches along identified causal paths<br>‚Ä¢ Human-in-the-Loop CIG Refinement for Industrial Debugging: Incorporate developer feedback to edit sub-issues/edges and rapidly converge on precise causal structures<br>‚Ä¢ Runtime-Augmented Causal Reasoning for Performance and Concurrency Bugs: Fuse dynamic traces and profiling with RDFS/CIG to strengthen causal inference beyond static dependencies</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21008" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21008" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Mixture-of-Experts (MoE) architectures have advanced the scaling of Large Language Models (LLMs) by activating only a sparse subset of parameters per input, enabling state-of-the-art performance with reduced computational cost. As these models are increasingly deployed in critical domains, understanding and strengthening their alignment mechanisms is essential to prevent harmful outputs. However, existing LLM safety research has focused almost exclusively on dense architectures, leaving the unique safety properties of MoEs largely unexamined. The modular, sparsely-activated design of MoEs suggests that safety mechanisms may operate differently than in dense models, raising questions about their robustness. In this paper, we present GateBreaker, the first training-free, lightweight, and architecture-agnostic attack framework that compromises the safety alignment of modern MoE LLMs at inference time. GateBreaker operates in three stages: (i) gate-level profiling, which identifies safety experts disproportionately routed on harmful inputs, (ii) expert-level localization, which localizes the safety structure within safety experts, and (iii) targeted safety removal, which disables the identified safety structure to compromise the safety alignment. Our study shows that MoE safety concentrates within a small subset of neurons coordinated by sparse routing. Selective disabling of these neurons, approximately 3% of neurons in the targeted expert layers, significantly increases the averaged attack success rate (ASR) from 7.4% to 64.9% against the eight latest aligned MoE LLMs with limited utility degradation. These safety neurons transfer across models within the same family, raising ASR from 17.9% to 67.7% with one-shot transfer attack. Furthermore, GateBreaker generalizes to five MoE vision language models (VLMs) with 60.9% ASR on unsafe image inputs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Safety alignment in Mixture-of-Experts (MoE) LLMs is largely unexamined; sparse, input-dependent routing may concentrate safety in few experts, creating bypassable failure modes.<br>‚Ä¢ Existing attacks and analyses focus on dense models or coarse expert-level manipulations, lacking fine-grained localization of safety structures within MoE experts.<br>‚Ä¢ As MoE LLMs/VLMs are widely deployed in critical domains, adversaries can steer prompts to misaligned experts; there is a need for training-free, architecture-agnostic attacks to stress-test robustness with minimal utility loss.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>GateBreaker is a training-free, inference-time, architecture-agnostic attack that profiles gate routing to identify safety-relevant experts, localizes safety neurons via activation contrasts on harmful vs. benign inputs, and masks (~3%) of these neurons at runtime to disable safety alignment while preserving utility.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ GateGuard: Robust Safety-Aware Routing for MoE LLMs: Design and train routing mechanisms that enforce redundant activation of safety experts and resist gate-guided bypass attacks.<br>‚Ä¢ Safety Circuit Redundancy in MoE: Distributed Alignment Against Neuron-Pruning Attacks: Introduce and train redundant, dispersed safety neurons across experts with adversarial objectives to withstand targeted pruning.<br>‚Ä¢ MoE Runtime Shield: Detection and Recovery from Neuron-Level Tampering: Develop lightweight monitors for expert/neuron activation anomalies and dynamic re-routing or safety re-injection to counter inference-time masking.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 2</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-12-31 23:03:25 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 2;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>