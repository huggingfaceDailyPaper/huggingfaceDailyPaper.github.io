<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - January 14, 2026</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">January 14, 2026</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06789" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06789" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a "closed-world" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Code agents exhibit a closed-world limitation, fixing bugs from scratch or using only local context while ignoring rich historical human debugging experience on GitHub.<br>‚Ä¢ Real-world GitHub issues/PRs are unstructured, noisy, and heterogeneous (social chatter, non-standard terminology, varying repo structures), making them hard to convert into agent-usable knowledge.<br>‚Ä¢ Cross-repository transfer is difficult due to variability in terminology, module organization, and coding styles, leading to poor standardization and retrieval.<br>‚Ä¢ Existing methods largely perform within-repository retrieval or rely on self-generated experiences, missing cross-repo human expertise at scale.<br>‚Ä¢ Conventional RAG and single-shot retrieval rely on shallow semantic matching and lack iterative, logic-driven exploration needed for debugging.<br>‚Ä¢ Lack of governance/quality control risks memory pollution and hallucinations, undermining reliability of agent memory.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MemGovern governs raw GitHub issue‚ÄìPR‚Äìpatch data into standardized, high-fidelity experience cards (dual-layer Index vs. Resolution) via hierarchical selection, content purification, and checklist-based quality control, then enables logic-driven utilization through an agentic experience search with Searching/Browsing primitives and progressive, multi-round query refinement and analogical transfer. As a plug-in to existing agents (e.g., SWE-Agent), it supplies 135K governed cards and yields +4.65% resolution on SWE-bench Verified.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Cross-Repository Causal Alignment for Code Repair Memories: Learning causal, repo-agnostic root-cause and fix-pattern abstractions to further improve transfer across diverse codebases.<br>‚Ä¢ Continual Memory Governance for Evolving Repositories: Online updating, drift detection, and deprecation policies to maintain freshness and reliability of experience cards as projects evolve.<br>‚Ä¢ Program-of-Thought Agents with Experiential Planning: Integrating experience cards into explicit repair plans that chain root-cause patterns ‚Üí modification logic ‚Üí validation strategies with verifiable checkpoints.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Solar Open Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.07022" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.07022" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Severe data scarcity and quality deficits for underserved languages (e.g., Korean at ~0.8% of indexed web), leading to weak LLM performance.<br>‚Ä¢ Open models and datasets disproportionately target English/Chinese, lacking language- and culture-specific treatment, which degrades downstream task performance.<br>‚Ä¢ Suboptimal tokenization for non-Latin scripts (byte-level fallback) inflates sequence lengths and reduces semantic density, hurting reasoning and efficiency.<br>‚Ä¢ Lack of bilingual, stage-aware data curricula to balance domains, quality, and languages across pretraining/SFT/RL.<br>‚Ä¢ Scaling RL for reasoning/alignment is hindered by tightly coupled online pipelines, making multi-objective training expensive and brittle.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Solar Open is a 102B-parameter MoE bilingual LLM trained with a progressive, bilingual curriculum over 20T tokens, fueled by 4.5T high-quality synthetic, reasoning-targeted data, and optimized via SnapPO, a decoupled cyclic off-policy RL framework. Language-aware tokenization (large-vocab BPE with digit/whitespace rules) and a chat template with explicit reasoning segregation (<|think|>) improve numeric/code handling and enable precise RL reward modeling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ SnapPO++: Generalized Decoupled RL for Multi-Objective, Multi-Agent LLM Training: Extend SnapPO with adaptive replay, cross-domain reward composition, and offline‚Äìonline mixing to scale reasoning, safety, and cultural alignment simultaneously.<br>‚Ä¢ AutoCurriculum: Learning Bilingual, Reasoning-Targeted Data Schedules for Low-Resource LLMs: Automatically learn curriculum phase transitions, language/domain mixes, and quality thresholds from online evaluation signals to minimize token budgets.<br>‚Ä¢ Joint Tokenizer‚ÄìRouter Optimization for Bilingual MoE LLMs: Co-design language-aware tokenization and expert routing/gating to reduce active parameters, control sequence length, and enhance reasoning for non-Latin scripts.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.04745" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.04745" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Evaluation misalignment: existing long-horizon benchmarks equate memory retrieval with person understanding, failing to test inference of motivations, stable principles, evolving self-concepts, and affective triggers.<br>‚Ä¢ Low-density, decontextualized data: synthetic chats/sparse dialogues compress experience and omit inner monologue and sensory grounding, weakening evidence for person-model inference.<br>‚Ä¢ Temporal-structural loss: non-linear autobiographical narratives get causally scrambled without flashback-aware alignment, undermining temporally grounded explanations.<br>‚Ä¢ Lack of evidence-grounded assessment: "deep" questions often invite speculation and lack explicit evidence linkage, reducing auditability.<br>‚Ä¢ Importance: building lifelong digital companions requires robust person understanding to enable explanation, anticipation, and alignment over extended horizons.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>KnowMe-Bench reconstructs long-form autobiographical narratives into a flashback-aware, time-anchored cognitive stream with multiple textual modalities (visual, auditory, situational context, background knowledge, inner monologue), then evaluates models via an evidence-linked, hierarchical suite spanning factual recall, subjective state attribution, and principle-level reasoning with expert verification.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Memory Mechanisms Beyond Retrieval for Person Understanding: Develop agent architectures that learn, update, and reason over explicit person models (principles, triggers, self-concepts) using causal chains from aligned autobiographical streams.<br>‚Ä¢ Flashback-Aware Timeline Construction for LLM Agents: Design ingestion/runtime mnestic realignment algorithms and measure gains in temporally grounded explanations, predictions, and conflict resolution.<br>‚Ä¢ Multimodal Autobiographical Benchmarks for Lifelong Companions: Extend KnowMe-Bench with real-world audio, vision, and sensor data to evaluate cross-modal integration for subjective attribution and principle-level reasoning.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.08225" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.08225" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in "solely task-solving" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Static, predefined toolsets constrain agents‚Äô ability to generalize to open-ended, evolving tool compositions in real-world collaboration.<br>‚Ä¢ Existing synthetic data pipelines favor single-shot, minimal-turn trajectories, failing to capture iterative user behaviors like incremental requests and turn-by-turn feedback.<br>‚Ä¢ Task-oriented generators optimize for efficiency, yielding ‚Äúsolely task-solving‚Äù dialogues that lack long-horizon coherence and realistic back-and-forth, hindering robust agent training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A user-oriented simulation framework that decouples task generation from user interaction, combining dynamic tool and database schema synthesis with a dedicated user simulator that enforces incremental requests and feedback to induce authentic multi-turn dialogues. The modular, plug-and-play pipeline produces high-density, verifiable trajectories (multiple task completions per session) and can start from any state for scalable data augmentation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Human-Calibrated User Simulators for Multi-Turn Tool-Use Agents: Train user simulators on real human‚Äìagent logs to better model incremental requests, feedback, ambiguity, and noise.<br>‚Ä¢ Lifelong Dynamic Tool and Schema Synthesis for Open-World Agentic Reasoning: Develop continual learning systems that expand and refine synthesized tools and databases across domains with verification and safety checks.<br>‚Ä¢ Consistency-Aware Training Objectives for Long-Horizon Tool Use: Design loss functions and evaluation protocols that enforce repeatable, consistent tool-calling behavior across runs and extended sessions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ShowUI-œÄ: Flow-based Generative Models as GUI Dexterous Hands</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24965" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24965" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progress bar) that require continuous, on-the-fly perception and adjustment. In this work, we develop ShowUI-œÄ, the first flow-based generative model as GUI dexterous hand, featuring the following designs: (i) Unified Discrete-Continuous Actions, integrating discrete clicks and continuous drags within a shared model, enabling flexible adaptation across diverse interaction modes; (ii) Flow-based Action Generation for drag modeling, which predicts incremental cursor adjustments from continuous visual observations via a lightweight action expert, ensuring smooth and stable trajectories; (iii) Drag Training data and Benchmark, where we manually collect and synthesize 20K drag trajectories across five domains (e.g. PowerPoint, Adobe Premiere Pro), and introduce ScreenDrag, a benchmark with comprehensive online and offline evaluation protocols for assessing GUI agents' drag capabilities. Our experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g. Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-œÄ achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach. We hope this work advances GUI agents toward human-like dexterous control in digital world. The code is available at https://github.com/showlab/showui-pi.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing GUI agents tokenize actions as discrete clicks, preventing free-form, closed-loop dragging that requires continuous, on-the-fly perception and incremental adjustment.<br>‚Ä¢ Current datasets/benchmarks reduce drags to start‚Äìend points and single screenshots, ignoring dense trajectories, intermediate UI state changes, and multiple valid paths, hindering evaluation and training for continuous manipulation.<br>‚Ä¢ Lack of a unified architecture that handles both clicks and drags within one model; proprietary agents underperform on continuous tasks, limiting progress toward human-like dexterous control in digital environments.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ShowUI-œÄ is a flow-based VLA that unifies clicks and drags as sequences of (x, y, m) and uses a lightweight transformer action expert trained with flow matching to incrementally predict cursor adjustments from streaming visual observations, generating smooth, stable trajectories.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Hierarchical Reasoning Meets Flow-Based Control: A Unified Planner‚ÄìController for Complex GUI Tasks: Integrate LLM planning with the flow-based action head to handle multi-step workflows, conditional branches, and dynamic UI changes.<br>‚Ä¢ ScreenDrag-X: Multimodal Continuous GUI Benchmark across Touch, Stylus, and Mobile: Extend ScreenDrag to touch/stylus gestures, multi-pointer and mobile/VR interfaces, with richer metrics for trajectory quality and uncertainty.<br>‚Ä¢ Rectified Flow and Uncertainty-Aware Policies for Real-Time GUI Dexterity: Explore rectified flow and uncertainty estimation to improve sampling efficiency, stability, and robustness under noisy or rapidly changing screen observations.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MemoBrain: Executive Memory as an Agentic Brain for Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.08079" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.08079" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons. We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation. We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Long-horizon tool-augmented reasoning accumulates transient traces and tool artifacts that exceed LLM context limits, causing cognitive overload and loss of task alignment.<br>‚Ä¢ Absence of explicit, in-process memory disrupts logical continuity and goal-directed reasoning under bounded context budgets.<br>‚Ä¢ Existing cross-task and long-term memory mechanisms are passive and not adaptive to reasoning progress; they fail to track dependencies among intermediate conclusions.<br>‚Ä¢ Prior context-reduction methods (summarization, folding) mainly optimize efficiency and lack executive control for global task awareness and trajectory-level guidance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MemoBrain is a standalone executive memory model that operates as a copilot, abstracting each reasoning episode into dependency-linked thoughts and actively managing context via FOLD (sequential trajectory folding) and FLUSH (selective compaction) to maintain a high-salience reasoning backbone. It is trained with supervised fine-tuning for thought formation and Direct Preference Optimization for memory management decisions under a fixed context budget.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Meta-Executive Memory: Transferable Control Policies for Long-Horizon Agents: Learn task-general executive policies that initialize per task yet transfer control strategies across domains to improve robustness and sample efficiency.<br>‚Ä¢ Multimodal MemoBrain: Executive Memory for Text‚ÄìCode‚ÄìVision Toolchains: Extend thought abstraction and dependency modeling to multimodal inputs and tool outputs (web, code, images) to sustain coherent reasoning in multimodal agent systems.<br>‚Ä¢ Verified Reasoning Graphs: Formal Guarantees for Executive Memory Operations: Introduce formal methods to verify that FOLD/FLUSH preserve semantic correctness and task alignment, providing provable safety for long-horizon reasoning.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06487" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06487" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ RL for open-ended agent tasks lacks objective ground-truth, so pointwise scalar reward models provide weak and unreliable optimization signals.<br>‚Ä¢ Pointwise scoring causes discrimination collapse‚Äîscores compress within groups and fail to capture subtle trajectory advantages‚Äîleading to noise-dominated rewards and training stagnation.<br>‚Ä¢ Existing methods are not process-aware and either rely on noisy final-output scoring or costly full pairwise comparisons (O(N^2)); moreover, the field lacks full-cycle benchmarks for supervised fine-tuning, RL training, and multi-dimensional automated evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ArenaRL replaces pointwise scalar rewards with intra-group relative ranking via process-aware pairwise evaluations guided by multi-level rubrics, and uses an adversarial arena with a seeded single-elimination tournament to produce stable advantage signals with O(N) complexity that closely approximates full O(N^2) pairwise accuracy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Rubrics for Process-Aware Relative Ranking in Open-Ended RL: Automatically induce and refine multi-level rubrics via meta-learning or preference distillation to reduce human design and improve evaluation fidelity.<br>‚Ä¢ Active Tournament-Based RL for LLM Agents: Uncertainty-Aware Bracketing and Comparison Budgeting: Develop adaptive tournament schedules that target informative comparisons, minimizing evaluation cost while maximizing advantage estimation accuracy.<br>‚Ä¢ ArenaRL for Multi-Modal and Tool-Use Agents: Extending Relative Ranking to Complex Real-World Environments: Generalize the arena and rubric framework to multi-modal inputs and tool-rich settings, with process-aware evaluation across interaction steps.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Ministral 3</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.08584" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.08584" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ State-of-the-art language models require massive compute and training data (tens of trillions of tokens), making them impractical for compute- and memory-constrained deployments.<br>‚Ä¢ Existing small models trained from scratch or via naive distillation often underperform; a systematic, FLOP-efficient method to transfer knowledge from a strong parent to compact children is needed.<br>‚Ä¢ Long-context processing (up to 256k tokens) and integrated image understanding are increasingly essential yet rarely supported efficiently in small open-weight models.<br>‚Ä¢ There is limited guidance on effective pretraining distillation, including the capacity gap and whether post-trained or preference-optimized teachers yield better students.<br>‚Ä¢ Open, Apache-licensed models at 3B‚Äì14B scales with competitive performance are needed for broad, practical use.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Cascade Distillation: an iterative prune‚Äìdistill‚Äìrepeat pipeline that prunes a strong parent (layer importance via input/output norm ratios, hidden-dimension reduction with global PCA rotation, FFN gating-dimension selection via SwiGLU importance) and up-trains children via forward-KL logit distillation first at short context (16k) then long context, shrinking from 14B to 8B to 3B. Base models are post-trained into Instruct (SFT + ODPO) and Reasoning (SFT with chain-of-thought + GRPO + ODPO) variants and include a frozen 410M ViT with learned projection for image understanding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Teacher Scheduling to Close the Capacity Gap in Cascade Distillation: Investigate stage-wise teacher choices (pretrained, post-trained, preference-optimized) and curricula to improve student pretraining while preserving generalization.<br>‚Ä¢ Joint Multimodal Distillation: Learning Vision‚ÄìLanguage Projections During Cascade Pruning: Train and distill the projection (and potentially the encoder) jointly to better align visual and textual representations across sizes without sacrificing parameter efficiency.<br>‚Ä¢ Temperature-Aware Attention and Dynamic RoPE for Robust 256k-Token Contexts: Optimize position-dependent softmax temperature and rotary embeddings to enhance stability, retrieval, and accuracy at ultra-long contexts in compact models.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">3AM: Segment Anything with Geometric Consistency in Videos</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.08831" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.08831" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ 2D VOS methods (e.g., SAM2) rely on appearance features and fail under large viewpoint changes, occlusions, reappearances, and similar distractors, leading to identity drift and inconsistent masks.<br>‚Ä¢ 3D instance segmentation achieves viewpoint consistency but depends on camera poses, depth maps, offline preprocessing, and scales poorly; 2D-to-3D lifting often yields view-inconsistent masks and error accumulation.<br>‚Ä¢ There is a need for a promptable, efficient, streaming segmentation approach that is 3D-aware and robust to wide-baseline motion, without requiring explicit 3D inputs at inference.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>3AM augments SAM2 by fusing multi-level MUSt3R reconstruction features (implicit geometric correspondence) with SAM2‚Äôs appearance features via a lightweight Feature Merger using cross-attention and convolution, trained with a field-of-view-aware sampling strategy. At inference, with only RGB and prompts, the merged representations drive SAM2‚Äôs memory attention and mask decoder to produce geometry-consistent object tracks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ 3AM-OpenVocab: Language-Prompted Geometry-Consistent Video Segmentation: Integrate text prompts (e.g., CLIP) with geometry-aware features to enable open-vocabulary, cross-view consistent VOS.<br>‚Ä¢ Pose-Free 3D Instance Tracking via Implicit Reconstruction in Streaming VOS: Couple 3AM with an online implicit 3D latent model to maintain identity across prolonged occlusions and scene transitions without camera poses.<br>‚Ä¢ Adaptive Field-of-View-Aware Curriculum for Wide-Baseline VOS: Develop an adaptive sampling and curriculum strategy that dynamically selects training frames to maximize geometric consistency under extreme motion and zoom.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.07264" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.07264" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Tool-use agents exhibit miscalibrated verbalized confidence, undermining trustworthiness in multi-turn, high-stakes deployments.<br>‚Ä¢ Existing calibration studies focus on static LLMs or narrow search scenarios and do not explain or address how different tool types (evidence vs. verification) systematically affect confidence.<br>‚Ä¢ Prompt engineering and standard tool-use RL fail to fix overconfidence induced by noisy evidence tools; a training framework that jointly optimizes accuracy and calibration with robust reward design and real-world generalization is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes Calibration Agentic RL (CAR), an RL fine-tuning framework that jointly optimizes task accuracy and verbalized calibration via a structured format reward and a calibration-aware outcome reward, introducing the Margin-Separated Calibration Reward (MSCR) to enforce strict incentive separation between correct and incorrect predictions. CAR is trained with GRPO and confidence elicitation tags, and validated across search and code-interpreter settings, showing reduced ECE while maintaining accuracy and generalizing from local retrievers to noisy API environments.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Tool-Type-Aware Calibration Policies for Autonomous Agents: Learn meta-policies that detect evidence vs. verification tool contexts and adapt confidence shaping and rewards dynamically to minimize overconfidence.<br>‚Ä¢ Trajectory-Level Calibration in Multi-Turn Tool Use: Extend CAR to reward step-wise uncertainty alignment across reasoning‚Äìaction‚Äìobservation chains, improving transparency and error localization beyond final-answer calibration.<br>‚Ä¢ Scaling CAR to Frontier and Multimodal Agents: Generalize MSCR-based training to larger models and multimodal tools (vision, speech, SQL, symbolic solvers), evaluating cross-domain robustness in high-stakes applications.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Parallel Context-of-Experts Decoding for Retrieval Augmented Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.08670" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.08670" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated "experts", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Long-context RAG concatenation creates prefill latency bottlenecks and degrades multi-document reasoning ("lost in the middle").<br>‚Ä¢ Parallel KV cache encoding removes prefill but breaks cross-document attention, harming multi-hop evidence aggregation.<br>‚Ä¢ Existing cache-merging or agentic pipelines require recomputation, training, or multiple LLM calls; context-aware decoding typically assumes a single supportive context, conflicting with independent per-document caches.<br>‚Ä¢ There is a need for a training-free, decode-time mechanism that aggregates evidence across separately encoded documents while leveraging retrieval scores to suppress distractors.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>PCED treats each retrieved document (plus a no-context prior) as a parallel expert that produces its own logits from an independent KV cache, and selects each next token via retrieval-aware contrastive decoding that calibrates expert logits against the amateur prior (Œ≤) and gates them by fused retrieval/reranker relevance (Œ≥¬∑log r). This yields token-level expert switching and cross-document reasoning without constructing a joint attention context.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ End-to-End Parallel Expert Gating for RAG: Train LLMs to accept parallel contextual inputs and learn token-level expert selection, reducing reliance on external retrievers and improving robustness.<br>‚Ä¢ PCED without Logits: Contrastive Decoding under API Constraints: Develop surrogate methods using limited log-prob APIs, candidate filtering, or distillation to approximate expert logits and enable PCED with closed-source models.<br>‚Ä¢ Compressing KV Caches for Scalable PCED: Explore quantization, low-rank/prefix pruning, and cache sharing to cut storage and memory while preserving accuracy, enabling PCED on very large corpora.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.08620" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.08620" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Real-world RAG must interpret visual elements (tables, charts, images) that text-only pipelines and benchmarks often ignore.<br>‚Ä¢ Users‚Äô information needs frequently require multi-document synthesis, comparison, and multi-hop reasoning, beyond single-document extractive QA emphasized by existing datasets.<br>‚Ä¢ Trustworthy systems need fine-grained source grounding (e.g., bounding boxes) to mitigate hallucinations, which most benchmarks do not evaluate jointly with retrieval and generation.<br>‚Ä¢ Current multimodal datasets are limited by short extractive tasks, synthetic queries, English-only coverage, and lack of multilingual diversity and free-form visual grounding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces ViDoRe V3, a multimodal, multilingual RAG benchmark built via a three-stage human-in-the-loop process: diverse document collection, balanced query generation (human extractive, human blind contextual, and synthetic blind contextual) across 7 types and 3 formats, and grounded answering with relevance labels and bounding boxes. VLM-assisted pre-selection, human verification, and response aggregation yield high-quality annotations enabling joint evaluation of retrieval, generation, and visual grounding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Fine-Grained Visual Grounding for Multimodal RAG at Scale: Develop models and metrics to improve bounding-box localization and evidence attribution in visually rich documents.<br>‚Ä¢ Chart- and Table-Aware RAG: Structured Reasoning over Non-Textual Elements: Integrate parsers and VLM modules that explicitly model charts/tables to boost retrieval and generation accuracy.<br>‚Ä¢ Multilingual Late-Interaction Visual Retrievers with Reranking for Real-World Corpora: Design and evaluate cross-lingual late-interaction retrievers with textual reranking tailored to ViDoRe-style datasets.<br>‚Ä¢ End-to-End Training of Retriever‚ÄìReader with Grounded Supervision: Jointly optimize retrieval and generation using page-level relevance and bounding-box signals to reduce hallucinations.<br>‚Ä¢ Hybrid Context Construction: Optimizing Visual+Textual Contexts for Open-Ended Queries: Investigate strategies to assemble and weight visual and textual contexts that maximize answer quality on open-ended tasks.<br>‚Ä¢ Synthetic Query Generation Beyond KG Traversal: LLM-Guided Realistic Multimodal Queries: Build improved synthetic pipelines that mimic human blind-context queries to expand coverage while preserving realism.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.08303" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.08303" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Transformer-based diffusion models deliver state-of-the-art quality but are impractical on-device due to billion-scale parameters, quadratic attention complexity, and large memory/compute demands.<br>‚Ä¢ Existing on-device text-to-image systems rely on U-Net backbones that lag behind DiTs in scalability, fidelity, editing flexibility, and personalization.<br>‚Ä¢ High-resolution generation (e.g., 1024√ó1024) causes attention bottlenecks and out-of-memory errors, requiring attention designs that preserve global context and local detail efficiently.<br>‚Ä¢ Real-world deployment spans heterogeneous hardware; static models cannot adapt to diverse compute, memory, and power budgets without retraining.<br>‚Ä¢ Achieving real-time, few-step sampling on devices needs effective distillation that preserves teacher-level distribution and perceptual quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces a three-stage Efficient DiT with adaptive global‚Äìlocal sparse self-attention (KV compression for global context plus blockwise neighborhood attention for local detail), trained via an elastic supernetwork that optimizes multiple sub-DiTs for different hardware, and distilled with K-DMD, which integrates DMD with few-step teacher guidance to enable high-fidelity 4-step on-device generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Sparsity Discovery for Diffusion Transformers on Edge: Learn input-aware, per-head sparsity patterns that dynamically choose global‚Äìlocal attention mixes to further reduce latency while preserving fidelity.<br>‚Ä¢ Once-for-All Generative Supernetworks for Multimodal Edge AI: Extend elastic DiT training to unified text‚Äìimage‚Äìvideo generation, enabling one supernetwork to serve diverse devices and tasks without retraining.<br>‚Ä¢ Quantization-Aware K-DMD for Sub-Second Few-Step Generation: Combine K-DMD with low-bit quantization and hardware-aware kernels to achieve 2‚Äì4 step, 1K-resolution synthesis under 1 second on mobile with minimal quality loss.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Motion Attribution for Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.08828" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.08828" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of motion-specific data attribution: current practices do not reveal which training clips drive temporal dynamics in generated videos<br>‚Ä¢ Image-centric attribution limitations: na√Øve extensions conflate appearance with motion and fail to localize dynamic regions or capture velocity/trajectory coherence<br>‚Ä¢ Scalability and bias issues: gradient-based attribution for videos is computationally heavy and biased by timestep magnitude and clip length, hindering fair, efficient selection for fine-tuning</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Motive computes motion-aware gradient influence by masking the diffusion loss with optical-flow-derived motion weights to emphasize dynamic regions. It ensures scalability and fairness via single-timestep/common-noise gradients, Fastfood projection for compact storage, and frame-length normalization, then ranks clips to select high-influence data for motion-focused fine-tuning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Latent Motion Masks for Attribution: Replace external optical flow with a learned latent-space motion estimator aligned to the VAE, improving robustness and efficiency of motion weighting<br>‚Ä¢ Causal Motion Influence via Controlled Fine-tuning: Establish causal estimates of motion influence using interventions and randomized fine-tuning trials rather than gradient similarity alone<br>‚Ä¢ Attribution-Guided Curriculum for Temporal Dynamics in Video Diffusion: Design training curricula that progressively prioritize motion patterns identified as influential to boost temporal consistency and physical plausibility</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.08665" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.08665" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing VLA navigation models are largely reactive and operate under a fixed compute budget, lacking adaptive, explicit reasoning when scenes are ambiguous or complex.<br>‚Ä¢ They lack persistent, interpretable cross-modal memory; short context windows lead to loops, redundant exploration, and weak handling of dynamic targets/environments.<br>‚Ä¢ Imitation learning dominates training, causing covariate shift and limited generalization; RL for embodied navigation‚Äîespecially with continuous control‚Äîremains underexplored.<br>‚Ä¢ Action discretization reduces motion precision, while growing video tokens inflate latency and hinder real-world deployment.<br>‚Ä¢ Limited interpretability and inability to decide when to deliberate impede reliable, safe real-world use.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VLingNav introduces Adaptive Chain-of-Thought (AdaCoT) that toggles explicit reasoning only when needed and a Visual-Assisted Linguistic Memory (VLingMem) that stores step summaries aligned with key visual cues for long-horizon recall, while an efficient Gaussian MLP head outputs continuous trajectories from VLM states. It is trained with a large adaptive-CoT dataset (Nav-AdaCoT-2.9M) and online expert-guided RL, plus efficiency modules (dynamic frame sampling, grid pooling, temporal tokens), yielding SOTA performance and zero-shot sim-to-real transfer.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Uncertainty-Driven AdaCoT: Calibrating Deliberation Triggers for Safe, Real-Time Navigation: Use uncertainty/entropy, value disagreement, or collision risk to trigger CoT under strict latency budgets and improve safety.<br>‚Ä¢ Map- and Memory-Integrated VLAs for Long-Horizon Spatial Reasoning: Fuse explicit topological/metric maps and scene graphs with linguistic memory to reduce revisits and enable global planning.<br>‚Ä¢ Demonstration-Augmented RL for Continuous VLA Policies with Outcome-Based Rewards: Scale hybrid PPO/GRPO with expert priors and sparse outcome rewards to improve robustness, efficiency, and recovery behaviors in long-horizon navigation.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">End-to-End Video Character Replacement without Structural Guidance</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.08587" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.08587" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Eliminate reliance on dense per-frame masks and explicit structural guidance (pose, depth) in video character replacement, which are fragile and costly.<br>‚Ä¢ Address failure modes in complex scenarios (occlusions, multi-character interactions, unusual poses, challenging illumination) that cause artifacts and temporal inconsistency.<br>‚Ä¢ Reduce computational overhead and improve flexibility/generalizability compared to reconstruction-based pipelines.<br>‚Ä¢ Overcome scarcity of high-quality paired training data for motion- and scene-consistent character replacement.<br>‚Ä¢ Improve facial identity preservation and expression fidelity in the generated character while maintaining original motion and background.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MoCha is an end-to-end video diffusion framework that uses in-context learning with condition-aware 3D RoPE to fuse a source video, a single arbitrary-frame mask, and multiple reference images, transferring motion and expressions onto the new identity without structural guidance. An RL-based post-training (LoRA) with ArcFace identity reward and pixel-wise MSE further enhances facial consistency, trained on a composite paired dataset built via UE5 rendering, portrait animation, and augmented video‚Äìmask pairs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Mask-Free Subject Replacement via Attention-Guided Tracking: Remove the single-frame mask by leveraging learned attention maps and zero-shot tracking to localize and follow the target subject purely from references.<br>‚Ä¢ Physically-Consistent MoCha: Illumination and Contact-Aware Replacement: Integrate differentiable relighting and contact-shadow rewards or implicit light estimation to enforce shading and interaction realism under complex lighting.<br>‚Ä¢ Multi-Actor MoCha: Joint Replacement with Occlusion and Interaction Reasoning: Extend to simultaneous multi-character replacement with occlusion-aware attention, collision constraints, and social motion priors to preserve interactions and temporal coherence.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">JudgeRLVR: Judge First, Generate Second for Efficient Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.08468" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.08468" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Outcome-focused RLVR induces verbose, unstructured trial-and-error reasoning with low information density and frequent backtracking, increasing token cost.<br>‚Ä¢ Heuristic length penalties reduce verbosity but often truncate essential steps, causing an unfavorable accuracy‚Äìefficiency trade-off.<br>‚Ä¢ Existing approaches lack mechanisms to build discriminative capability for early pruning; RLVR tends to reweight existing paths and can benefit from spurious rewards, limiting improvements in reasoning style and generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>JudgeRLVR trains a single LLM in two stages: first as a discriminative judge that classifies solution responses as correct/incorrect using verifiable final-answer rewards, then as a generator via vanilla RLVR initialized from the judge to transfer pruning priors. This achieves more direct, concise reasoning without explicit length penalties while improving accuracy and generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Joint Judge‚ÄìGenerator Co-Training for Process-Consistent Reasoning: Simultaneously optimize judging and generation with shared parameters and consistency losses to further regularize reasoning style and reduce backtracking.<br>‚Ä¢ Cross-Domain Judge Pretraining for Transferable Pruning Priors: Pretrain the judge on diverse domains (math, code, logic) to study transfer of discriminative priors that enhance out-of-domain generalization and efficiency.<br>‚Ä¢ Process-Aware Verifiable Rewards Beyond Final Answers: Integrate step-level verifiers and structured parse-based rewards into JudgeRLVR to encourage coherent intermediate reasoning while preserving conciseness.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">UM-Text: A Unified Multimodal Model for Image Understanding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.08321" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.08321" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Manual specification of text content, layout, and attributes is cumbersome and fails to ensure stylistic consistency with the reference image.<br>‚Ä¢ Mainstream T2I/diffusion models struggle to render complex, multilingual characters (e.g., Chinese) and preserve harmony with background context.<br>‚Ä¢ LLM-based layout planners largely ignore visual context and are ill-suited for editing, which requires precise localization of target text regions.<br>‚Ä¢ Existing condition representations are weak: line-level OCR or single-token text embeddings miss character-level stroke detail and do not fuse instruction, image, and glyph cues.<br>‚Ä¢ Training lacks fine-grained supervision for glyph structures, causing blurred/incorrect strokes and instability during masked editing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>UM-Text unifies a VLM (UM-Designer) with a diffusion transformer via UM-Encoder, which fuses T5 instruction embeddings, character-level OCR glyph embeddings, and VLM-derived context to automatically design text content, layout, and implicit attributes for editing/generation; it is optimized with dual-space Regional Consistency Loss and a three-stage training regimen (VLM pretrain on UM-DATA-200K, diffusion pretrain, semantic alignment).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Aesthetic-Aware Layouts for Visual Text Editing: Reinforcement and preference-based training to align auto-designed layouts/attributes with human aesthetics.<br>‚Ä¢ UM-Text-Video: Context-Aware Multimodal Text Editing in Videos with Temporal Consistency: Extend UM-Encoder/VLM conditioning and RC losses to spatiotemporal settings.<br>‚Ä¢ Few-Shot Cross-Script Adaptation for Complex Glyphs in Unified Multimodal Editing: Meta-learning and adapter tuning to handle rare scripts, handwriting, and stylized fonts with minimal data.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.07290" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.07290" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This paper presents VideoLoom, a unified Video Large Language Model (Video LLM) for joint spatial-temporal understanding. To facilitate the development of fine-grained spatial and temporal localization capabilities, we curate LoomData-8.7k, a human-centric video dataset with temporally grounded and spatially localized captions. With this, VideoLoom achieves state-of-the-art or highly competitive performance across a variety of spatial and temporal benchmarks (e.g., 63.1 J&F on ReVOS for referring video object segmentation, and 48.3 R1@0.7 on Charades-STA for temporal grounding). In addition, we introduce LoomBench, a novel benchmark consisting of temporal, spatial, and compositional video-question pairs, enabling a comprehensive evaluation of Video LLMs from diverse aspects. Collectively, these contributions offer a universal and effective suite for joint spatial-temporal video understanding, setting a new standard in multimodal intelligence.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing Video LLMs typically address either spatial or temporal understanding in isolation, limiting holistic interpretation of complex video events.<br>‚Ä¢ There is a scarcity of high-quality datasets with fine-grained, jointly annotated spatial‚Äìtemporal labels; most datasets provide only temporal segments or only spatial trajectories.<br>‚Ä¢ Na√Øve joint training on heterogeneous datasets suffers from annotation format and distribution mismatches, causing unstable training and weak spatial‚Äìtemporal associations.<br>‚Ä¢ Spatial and temporal tasks demand different input granularities (high-resolution for spatial detail vs long context for temporal reasoning), complicating unified modeling and optimization.<br>‚Ä¢ Lack of comprehensive evaluation that simultaneously probes temporal, spatial, and compositional reasoning in Video LLMs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VideoLoom is a unified Video LLM trained on LoomData-8.7k, a human-centric dataset with temporally grounded and spatially localized captions, to learn coherent joint spatial‚Äìtemporal representations. The work also introduces LoomBench, a comprehensive benchmark of temporal, spatial, and compositional video‚Äìquestion pairs, enabling thorough evaluation and demonstrating state-of-the-art or competitive performance across diverse tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Scaling VideoLoom: Multi-Resolution and Long-Context Modeling for Unified Spatial‚ÄìTemporal Understanding: Develop architectures and training strategies that efficiently reconcile high-resolution spatial detail with long-range temporal context.<br>‚Ä¢ LoomData++: Large-Scale, Multi-Domain Spatial‚ÄìTemporal Annotations for Robust Video LLMs: Expand and diversify human-centric datasets with richer joint annotations (e.g., masks, relations) to improve generalization across domains and tasks.<br>‚Ä¢ Compositional Video Reasoning with Structured Supervision: Integrate programmatic or graph-based supervision to enhance multi-step, compositional reasoning that spans spatial localization and temporal grounding.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.06786" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.06786" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Improving the reasoning abilities of large language models (LLMs) has largely relied on iterative self-training with model-generated data. While effective at boosting accuracy, existing approaches primarily reinforce successful reasoning paths, incurring a substantial calibration cost: models become overconfident and lose the ability to represent uncertainty. This failure has been characterized as a form of model collapse in alignment, where predictive distributions degenerate toward low-variance point estimates. We address this issue by reframing reasoning training as an epistemic learning problem, in which models must learn not only how to reason, but also when their reasoning should be trusted. We propose epistemically-calibrated reasoning (EpiCaR) as a training objective that jointly optimizes reasoning performance and calibration, and instantiate it within an iterative supervised fine-tuning framework using explicit self-evaluation signals. Experiments on Llama-3 and Qwen-3 families demonstrate that our approach achieves Pareto-superiority over standard baselines in both accuracy and calibration, particularly in models with sufficient reasoning capacity (e.g., 3B+). This framework generalizes effectively to OOD mathematical reasoning (GSM8K) and code generation (MBPP). Ultimately, our approach enables a 3X reduction in inference compute, matching the K=30 performance of STaR with only K=10 samples in capable models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLMs are miscalibrated in reasoning: they often assign high confidence to incorrect answers, failing to represent uncertainty and undermining reliability in high-stakes settings.<br>‚Ä¢ Dominant iterative self-training methods (e.g., STaR, ReST) improve accuracy but incur a calibration cost, reinforcing only successful paths and causing epistemic signal truncation/model collapse that erodes uncertainty estimation.<br>‚Ä¢ Existing fixes‚Äîpost-hoc calibration, separate verifiers, and inference-time scaling (self-consistency, slow-thinking RL)‚Äîare brittle or compute-heavy and do not correct the base policy‚Äôs miscalibration, motivating an integrated training objective that balances accuracy and calibration.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>EPICAR is an epistemically-calibrated reasoning objective embedded in iterative supervised fine-tuning that jointly optimizes reasoning accuracy and uncertainty calibration using explicit self-evaluation signals (verbalized yes/no confidence). This dual-task training guides models to learn both how to reason and when to trust their outputs, achieving Pareto-superior accuracy‚Äìcalibration trade-offs and reducing inference compute.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Epistemically-Calibrated RL for Slow-Thinking LLMs: Combine EPICAR with GRPO-style reinforcement learning to reward both correctness and well-calibrated self-verification/backtracking.<br>‚Ä¢ Prompt-Robust Confidence from Hidden States: Train a confidence head on hidden representations co-optimized with EPICAR to reduce prompt sensitivity and improve calibration in long-form reasoning.<br>‚Ä¢ Adaptive Self-Consistency via Epistemic Uncertainty: Use calibrated confidence to dynamically allocate sample counts (K) and stopping criteria, minimizing inference compute while preserving accuracy.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.07632" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.07632" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Discrete motion tokenization has recently enabled Large Language Models (LLMs) to serve as versatile backbones for motion understanding and motion-language reasoning. However, existing pipelines typically decouple motion quantization from semantic embedding learning, linking them solely via token IDs. This approach fails to effectively align the intrinsic geometry of the motion space with the embedding space, thereby hindering the LLM's capacity for nuanced motion reasoning. We argue that alignment is most effective when both modalities share a unified geometric basis. Therefore, instead of forcing the LLM to reconstruct the complex geometry among motion tokens from scratch, we present a novel framework that explicitly enforces orthogonality on both the motion codebook and the LLM embedding space, ensuring that their relational structures naturally mirror each other. Specifically, we employ a decoder-only quantizer with Gumbel-Softmax for differentiable training and balanced codebook usage. To bridge the modalities, we use a sparse projection that maps motion codes into the LLM embedding space while preserving orthogonality. Finally, a two-stage orthonormal regularization schedule enforces soft constraints during tokenizer training and LLM fine-tuning to maintain geometric alignment without hindering semantic adaptation. Extensive experiments on HumanML3D demonstrate that our framework achieves a 20% performance improvement over current state-of-the-art methods, validating that a unified geometric basis effectively empowers the LLM for nuanced motion reasoning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing motion‚ÄìLLM pipelines link modalities only via token IDs, decoupling motion quantization from semantic embedding learning and ignoring inter-code geometric relationships.<br>‚Ä¢ The lack of a unified geometric basis between the motion codebook and LLM embedding space causes geometric mismatch, limiting nuanced motion reasoning and caption fidelity.<br>‚Ä¢ Standard VQ-VAE relies on non-differentiable nearest-neighbor assignment, making it hard to impose geometry-aware constraints and leading to codebook collapse/poor utilization.<br>‚Ä¢ There is no structure-preserving mechanism to transfer motion-space geometry into LLM embeddings without distortion, hindering effective motion‚Äìlanguage alignment.<br>‚Ä¢ Improving motion understanding is critical for embodied agents; better alignment directly enhances caption quality, semantic consistency, and downstream reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>GeoMotionGPT aligns motion and language via a shared orthogonal basis using a decoder-only Gumbel-Softmax quantizer with orthogonality and utilization regularization, a fixed sparse one-to-one projection into the LLM embedding space, and a two-stage orthonormal regularization during tokenizer training and LLM fine-tuning to preserve geometry while allowing semantic adaptation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Orthogonal-Consistent Adapters for Multimodal LLMs: Learn lightweight adapters that preserve orthogonality/isometry while flexibly adapting motion tokens to diverse LLM backbones.<br>‚Ä¢ Hierarchical Geometric Alignment for Long-Horizon Motion: Design multi-scale codebooks with layered orthogonality to capture fine-to-coarse temporal structure and improve long-sequence understanding/generation.<br>‚Ä¢ Beyond Orthogonality: Comparative Study of Geometric Bases for Motion‚ÄìLanguage Alignment: Systematically evaluate orthogonal, isometric, hyperbolic, and spherical bases for codebook‚ÄìLLM alignment across tasks and model sizes.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.04582" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.04582" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLMs often generate runnable code but produce charts that are semantically misaligned and visually unclear‚Äîqualities only assessable after execution/rendering.<br>‚Ä¢ Open-source models (7B‚Äì14B) frequently yield non-executable or low-quality visualizations; closed-source models are strong but costly and raise privacy/compliance concerns.<br>‚Ä¢ Supervised fine-tuning improves syntax and executability but cannot optimize post-execution qualities (alignment, readability) because token-level loss lacks visual feedback.<br>‚Ä¢ Existing RL methods for code rely on single-modal signals (numeric correctness/executability) and fail to jointly optimize text answer correctness, code validity, and visual chart quality.<br>‚Ä¢ There is a need for an on-prem, cost-effective framework that aligns text, code, and vision via multimodal, post-execution rewards to improve end-to-end visualization generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RL-Text2Vis uses Group Relative Policy Optimization with a novel multi-objective, post-execution reward to jointly optimize code validity, chart clarity/alignment, and textual answer correctness. The system generates code, executes and renders the chart, scores all three objectives, and updates a Qwen2.5 (7B/14B) policy accordingly.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Visual Reward Models for Text-to-Visualization via Preference Data: Train a learned, vision-language reward model from human preferences to better capture chart clarity, alignment, and interpretability, improving RL-Text2Vis training beyond heuristic signals.<br>‚Ä¢ Interactive Agentic Text2Vis with Planning and Tool Use: Develop a planning-based agent that decomposes queries, performs table operations (SQL/Pandas), and designs charts under constraints, optimized end-to-end with multimodal RL.<br>‚Ä¢ RL-Text2Vis for Multi-Chart Dashboards and Narrative Visualization: Extend the framework to generate coherent multi-chart dashboards and visual narratives, introducing rewards for layout, cross-chart consistency, and storytelling quality.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02669" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02669" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing fact-checking evaluations largely assess only claim verification, neglecting claim extraction and evidence retrieval, resulting in incomplete views of end-to-end competence.<br>‚Ä¢ Static, accuracy-only benchmarks risk test set leakage and lack adaptability to emerging/LLM-generated misinformation; they fail to capture open-form, multi-stage reasoning quality, robustness, and justification soundness.<br>‚Ä¢ Human arena-style comparisons are costly, and LLM-as-judge can be biased without standardized guidelines, limiting fair, scalable cross-model assessment.<br>‚Ä¢ There is no dynamic mechanism to generate harder, semantically controlled claims, hindering the diagnosis of factual blind spots and robustness boundaries.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>FactArena is an automated, multi-agent arena framework that benchmarks LLMs stage-wise across claim extraction, tool-augmented evidence retrieval, and justification-based verdict prediction using consolidated guidelines for LLM judges and pairwise comparisons scored by Elo/Bradley‚ÄìTerry. It further introduces arena-driven claim evolution that reverses and iteratively hardens claims to expose robustness gaps beyond fixed test sets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Calibrating LLM Judges for Fact-Checking: Debiasing and Reliability Guarantees: Develop bias detection, calibration with human ground truth, and confidence-weighted aggregation to improve judge consistency and trust.<br>‚Ä¢ Cross-Domain and Multilingual FactArena: Stage-wise Auditing Across Web, Scientific, and Social Media Claims: Extend the framework to multiple domains and languages with specialized retrieval tools, evaluating transferability and domain robustness.<br>‚Ä¢ Adversarial Claim Evolution for Stress-Testing Fact-Checking Pipelines: Design adversarial and confounding claim/evidence generation to probe resilience under noisy, conflicting, or misleading sources and map failure modes.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 8</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2026-01-14 23:05:38 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 8;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>