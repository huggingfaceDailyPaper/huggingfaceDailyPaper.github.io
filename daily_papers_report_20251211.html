<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - December 11, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">December 11, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.09363" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.09363" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ High demand for comfortable, high-fidelity stereo video for XR devices, while dual-camera capture is costly and inaccessible; converting abundant monocular videos to stereo is desirable.<br>‚Ä¢ Reconstruction-based NVS pipelines (SfM/NeRF/3DGS) are brittle under pose errors and non-rigid dynamics, causing unstable geometry and temporal inconsistency in stereo.<br>‚Ä¢ Depth-warp-inpaint diffusion approaches decouple geometry from synthesis, breaking pixel-level correspondence and causing artifacts, color shifts, and stereo discomfort.<br>‚Ä¢ Lack of large-scale, IPD-aligned stereo datasets; existing wide-baseline data exaggerate parallax and reduce comfort on XR devices.<br>‚Ä¢ Practical constraints of video diffusion (short clips, limited resolution) hinder scalable, high-resolution, long-duration stereo generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>StereoWorld adapts a pretrained video diffusion model to directly generate right-eye views from a left-view video via latent-frame concatenation (monocular conditioning) and geometry-aware regularization that combines disparity supervision (with a differentiable stereo projector) and joint RGB‚Äìdepth diffusion using dual-branch DiT heads. Spatio-temporal tiling enables efficient high-resolution, long-duration synthesis, trained on a curated 11M-frame IPD-aligned stereo dataset.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Baseline-Controllable Stereo Diffusion: Explicitly condition on desired baseline/IPD or target parallax to provide user/device-adjustable stereo strength.<br>‚Ä¢ Real-Time Stereo Video via Consistency Distillation: Distill the diffusion model into a one-/few-step generator with caching and token pruning for interactive XR playback.<br>‚Ä¢ Self-Supervised Geometry for Stereo Diffusion: Learn disparity and depth jointly without external estimators using cycle consistency, cross-view reprojection, and temporal constraints.<br>‚Ä¢ Occlusion- and Layer-Aware Stereo Video Generation: Integrate layered or segment-based representations to better handle occlusions and disocclusions across views and time.<br>‚Ä¢ From Stereo to Multi-View and 6-DoF Video: Extend the end-to-end framework to generate multiple viewpoints or short camera trajectories for light-field/XR experiences.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08560" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08560" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ fMRI signals are high-dimensional and complex, while the space of visual concepts is vast, making interpretation challenging.<br>‚Ä¢ Existing studies are small-scale, rely on manual inspection, and often focus on specific regions (e.g., FFA, PPA) or a few hand-picked categories.<br>‚Ä¢ Limited image‚ÄìfMRI datasets hinder robust, data-driven discovery and stable decomposition (e.g., ICA) of nuanced representations.<br>‚Ä¢ Analyses at the voxel level are too local and at the ROI level too coarse, obscuring overlapping and fine-grained concepts.<br>‚Ä¢ There is no scalable, automated interpretability pipeline for the brain, and systematic validation across many patterns and concepts is rare.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>BrainExplore is an automated pipeline that performs per-ROI unsupervised fMRI decomposition (PCA, NMF, ICA, Sparse Autoencoders), retrieves top-activating images, builds a brain-inspired hypothesis dictionary via VLM/LLM, labels images, and scores pattern‚Äìhypothesis alignments to surface interpretable representations. It scales by augmenting data with predicted fMRI responses from an image-to-fMRI encoder and integrates multiple methods to select the most consistent explanations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Sparse Autoencoders with Spatial Priors for fMRI Decomposition: Incorporate anatomical/topological constraints into SAEs to enhance spatial localization and interpretability of higher-level visual representations.<br>‚Ä¢ Closed-Loop Stimulus Optimization for Interpretable Brain Pattern Discovery: Use image-to-fMRI encoders with generative models to synthesize stimuli that maximize specific component activations, validating concepts with measured fMRI.<br>‚Ä¢ Cross-Subject Alignment of Interpretable Brain Components via Shared Latent Spaces: Learn subject-invariant latent representations that align discovered patterns across participants, improving generalization and comparative neuroscience.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Composing Concepts from Images and Videos via Concept-prompt Binding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.09824" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.09824" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing methods struggle to accurately extract and decouple complex visual concepts (occlusions, temporal changes, styles, non-object attributes) from images/videos<br>‚Ä¢ Reliance on masks, LoRA fusion, or learnable embeddings limits control, scalability, and predictability of what each token learns<br>‚Ä¢ Prior works mainly animate an image subject with video motion, lacking flexible composition of diverse attributes (style, lighting, appearance, motion) across modalities<br>‚Ä¢ Creators need one-shot, mask-free, prompt-controlled concept composition that binds visual concepts to textual tokens for intuitive reuse in generation<br>‚Ä¢ Temporal heterogeneity between images and videos causes incompatibility when directly composing concepts from both sources<br>‚Ä¢ Many approaches depend on specific modulation architectures, reducing universality across modern DiT-based T2V models</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>BiCo binds visual concepts to specific prompt tokens via hierarchical binder modules attached to DiT cross-attention, enabling mask-free decomposition and prompt-controlled recomposition across images and videos. It boosts binding accuracy with a VLM-driven Diversify-and-Absorb Mechanism and ensures image‚Äìvideo compatibility through a two-stage Temporal Disentanglement Strategy with dual-branch spatial‚Äìtemporal binders.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Interactive Token-Level Concept Editing for Video Generation: A human-in-the-loop interface to select, gate, and edit bound tokens and their spatial/temporal scopes for fine-grained control.<br>‚Ä¢ Universal Binder Adapters for Cross-Model Concept Composition: Model-agnostic binder designs that transfer concept‚Äìtoken bindings across different DiT/T2V backbones and scales without retraining.<br>‚Ä¢ Composable 3D Spatiotemporal Concepts for Multiview and VR Content: Extend concept-prompt binding to 3D/NeRF pipelines to compose geometry, appearance, and motion across multiview sequences.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">OmniPSD: Layered PSD Generation with Diffusion Transformer</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.09247" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.09247" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Most generative models output flat RGB images without layer-wise structure or alpha channels, preventing element-level editing in professional PSD workflows.<br>‚Ä¢ Post-hoc ‚Äúgenerate-then-decompose‚Äù pipelines (segmentation/matting after RGB synthesis) accumulate errors and poorly model inter-layer relationships and transparency.<br>‚Ä¢ No unified framework jointly supports text-to-PSD generation and image-to-PSD decomposition with transparency-aware reasoning.<br>‚Ä¢ Existing RGBA autoencoders are not robust to design-specific transparency (e.g., semi-transparent text, shadows, blending), degrading reconstruction quality.<br>‚Ä¢ Lack of large-scale, PSD-style RGBA-layer datasets and benchmarks tailored to layered design generation and reconstruction.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>OmniPSD couples a retrained transparency-preserving RGBA-VAE with Flux-dev (text-to-image) and Flux-Kontext (image editing) diffusion transformers to unify text-to-PSD generation and image-to-PSD decomposition. It jointly generates layers via a 2√ó2 grid with hierarchical prompts for spatial in-context learning, and iteratively decomposes flattened images using flow-matched LoRA editors for foreground extraction/erasure, with an OCR‚Äìfont-recovery pipeline to rebuild editable text layers.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ VectorAware-OmniPSD: Joint Raster‚ÄìVector Layer Synthesis for Fully Editable Designs: Extend OmniPSD to co-generate RGBA layers and SVG/shape primitives with differentiable rasterization for true PSD/SVG editability.<br>‚Ä¢ OmniPSD-Interact: Human-in-the-Loop Constraint-Guided Layered Diffusion: Incorporate layout constraints, selection masks, and incremental user edits to enable responsive, controllable layer synthesis and refinement.<br>‚Ä¢ OmniPSD-Video: Temporally Consistent Layered Motion Graphics Generation: Generalize layered RGBA generation/decomposition to video with cross-frame consistency of layers, alphas, and editable text for motion design.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08829" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08829" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Transformer VLMs suffer quadratic attention cost and ever-growing KV caches, causing prohibitive latency and memory in long-context/streaming scenarios, especially on edge devices.<br>‚Ä¢ Window-based VLMs degrade once sequences exceed the window; they discard past context and lose long-term situational coherence.<br>‚Ä¢ Existing linear-attention VLMs compress history into fixed-size states, underperforming on information-dense tasks (OCR, documents) and lacking effective training strategies.<br>‚Ä¢ Need an architecture that supports unlimited multimodal input with constant latency/memory, reuses pretrained Transformer weights, and remains self-contained without external memory.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A hybrid VLM that interleaves Sliding Window Attention layers for fine-grained local modeling with Gated DeltaNet (linear attention with gated delta updates and Householder-style rotation) for efficient long-range memory, yielding constant-latency, constant-memory inference. Trained via a three-stage pipeline: distillation from a strong Transformer VLM, instruction SFT, and long-sequence SFT (via LoRA) to activate length generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Hybrid Attention Scheduling for Streaming Vision-Language Models: Learn to dynamically allocate and route tokens through SWA vs. Gated DeltaNet layers based on content and sequence dynamics to optimize detail retention and long-term memory.<br>‚Ä¢ Theoretical Bounds on Information Retention in Gated DeltaNet Memory: Formal analysis of memory collisions, recency bias, and retrieval fidelity in gated delta-rule updates, with principled designs for gates and rotations.<br>‚Ä¢ Cross-Teacher Distillation for Linear-Hybrid VLMs: Multi-teacher, multi-stage distillation (from diverse Transformer VLMs) to enhance generalization on information-dense tasks while maintaining linear-time efficiency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.09928" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.09928" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ VLAs suffer from temporal myopia: they assume Markovian inputs and rely on the current frame, degrading long-horizon coherence and causal consistency.<br>‚Ä¢ History via frame stacking is redundant and slow: stacking RGB frames inflates computation and latency while burying task-relevant dynamics under static pixels.<br>‚Ä¢ Foresight via pixel-level subgoals is brittle: future image prediction introduces semantic drift, redundancy, and weak temporal continuity.<br>‚Ä¢ Lack of bidirectional temporal reasoning: existing methods rarely unify hindsight (past dynamics) and foresight (anticipated motion) within a compact, structured representation.<br>‚Ä¢ Real-time constraints: long temporal context must be modeled with negligible inference overhead to be practical for robotic control.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>HiF-VLA encodes past dynamics as compact Motion Vectors (hindsight) from video codecs, predicts future motion and action tokens in parallel (foresight) using a non-causal VLM, and fuses them via a Hindsight-Modulated Joint Expert with AdaLN conditioning to output temporally coherent actions with low latency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Beyond Codec Motion: Self-Supervised, Robust Motion Tokens for VLA: Replace/augment codec MVs with learned, noise-robust motion primitives (e.g., flow/warp tokens) to handle highly dynamic or low-texture scenes.<br>‚Ä¢ Hierarchical HiF-VLA: Motion-Grounded Subgoals and Long-Horizon Task Decomposition: Build a two-level policy where motion-conditioned high-level planners set subgoals and low-level HiF-VLA executes, improving scalability to very long tasks.<br>‚Ä¢ Multimodal HiF-VLA: Fusing Vision, Proprioception, and Tactile Hindsight/Foresight for Occlusion-Robust Manipulation: Extend motion reasoning to include force/tactile and proprioceptive dynamics to maintain reliability under occlusion and subtle state changes.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.02892" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.02892" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves 3.8-4.0times speedups while retaining 99.8-100% of the baseline score on average. On base models, SchED yields consistent speedup gains with 99.1-100% performance retention, with up to 2.34times under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, Œ≥{=}4), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Diffusion LLMs suffer from slow, iterative reverse-diffusion decoding; conservative, fixed step budgets and transfer schedules cause unnecessary computation on easy inputs and instability when overly aggressive.<br>‚Ä¢ Existing acceleration methods often require retraining, auxiliary models, or brittle heuristics; prior confidence-based early-commit approaches degrade on long-form generation and lack progress awareness.<br>‚Ä¢ There is a need for a simple, training-free, model-agnostic early-exit principle that triggers precisely when predictions stabilize rather than at preset budgets.<br>‚Ä¢ Current reporting separates speed and quality, obscuring trade-offs; a quality-penalized metric is needed to compare methods fairly.<br>‚Ä¢ Understanding why instruction tuning enables earlier, safer exits (e.g., faster entropy decay) can guide principled early-exit design.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SchED computes top-2 logit margins for all tokens in the answer span at each denoising step, aggregates them (mean), and stops decoding once this confidence exceeds a smooth, progress-aware threshold œÑ(p) (linear/cosine/exponential), then fills remaining masks with current argmax predictions. It is training-free, model-agnostic, and composes with single-block and block-diffusion transfer schedules, delivering 3.8‚Äì4.0√ó speedups on instruct models with near-parity quality and outperforming prior early-commit baselines on a conservative QPS metric.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Progress-Aware Confidence Schedules via Meta-Optimization: Learn instance- and task-conditioned œÑ(p) from data (or with RL) to adaptively balance quality and speed beyond fixed linear/cosine/exponential forms.<br>‚Ä¢ Joint Early-Exit and Remasking for Robust Diffusion Decoding: Integrate schedule-based stopping with selective re-masking of low-confidence tokens to correct residual errors before termination.<br>‚Ä¢ Theoretical Guarantees for Schedule-Based Early Exit in Discrete Diffusion: Derive bounds on error and stopping optimality as a function of margin statistics and progress, providing principled thresholds.<br>‚Ä¢ Hardware-Aware Multi-Objective Scheduling for Diffusion Inference: Co-optimize œÑ(p) with caching and speculative mechanisms to minimize end-to-end latency under real hardware constraints.<br>‚Ä¢ Extending Progress-Aware Early Exit to Multimodal and Code Diffusion Models: Generalize margin aggregation and scheduling to multimodal LMs and code generation, with modality- or domain-specific confidence signals.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Rethinking Chain-of-Thought Reasoning for Videos</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.09616" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.09616" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ CoT for video MLLMs incurs heavy training and inference overhead due to long reasoning traces and thousands of redundant visual tokens<br>‚Ä¢ Two-stage post-training (SFT on CoT + RL) requires costly CoT annotations and long sequences, slowing iteration and increasing carbon footprint<br>‚Ä¢ Empirically, CoT yields only modest accuracy gains over direct answering while drastically increasing latency and memory<br>‚Ä¢ Pretrained models are misaligned with concise reasoning (short rationales underperform direct answers), limiting practical explainability<br>‚Ä¢ Training-free token compression degrades performance, especially for concise/CoT decoding, revealing fragility to input token perturbations<br>‚Ä¢ Efficient deployment for long videos and resource-constrained settings demands fewer input/output tokens without sacrificing accuracy</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Directly fine-tune a pretrained video MLLM with GRPO-based reinforcement learning (no SFT, no CoT annotations) to produce concise reasoning before answers while integrating trainable token compression (token merging + physical pruning) during training and inference. The framework maintains FlashAttention efficiency by pruning only in a few layers, enabling more frames under the same compute and yielding brief, effective reasoning with improved accuracy and latency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning to Budget: Adaptive Reasoning-Length Control for Video MLLMs via Rewarded Compactness: Train policies that dynamically allocate rationale length per query, balancing accuracy and latency with compactness-aware rewards<br>‚Ä¢ End-to-End Token Compression Policies for Long-Video Reasoning: Jointly learn frame selection and token pruning/merging with differentiable or RL objectives to maximize accuracy under strict token budgets<br>‚Ä¢ Beyond Videos: Generalizing Concise Reasoning with Compression to Multimodal Streams (Audio, Sensors, and Images): Extend the concise-reasoning + compression paradigm to multi-sensor inputs and study cross-modal robustness and alignment</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">EtCon: Edit-then-Consolidate for Reliable Knowledge Editing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04753" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04753" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This work's empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is a critical absence of a knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs' inference-time behavior under autoregressive generation, thereby leading to a mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, a novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised Fine-Tuning (TPSFT) that localizes the edit via a trust-region objective to limit policy drift; (2) Then, a consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLMs rapidly become outdated, yet full retraining to inject new knowledge is impractical due to cost and data requirements.<br>‚Ä¢ Existing knowledge editing methods perform well in teacher-forcing and single-edit settings but degrade under real-world, autoregressive, and lifelong editing scenarios.<br>‚Ä¢ Traditional parametric edits overfit to the new fact, causing policy drift and damaging pre-trained capabilities such as reasoning, fluency, and robustness.<br>‚Ä¢ A missing consolidation phase leaves a mismatch between updated parametric knowledge and inference-time behavior, yielding inconsistent activation and contradictory generations.<br>‚Ä¢ Current approaches lack mechanisms to localize edits (preserve locality) and to align CoT-based inference policy with the edited knowledge.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>EtCon is a two-stage paradigm: Targeted Proximal Supervised Fine-Tuning (TPSFT) applies trust-region‚Äìconstrained updates to identified FFN knowledge layers to localize edits and mitigate overfitting, followed by Group Relative Policy Optimization (GRPO) to consolidate the edit by aligning chain-of-thought inference via trajectory-level rewards.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Online EtCon: Continual Edit-and-Consolidate for Lifelong LLM Updating: Develop scheduling, replay, and interference mitigation to support scalable, streaming edits with periodic consolidation.<br>‚Ä¢ Uncertainty-Guided Consolidation for Reliable Knowledge Editing: Use epistemic/aleatoric uncertainty to adapt consolidation strength and reward shaping, improving safety and edit reliability.<br>‚Ä¢ Automated Layer Selection for Targeted Proximal Editing: Learn or probe which layers/heads store target knowledge to further localize TPSFT and reduce collateral drift.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.09864" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.09864" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ End-to-end autonomous driving struggles in long-tail and unstructured scenarios due to limited world knowledge and weak visual dynamic modeling, harming generalization and safety.<br>‚Ä¢ VLA-based models cannot exploit large unlabeled driving video datasets for visual causal learning, while world models learn dynamics but lack LLM-level reasoning, world knowledge, and interactive capability.<br>‚Ä¢ Existing unified models do not jointly deliver chain-of-thought reasoning, coherent future video generation, and physically consistent trajectory planning; they also underutilize pre-trained VLMs/video generative models, diverse data sources, and lack comprehensive evaluation across understanding, reasoning, and planning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>UniUGP proposes a unified Understanding‚ÄìGeneration‚ÄìPlanning framework with a hybrid expert architecture that integrates pre-trained VLMs and video generation/world models to jointly output interpretable chain-of-thought reasoning, coherent future videos, and physically consistent trajectories from multi-frame inputs and language instructions. A multi-term loss (for CoT logic, trajectory smoothness, and video coherence) and a four-stage training strategy across diverse AD datasets ensure cross-modal causal alignment and robust performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ UniUGP-RL: Closed-loop reinforcement learning with real-time language feedback to refine reasoning and planning under long-tail scenarios.<br>‚Ä¢ UniUGP-Active: Active and self-supervised learning on unlabeled driving videos to maximize visual causal learning and scenario coverage.<br>‚Ä¢ Trustworthy UniUGP: Uncertainty-aware reasoning and formal safety verification integrated into unified CoT‚Äìvideo‚Äìtrajectory outputs for dependable AD.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">WonderZoom: Multi-Scale 3D World Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.09164" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.09164" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to "zoom into" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing 3D/world generators are single-scale and cannot coherently create content from macro (landscape/room/city) to micro (object/surface) levels from a single image.<br>‚Ä¢ Lack of a scale-adaptive 3D representation that can grow dynamically as finer details are generated, while supporting real-time rendering and avoiding aliasing across overlapping scales.<br>‚Ä¢ Generative pipelines need to synthesize new, previously non-existent structures at finer scales that obey user prompts and remain geometrically and visually consistent with coarser context.<br>‚Ä¢ Traditional LoD and recent hierarchical (e.g., mip/GS) methods assume pre-existing or fully supervised multi-scale data and static hierarchies, making them incompatible with progressive generation.<br>‚Ä¢ Naively generating all scales simultaneously is computationally intractable and conflicts with the coarse-to-fine nature of multi-scale synthesis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>WonderZoom introduces scale-adaptive Gaussian surfels with native-scale-aware opacity modulation for dynamic, append-only multi-scale 3D representations that render in real time, and a progressive detail synthesizer that autoregressively generates finer-scale images (with depth and auxiliary views) conditioned on coarse geometry and user prompts, then integrates them by adding new surfels without re-optimizing existing ones.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Dynamic 4D WonderZoom: Multi-Scale Spatiotemporal World Generation: Extend the framework to time-varying scenes with motion-aware surfels and temporal coherence across scales.<br>‚Ä¢ Uncertainty-Aware Scale-Adaptive Surfels for Guided Detail Synthesis: Model and propagate per-surfel uncertainty to decide where and when to generate new details and to improve opacity blending and consistency.<br>‚Ä¢ End-to-End Learned Multi-Scale 3D Generation from Sparse Inputs: Replace hand-crafted SR/VLM stages with a unified diffusion-based generator trained to produce images, depths, and surfel parameters across scales from minimal inputs.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Learning Unmasking Policies for Diffusion Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.09106" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.09106" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their autoregressive counterparts on many tasks, while holding the promise of being more efficient during inference. One particularly successful variant is masked discrete diffusion, in which a buffer filled with special mask tokens is progressively replaced with tokens sampled from the model's vocabulary. Efficiency can be gained by unmasking several tokens in parallel, but doing too many at once risks degrading the generation quality. Thus, one critical design aspect of dLLMs is the sampling procedure that selects, at each step of the diffusion process, which tokens to replace. Indeed, recent work has found that heuristic strategies such as confidence thresholding lead to both higher quality and token throughput compared to random unmasking. However, such heuristics have downsides: they require manual tuning, and we observe that their performance degrades with larger buffer sizes. In this work, we instead propose to train sampling procedures using reinforcement learning. Specifically, we formalize masked diffusion sampling as a Markov decision process in which the dLLM serves as the environment, and propose a lightweight policy architecture based on a single-layer transformer that maps dLLM token confidences to unmasking decisions. Our experiments show that these trained policies match the performance of state-of-the-art heuristics when combined with semi-autoregressive generation, while outperforming them in the full diffusion setting. We also examine the transferability of these policies, finding that they can generalize to new underlying dLLMs and longer sequence lengths. However, we also observe that their performance degrades when applied to out-of-domain data, and that fine-grained tuning of the accuracy-efficiency trade-off can be challenging with our approach.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Selecting which tokens to unmask at each diffusion step is critical for balancing efficiency and generation quality in diffusion LLMs, but existing random strategies underperform.<br>‚Ä¢ Heuristic policies (e.g., confidence thresholding) improve quality and throughput yet require manual tuning and their performance degrades with larger buffer sizes.<br>‚Ä¢ There is no principled, learnable sampling procedure that adapts across models and sequence lengths while optimizing the accuracy‚Äìefficiency trade-off, limiting scalability and robustness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper formalizes masked diffusion sampling as a Markov decision process where the dLLM is the environment, and trains a lightweight single-layer transformer policy via reinforcement learning to map token confidences to unmasking decisions. This learned policy matches heuristic performance in semi-autoregressive settings and surpasses them in full diffusion, with partial transferability across models and longer sequences.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Meta-Learned Unmasking Policies for Domain-Robust Diffusion Language Models: Use meta-learning and domain adaptation to improve policy generalization to out-of-domain data.<br>‚Ä¢ Multi-Objective RL for Fine-Grained Accuracy‚ÄìEfficiency Control in Diffusion Sampling: Optimize a Pareto front over quality and throughput to enable adjustable trade-offs at inference time.<br>‚Ä¢ Uncertainty-Aware Unmasking via Calibrated Confidence in Diffusion LMs: Incorporate calibrated or Bayesian uncertainty estimates into the policy inputs to make more reliable unmasking decisions.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">Towards a Science of Scaling Agent Systems</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08296" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08296" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of principled, quantitative scaling laws for LM-based agent systems forces practitioners to rely on heuristics when choosing number of agents and coordination topology.<br>‚Ä¢ Unclear trade-offs among agents, coordination structure, model capability, and task properties lead to inefficiency and suboptimal performance in real-world applications.<br>‚Ä¢ Prior evaluations are confounded by differing tools, prompts, and token budgets, making it hard to isolate architectural effects and generalize findings.<br>‚Ä¢ Overgeneralized claims that multi-agent systems always outperform single-agent baselines ignore capability saturation and error amplification, risking degraded outcomes on sequential tasks.<br>‚Ä¢ Absence of predictive models and empirical metrics to anticipate when coordination helps or hurts prevents reliable design choices across diverse domains.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Conduct a controlled, cross-domain evaluation of five agent architectures across four benchmarks and three LLM families (180 configurations), standardizing tools, prompts, and token budgets to isolate architectural effects, then derive a predictive model using coordination metrics (efficiency, overhead, error amplification, redundancy) to forecast performance and optimal coordination strategies.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Coordination Topologies for Task-Contingent Agent Systems: Develop meta-controllers that select and switch between centralized, decentralized, independent, and hybrid coordination based on online estimates of parallelizability, tool intensity, and model capability.<br>‚Ä¢ Bounding Error Propagation in Multi-Agent LMs via Verification and Control: Design auditing, consensus, and redundancy protocols that explicitly limit topology-dependent error amplification while preserving coordination efficiency.<br>‚Ä¢ Cost- and Capability-Aware Agentic Scaling Laws Across Modalities: Extend the predictive framework to multimodal and embodied tasks, integrating API/tool costs and token budgets with capability estimates to generalize beyond web, finance, and planning domains.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.09663" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.09663" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ MLLMs are primarily trained and evaluated on RGB/natural images, leaving their capability on infrared (IR) images largely unexplored.<br>‚Ä¢ Significant domain shift between IR inputs and RGB-trained MLLMs degrades fine-grained perception and overall understanding.<br>‚Ä¢ Prior IR-focused evaluations have narrow task coverage, limited model diversity, and lack rigorous human calibration, obscuring true capabilities.<br>‚Ä¢ Scarcity of high-quality IR‚Äìtext data makes supervised fine-tuning and model-specific adaptations impractical.<br>‚Ä¢ Lack of standardized, reliable evaluation protocols (prompts, bilinguality, judgment) hinders fair comparison across models on IR tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces IF-Bench, a high-quality IR understanding benchmark (499 images, 680 VQA across 10 dimensions) with robust protocols (unified prompts, cyclic evaluation, bilingual assessments, hybrid judgments), and proposes GenViP, a training-free generative visual prompting method that edits IR images into semantically and spatially aligned RGB counterparts and co-feeds both to any MLLM. They further boost open-source editing via fine-tuning Qwen-Edit-2509 on 50k RGB‚ÄìT pairs, yielding consistent gains (up to 7%) across >40 MLLMs and surpassing some closed-source models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Task-Aware Reasoning Mode Switching for Multispectral MLLMs: Learn to dynamically enable/disable thinking mode based on task type (fine-grained perception vs. thermal reasoning) to mitigate observed trade-offs.<br>‚Ä¢ Domain-Invariant Visual Encoders for Unified RGB‚ÄìIR Understanding: Train shared encoders with domain adversarial learning and cross-modal alignment to reduce IR‚ÄìRGB distribution gaps without paired supervision.<br>‚Ä¢ Self-Supervised Infrared‚ÄìText Pretraining via Cross-Modal Pseudo-Labeling: Leverage large unlabeled RGB‚ÄìIR corpora with MLLM-generated captions and consistency losses to build scalable IR-aware vision‚Äìlanguage models.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05446" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05446" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Dynamic 3DGS (4DGS) lacks rate‚Äìdistortion-optimized compression; most prior work targets memory or speed rather than storage/transmission efficiency.<br>‚Ä¢ Existing dynamic representations have critical trade-offs: space-time 4DGS inflates bitrate with many short-lived primitives, while canonical 3DGS with deformation lacks explicit temporal activation, causing unnatural motion to handle occlusion/disocclusion and training instability.<br>‚Ä¢ Compression models are suboptimal: grid-based hyperpriors add signaling overhead and prior methods underexploit intra-anchor correlations; multi-view color inconsistencies further destabilize optimization and degrade RD.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TED-4DGS builds on ScaffoldGS anchors and introduces per-anchor embedding-based deformation via a shared global deformation bank plus learnable temporal activation to explicitly control visibility; it couples an INR-based hyperprior and channel-wise autoregressive coding to entropy-model anchor attributes, trained progressively for RD-optimized dynamic 3DGS compression.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Temporal-Perceptual RD Optimization for Dynamic 3DGS: Integrate motion-aware perceptual losses with temporal activation to improve perceived quality at ultra-low bitrates.<br>‚Ä¢ Cross-Scene Deformation Bank Distillation for Transferable 4DGS Compression: Learn and distill a shared deformation bank across scenes to reduce per-scene signaling and enable few-shot compression.<br>‚Ä¢ Occlusion-Aware Temporal Activation with Visibility Supervision: Use explicit visibility/occlusion cues (e.g., depth masks or scene flow) to guide activation parameters, enhancing stability and RD efficiency.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08006" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08006" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Lightweight, real-time text-to-speech systems are crucial for accessibility. However, the most efficient TTS models often rely on lightweight phonemizers that struggle with context-dependent challenges. In contrast, more advanced phonemizers with a deeper linguistic understanding typically incur high computational costs, which prevents real-time performance. This paper examines the trade-off between phonemization quality and inference speed in G2P-aided TTS systems, introducing a practical framework to bridge this gap. We propose lightweight strategies for context-aware phonemization and a service-oriented TTS architecture that executes these modules as independent services. This design decouples heavy context-aware components from the core TTS engine, effectively breaking the latency barrier and enabling real-time use of high-quality phonemization models. Experimental results confirm that the proposed system improves pronunciation soundness and linguistic accuracy while maintaining real-time responsiveness, making it well-suited for offline and end-device TTS applications.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Real-time, offline TTS for accessibility must balance speed, footprint, and naturalness; existing lightweight or rule-based systems degrade naturalness, while larger neural models break latency constraints.<br>‚Ä¢ In G2P-aided TTS, lightweight phonemizers lack context awareness, causing mispronunciations in ambiguous languages (e.g., Persian homographs and Ezafe), reducing intelligibility.<br>‚Ä¢ Directly integrating strong context-aware G2P models increases computational load and latency, preventing real-time performance on end devices.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A service-oriented TTS architecture that offloads heavy, context-aware phonemizers to independent services and augments them with lightweight statistical modules for partial context awareness, enabling low-latency phonemization. Implemented via a service-oriented adaptation of PiperTTS with an enhanced eSpeak-based Persian phonemizer and a new Persian voice, achieving improved accuracy while maintaining real-time responsiveness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Latency-Aware Orchestration of Context-Aware Phonemizers for Multilingual TTS: Design a controller that dynamically selects between lightweight and heavy phonemizers based on context complexity and device constraints across languages.<br>‚Ä¢ Privacy-Preserving On-Device Adaptation of Phonemizers via Federated Learning: Personalize context-aware phonemizers using federated training to capture user-specific vocabulary and grammar without sharing raw data.<br>‚Ä¢ Benchmarking and Metrics for Context-Aware G2P in Low-Resource Languages: Develop standardized datasets and latency-aware evaluation metrics focused on phenomena like homograph resolution and Ezafe detection.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04519" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04519" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Autoregressive (AR) diffusion enables streaming, interactive long-video generation by producing frames causally, yet maintaining coherence over minute-scale horizons remains challenging due to accumulated errors, motion drift, and content repetition. We approach this problem from a memory perspective, treating video synthesis as a recurrent dynamical process that requires coordinated short- and long-term context. We propose VideoSSM, a Long Video Model that unifies AR diffusion with a hybrid state-space memory. The state-space model (SSM) serves as an evolving global memory of scene dynamics across the entire sequence, while a context window provides local memory for motion cues and fine details. This hybrid design preserves global consistency without frozen, repetitive patterns, supports prompt-adaptive interaction, and scales in linear time with sequence length. Experiments on short- and long-range benchmarks demonstrate state-of-the-art temporal consistency and motion stability among autoregressive video generator especially at minute-scale horizons, enabling content diversity and interactive prompt-based control, thereby establishing a scalable, memory-aware framework for long video generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ AR diffusion for long videos suffers from error accumulation, motion drift, and content repetition over minute-scale horizons.<br>‚Ä¢ Sliding-window attention (KV cache) is efficient but loses information outside the window, causing drift as early tokens are evicted.<br>‚Ä¢ Attention-sink strategies stabilize long-range context but freeze global memory, leading to repetitive, non-dynamic generation.<br>‚Ä¢ Full bidirectional attention has quadratic cost, limiting scalability and real-time, interactive use.<br>‚Ä¢ 3D-coupled spatial memories in world models do not transfer well to open-ended, free-view long-video synthesis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VideoSSM augments an autoregressive DiT with a hybrid memory: a causal sliding-window KV cache for local, lossless detail and an SSM-based global compressed state that continuously absorbs evicted tokens via gated updates and controlled forgetting; queries retrieve from both and are fused by a router for O(TL) streaming. The model is trained via Self Forcing distillation with DMD loss from a bidirectional teacher, improving long-range consistency and prompt-adaptive interaction.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Memory Routing for Multimodal AR Video Generation: Learn content- and prompt-aware routers to dynamically weight local vs. global memory across text, audio, and vision.<br>‚Ä¢ Hierarchical State-Space Memory for Hour-Scale Video: Stack multi-timescale SSMs to capture short-, mid-, and long-term dynamics for stable generation beyond minutes.<br>‚Ä¢ World-State-Augmented SSMs for Controllable Camera Motion: Integrate lightweight 3D scene proxies into the SSM to enhance global consistency and enable explicit camera and viewpoint control in free-view settings.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">GimbalDiffusion: Gravity-Aware Camera Control for Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.09112" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.09112" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GimbalDiffusion, a framework that enables camera control grounded in physical-world coordinates, using gravity as a global reference. Instead of describing motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360-degree videos to construct a wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the model's reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish a benchmark for camera-aware video generation by rebalancing SpatialVID-HQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of absolute, gravity-referenced camera control in text-to-video; most methods define poses relative to the first frame, making pitch/roll ambiguous and preventing explicit global orientation control.<br>‚Ä¢ Dataset bias toward forward-facing, level-horizon shots limits learning of extreme viewpoints (e.g., top-down, high roll/Dutch angles), reducing controllability and robustness.<br>‚Ä¢ Entanglement between text semantics and camera angle causes models to ignore camera conditioning when prompts conflict with intended viewpoints.<br>‚Ä¢ Absence of a standardized benchmark and metrics for extreme camera angles impedes fair evaluation of gravity-aligned control.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>GimbalDiffusion conditions video diffusion with absolute, gravity-aligned camera parameters by deriving absolute poses from 360¬∞ videos (SfM translations + sampled rotations) and encoding them as absolute Pl√ºcker rays, while using null-pitch captioning to decouple text content from camera angle; a new SpatialVID-extreme benchmark evaluates adherence across wide pitch/roll ranges.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ 6-DoF Gravity-Aware Camera Control for Text-to-Video: Extend from rotation-only to full translation+rotation control with explicit parallax, supervised by novel view synthesis or Gaussian splatting.<br>‚Ä¢ Disentangled Prompt‚ÄìCamera Conditioning via Multi-View Captioning: Generalize null-pitch to multi-view, view-invariant captioning and losses that explicitly separate content semantics from viewpoint across frames.<br>‚Ä¢ CineBench-Extreme: Robust Metrics and Dataset for Extreme Viewpoints: Build a larger, IMU-fused benchmark and roll-robust orientation estimators to reliably assess absolute/relative pose accuracy under near-pole conditions.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">Pay Less Attention to Function Words for Free Robustness of Vision-Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07222" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07222" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Robust vision-language models (VLMs) suffer a trade-off: adversarial training improves robustness but significantly degrades clean performance and requires high computational cost.<br>‚Ä¢ Cross-modal adversarial attacks exploit misalignment by pushing attention toward ubiquitous, non-specific function words, a vulnerability largely overlooked by existing defenses.<br>‚Ä¢ Current defenses (e.g., purification, detection, AT variants) do not address token-type granularity in fusion cross-attention and fail to deliver lightweight robustness that preserves performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Function-word De-Attention (FDA) adds a parallel pipeline in the fusion encoder to compute cross-attention between function words and image tokens, then differentially subtracts these 'distraction' scores from the original cross-attention (per head) via a control gate, refocusing attention on content words for improved robustness without retraining.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Adaptive De-Attention Gates for Multilingual VLMs: Train per-layer/per-head gates and language-aware function-word sets to generalize FDA across multilingual and code-mixed inputs.<br>‚Ä¢ Beyond Function Words: Automatic Discovery and Suppression of Cross-Modal Distractor Tokens: Use attribution or influence-function methods to identify and dynamically de-attend low-specificity tokens and phrases beyond hand-crafted stopword lists.<br>‚Ä¢ Theoretical and Empirical Analysis of Differential Attention for Robust Vision-Language Alignment: Develop a formal robustness framework explaining subtractive attention's effects under perturbations and optimize layer/head placement with end-to-end objectives.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05402" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05402" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of data-driven guidance on when to buy ASIC miners amid volatile BTC price, difficulty changes, and halving cycles, leading to mistimed purchases and extreme payback extensions (e.g., S19j Pro case).<br>‚Ä¢ High-stakes, capital-intensive decisions where electricity dominates OPEX and hardware rapidly obsoletes, making acquisition timing economically critical to avoid unprofitable operations.<br>‚Ä¢ Existing work focuses on price/income prediction rather than ROI-based buy/no-buy decisions; typically omits capex, 1-year ROI horizons, resale considerations, and geographic electricity variability, offering limited decision support.<br>‚Ä¢ Traditional and standard sequential models struggle with multi-scale/periodic patterns (difficulty adjustments, halvings), lacking robust mechanisms to capture long-range cyclic dependencies needed for timing decisions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Formulates ASIC purchase timing as a three-class time-series classification of 1-year ROI (‚â§0, 0‚Äì1, ‚â•1) and introduces MineROI-Net, a Transformer augmented with FFT-based spectral filtering to model periodicities and squeeze-and-excitation-style channel mixing for cross-feature interactions. Trained with weighted cross-entropy and evaluated via expanding-window cross-regimes CV on 20 ASICs (2015‚Äì2024), it outperforms LSTM and TSLANet, achieving 83.7% accuracy and 83.1% macro F1 on the final test split.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Liquidation Strategies for Mining ROI Prediction: Learn ROI under alternative BTC selling policies (e.g., threshold-based, weekly batching, HODL) and jointly optimize timing and liquidation.<br>‚Ä¢ Incorporating Hardware Depreciation and Resale Markets into ROI Timing: Extend the ROI framework to include secondary market prices, depreciation curves, and upgrade/swap decisions.<br>‚Ä¢ Uncertainty-Aware MineROI-Net with Conformal Risk Controls: Calibrate predictive intervals and abstention policies to minimize downside risk and align with miner-specific utility/risk constraints.<br>‚Ä¢ Portfolio-Level Hardware Acquisition via Reinforcement Learning: Optimize multi-machine, multi-period purchase schedules under budget, power, and price constraints using RL on top of ROI forecasts.<br>‚Ä¢ Causal Effects of Halving and Policy Shocks on Mining ROI Timing: Apply causal inference to isolate regime-specific impacts of halvings, fee spikes, and regulatory shocks on acquisition decisions.<br>‚Ä¢ Geo-aware ROI Prediction with Dynamic Energy Markets and Renewables: Model location-specific electricity volatility, curtailment, and renewable integration to produce site-optimized timing signals.<br>‚Ä¢ Cross-Chain and Merge-Mining ROI Timing Framework: Generalize to multi-coin SHA-256 (and merge-mined) revenues to time hardware purchases across chains.<br>‚Ä¢ Explainable MineROI-Net: Interpreting ROI Drivers Across Market Regimes: Use time-series attribution (e.g., integrated gradients in frequency/time) to explain feature/period contributions to buy/avoid signals.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01453" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01453" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Clinical dialogue represents a complex duality requiring both the empathetic fluency of natural conversation and the rigorous precision of evidence-based medicine. While Large Language Models possess unprecedented linguistic capabilities, their architectural reliance on reactive and stateless processing often favors probabilistic plausibility over factual veracity. This structural limitation has catalyzed a paradigm shift in medical AI from generative text prediction to agentic autonomy, where the model functions as a central reasoning engine capable of deliberate planning and persistent memory. Moving beyond existing reviews that primarily catalog downstream applications, this survey provides a first-principles analysis of the cognitive architecture underpinning this shift. We introduce a novel taxonomy structured along the orthogonal axes of knowledge source and agency objective to delineate the provenance of clinical knowledge against the system's operational scope. This framework facilitates a systematic analysis of the intrinsic trade-offs between creativity and reliability by categorizing methods into four archetypes: Latent Space Clinicians, Emergent Planners, Grounded Synthesizers, and Verifiable Workflow Automators. For each paradigm, we deconstruct the technical realization across the entire cognitive pipeline, encompassing strategic planning, memory management, action execution, collaboration, and evolution to reveal how distinct architectural choices balance the tension between autonomy and safety.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Clinical dialogue is a high-stakes, goal-oriented process requiring both empathetic communication and rigorous evidence-based reasoning; healthcare workforce shortages make scalable automation essential.<br>‚Ä¢ Pipeline-based and retrieval-based systems are either rigid with cascading errors or lack generative flexibility for nuanced, long-tail patient interactions, resulting in impersonal or fragmented communication.<br>‚Ä¢ Standard LLMs are reactive and stateless, prioritizing fluency over factuality, leading to hallucinations; they lack persistent longitudinal memory and agency to query EHRs/tools, limiting real-world clinical workflow orchestration.<br>‚Ä¢ Existing surveys are task-centric or open-domain and lack a unified, first-principles framework that explains agent design choices, trade-offs between autonomy and safety, and how knowledge sources shape reliability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces a first-principles 2√ó2 taxonomy for clinical agents along orthogonal axes of knowledge source (implicit vs explicit) and agency objective (event cognition vs goal execution), yielding four paradigms‚ÄîLatent Space Clinicians, Grounded Synthesizers, Emergent Planners, and Verifiable Workflow Automators‚Äîand systematically deconstructs each across planning, memory, action, collaboration, and evolution, grounded in a POMDP formulation of clinical dialogue.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Neuro-Symbolic Clinical Agents: Verifiable Reasoning Over Guidelines and Patient Graphs: Integrate LLMs with formal medical knowledge (ontologies, decision rules) and constraint-checking to reduce hallucinations and improve interpretability.<br>‚Ä¢ Longitudinal Memory for Chronic Care: Belief-State Updating and Privacy-Preserving Continuity: Design persistent memory architectures that approximate Bayesian belief updates across encounters, fuse EHR data, and enable safe continual learning.<br>‚Ä¢ Clinical Workflow Formalization and Verification for Agentic Automation: From Guidelines to Executable Decision Trees: Convert clinical guidelines into machine-checkable workflows with runtime validation and safety guards to enable trustworthy VWA agents.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 7</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-12-11 23:06:35 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 7;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>