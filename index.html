<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - October 24, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            font-weight: 300;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #2c3e50;
            margin-bottom: 15px;
            line-height: 1.3;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.3em;
            display: flex;
            align-items: center;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 20px;
            border-radius: 6px;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 20px;
            border-radius: 6px;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 20px;
            border-radius: 6px;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 20px;
            border-radius: 6px;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">October 24, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.19600" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.19600" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In the quest for scientific progress, communicating research is as vital as the discovery itself. Yet, researchers are often sidetracked by the manual, repetitive chore of building project webpages to make their dense papers accessible. While automation has tackled static slides and posters, the dynamic, interactive nature of webpages has remained an unaddressed challenge. To bridge this gap, we reframe the problem, arguing that the solution lies not in a single command, but in a collaborative, hierarchical process. We introduce AutoPage, a novel multi-agent system that embodies this philosophy. AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline from narrative planning to multimodal content generation and interactive rendering. To combat AI hallucination, dedicated "Checker" agents verify each step against the source paper, while optional human checkpoints ensure the final product aligns perfectly with the author's vision, transforming the system from a mere tool into a powerful collaborative assistant. To rigorously validate our approach, we also construct PageBench, the first benchmark for this new task. Experiments show AutoPage not only generates high-quality, visually appealing pages but does so with remarkable efficiency in under 15 minutes for less than \0.1. Code and dataset will be released at https://mqleet.github.io/AutoPage_ProjectPage/{Webpage}$.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>è®ºæ–‡èšç„¦å°†å­¦æœ¯è®ºæ–‡è‡ªåŠ¨ç”Ÿæˆäº¤äº’å¼é¡¹ç›®ç½‘é¡µï¼ˆpaper-to-pageï¼‰ï¼Œä»¥å‡å°‘ç ”ç©¶è€…åœ¨æ¨¡æ¿æ”¹é€ ä¸Šçš„é‡å¤åŠ³åŠ¨å¹¶æå‡ä¼ æ’­æ•ˆç‡ã€‚ç°æœ‰è‡ªåŠ¨åŒ–å¤§å¤šé¢å‘é™æ€æµ·æŠ¥/å¹»ç¯ç‰‡/è§†é¢‘ï¼Œéš¾ä»¥å¤„ç†ç½‘é¡µçš„å¯æ»šåŠ¨ç»“æ„ã€äº¤äº’å…ƒç´ ä¸é£æ ¼åå¥½ï¼ˆè§ç¬¬1é¡µFig.1ï¼‰ã€‚ç«¯åˆ°ç«¯LLMæ–¹æ¡ˆå¸¸å‡ºç°ä¸åˆç†å¸ƒå±€ã€ç¼ºä¹äººç±»åé¦ˆä¸”æ˜“å¹»è§‰ï¼Œéš¾ä»¥ä¿è¯äº‹å®ä¸€è‡´æ€§ä¸è§†è§‰å“è´¨ã€‚ä¸ºæ­¤ï¼Œä½œè€…å°†ä»»åŠ¡é‡æ„ä¸ºâ€œåä½œå¼ã€åˆ†å±‚ã€ç²—åˆ°ç»†â€çš„æµç¨‹ï¼Œå¹¶å¼•å…¥å¤šè½®éªŒè¯ä¸å¯é€‰äººç±»æ ¡å¯¹ä»¥ç¡®ä¿å¯æ§æ€§ä¸å¯é æ€§ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>ä½œè€…æå‡ºAutoPageå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé‡‡ç”¨ä¸‰é˜¶æ®µç²—åˆ°ç»†ç®¡çº¿ï¼ˆç¬¬4é¡µFig.2ï¼‰ï¼š(1) å™äº‹è§„åˆ’ä¸ç»“æ„åŒ–ï¼šç”¨MinerUä¸Doclingå°†PDFè§£æä¸ºMarkdownä¸èµ„äº§åº“ï¼ŒPage Content Plannerç”Ÿæˆç½‘é¡µå¤§çº²ï¼Œå¹¶ç”±æ£€æŸ¥å™¨æ ¡éªŒã€‚ (2) å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆï¼šåšæŒâ€œå…ˆæ–‡åå›¾â€ï¼Œå…ˆç”Ÿæˆæ®µè½ï¼Œå†ä»èµ„äº§åº“æŒ‘é€‰å¹¶æ’å…¥æœ€ç›¸å…³å›¾è¡¨ï¼›éšåç”±Content Checkeræ ¸éªŒæ–‡æœ¬â€“è§†è§‰ä¸€è‡´æ€§ï¼Œå¹¶æä¾›å¯é€‰äººç±»å¾®è°ƒã€‚ (3) äº¤äº’å¼æ¸²æŸ“ï¼šåŸºäºå¸¦æ ‡ç­¾çš„æ¨¡æ¿åº“è¿›è¡ŒåŒ¹é…ï¼Œæ•´åˆå†…å®¹å¹¶ç”ŸæˆHTML/CSS/JSï¼Œæœ€åç”±HTML Checkeråšç‰ˆå¼ä¸æ ·å¼ä½“æ£€ï¼Œå¹¶å¯æ¥å—äººç±»æŒ‡ä»¤å¾®è°ƒæ ·å¼ã€‚åŒæ—¶æ„å»ºPageBenchåŸºå‡†ï¼ˆç¬¬5â€“6é¡µï¼‰ï¼šæ±‡é›†1500+é¡¹ç›®é¡µï¼Œå«100ç¯‡æµ‹è¯•é›†ä¸87ä¸ªé£æ ¼æ¨¡æ¿ï¼Œå¹¶æå‡ºå†…å®¹ï¼ˆPPLã€è¯­ä¹‰ä¿çœŸã€å‹ç¼©æ„ŸçŸ¥ä¿¡æ¯å‡†ç¡®åº¦ï¼‰ä¸è§†è§‰ï¼ˆVLMè£åˆ¤çš„è§†è§‰å‡†ç¡®æ€§/ç‰ˆå¼ä¸€è‡´æ€§/ç¾å­¦åˆ†ï¼‰ä¸¤ç±»æŒ‡æ ‡ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ“Š</span>å®éªŒç»“æœ</h3>
            <div class="results">
                <p>åœ¨PageBenchä¸Šï¼ŒAutoPageæ˜¾è‘—æå‡ä¸åŒéª¨å¹²æ¨¡å‹çš„é¡µé¢è´¨é‡ï¼ˆè¡¨1ï¼Œç¬¬6é¡µï¼‰ï¼šç›¸å¯¹GPTâ€‘4oâ€‘miniï¼Œå®¡ç¾åˆ†ç”±2.71å‡è‡³2.95ã€ç‰ˆå¼ä¸å‡ç»ƒç”±2.08å‡è‡³2.38ï¼›ç›¸å¯¹Geminiâ€‘2.5â€‘Flashï¼Œè¯­ä¹‰ä¿çœŸç”±0.684å‡è‡³0.742ã€è§†è§‰å…ƒç´ å‡†ç¡®æ€§ç”±2.82å‡è‡³3.13ï¼Œå‹ç¼©æ„ŸçŸ¥ä¿¡æ¯å‡†ç¡®åº¦ç”±1.276å‡è‡³1.591ï¼ˆè¡¨3ï¼Œç¬¬12é¡µï¼‰ã€‚å¯¹è¾ƒå¼±éª¨å¹²ï¼ˆå¦‚Qwenï¼‰æå‡æ›´å¤§ï¼šè§†è§‰å‡†ç¡®æ€§ç”±2.52è·ƒå‡è‡³3.01ï¼Œæ˜¾è‘—ç¼©å°ä¸å¼ºæ¨¡å‹çš„å·®è·ï¼ˆè¡¨1ï¼‰ã€‚ç”¨æˆ·ç ”ç©¶ï¼ˆ20äººï¼Œå¼ºåˆ¶æ‰“åˆ†ï¼‰æ˜¾ç¤ºAutoPageå¹³å‡å¾—åˆ†7.16ï¼Œä¼˜äºGrok4â€‘fast(6.93)ä¸Gemini(6.79)ï¼ˆç¬¬7é¡µFig.3ï¼‰ã€‚éªŒè¯å™¨æ¶ˆèè¡¨æ˜å…¶å…³é”®æ€§ï¼šç§»é™¤å…¨éƒ¨æ£€æŸ¥å™¨åï¼Œè§†è§‰å‡†ç¡®æ€§ç”±3.13é™è‡³2.75ã€ç¾å­¦åˆ†ç”±2.69é™è‡³1.90ã€ç‰ˆå¼ç”±2.15é™è‡³1.60ï¼ˆè¡¨2ï¼Œç¬¬11é¡µï¼‰ï¼›ç³»ç»Ÿç”Ÿæˆä¸€é¡µçº¦4â€“20åˆ†é’Ÿã€æˆæœ¬$0.06â€“$0.20ï¼Œå¸¸è§é…ç½®< $0.1ï¼ˆç¬¬8é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>å¯å°†æ¨¡æ¿åŒ¹é…ä»é™æ€æ ‡ç­¾æ‰©å±•ä¸ºå­¦ä¹ å¼é£æ ¼æ£€ç´¢ä¸è‡ªé€‚åº”ä¸»é¢˜ç”Ÿæˆï¼Œå¢å¼ºä¸ªæ€§åŒ–ä¸è·¨é¢†åŸŸæ³›åŒ–ã€‚é’ˆå¯¹å†…å®¹ä¸€è‡´æ€§ï¼Œå¯è®­ç»ƒæ›´å¼ºçš„å¤šæ¨¡æ€â€œæ£€æŸ¥å‘˜â€ï¼ˆVLM-as-Judge/å¥–åŠ±æ¨¡å‹ï¼‰ï¼Œç»“åˆæ£€ç´¢ä¸å¯éªŒè¯æ¨ç†è¿›ä¸€æ­¥æŠ‘åˆ¶å¹»è§‰ã€‚æ‹“å±•äº¤äº’èƒ½åŠ›ï¼Œå¦‚åµŒå…¥å¯æ‰§è¡Œæ¼”ç¤ºã€åœ¨çº¿å¯è§†åŒ–ä¸ä»£ç å¤ç°å®éªŒæµæ°´çº¿ï¼Œå¹¶æ”¯æŒå¤šè¯­è¨€ä¸æ— éšœç¢è®¾è®¡ã€‚è¯„æµ‹å±‚é¢ï¼Œå¯ä¸°å¯Œå®¢è§‚è§†è§‰æŒ‡æ ‡ä¸äººæœºè”åˆè¯„æµ‹åè®®ï¼Œæ„å»ºè·¨é¢†åŸŸã€æ›´å¤§è§„æ¨¡çš„å¯¹é½æ ‡æ³¨ï¼›åŒæ—¶æ¢ç´¢åŸºäºRL/è‡ªåæ€çš„ç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œä½¿å‹ç¼©â€“å‡†ç¡®åº¦åœ¨ä»»åŠ¡çº§ç›®æ ‡ä¸Šæœ€ä¼˜ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.19779" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.19779" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, a goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target model's knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, a novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes a reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of a draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15\%). The code is publicly available at https://github.com/yuezhouhu/adaspec.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>è®ºæ–‡å…³æ³¨åŠ é€Ÿå¤§æ¨¡å‹æ¨ç†çš„Speculative Decodingï¼ˆSDï¼‰ä¸­ï¼Œè‰ç¨¿æ¨¡å‹ä¸ç›®æ ‡æ¨¡å‹å¯¹é½ä¸è¶³å¯¼è‡´çš„ä½æ¥å—ç‡é—®é¢˜ã€‚ç°æœ‰åšæ³•å¤šç”¨çŸ¥è¯†è’¸é¦ï¼ˆå¦‚æœ€å°åŒ–å…¨tokençš„å‰å‘KLæˆ–TVDï¼‰æ¥å¯¹é½åˆ†å¸ƒï¼Œä½†è¿™ä¸SDçœŸæ­£ç›®æ ‡ï¼ˆæœ€å¤§åŒ–æ¥å—ç‡ï¼‰ä¸ä¸€è‡´ï¼Œä¸”ä¼šæŠŠæœ‰é™å®¹é‡æµªè´¹åœ¨éš¾å­¦ã€åæ­£ä¹Ÿéš¾è¢«æ¥å—çš„tokenä¸Šï¼Œå¯¼è‡´æ”¶æ•›å›°éš¾ä¸æ¬¡ä¼˜æ€§èƒ½ã€‚ä½œè€…è§‚å¯Ÿåˆ°tokenå¯å­¦ä¹ æ€§å·®å¼‚å¾ˆå¤§ï¼Œå°æ¨¡å‹éš¾ä»¥åŒæ—¶æ‹Ÿåˆæ‰€æœ‰tokenï¼Œéœ€é¢å‘SDç›®æ ‡çš„é€‰æ‹©æ€§è®­ç»ƒç­–ç•¥ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºAdaSPECï¼šä¸€ç§é€‰æ‹©æ€§çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œåˆ†ä¸¤æ­¥è¿›è¡Œã€‚é¦–å…ˆè®­ç»ƒå‚è€ƒæ¨¡å‹ï¼ˆç”±è‰ç¨¿æ¨¡å‹åˆå§‹åŒ–ï¼‰ç”¨ç›®æ ‡æ¨¡å‹åšæ•™å¸ˆï¼Œä»¥å‰å‘KLåœ¨ä¸‹æ¸¸æ•°æ®ä¸Šè’¸é¦ï¼Œå¾—åˆ°å¯¹æ•™å¸ˆæ›´è´´è¿‘çš„â€œå¯å­¦ä¹ æ€§æ¢é’ˆâ€ã€‚éšåå¯¹æ¯ä¸ªtokenè®¡ç®—ç›¸å¯¹ç›®æ ‡æ¨¡å‹çš„tokençº§æŸå¤±ï¼šLdraftä¸Lrefï¼Œå¹¶ä»¥Î”L=Ldraftâˆ’Lrefé€‰å–Top-kï¼ˆé€šå¸¸kâ‰ˆ0.4ï¼‰ä½œä¸ºâ€œæœ€æœ‰æ•ˆå­¦ä¹ â€çš„tokenå­é›†ï¼Œè¿‡æ»¤æ‰éš¾å­¦æˆ–å·²å­¦ä¼šçš„tokenï¼Œä»…åœ¨é€‰ä¸­å­é›†ä¸Šå¯¹è‰ç¨¿æ¨¡å‹è¿›è¡ŒKLè’¸é¦ã€‚å…³é”®è´¡çŒ®åŒ…æ‹¬ï¼šåŸºäºå‚è€ƒæ¨¡å‹çš„tokenå¯å­¦ä¹ æ€§åˆ¤åˆ«ã€ä¸SDæ¥å—ç‡ç›®æ ‡å¯¹é½çš„é€‰æ‹©æ€§è’¸é¦æŸå¤±ã€ä»¥åŠç®€æ´æ˜“å¤ç”¨çš„è®­ç»ƒæµç¨‹ï¼ˆç®—æ³•ä¸æœ€å°å®ç°è§æ–‡ä¸­ç®—æ³•ä¸ä»£ç ç‰‡æ®µï¼Œé¡µ13-15ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ“Š</span>å®éªŒç»“æœ</h3>
            <div class="results">
                <p>åœ¨Pythia-31Mâ†’1.4Bä¸CodeGen-350Mâ†’Phi-2ä¸¤ç§é…ç½®ã€GSM8K/Alpaca/MBPP/CNN-DM/XSUMäº”é¡¹ä»»åŠ¡ä¸Šï¼ŒAdaSPECçš„æ¥å—ç‡åœ¨3-epochä¸â€œæœ€ä¼˜epochâ€ä¸¤ç§è®¾ç½®ä¸‹å‡ä¼˜äºDistillSpecï¼ˆè¡¨1ï¼Œé¡µ6ï¼‰ï¼Œæœ€é«˜æå‡çº¦15%ï¼ˆå¦‚MBPPä»49.88%è‡³65.12%ï¼‰ã€‚åˆ†å¸ƒå±‚é¢ï¼ŒAdaSPECæ˜¾è‘—æé«˜æ­£logit marginæ¯”ä¾‹ã€é™ä½è´Ÿmarginä¸tokençº§KLï¼ˆå›¾2ï¼Œé¡µ7ï¼‰ï¼Œè¡¨æ˜å¯¹é½æ›´ç´§å¯†ã€‚ç«¯åˆ°ç«¯åŠ é€Ÿä¸Šï¼ŒåŸºäºvLLMçš„å•å¡A100æ¨ç†åœ¨å¤šä»»åŠ¡ä¸Šè·å¾—çº¦10â€“20%é€Ÿåº¦æå‡ï¼ˆè¡¨5ï¼Œé¡µ9ï¼‰ï¼Œä¸å…ˆè¿›SDç®—æ³•EAGLEé›†æˆä¹Ÿå¸¦æ¥ç²¾åº¦ä¸é€Ÿåº¦åŒæå‡ï¼ˆè¡¨6ï¼Œé¡µ9ï¼‰ï¼›æ‰©å±•åˆ°æ›´å¤§ç»„åˆï¼ˆQwen0.5Bâ†’32Bï¼‰åŒæ ·æå‡æ¥å—ç‡ï¼ˆ84.43%â†’86.21%ï¼Œè¡¨7ï¼Œé¡µ9ï¼‰ï¼Œæ··åˆä»»åŠ¡è®­ç»ƒä¸­äº¦æ›´å°‘é—å¿˜ï¼ˆè¡¨8ï¼Œé¡µ9ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>å¯è¿›ä¸€æ­¥æ¢ç´¢è‡ªé€‚åº”/åŠ¨æ€è¿‡æ»¤ï¼ˆéšè®­ç»ƒè¿›åº¦æˆ–ä¸Šä¸‹æ–‡éš¾åº¦è°ƒæ•´kä¸é˜ˆå€¼ï¼‰ï¼Œä»¥åŠåœ¨çº¿SDåœºæ™¯çš„å³æ—¶å¯å­¦ä¹ æ€§ä¼°è®¡ã€‚ç›®æ ‡å‡½æ•°å±‚é¢ï¼Œå¯ç›´æ¥ä¼˜åŒ–æ¥å—ç‡/å—æ•ˆç‡æˆ–å°†å…¶ä½œä¸ºå¼ºåŒ–å­¦ä¹ /æœ€ä¼˜åŒ–çº¦æŸï¼Œæˆ–è¿›è¡Œspan/å¥å­çº§çš„é€‰æ‹©æ€§è’¸é¦ä»¥æ•æ‰é•¿ç¨‹ä¾èµ–ã€‚ä½“ç³»ç»“æ„ä¸Šï¼Œå¯ä¸æ ‘å¼/å¤šæ­¥éªŒè¯ï¼ˆå¦‚EAGLE-2ï¼‰æˆ–å¤šå¤´/å¤štokenè‰ç¨¿ç”Ÿæˆç»„åˆï¼Œå¹¶ä¸é‡åŒ–ã€å‰ªæç­‰é«˜æ•ˆåŒ–æ‰‹æ®µååŒã€‚è·¨ä»»åŠ¡ä¸å¤šæ•°æ®é…æ¯”çš„è¯¾ç¨‹å­¦ä¹ ã€è·¨æ¨¡å‹æ—çš„tokenæ˜ å°„ä¸æ ¡å‡†ã€ä»¥åŠæ›´é²æ£’çš„æ¸©åº¦/æ ¡å‡†ç­–ç•¥ä¹Ÿå€¼å¾—ç ”ç©¶ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20579" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20579" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Most video reasoning models only generate textual reasoning traces without indicating when and where key evidence appears. Recent models such as OpenAI-o3 have sparked wide interest in evidence-centered reasoning for images, yet extending this ability to videos is more challenging, as it requires joint temporal tracking and spatial localization across dynamic scenes. We introduce Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal evidence into video reasoning, and carefully collect training data and design training strategies to address the aforementioned challenges. The model highlights key timestamps, objects, and bounding boxes alongside its answers, allowing reasoning to be grounded in concrete visual observations. To enable this functionality, we first curate and build two high-quality datasets, STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed temporal and spatial annotations, since most existing datasets offer either temporal spans for videos or spatial boxes on images, lacking unified spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start reinforcement learning strategy with multiple specially designed rewards that jointly encourage answer accuracy, temporal alignment, and spatial precision. On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance, raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent improvements are also observed on a broad range of video understanding benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond accuracy, the reasoning traces produced by Open-o3 Video also provide valuable signals for test-time scaling, enabling confidence-aware verification and improving answer reliability.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>ç°æœ‰è§†é¢‘æ¨ç†æ¨¡å‹å¤šåªè¾“å‡ºæ–‡æœ¬å¼â€œç†ç”±â€ï¼Œç¼ºå°‘æ˜ç¡®çš„æ—¶é—´ç‚¹ä¸ç©ºé—´ä½ç½®è¯æ®ï¼Œéš¾ä»¥éªŒè¯ç­”æ¡ˆæ˜¯å¦ä¸ç”»é¢ä¸€è‡´ï¼›è€Œè§†é¢‘ç›¸æ¯”å›¾åƒè¿˜éœ€åœ¨åŠ¨æ€åœºæ™¯ä¸­åŒæ—¶è¿›è¡Œæ—¶åºè·Ÿè¸ªä¸ç©ºé—´å®šä½ã€‚è®ºæ–‡æŒ‡å‡ºä¸¤å¤§ç“¶é¢ˆï¼šä¸€æ˜¯ç¼ºä¹åŒæ—¶å«æœ‰æ—¶é—´ä¸ç©ºé—´æ ‡æ³¨ã€ä¸”å¸¦æ¨ç†é“¾çš„é«˜è´¨é‡æ•°æ®ï¼›äºŒæ˜¯è®­ç»ƒä¸Šç©ºé—´å¥–åŠ±ä¾èµ–å‡†ç¡®æ—¶é—´æˆ³ï¼Œæ—©æœŸæ—¶é—´é¢„æµ‹ä¸å‡†ä¼šå¯¼è‡´ç©ºé—´å¥–åŠ±ç¨€ç–ä¹ƒè‡³â€œç©ºé—´å´©å¡Œâ€ã€‚è¯¥é—®é¢˜çš„é‡è¦æ€§åœ¨äºæé«˜å¯è§£é‡Šæ€§ä¸å¯éªŒè¯æ€§ï¼Œå°¤å…¶åœ¨é•¿è§†é¢‘ä¸é®æŒ¡ã€è¿åŠ¨é¢‘ç¹çš„å¤æ‚åœºæ™¯ä¸­ï¼ˆè§ç¬¬1â€“2é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>ä½œè€…æå‡ºOpen-o3 Videoï¼šä¸€ä¸ªéAgentçš„ä¸€ä½“åŒ–æ¡†æ¶ï¼Œåœ¨<think>/<answer>ä¸­æ˜¾å¼ç”Ÿæˆå¸¦æ—¶é—´æˆ³ä¸è¾¹æ¡†çš„è¯æ®ï¼ˆâ€œthink with framesâ€ï¼‰ï¼Œå®ç°ç­”æ¡ˆä¸ä½•æ—¶ä½•åœ°çš„å¯éªŒè¯å¯¹é½ã€‚ä¸ºæ­¤æ„å»ºä¸¤å¥—æ•°æ®ï¼šç”¨äºSFTçš„STGR-CoT-30kä¸ç”¨äºRLçš„STGR-RL-36kï¼Œå¹¶æ–°å¢5.9ké«˜è´¨é‡æ—¶ç©ºæ ·æœ¬ï¼›é…å¥—çš„æ ‡æ³¨æµæ°´çº¿åŒ…å«Gemini 2.5 Proåˆæ ‡ã€è¾¹æ¡†è¿‡æ»¤ä¸ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆè§å›¾2ï¼Œç¬¬4é¡µï¼‰ã€‚è®­ç»ƒé‡‡ç”¨â€œä¸¤é˜¶æ®µâ€ï¼šå…ˆå†·å¯åŠ¨SFTå­¦ä¼šç»“æ„åŒ–ã€è½åœ°å¼è¾“å‡ºï¼Œå†ç”¨GSPOè¿›è¡Œåºåˆ—çº§RLï¼Œè”åˆä¸‰ç±»å¥–åŠ±ï¼ˆç­”æ¡ˆ/æ€è€ƒ/æ ¼å¼ï¼‰ï¼›å…¶ä¸­æ€è€ƒå¥–åŠ±å«â€œè‡ªé€‚åº”æ—¶é—´æ¥è¿‘åº¦â€ï¼ˆÏƒé€€ç«ä»¥ç¼“è§£ç¨€ç–å¹¶é€æ­¥æ”¶ç´§å¯¹é½ï¼‰ä¸â€œæ—¶é—´é—¨æ§â€ï¼ˆä»…åœ¨æ—¶é—´æ¥è¿‘æ—¶è®¡ç®—ç©ºé—´IoUï¼‰ï¼Œå¹¶ä»¥16å¸§å‡åŒ€é‡‡æ ·ä¸ç»å¯¹æ—¶é—´æˆ³å¢å¼ºæ—¶åºæ„Ÿï¼ˆè§å›¾3ï¼Œç¬¬5â€“7é¡µï¼›é™„å½•A.1ï¼‰ã€‚å…³é”®æŠ€æœ¯è´¡çŒ®åŒ…æ‹¬ï¼šç»Ÿä¸€çš„æ—¶ç©ºè½åœ°æ¨ç†æ ¼å¼ã€æˆä½“ç³»çš„STGRæ•°æ®ã€GSPOç¨³å®šé•¿é“¾ä¼˜åŒ–ã€ä»¥åŠè‡ªé€‚åº”æ—¶é—´æ¥è¿‘ä¸æ—¶é—´é—¨æ§ä¸¤é¡¹å¥–åŠ±è®¾è®¡ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ“Š</span>å®éªŒç»“æœ</h3>
            <div class="results">
                <p>åœ¨V-STARåŸºå‡†ä¸Šï¼ŒOpen-o3 Videoå–å¾—SOTAï¼šWhatå‡†ç¡®ç‡61.0ã€mAM 33.7ã€mLGM 46.6ï¼Œç›¸æ¯”Qwen2.5-VL-7Bæå‡+14.4%ï¼ˆmAMï¼‰ä¸+24.2%ï¼ˆmLGMï¼‰ï¼Œå¹¶è¶…è¿‡GPT-4oï¼ˆè§è¡¨1ï¼Œç¬¬8é¡µï¼‰ã€‚åœ¨VideoMMEã€WorldSenseã€VideoMMMUã€TVGBenchä¸Šäº¦æœ‰ä¸€è‡´å¢ç›Šï¼Œå¦‚é•¿è§†é¢‘+4.1%ã€WorldSenseè¯†åˆ«+3.1%ã€VideoMMMUæ„ŸçŸ¥+3.3%ã€TVGBench mIoU +4.5ï¼ˆè§è¡¨2ï¼Œç¬¬9é¡µï¼‰ã€‚æ¶ˆèæ˜¾ç¤ºï¼šRL> SFTï¼ŒSFT+RLæœ€ä½³ï¼›GSPOä¼˜äºGRPOï¼›å»é™¤è‡ªé€‚åº”æ—¶é—´æ¥è¿‘æˆ–æ—¶é—´é—¨æ§å‡æ˜¾è‘—é™åˆ†ï¼›é«˜è´¨é‡æ—¶ç©ºæ ‡æ³¨å¸¦æ¥æœ€å¤§æå‡ï¼ˆè§è¡¨3â€“5ï¼Œç¬¬9â€“10é¡µï¼‰ã€‚å¦å¤–ï¼Œåˆ©ç”¨æ˜¾å¼è¯æ®çš„â€œç½®ä¿¡æ„ŸçŸ¥æŠ•ç¥¨â€åœ¨æ¨ç†æ—¶æ¯”å¤šæ•°æŠ•ç¥¨æ›´ç¨³å¥ï¼ˆ+1.0ï¼Œè§è¡¨7ä¸å›¾6ï¼Œç¬¬16ä¸19é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>æ•°æ®å±‚é¢å¯è¿›ä¸€æ­¥æ‰©å±•æ›´é•¿æ—¶é•¿ã€æ›´å°ç›®æ ‡ä¸å¤šç›®æ ‡è·Ÿè¸ªçš„æ—¶ç©ºæ ‡æ³¨ï¼Œæ¢ç´¢åŠç›‘ç£/åˆæˆæ•°æ®å¢å¼ºä¸æ›´å¼ºä¸€è‡´æ€§æ ¡éªŒã€‚ç®—æ³•å±‚é¢å¯å¼•å…¥æ›´ä¸°å¯Œçš„å¥–åŠ±ï¼ˆå¦‚è·¨å¸§èº«ä»½ä¸è½¨è¿¹ä¸€è‡´æ€§ã€å› æœçº¦æŸï¼‰ã€æ›´ä¼˜çš„å¸§/ç‰‡æ®µé€‰æ‹©ç­–ç•¥ä¸è®°å¿†/å±‚çº§å¼æ—¶åºå»ºæ¨¡ï¼Œæˆ–ç»“åˆå¯å¾®è£å‰ª/å¤–éƒ¨å·¥å…·ä»¥ç²¾ç»†åŒ–è¯æ®è·å–ã€‚æ¨¡æ€å±‚é¢å°†æ–‡æœ¬â€“æ—¶é—´â€“ç©ºé—´ä¸éŸ³é¢‘/è¯­éŸ³å¯¹é½ï¼Œæ”¯æŒå¤šæ­¥æ¨ç†ä¸æ›´å¤æ‚äº‹ä»¶ç†è§£ï¼ˆè®ºæ–‡ä¹Ÿå°†å…¶ä½œä¸ºåç»­æ–¹å‘ï¼Œè§é™„å½•A.7ï¼‰ã€‚æ¨ç†å±‚é¢å¯æŠŠæ˜¾å¼è¯æ®ç”¨äºä¸ç¡®å®šæ€§æ ¡å‡†ã€è‡ªåŠ¨éªŒè¯ä¸äº¤äº’å¼â€œè¯æ®-ç­”æ¡ˆâ€å¾ªç¯ï¼ŒåŒæ—¶ä¼˜åŒ–æ•ˆç‡ä»¥ä¾¿é•¿è§†é¢‘ä½å»¶è¿Ÿéƒ¨ç½²ã€‚</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 1</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-10-24 13:16:40 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 1;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
</body>
</html>