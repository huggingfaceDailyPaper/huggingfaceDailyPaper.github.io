<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - October 27, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "SF Pro Display", -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Helvetica Neue", "Arial", "Noto Sans", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", sans-serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">October 27, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">DeepAgent: A General Reasoning Agent with Scalable Toolsets</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21618" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21618" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large reasoning models have demonstrated strong problem-solving abilities,<br>yet real-world tasks often require external tools and long-horizon<br>interactions. Existing agent frameworks typically follow predefined workflows,<br>which limit autonomous and global task completion. In this paper, we introduce<br>DeepAgent, an end-to-end deep reasoning agent that performs autonomous<br>thinking, tool discovery, and action execution within a single, coherent<br>reasoning process. To address the challenges of long-horizon interactions,<br>particularly the context length explosion from multiple tool calls and the<br>accumulation of interaction history, we introduce an autonomous memory folding<br>mechanism that compresses past interactions into structured episodic, working,<br>and tool memories, reducing error accumulation while preserving critical<br>information. To teach general-purpose tool use efficiently and stably, we<br>develop an end-to-end reinforcement learning strategy, namely ToolPO, that<br>leverages LLM-simulated APIs and applies tool-call advantage attribution to<br>assign fine-grained credit to the tool invocation tokens. Extensive experiments<br>on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,<br>TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,<br>HLE), demonstrate that DeepAgent consistently outperforms baselines across both<br>labeled-tool and open-set tool retrieval scenarios. This work takes a step<br>toward more general and capable agents for real-world applications. The code<br>and demo are available at https://github.com/RUC-NLPIR/DeepAgent.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç°å®ä»»åŠ¡éœ€è¦å¤–éƒ¨å·¥å…·ä¸é•¿ç¨‹äº¤äº’ï¼Œä½†ä¸»æµåŸºäºé¢„è®¾æµç¨‹çš„ä»£ç†ï¼ˆå¦‚ReActã€Plan-and-Solveï¼‰æ‰§è¡Œæ­¥éª¤ä¸æ•´ä½“è¿‡ç¨‹ç¼ºä¹è‡ªä¸»æ€§ï¼Œéš¾ä»¥åœ¨ç»Ÿä¸€æ¨ç†ä¸­å®Œæˆæ€è€ƒ-æ£€ç´¢-æ‰§è¡Œã€‚<br>â€¢ ç°æœ‰â€œæ·±åº¦ç ”ç©¶â€ç±»æ–¹æ³•å¤šä»…æ”¯æŒå°‘é‡é¢„å®šä¹‰å·¥å…·ï¼ˆæœç´¢/æµè§ˆ/ä»£ç ï¼‰ï¼Œéš¾ä»¥æ‰©å±•åˆ°ä»»æ„è§„æ¨¡ã€å¼€æ”¾é›†çš„å·¥å…·ç”Ÿæ€ï¼Œé™åˆ¶çœŸå®åº”ç”¨åœºæ™¯é€‚ç”¨æ€§ã€‚<br>â€¢ é•¿ç¨‹å¤šè½®å·¥å…·è°ƒç”¨å¯¼è‡´ä¸Šä¸‹æ–‡é•¿åº¦çˆ†ç‚¸ä¸å†å²å™ªå£°ç´¯ç§¯ï¼Œç¼ºä¹ç¨³å®šã€å¯ç”¨ä¸”é«˜ä¿çœŸçš„äº¤äº’è®°å¿†ç®¡ç†æœºåˆ¶ã€‚<br>â€¢ å·¥å…·ä½¿ç”¨çš„RLè®­ç»ƒä¾èµ–çœŸå®APIï¼Œå­˜åœ¨ä¸ç¨³å®šã€é«˜å»¶è¿Ÿä¸é«˜æˆæœ¬é—®é¢˜ï¼›ä»…ä¾èµ–æœ€ç»ˆæˆåŠŸçš„ç¨€ç–å¥–åŠ±ï¼Œæ— æ³•å¯¹ä¸­é—´å·¥å…·è°ƒç”¨è¿›è¡Œç»†ç²’åº¦ä¿¡ç”¨åˆ†é…ã€‚<br>â€¢ ç°æœ‰å·¥å…·æ£€ç´¢å¤šä¸ºå•æ¬¡é¢„æ£€ç´¢ï¼Œéš¾ä»¥åœ¨ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­åŠ¨æ€å‘ç°ã€è¯•é”™å’Œåˆ‡æ¢å·¥å…·ï¼Œå½±å“å¼€æ”¾åœºæ™¯çš„é²æ£’æ€§ä¸å¯æ‰©å±•æ€§ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºDeepAgentï¼Œå°†æ€è€ƒã€å·¥å…·æ£€ç´¢ä¸è°ƒç”¨ä»¥åŠâ€œè®°å¿†æŠ˜å â€ç»Ÿä¸€åˆ°å•ä¸€è¿ç»­æ¨ç†æµä¸­ï¼šæŒ‰éœ€å¯†é›†æ£€ç´¢å·¥å…·å¹¶ä»¥è¾…åŠ©LLMå°†å†å²æŠ˜å ä¸ºæƒ…æ™¯/å·¥ä½œ/å·¥å…·ä¸‰ç±»JSONç»“æ„è®°å¿†ä»¥æ§é•¿ä¿å‡†ã€‚å¹¶æå‡ºToolPOå¼ºåŒ–å­¦ä¹ ï¼Œç”¨LLMæ¨¡æ‹ŸAPIç¨³å®šå¤§è§„æ¨¡è®­ç»ƒï¼Œå¹¶å¯¹å·¥å…·è°ƒç”¨/æŠ˜å ä»¤ç‰Œå®æ–½ä¼˜åŠ¿å½’å› ï¼Œç²¾ç»†æå‡ä¸­é—´åŠ¨ä½œä¸ç«¯åˆ°ç«¯æˆåŠŸç‡ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ å¼€æ”¾ç¯å¢ƒä¸‹çš„æ£€ç´¢-æ¨ç†è”åˆä¼˜åŒ–ï¼šåˆ©ç”¨åœ¨çº¿å·¥å…·åé¦ˆè‡ªé€‚åº”æ›´æ–°ç´¢å¼•ä¸è¡¨ç¤ºï¼Œå®ç°å·¥å…·æ£€ç´¢ä¸ç­–ç•¥ååŒè®­ç»ƒï¼Œå¢å¼ºå¼€æ”¾é›†æ³›åŒ–ã€‚<br>â€¢ å¯å­¦ä¹ çš„è®°å¿†æŠ˜å ç½‘ç»œï¼šä»¥å¯å¾®å‹ç¼©ä¸é€‰æ‹©æ€§å›æ”¾æ›¿ä»£å¤–éƒ¨æ‘˜è¦å™¨ï¼Œè”åˆä¼˜åŒ–å‹ç¼©ç‡ä¸è¯­ä¹‰ä¿çœŸï¼Œæ”¯æ’‘æ›´é•¿ç¨‹ä¸æ›´ç¨³å®šçš„æ¨ç†ã€‚<br>â€¢ å®‰å…¨å¯æ§çš„å·¥å…·æ™ºèƒ½ä½“ï¼šå¼•å…¥è°ƒç”¨éªŒè¯ã€æƒé™æ²»ç†ä¸å¤±è´¥æ¢å¤æœºåˆ¶ï¼Œç»“åˆå¯éªŒè¯æ‰§è¡Œè½¨è¿¹ä¸å› æœå¥–åŠ±ï¼Œæå‡å·¥å…·ä½¿ç”¨çš„å®‰å…¨æ€§ä¸å¯é æ€§ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Video-As-Prompt: Unified Semantic Control for Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20888" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20888" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Unified, generalizable semantic control in video generation remains a<br>critical open challenge. Existing methods either introduce artifacts by<br>enforcing inappropriate pixel-wise priors from structure-based controls, or<br>rely on non-generalizable, condition-specific finetuning or task-specific<br>architectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes<br>this problem as in-context generation. VAP leverages a reference video as a<br>direct semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via<br>a plug-and-play Mixture-of-Transformers (MoT) expert. This architecture<br>prevents catastrophic forgetting and is guided by a temporally biased position<br>embedding that eliminates spurious mapping priors for robust context retrieval.<br>To power this approach and catalyze future research, we built VAP-Data, the<br>largest dataset for semantic-controlled video generation with over 100K paired<br>videos across 100 semantic conditions. As a single unified model, VAP sets a<br>new state-of-the-art for open-source methods, achieving a 38.7% user preference<br>rate that rivals leading condition-specific commercial models. VAP's strong<br>zero-shot generalization and support for various downstream applications mark a<br>significant advance toward general-purpose, controllable video generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç»Ÿä¸€è¯­ä¹‰æ§åˆ¶éš¾é¢˜ï¼šåœ¨æ¦‚å¿µã€é£æ ¼ã€è¿åŠ¨ã€æœºä½ç­‰éåƒç´ å¯¹é½æ¡ä»¶ä¸‹ç¼ºä¹é€šç”¨æ¡†æ¶ï¼Œç›´æ¥è¿ç§»ç»“æ„æ§åˆ¶æ–¹æ³•ä¼šå¼ºåŠ åƒç´ çº§æ˜ å°„å…ˆéªŒï¼Œäº§ç”Ÿå¤åˆ¶/æ‹¼è´´ä¼ªå½±ä¸è¯­ä¹‰é”™é…ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•ç¢ç‰‡åŒ–ä¸æ³›åŒ–å¼±ï¼šè¦ä¹ˆæŒ‰æ¡ä»¶å¾®è°ƒä¸»å¹²/LoRAï¼ˆé«˜æˆæœ¬ã€æ˜“é—å¿˜ã€å¯¹æœªè§è¯­ä¹‰é›¶æ ·æœ¬æ³›åŒ–å·®ï¼‰ï¼Œè¦ä¹ˆä¸ºç‰¹å®šä»»åŠ¡å®šåˆ¶æ¨¡å—/æ¨ç†ç­–ç•¥ï¼ˆé˜»ç¢ç»Ÿä¸€ä¸æ‰©å±•ï¼‰ã€‚<br>â€¢ æ•°æ®ä¸è¯„æµ‹ç¼ºå£ï¼šç¼ºå°‘è¦†ç›–å¹¿æ³›è¯­ä¹‰æ¡ä»¶çš„å¤§è§„æ¨¡é…å¯¹æ•°æ®ï¼Œé™åˆ¶ç»Ÿä¸€è®­ç»ƒä¸å…¬å¹³å¯¹æ¯”ï¼Œéš¾ä»¥ç³»ç»Ÿè¯„ä¼°ç”¨æˆ·åå¥½ä¸è·¨ä»»åŠ¡è¡¨ç°ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºVideo-As-Promptï¼ˆVAPï¼‰ï¼šå°†å«ç›®æ ‡è¯­ä¹‰çš„å‚è€ƒè§†é¢‘ç›´æ¥ä½œä¸ºâ€œè§†é¢‘æç¤ºâ€ï¼Œåœ¨å†»ç»“çš„Video DiTä¸Šå¹¶è”ä¸€ä¸ªå¯æ’æ‹”Mixture-of-Transformersä¸“å®¶ï¼Œé€šè¿‡å…¨æ³¨æ„åŠ›è¿›è¡Œå±‚çº§åŒæ­¥å¼•å¯¼ï¼Œå®ç°é€šç”¨çš„ä¸Šä¸‹æ–‡è¯­ä¹‰æ§åˆ¶ã€‚é…åˆæ—¶é—´åç½®RoPEï¼ˆå‚è€ƒåºåˆ—ç½®äºç›®æ ‡åºåˆ—ä¹‹å‰ã€ä¿æŒç©ºé—´ä½ç½®ä¸å˜ï¼‰ä»¥å»é™¤é”™è¯¯çš„åƒç´ æ˜ å°„å…ˆéªŒå¹¶å¼ºåŒ–ç¨³å¥æ£€ç´¢ï¼Œå¹¶æ„å»ºVAP-Dataï¼ˆ10ä¸‡+é…å¯¹ã€100ç±»è¯­ä¹‰ï¼‰æ”¯æ’‘è®­ç»ƒä¸è¯„æµ‹ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ å¤šä¸“å®¶åŠ¨æ€è·¯ç”±çš„VAPï¼šé€šè¿‡Mixture-of-Transformersçš„è‡ªé€‚åº”ä¸“å®¶é€‰æ‹©ä¸ç¨€ç–è·¯ç”±ï¼Œæå‡è·¨è¯­ä¹‰é›¶æ ·æœ¬æ³›åŒ–ä¸å¤æ‚åœºæ™¯çš„ç¨³å¥æ€§ã€‚<br>â€¢ é¢å‘é•¿æ—¶ç¨‹ä¸è·¨é•œå¤´çš„æ—¶é—´åç½®ç¼–ç ï¼šæ‰©å±•æ—¶é—´åç½®RoPEåˆ°é•¿è§†é¢‘ä¸å¤šé•œå¤´å‰ªè¾‘ï¼Œå¢å¼ºå…¨å±€ä¸€è‡´æ€§ä¸è¯­ä¹‰å»¶ç»­ã€‚<br>â€¢ å¤šæ¨¡æ€è§†é¢‘å³æç¤ºç”Ÿæˆï¼šå°†éŸ³é¢‘/æ–‡æœ¬/åŠ¨ä½œä¼ æ„Ÿå™¨ç­‰å¤šæ¨¡æ€ä¿¡å·ä¸è§†é¢‘æç¤ºè”åˆå»ºæ¨¡ï¼Œç»†åŒ–æ—¶åºä¸è¯­ä¹‰æ§åˆ¶ç²¾åº¦ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">UI-Ins: Enhancing GUI Grounding with Multi-Perspective<br> Instruction-as-Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20286" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20286" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>GUI grounding, which maps natural-language instructions to actionable UI<br>elements, is a core capability of GUI agents. Prior works largely treats<br>instructions as a static proxy for user intent, overlooking the impact of<br>instruction diversity and quality on grounding performance. Through a careful<br>investigation of existing grounding datasets, we find a 23.3% flaw rate in<br>their instructions and show that inference-time exploitation of instruction<br>diversity yields up to a substantial 76% relative performance improvement. In<br>this paper, we introduce the Instruction-as-Reasoning paradigm, treating<br>instructions as dynamic analytical pathways that offer distinct perspectives<br>and enabling the model to select the most effective pathway during reasoning.<br>To achieve this, we propose a two-stage training framework: supervised<br>fine-tuning (SFT) on synthesized, diverse instructions to instill<br>multi-perspective reasoning, followed by reinforcement learning (RL) to<br>optimize pathway selection and composition. Our resulting models, UI-Ins-7B and<br>UI-Ins-32B, achieve state-of-the-art results on five challenging grounding<br>benchmarks and exhibit emergent reasoning, selectively composing and<br>synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B<br>attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on<br>ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model<br>demonstrates strong agentic potential, achieving a 74.1% success rate on<br>AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals<br>additional insights such as how reasoning can be formulated to enhance rather<br>than hinder grounding performance, and how our method mitigates policy collapse<br>in the SFT+RL framework. All code and model checkpoints will be publicly<br>released in https://github.com/alibaba/UI-Ins.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ æŒ‡ä»¤å¤šæ ·æ€§ä¸è´¨é‡è¢«å¿½è§†ï¼Œç›´æ¥é™åˆ¶GUI groundingå‡†ç¡®ç‡ï¼šå¤šæ•°æ®é›†çº¦23.3%çš„æŒ‡ä»¤å­˜åœ¨æ­§ä¹‰/ä¸åŒ¹é…ç­‰ç¼ºé™·ï¼Œæ¸…æ´—åæ˜¾è‘—å¢ç›Šï¼›ä»…åœ¨æ¨ç†æ—¶æ¢ç”¨æ›´åˆé€‚çš„è§†è§’å¯å¸¦æ¥æœ€é«˜76%çš„ç›¸å¯¹æå‡ï¼Œè¯´æ˜â€œè§†è§’é€‰æ‹©â€å¯¹æ€§èƒ½è‡³å…³é‡è¦ï¼ˆè§ç¬¬3é¡µå›¾2(a)/(b)/(c)ï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šå°†æŒ‡ä»¤å½“ä½œé™æ€è¾“å…¥ã€å•é£æ ¼SFTæ˜ å°„ï¼Œç¼ºä¹è·¨è§†è§’æ¨ç†ä¸é€‰æ‹©èƒ½åŠ›ï¼›è‡ªç”±å¼æ¨ç†ï¼ˆFFRï¼‰åœ¨RLé˜¶æ®µå¸¸å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼›SFT+RLå› åæ ‡å¼ç›‘ç£è¾“å‡ºé«˜åº¦åŒè´¨ï¼Œæ¢ç´¢ä¸è¶³ï¼Œæ˜“å‘ç”Ÿç­–ç•¥åå¡Œï¼ˆè§ç¬¬12é¡µè¡¨8/è¡¨9ï¼‰ã€‚<br>â€¢ è¿«åˆ‡éœ€æ±‚ï¼šæ„å»ºèƒ½æŠŠâ€œæŒ‡ä»¤å½“ä½œæ¨ç†è·¯å¾„â€çš„æ¨¡å‹ï¼Œæ—¢éœ€é«˜è´¨é‡ã€å¤šè§†è§’æ•°æ®ï¼Œä¹Ÿéœ€ç¨³å®šçš„è®­ç»ƒèŒƒå¼ï¼Œä½¿æ¨¡å‹åœ¨ä¸åŒGUIåœºæ™¯ä¸­è‡ªé€‚åº”é€‰å–/åˆæˆæœ€ä¼˜è§†è§’ï¼Œå¹¶è½¬åŒ–ä¸ºç«¯åˆ°ç«¯ä»£ç†æ€§èƒ½ï¼ˆAndroidWorldæˆåŠŸç‡74.1%ï¼Œè§ç¬¬10-11é¡µè¡¨5ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºInstruction-as-Reasoningï¼šå…ˆé€šè¿‡æ•°æ®ç®¡çº¿æ¸…æ´—æ ‡æ³¨å¹¶ç”¨GPT-4.1åˆæˆå¤–è§‚/åŠŸèƒ½/ä½ç½®/æ„å›¾å››ç±»é«˜è´¨é‡å¤šè§†è§’æŒ‡ä»¤ï¼Œå†ä»¥ä¸¤é˜¶æ®µè®­ç»ƒå®ç°â€œå…ˆæƒ³åç­”â€ã€‚SFTé˜¶æ®µå­¦ä¹ ç”Ÿæˆä¸­é—´æ¨ç†å¹¶è¾“å‡ºåæ ‡ï¼ŒRLé˜¶æ®µç”¨GRPOä¸point-in-boxå¥–åŠ±åœ¨å¼€æ”¾æ¨ç†ç©ºé—´ä¸­é€‰æ‹©/ç»„åˆæœ€ä¼˜è§†è§’ï¼Œç¨³å®šæå‡Groundingè¡¨ç°ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ çŸ¥è¯†å¢å¼ºçš„GUI Groundingï¼šç»“åˆå¤–éƒ¨çŸ¥è¯†åº“/RAGä¸å¤šè§†è§’æ¨ç†ï¼Œæ¶ˆè§£å“ç‰Œ/é¢†åŸŸå®ä½“ç­‰çŸ¥è¯†ç¼ºå£å¯¼è‡´çš„å®šä½é”™è¯¯ã€‚<br>â€¢ ç»“æ„ä¸çŠ¶æ€æ„ŸçŸ¥çš„å¤šè§†è§’æ¨ç†ï¼šå¼•å…¥UIå¸ƒå±€å›¾ä¸ç»„ä»¶ç±»å‹/çŠ¶æ€å»ºæ¨¡ï¼Œæ‰©å±•è§†è§’ç©ºé—´ä»¥æå‡å¤æ‚ç•Œé¢ä¸è¿‘é‚»å¹²æ‰°ä¸‹çš„é²æ£’æ€§ã€‚<br>â€¢ è‡ªå‘ç°è§†è§’ä¸ç­–ç•¥è’¸é¦ï¼šä»GRPOæ¢ç´¢ä¸­è‡ªåŠ¨æŒ–æ˜æ–°è§†è§’å¹¶è¿›è¡Œå¤šè§†è§’ç­–ç•¥è’¸é¦ï¼Œå‡å°‘æ¨ç†å¼€é”€åŒæ—¶é¿å…SFT+RLçš„ç­–ç•¥åå¡Œã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">From Denoising to Refining: A Corrective Framework for Vision-Language<br> Diffusion Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.19871" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.19871" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Discrete diffusion models have emerged as a promising direction for<br>vision-language tasks, offering bidirectional context modeling and theoretical<br>parallelization. However, their practical application is severely hindered by a<br>train-inference discrepancy, which leads to catastrophic error cascades:<br>initial token errors during parallel decoding pollute the generation context,<br>triggering a chain reaction of compounding errors and leading to syntactic<br>errors and semantic hallucinations. To address this fundamental challenge, we<br>reframe the generation process from passive denoising to active refining. We<br>introduce ReDiff, a refining-enhanced diffusion framework that teaches the<br>model to identify and correct its own errors. Our approach features a two-stage<br>training process: first, we instill a foundational revision capability by<br>training the model to revise synthetic errors; second, we implement a novel<br>online self-correction loop where the model is explicitly trained to revise its<br>own flawed drafts by learning from an expert's corrections. This mistake-driven<br>learning endows the model with the crucial ability to revisit and refine its<br>already generated output, effectively breaking the error cascade. Extensive<br>experiments demonstrate that ReDiff significantly improves the coherence and<br>factual accuracy of generated content, enabling stable and efficient parallel<br>generation far superior to traditional denoising methods. Our codes and models<br>are available at https://rediff-hku.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç¦»æ•£æ‰©æ•£å¼VLMåœ¨å¹¶è¡Œè§£ç ä¸­å­˜åœ¨è®­ç»ƒ-æ¨ç†åˆ†å¸ƒä¸ä¸€è‡´ï¼Œåˆå§‹å°‘é‡tokené”™è¯¯ä¼šæ±¡æŸ“å…¨å±€ä¸Šä¸‹æ–‡å¹¶è§¦å‘è¯¯å·®çº§è”ï¼Œå¯¼è‡´è¯­æ³•æ··ä¹±ä¸è§†è§‰å¹»è§‰ï¼Œä¸”æ¨¡å‹æ— æ³•å›çœ‹ä¿®æ­£ï¼ˆå›¾1ä¸ç¤ºä¾‹ï¼Œç¬¬2é¡µï¼›ç¬¬3-4é¡µï¼‰<br>â€¢ ç°æœ‰mask-predæ–¹æ³•ä»…åœ¨å¹²å‡€GTä¸Šè¢«åŠ¨å»å™ªï¼Œæ¨ç†é˜¶æ®µæŠŠå·²è§£ç tokenè§†ä¸ºå›ºå®šæ¡ä»¶ï¼Œç¼ºä¹å¯¹æ—¢æœ‰è¾“å‡ºçš„ä¸»åŠ¨ä¿®è®¢èƒ½åŠ›ï¼Œæœªå……åˆ†åˆ©ç”¨åŒå‘æ³¨æ„åŠ›çš„ç»“æ„æ½œåŠ›ï¼ˆç¬¬3é¡µï¼‰<br>â€¢ å¹¶è¡ŒåŒ–æ½œåŠ›æœªè¢«å…‘ç°ï¼šå¤štoken/æ­¥æ—¶è´¨é‡æ˜¾è‘—ä¸‹é™ï¼Œå¸¸è¢«è¿«é€€åŒ–ä¸º1 token/æ­¥ï¼Œé€ æˆæ•ˆç‡ä¸ç¨³å®šæ€§çŸ›ç›¾ï¼ˆå›¾1(c)ç¬¬2é¡µï¼›è¡¨2-3ç¬¬7-8é¡µï¼‰<br>â€¢ å¤šæ¨¡æ€å¹»è§‰åœ¨æ‰©æ•£å¹¶è¡Œæƒ…æ™¯æ›´çªå‡ºï¼Œç¼ºå°‘é¢å‘â€œè¯­æ³•é”™è¯¯/å¹»è§‰â€ä¸¤ç±»é”™è¯¯çš„é’ˆå¯¹æ€§å­¦ä¹ ä¿¡å·ä¸çº é”™æœºåˆ¶ï¼ˆç¬¬3-5é¡µï¼‰</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºReDiffï¼Œå°†ç¦»æ•£æ‰©æ•£ä»è¢«åŠ¨å»å™ªè½¬ä¸ºä¸»åŠ¨ç²¾ç‚¼ï¼šé˜¶æ®µIä»¥åˆæˆè¯­æ³•/å¹»è§‰é”™è¯¯è¿›è¡Œâ€œåŸºç¡€ä¿®è®¢è®­ç»ƒâ€ï¼Œé˜¶æ®µIIæ„å»ºâ€œåœ¨çº¿è‡ªçº é”™é—­ç¯â€ï¼Œç”¨æ¨¡å‹è‰ç¨¿ä¸å¤–éƒ¨ä¸“å®¶ä¿®è®¢å¯¹å½¢æˆå®šå‘ç›‘ç£ã€‚æ¨ç†ä¸­å…è®¸å¯¹å·²ç”Ÿæˆtokenè¿­ä»£æ›¿æ¢ä¸ç²¾ç‚¼ï¼ŒåŒæ—¶å¹¶è¡Œè§£ç ä¸è‡ªæˆ‘æ ¡æ­£ä»¥æ‰“ç ´è¯¯å·®çº§è”ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ RefineFlowï¼šæ— å¤–éƒ¨ä¸“å®¶çš„è‡ªä¸¾å¼åœ¨çº¿è‡ªçº é”™ç¦»æ•£æ‰©æ•£â€”â€”ä»¥è‡ªè’¸é¦/ä¸€è‡´æ€§è®­ç»ƒæ›¿ä»£å¤§æ¨¡å‹ä¸“å®¶ï¼Œé™ä½æˆæœ¬å¹¶æŒç»­è¿­ä»£ç²¾ç‚¼èƒ½åŠ›<br>â€¢ RLAIF-Refineï¼šèåˆäºº/AIåé¦ˆçš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å¹¶è¡Œç²¾ç‚¼ç­–ç•¥â€”â€”ä»¥å¥–åŠ±é©±åŠ¨åŠ¨æ€é€‰æ‹©é‡æ©/æ›¿æ¢ä½ç½®ä¸æ­¥é•¿ï¼Œæƒè¡¡é€Ÿåº¦ä¸ç¨³å®šæ€§<br>â€¢ Grounded-ReDiffï¼šå¼•å…¥åŒºåŸŸçº§è§†è§‰å¯¹é½ç›‘ç£æŠ‘åˆ¶å¤šæ¨¡æ€å¹»è§‰â€”â€”ç»“åˆæ£€æµ‹/åˆ†å‰²/ROIæˆ–åœºæ™¯å›¾çº¦æŸï¼Œæå‡äº‹å®ä¸€è‡´æ€§ä¸å¯è§£é‡Šæ€§<br>â€¢ ç¦»æ•£æ‰©æ•£å¹¶è¡Œè§£ç è¯¯å·®çº§è”çš„ç†è®ºåˆ†æä¸è°ƒåº¦â€”â€”å»ºç«‹è¯¯å·®ä¼ æ’­ä¸Šç•Œä¸æ”¶æ•›åˆ†æï¼Œå¯¼å‡ºæœ€ä¼˜tokené€‰æ‹©ä¸æ­¥æ•°è°ƒåº¦ç­–ç•¥</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via<br> Hierarchical Model Merging</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20479" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20479" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We unveil that internal representations in large language models (LLMs) serve<br>as reliable proxies of learned knowledge, and propose RECALL, a novel<br>representation-aware model merging framework for continual learning without<br>access to historical data. RECALL computes inter-model similarity from<br>layer-wise hidden representations over clustered typical samples, and performs<br>adaptive, hierarchical parameter fusion to align knowledge across models. This<br>design enables the preservation of domain-general features in shallow layers<br>while allowing task-specific adaptation in deeper layers. Unlike prior methods<br>that require task labels or incur performance trade-offs, RECALL achieves<br>seamless multi-domain integration and strong resistance to catastrophic<br>forgetting. Extensive experiments across five NLP tasks and multiple continual<br>learning scenarios show that RECALL outperforms baselines in both knowledge<br>retention and generalization, providing a scalable and data-free solution for<br>evolving LLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æŒç»­/å¤šåŸŸå­¦ä¹ ä¸­æ˜“å‘ç”Ÿç¾éš¾æ€§é—å¿˜ï¼Œéš¾ä»¥åŒæ—¶ä¿æŒåŸæœ‰é€šç”¨èƒ½åŠ›ä¸æ–°ä»»åŠ¡ä¸“é•¿<br>â€¢ ç°æœ‰æ•°æ®é‡æ”¾æ–¹æ³•ä¾èµ–å†å²æ ·æœ¬ï¼Œå—å­˜å‚¨ä¸éšç§é™åˆ¶ï¼›æ¨¡å‹çº¦æŸ/ç»“æ„é€‚é…æ–¹æ³•ä¼˜åŒ–ç©ºé—´å—é™ã€å¸¸éœ€ä»»åŠ¡æ ‡è¯†ä¸”æ˜“å¢åŠ å¤æ‚åº¦ä¸å¹²æ‰°<br>â€¢ æ— å†å²æ•°æ®ä¸æ— æ˜¾å¼ä»»åŠ¡è¾¹ç•Œæ—¶ï¼Œéš¾ä»¥åˆ¤æ–­åº”ä¿ç•™çš„çŸ¥è¯†ä¸æ›´æ–°æ–¹å‘ï¼›ç®€å•æƒé‡å¹³å‡å¿½è§†å±‚é—´åŠŸèƒ½å·®å¼‚ä¸æ·±å±‚è¡¨ç¤ºåˆ†åŒ–ï¼Œæ˜“äº§ç”Ÿè¯­ä¹‰å†²çª</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºRECALLï¼šå…ˆç”¨KMeansä»æ–°ä»»åŠ¡æ•°æ®ä¸­é€‰å–å°‘é‡â€œå…¸å‹æ ·æœ¬â€ï¼Œåœ¨å„æ¨¡å‹æŒ‰å±‚æå–è¡¨ç¤ºå¹¶ä»¥RBFæ ¸è®¡ç®—è·¨æ¨¡å‹è¡¨ç¤ºç›¸ä¼¼åº¦ï¼Œå°†ç›¸ä¼¼åº¦softmaxæˆå±‚çº§åˆå¹¶æƒé‡ï¼Œè¿›è¡Œé€å±‚åŠ æƒå‚æ•°èåˆï¼Œä»è€Œå¯¹é½è¡¨ç¤ºã€ä¿ç•™æµ…å±‚é€šç”¨ç‰¹å¾å¹¶å…è®¸æ·±å±‚ä»»åŠ¡ç‰¹åŒ–ï¼Œæ— éœ€è®¿é—®å†å²æ•°æ®ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è·¨æ¶æ„LLMçš„è¡¨ç¤ºå¯¹é½æ¨¡å‹èåˆï¼šé¢å‘ä¸åŒæ¶æ„/è¯è¡¨/Tokenizerçš„æ¨¡å‹ï¼Œå­¦ä¹ å…±äº«å¯¹é½ç©ºé—´ä¸é€‚é…å™¨ï¼Œå®ç°å¼‚æ„æ¨¡å‹çš„å±‚çº§åˆå¹¶<br>â€¢ è¡¨ç¤ºé©±åŠ¨çš„åœ¨çº¿æŒç»­å­¦ä¹ ä¸è‡ªé€‚åº”æ ·æœ¬é€‰æ‹©ï¼šåœ¨æµå¼/ä»»åŠ¡æœªçŸ¥åœºæ™¯ä¸‹ï¼Œç»“åˆä¸»åŠ¨ä¸ä¸ç¡®å®šæ€§é‡‡æ ·åŠ¨æ€é€‰å–å…¸å‹æ ·æœ¬ï¼Œå®æ—¶æ›´æ–°å±‚çº§åˆå¹¶æƒé‡<br>â€¢ è®­ç»ƒ-åˆå¹¶ååŒçš„ç»ˆèº«å­¦ä¹ ï¼šå°†è¡¨ç¤ºä¸€è‡´æ€§æ­£åˆ™ä¸EWC/LoRAç­‰è®­ç»ƒèŒƒå¼è”åˆï¼Œå½¢æˆè®­ç»ƒæœŸä¸åˆå¹¶æœŸååŒä¼˜åŒ–ä»¥æ”¯æŒå¤šä»»åŠ¡å¤§è§„æ¨¡æ‰©å±•</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via<br> Data Alignment and Test-Time Scaling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20206" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20206" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Prompt design plays a crucial role in text-to-video (T2V) generation, yet<br>user-provided prompts are often short, unstructured, and misaligned with<br>training data, limiting the generative potential of diffusion-based T2V models.<br>We present RAPO++, a cross-stage prompt optimization framework that<br>unifies training-data--aligned refinement, test-time iterative scaling, and<br>large language model (LLM) fine-tuning to substantially improve T2V generation<br>without modifying the underlying generative backbone. In Stage 1,<br>Retrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with<br>semantically relevant modifiers retrieved from a relation graph and refactors<br>them to match training distributions, enhancing compositionality and<br>multi-object fidelity. Stage 2 introduces Sample-Specific Prompt<br>Optimization (SSPO), a closed-loop mechanism that iteratively refines prompts<br>using multi-source feedback -- including semantic alignment, spatial fidelity,<br>temporal coherence, and task-specific signals such as optical flow -- yielding<br>progressively improved video generation quality. Stage 3 leverages<br>optimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing<br>task-specific optimization patterns and enabling efficient, high-quality prompt<br>generation even before inference. Extensive experiments across five<br>state-of-the-art T2V models and five benchmarks demonstrate that RAPO++<br>achieves significant gains in semantic alignment, compositional reasoning,<br>temporal stability, and physical plausibility, outperforming existing methods<br>by large margins. Our results highlight RAPO++ as a model-agnostic,<br>cost-efficient, and scalable solution that sets a new standard for prompt<br>optimization in T2V generation. The code is available at<br>https://github.com/Vchitect/RAPO.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç”¨æˆ·æä¾›çš„T2Væ–‡æœ¬æç¤ºå¾€å¾€ç®€çŸ­ã€æ— ç»“æ„ï¼Œä¸”ä¸è®­ç»ƒæ•°æ®çš„é£æ ¼/é•¿åº¦åˆ†å¸ƒä¸åŒ¹é…ï¼Œé™åˆ¶æ‰©æ•£å¼è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ½œåŠ›<br>â€¢ ç°æœ‰T2Iæç¤ºä¼˜åŒ–æ–¹æ³•è¿ç§»åˆ°è§†é¢‘æ—¶å¯¹æ—¶é—´ä¸€è‡´æ€§ï¼ˆè¿åŠ¨å¹³æ»‘ã€é—ªçƒï¼‰ä¸ç‰©ç†åˆç†æ€§æå‡æœ‰é™<br>â€¢ T2Væç¤ºå·¥ç¨‹å¤šä¸ºæ¨¡å‹ç‰¹å®šï¼Œç¼ºå°‘å¯æ³›åŒ–ã€å¯æ‰©å±•ä¸”ä¸è®­ç»ƒåˆ†å¸ƒå¯¹é½çš„é€šç”¨ä¼˜åŒ–ç­–ç•¥<br>â€¢ ç›´æ¥ç”¨LLMé‡å†™å®¹æ˜“å¼•å…¥è¯¯å¯¼ä¿¡æ¯ï¼Œå¯¼è‡´è¯­ä¹‰åç§»ä¸æ ¼å¼å¤±é…ï¼Œå‰Šå¼±ç»„åˆæ€§å’Œå¤šç‰©ä½“å¯¹é½<br>â€¢ åŸºäºRLHFçš„æç¤ºä¼˜åŒ–éœ€å¤§é‡ç”Ÿæˆå›æ”¾ä¸å¥–åŠ±è¯„ä¼°ï¼Œè§†é¢‘ç”Ÿæˆæˆæœ¬é«˜ã€éš¾ä»¥åœ¨T2Vä¸Šå®é™…è½åœ°<br>â€¢ ç¼ºä¹åœ¨æ¨ç†æ—¶åˆ©ç”¨å¤šæºåé¦ˆé—­ç¯è¿­ä»£ä¼˜åŒ–æç¤ºçš„é€šç”¨æ¡†æ¶ï¼Œéš¾ä»¥åŠ¨æ€æå‡å•æ ·æœ¬ç”Ÿæˆè´¨é‡</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>RAPO++æå‡ºä¸‰é˜¶æ®µè·¨é˜¶æ®µæç¤ºä¼˜åŒ–ï¼šé˜¶æ®µ1ä»¥è®­ç»ƒè¯­æ–™æ„å»ºå…³ç³»å›¾è¿›è¡Œæ£€ç´¢å¢å¼ºï¼Œå¹¶ç”±LLMé‡æ„ä¸åˆ¤åˆ«é€‰æ‹©ï¼Œä½¿æç¤ºåœ¨è¯­ä¹‰ä¸ç»“æ„ä¸Šå¯¹é½è®­ç»ƒåˆ†å¸ƒï¼›é˜¶æ®µ2åœ¨æ¨ç†æ—¶è¿›è¡Œæ ·æœ¬ç‰¹å®šçš„é—­ç¯è¿­ä»£ä¼˜åŒ–ï¼ŒèåˆVLMéªŒè¯å™¨ä¸ä»»åŠ¡ä¿¡å·ï¼ˆå¦‚å…‰æµã€è®¡æ•°ï¼‰ç”Ÿæˆåé¦ˆï¼Œå¾ªç¯æ”¹å†™ä¸å¹³å‡æ’åé€‰æ‹©ï¼›é˜¶æ®µ3ç”¨é˜¶æ®µ2æ”¶é›†çš„ä¼˜è´¨æç¤ºå¯¹é‡å†™LLMè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œå°†ä¼˜åŒ–æ¨¡å¼å†…åŒ–ï¼Œé™ä½æ¨ç†æˆæœ¬å¹¶å¢å¼ºæ³›åŒ–ã€‚å…¶æ•´ä½“ä¸ºæ¨¡å‹æ— å…³ã€æ— éœ€æ”¹åŠ¨ç”Ÿæˆå™¨çš„æµ‹è¯•æ—¶æ‰©å±•æ–¹æ¡ˆã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ ç‰©ç†å…ˆéªŒæ³¨å…¥çš„SSPOï¼šç»“åˆå¯å¾®ç‰©ç†å¼•æ“/ä»¿çœŸå™¨åé¦ˆï¼Œçº¦æŸè¿åŠ¨ä¸äº¤äº’çš„ç‰©ç†ä¸€è‡´æ€§<br>â€¢ å…³ç³»å›¾è‡ªè¿›åŒ–çš„RAPOï¼šä»å¼±æ ‡æ³¨ç½‘ç»œè§†é¢‘æŒç»­æŒ–æ˜æ–°åœºæ™¯-ä¿®é¥°è¯å¯¹ï¼Œè¿›è¡Œåœ¨çº¿æ‰©å±•ä¸å»å™ªæ›´æ–°<br>â€¢ è·¨æ¨¡æ€éŸ³ç”»å…±ä¼˜åŒ–çš„æç¤ºç¼–è¯‘å™¨ï¼šè”åˆä¼˜åŒ–æ–‡æœ¬åˆ°è§†é¢‘ä¸éŸ³é¢‘ç”Ÿæˆçš„æç¤ºï¼Œæå‡è¯­ä¹‰ä¸èŠ‚å¥å¯¹é½</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">A Definition of AGI</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.18212" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.18212" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The lack of a concrete definition for Artificial General Intelligence (AGI)<br>obscures the gap between today's specialized AI and human-level cognition. This<br>paper introduces a quantifiable framework to address this, defining AGI as<br>matching the cognitive versatility and proficiency of a well-educated adult. To<br>operationalize this, we ground our methodology in Cattell-Horn-Carroll theory,<br>the most empirically validated model of human cognition. The framework dissects<br>general intelligence into ten core cognitive domains-including reasoning,<br>memory, and perception-and adapts established human psychometric batteries to<br>evaluate AI systems. Application of this framework reveals a highly "jagged"<br>cognitive profile in contemporary models. While proficient in<br>knowledge-intensive domains, current AI systems have critical deficits in<br>foundational cognitive machinery, particularly long-term memory storage. The<br>resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify<br>both rapid progress and the substantial gap remaining before AGI.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç¼ºä¹å…·ä½“ã€å¯é‡åŒ–ã€ä¸äººç±»è®¤çŸ¥å¯¹é½çš„AGIå®šä¹‰ï¼Œå¯¼è‡´â€œç§»åŠ¨é—¨æ§›â€å’Œè®¨è®ºæ··ä¹±ï¼›ç°æœ‰è¯„ä¼°å¤šä»¥å•ä¸€åŸºå‡†æˆ–ç»æµäº§å‡ºæ›¿ä»£ï¼Œéš¾ä»¥è¡¡é‡â€œå¹¿åº¦+ç†Ÿç»ƒåº¦â€çš„äººç±»æ°´å¹³ï¼ˆè§å›¾2ç¬¬6é¡µï¼‰<br>â€¢ ç°æœ‰æ¨¡å‹èƒ½åŠ›å‘ˆâ€œé”¯é½¿çŠ¶â€ï¼ŒçŸ¥è¯†/å†™ä½œ/æ•°å­¦å¼ºï¼Œä½†åœ¨é•¿æœŸè®°å¿†å­˜å‚¨ã€è§†è§‰/å¬è§‰å¤„ç†ã€é€Ÿåº¦ç­‰åŸºç¡€èƒ½åŠ›ä¸Šå­˜åœ¨æ˜æ˜¾çŸ­æ¿ï¼Œéš¾ä»¥æ”¯æ’‘é€šç”¨æ™ºèƒ½ï¼ˆè¡¨1ç¬¬3é¡µï¼›å„åˆ†é¡¹ç»“æœè§ç¬¬10â€“13é¡µï¼‰<br>â€¢ è‡ªåŠ¨åŒ–æ•°æ®é›†æ˜“è¢«è®­ç»ƒåˆ†å¸ƒæ±¡æŸ“ï¼Œä¸”éš¾ä»¥åˆ†ç¦»åº•å±‚è®¤çŸ¥èƒ½åŠ›ï¼›RAGä¸è¶…é•¿ä¸Šä¸‹æ–‡ç­‰â€œèƒ½åŠ›æ‰­æ›²â€æ©ç›–å†…åœ¨ç¼ºé™·ï¼ˆå¦‚å†…éƒ¨æ£€ç´¢ä¸å‡†ã€ä¸ä¼šæŒç»­å­¦ä¹ ï¼‰ï¼ˆè®¨è®ºæ®µç¬¬13â€“14é¡µï¼‰<br>â€¢ ç°æœ‰æ¡†æ¶ç¼ºå°‘ä¸äººç±»å¿ƒç†æµ‹è¯„ä½“ç³»çš„ä¸€ä¸€å¯¹åº”æ˜ å°„ï¼Œæ— æ³•é‡åŒ–ä¸â€œå—è¿‡è‰¯å¥½æ•™è‚²æˆäººâ€çš„å·®è·ä¸è·¯å¾„ï¼ˆå¼•è¨€ä¸æ–¹æ³•ç»¼è¿°ï¼Œç¬¬1â€“6é¡µï¼‰</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>ä»¥CHCï¼ˆCattellâ€“Hornâ€“Carrollï¼‰äººç±»è®¤çŸ¥ç†è®ºä¸ºè“æœ¬ï¼Œå°†é€šç”¨æ™ºèƒ½åˆ†è§£ä¸º10ä¸ªç­‰æƒé‡ï¼ˆå„10%ï¼‰çš„æ ¸å¿ƒè®¤çŸ¥åŸŸï¼Œå¹¶ä¸ºæ¯ä¸ªåŸŸè®¾è®¡/æ”¹ç¼–å¿ƒç†æµ‹è¯„å¼ä»»åŠ¡ï¼ˆè·¨æ–‡æœ¬ã€è§†è§‰ã€éŸ³é¢‘ï¼‰ï¼Œç»™å‡ºæ ‡å‡†åŒ–â€œAGIåˆ†æ•°â€å’Œèƒ½åŠ›å‰–é¢ã€‚è¯„ä¼°å¼ºè°ƒå¹¿åº¦ä¸é²æ£’æ€§ï¼Œç»“åˆäººå·¥åˆ¤åˆ†ä¸ä»£è¡¨æ€§åŸºå‡†ï¼Œé¿å…å•ä¸€æ•°æ®é›†è¿‡æ‹Ÿåˆã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ é¢å‘AGIçš„æŒç»­ä¸ä¸ªæ€§åŒ–é•¿æœŸè®°å¿†ï¼šä»æµ‹è¯„åˆ°æ¨¡å—åŒ–æƒé‡æ›´æ–°â€”â€”æå‡ºå¯å¤ç°å®éªŒä¸åŸºå‡†ï¼Œæ¢ç´¢LoRA/è®°å¿†æ¨¡å—å®ç°è·¨ä¼šè¯çš„å…³è”ã€è¯­ä¹‰ä¸é€å­—è®°å¿†å·©å›º<br>â€¢ ä»å¤–éƒ¨æ£€ç´¢åˆ°å†…éƒ¨ç²¾æ£€ç´¢ï¼šé™ä½å¹»è§‰çš„å‚æ•°åŒ–è®°å¿†ç²¾å‡†åº¦è®­ç»ƒä¸è¯„æµ‹â€”â€”æ„å»ºæ— å·¥å…·çš„äº‹å®æå–åŸºå‡†ä¸è®­ç»ƒæ–¹æ³•ï¼Œç³»ç»Ÿè¯„ä¼°å¹¶ä¼˜åŒ–æ¨¡å‹çš„å†…åœ¨æ£€ç´¢ç²¾åº¦ä¸ç½®ä¿¡æ ¡å‡†<br>â€¢ è·¨æ¨¡æ€å·¥ä½œè®°å¿†ä¸è§†è§‰-ç©ºé—´æ¨ç†ä¸€ä½“åŒ–è¯„æµ‹ï¼šé¢å‘é•¿è§†é¢‘ç†è§£ä¸ç©ºé—´å¯¼èˆªçš„MM-LLMèƒ½åŠ›åˆ»ç”»â€”â€”æ•´åˆé•¿è§†é¢‘é—®ç­”ã€ç©ºé—´å¯¼èˆªã€å…·èº«æ¨ç†ä»»åŠ¡ï¼Œç ”ç©¶è®°å¿†-æ³¨æ„-è§„åˆ’ååŒæœºåˆ¶</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Visual Diffusion Models are Geometric Solvers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21697" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21697" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In this paper we show that visual diffusion models can serve as effective<br>geometric solvers: they can directly reason about geometric problems by working<br>in pixel space. We first demonstrate this on the Inscribed Square Problem, a<br>long-standing problem in geometry that asks whether every Jordan curve contains<br>four points forming a square. We then extend the approach to two other<br>well-known hard geometric problems: the Steiner Tree Problem and the Simple<br>Polygon Problem.<br> Our method treats each problem instance as an image and trains a standard<br>visual diffusion model that transforms Gaussian noise into an image<br>representing a valid approximate solution that closely matches the exact one.<br>The model learns to transform noisy geometric structures into correct<br>configurations, effectively recasting geometric reasoning as image generation.<br> Unlike prior work that necessitates specialized architectures and<br>domain-specific adaptations when applying diffusion to parametric geometric<br>representations, we employ a standard visual diffusion model that operates on<br>the visual representation of the problem. This simplicity highlights a<br>surprising bridge between generative modeling and geometric problem solving.<br>Beyond the specific problems studied here, our results point toward a broader<br>paradigm: operating in image space provides a general and practical framework<br>for approximating notoriously hard problems, and opens the door to tackling a<br>far wider class of challenging geometric tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç›®æ ‡é—®é¢˜ï¼šæŠŠæ‰©æ•£æ¨¡å‹ç”¨äºå‡ ä½•æ¨ç†ä¸ç»„åˆä¼˜åŒ–ï¼Œç›´æ¥åœ¨åƒç´ ç©ºé—´æ±‚è§£ç»å…¸è€Œå›°éš¾çš„å‡ ä½•é—®é¢˜ï¼ˆå†…æ¥æ–¹å½¢ã€æ¬§æ°Steineræ ‘ã€æœ€å¤§é¢ç§¯å¤šè¾¹å½¢åŒ–ï¼‰<br>â€¢ é‡è¦æ€§ï¼šè¿™äº›é—®é¢˜æ™®éNPéš¾/å¤šè§£ä¸”çº¦æŸä¸¥æ ¼ï¼Œå½±å“é€šä¿¡/PCBå¸ƒçº¿/åŸºç¡€è®¾æ–½ç­‰åº”ç”¨ï¼›èƒ½äº§ç”Ÿå¤šæ ·é«˜è´¨é‡è¿‘ä¼¼è§£çš„é€šç”¨æ–¹æ³•å…·æœ‰å®è·µä»·å€¼<br>â€¢ ç°æœ‰é™åˆ¶ï¼šå¤šæ•°æ‰©æ•£æ±‚è§£å™¨ä¾èµ–å›¾/ç¬¦å·å‚æ•°åŒ–ä¸é—®é¢˜ç‰¹å®šæ¶æ„æˆ–æŠ€å·§ï¼Œéš¾ä»¥è·¨ä»»åŠ¡å¤ç”¨ï¼›åƒç´ åŸŸæ–¹æ³•ä¹Ÿå¸¸éœ€å¤æ‚é‡‡æ ·é¡ºåºæˆ–å¯å¾®æ¸²æŸ“+é€ä¾‹ä¼˜åŒ–ï¼Œé€šç”¨æ€§ä¸æ˜“ç”¨æ€§ä¸è¶³<br>â€¢ æ …æ ¼è§†è§’æœºé‡ï¼šå°†å‡ ä½•å®ä¾‹è§†ä½œå›¾åƒï¼Œè®­ç»ƒæ¨¡å‹ä»å™ªå£°æ¢å¤â€œè§£åˆ†å¸ƒâ€ï¼Œå¯è‡ªç„¶åˆ»ç”»å¤šè§£æ€§ï¼›åŒæ—¶æ„é€ æœ‰æ•ˆè§£æ•°æ®é€šå¸¸æ¯”å¯¹å•å®ä¾‹ç²¾ç¡®æ±‚è§£æ›´å®¹æ˜“</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>å°†é—®é¢˜å®ä¾‹æ¸²æŸ“ä¸ºæ¡ä»¶å›¾åƒï¼Œè®­ç»ƒæ ‡å‡†è§†è§‰æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆU-Net+è‡ªæ³¨æ„ï¼‰ä»é«˜æ–¯å™ªå£°ç”Ÿæˆâ€œè§£å›¾åƒâ€ï¼Œå†ä»¥è½»é‡å‡ ä½•åå¤„ç†ï¼ˆå¦‚é¡¶ç‚¹å¸é™„ã€èŠ‚ç‚¹/è¾¹æ£€æµ‹ä¸æ— äº¤å‰æ ¡éªŒï¼‰æå–å¯è¡Œè¿‘ä¼˜è§£ã€‚æ¶æ„ä¸å˜ï¼ŒæŒ‰ä»»åŠ¡ä»…æ›´æ¢è®­ç»ƒæ•°æ®ä¸åå¤„ç†è§£ç å™¨ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ å¯å¾®å‡ ä½•çº¦æŸå¼•å¯¼çš„è§†è§‰æ‰©æ•£æ±‚è§£å™¨ï¼šåœ¨é‡‡æ ·ä¸­èå…¥æ— äº¤å‰ã€120Â°å¤¹è§’ç­‰å¯å¾®ä¸€è‡´æ€§/æŠ•å½±çº¦æŸï¼Œæå‡å¯è¡Œç‡ä¸æœ€ä¼˜æ€§<br>â€¢ ä»åƒç´ åˆ°çŸ¢é‡çš„ç«¯åˆ°ç«¯çŸ¢é‡åŒ–æ‰©æ•£ï¼šåœ¨æ‰©æ•£æœ«ç«¯æ¥å…¥å¯å­¦ä¹ çš„çŸ¢é‡è§£ç å¤´ï¼Œç›´æ¥è¾“å‡ºå‚æ•°åŒ–æ›²çº¿/æ ‘/å¤šè¾¹å½¢ä»¥å‡å°‘æ …æ ¼è¯¯å·®å¹¶æ”¯æŒé«˜åˆ†è¾¨ç‡<br>â€¢ ç»Ÿä¸€å¤šä»»åŠ¡ä¸ä¸‰ç»´å‡ ä½•æ±‚è§£çš„è§†è§‰æ‰©æ•£ï¼šé€šè¿‡å¤šæ¡ä»¶æç¤ºä¸å…±äº«éª¨å¹²ï¼Œè®©å•æ¨¡å‹è¦†ç›–æ›´å¤šå¹³é¢/ç©ºé—´å‡ ä½•é—®é¢˜å¹¶æ‰©å±•åˆ°3Dæ›²é¢ä¸ç½‘æ ¼</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">WorldGrow: Generating Infinite 3D World</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21682" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21682" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We tackle the challenge of generating the infinitely extendable 3D world --<br>large, continuous environments with coherent geometry and realistic appearance.<br>Existing methods face key challenges: 2D-lifting approaches suffer from<br>geometric and appearance inconsistencies across views, 3D implicit<br>representations are hard to scale up, and current 3D foundation models are<br>mostly object-centric, limiting their applicability to scene-level generation.<br>Our key insight is leveraging strong generation priors from pre-trained 3D<br>models for structured scene block generation. To this end, we propose<br>WorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our<br>method features three core components: (1) a data curation pipeline that<br>extracts high-quality scene blocks for training, making the 3D structured<br>latent representations suitable for scene generation; (2) a 3D block inpainting<br>mechanism that enables context-aware scene extension; and (3) a coarse-to-fine<br>generation strategy that ensures both global layout plausibility and local<br>geometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset,<br>WorldGrow achieves SOTA performance in geometry reconstruction, while uniquely<br>supporting infinite scene generation with photorealistic and structurally<br>consistent outputs. These results highlight its capability for constructing<br>large-scale virtual environments and potential for building future world<br>models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šå¦‚ä½•ç”Ÿæˆå¯æ— é™æ‰©å±•çš„3Dä¸–ç•Œï¼ŒåŒæ—¶ä¿æŒå…¨å±€å‡ ä½•ç»“æ„è¿è´¯ä¸å±€éƒ¨å¤–è§‚é€¼çœŸã€‚<br>â€¢ é‡è¦æ€§ï¼šæ”¯æ’‘æ¸¸æˆã€VR/ARã€CADä¸å½±è§†åˆ¶ä½œï¼Œæ›´æ˜¯æ„å»ºWorld Modelsä¸å…·èº«æ™ºèƒ½å¼€æ”¾å¼å­¦ä¹ çš„åŸºç¡€ç¯å¢ƒã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼š2Dç”Ÿæˆâ†’3Dæå‡æ–¹æ³•ç¼ºä¹æ•´ä½“3Dç†è§£ï¼Œæ˜“å‡ºç°è·¨è§†è§’å‡ ä½•ä¸çº¹ç†ä¸ä¸€è‡´ï¼Œéš¾ä»¥æ‰©å±•ï¼›ç›´æ¥3Dç”Ÿæˆå—åœºæ™¯çº§æ•°æ®è§„æ¨¡ä¸è¡¨ç¤ºèƒ½åŠ›é™åˆ¶ï¼Œå¤šä¸ºç‰©ä½“çº§æ¨¡å‹ä¸é€‚ç”¨äºåœºæ™¯ï¼›ç°æœ‰æ— ç•Œåœºæ™¯æ–¹æ³•å¤šä»…ç”Ÿæˆå‡ ä½•ã€çº¹ç†ä¾èµ–å¤–éƒ¨è´´å›¾æˆ–è®­ç»ƒè‡ªç”±åº¦æ–¹æ¡ˆå¯¼è‡´è§†è§’ä¸€è‡´æ€§å·®ä¸è´¨é‡éšæ‰©å±•è¡°å‡ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºWorldGrowï¼šä¸€ç§åŸºäºå—çš„æ— ç•Œ3Dåœºæ™¯ç”Ÿæˆæ¡†æ¶ï¼Œä¾æ‰˜æ•°æ®åˆ‡ç‰‡ä¸åŒå°ºåº¦ï¼ˆç²—/ç»†ï¼‰å—è®­ç»ƒã€åœºæ™¯å‹å¥½SLATè¡¨å¾ï¼ˆé®æŒ¡æ„ŸçŸ¥ç‰¹å¾èšåˆ+åœºæ™¯å—é‡è®­è§£ç å™¨ï¼‰ä¸ä¸‰ç»´å—ä¿®å¤ï¼ˆç»“æ„ä¸æ½œç‰¹å¾ä¸¤é˜¶æ®µæµåŒ¹é…ï¼‰ã€‚å…¶é‡‡ç”¨å¸¦é‡å çš„é€å—æ‰©å±•ï¼Œå…ˆç”¨ç²—ç»“æ„é“ºè®¾å…¨å±€å¸ƒå±€ï¼Œå†ä»¥ç»†ç»“æ„ä¸æ½œå˜é‡ç»†åŒ–å‡ ä½•ä¸å¤–è§‚ï¼Œæœ€ç»ˆè§£ç ä¸ºå¯æ¸²æŸ“çš„è¿ç»­å¤§è§„æ¨¡3Dä¸–ç•Œã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ WorldGrow-3D-Zï¼šé¢å‘å¤šå±‚å»ºç­‘ä¸åœ°å½¢çš„å‚ç›´å¯æ‰©å±•æ— é™3Dä¸–ç•Œç”Ÿæˆâ€”â€”å¼•å…¥Zè½´å—ç”Ÿé•¿ä¸è·¨å±‚ä¸Šä¸‹æ–‡çº¦æŸã€‚<br>â€¢ Semantics-in-the-Loop WorldGrowï¼šèåˆLLMå¸ƒå±€çº¦æŸçš„å¯æ§æ— ç•Œåœºæ™¯ç”Ÿæˆâ€”â€”æŒ‰æˆ¿å‹/åŠŸèƒ½åŒºä¸é£æ ¼æŒ‡ä»¤ç”Ÿæˆå¯ç¼–è¾‘ä¸–ç•Œã€‚<br>â€¢ UniLat-Worldï¼šå‡ ä½•-å¤–è§‚ç»Ÿä¸€æ½œç©ºé—´çš„å•é˜¶æ®µæ— ç•Œåœºæ™¯ç”Ÿæˆâ€”â€”æå‡æ•ˆç‡å¹¶é™ä½å¤šé˜¶æ®µè¯¯å·®ä¼ æ’­ã€‚<br>â€¢ SeamlessGrowï¼šè·¨å—è¾¹ç•Œä¸€è‡´æ€§ä¸é•¿ç¨‹ç¨³å®šæ€§çš„å­¦ä¹ å¼å¯¹é½â€”â€”æ˜¾å¼æ¥ç¼çº¦æŸä¸å¾ªç¯ä¸€è‡´æ€§æŠ‘åˆ¶è¿œè·ç¦»æ¼‚ç§»ã€‚<br>â€¢ SceneBench++ï¼šå¤§è§„æ¨¡æˆ·å¤–/æ··åˆåŸŸæ— ç•Œåœºæ™¯æ•°æ®é›†ä¸è¯„æµ‹åŸºå‡†â€”â€”å¼•å…¥èˆªæ‹ã€è¡—æ™¯ä¸ç¨‹åºèµ„äº§ä»¥æå‡å¤šåŸŸæ³›åŒ–ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Model Merging with Functional Dual Anchors</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21223" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21223" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Model merging is an efficient post-training strategy for integrating<br>knowledge from multiple finetuned checkpoints of a shared foundation model.<br>Existing methods operate in the parameter space, combining task vectors to<br>mitigate conflicts, but remain constrained by parameter inconsistencies. We<br>propose Functional Dual Anchors (FDAs), a framework that instead models the<br>input-representation space. FDAs are synthetic inputs whose induced gradients<br>align with task vectors, capturing task-specific functional shifts relative to<br>the pretrained model. This perspective bridges joint multi-task training and<br>post-hoc merging, offering both robustness and flexibility. We further<br>introduce a principled initialization scheme and show that FDAs are<br>complementary to parameter-space model merging. Comprehensive experiments<br>demonstrate the effectiveness of FDAs in model merging.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šå¤šä»»åŠ¡æ¨¡å‹åˆå¹¶ä¸»è¦åœ¨å‚æ•°ç©ºé—´ä¸­é€šè¿‡ä»»åŠ¡å‘é‡åŠ æƒå åŠ ï¼Œæ˜“å‡ºç°ä»»åŠ¡é—´çŸ¥è¯†å†²çªä¸åˆå§‹åŒ–æ•æ„Ÿï¼Œéš¾ä»¥ç¨³å®šæ•´åˆå¤šé¢†åŸŸèƒ½åŠ›ã€‚<br>â€¢ é‡è¦æ€§ï¼šç›¸è¾ƒå¤šä»»åŠ¡è”åˆè®­ç»ƒï¼ŒåéªŒåˆå¹¶æ— éœ€é‡è®­ã€æˆæœ¬ä½ï¼Œä½†è‹¥åˆå¹¶è´¨é‡ä¸ä½³ä¼šæ˜¾è‘—åç¦»çœŸå®æŸå¤±ç›†åœ°ï¼Œæ— æ³•åœ¨å®é™…ç³»ç»Ÿä¸­å¯é å¤ç”¨ä¸éƒ¨ç½²ï¼ˆè§è®ºæ–‡å›¾2çš„æŸå¤±æ™¯è§‚å¯¹æ¯”ï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šå¤§é‡æ–¹æ³•ä»…å»ºæ¨¡å‚æ•°ç©ºé—´ï¼ˆå¹…å€¼/ç›¸ä¼¼åº¦/æ­£äº¤/å­ç©ºé—´æˆ–å°‘é‡æ•°æ®å…ˆéªŒï¼‰ï¼Œå¯¹èµ·ç‚¹æ•æ„Ÿã€å¯¹ä¸åŒä»»åŠ¡æ¼‚ç§»é²æ£’æ€§å·®ï¼Œéš¾ä»¥å¤ç°è”åˆè®­ç»ƒçš„åŠŸèƒ½æ€§è¿ç§»ï¼›ä»»åŠ¡å‘é‡æä¾›çš„æ˜¯ä»Î¸0å‡ºå‘çš„å›ºå®šçº¿æ€§è·¯å¾„ï¼Œæ˜“åç¦»æœ€ä¼˜æ”¶æ•›è½¨è¿¹ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºåŠŸèƒ½åŒé”šï¼ˆFDAsï¼‰ï¼šä¸ºæ¯ä¸ªä¸‹æ¸¸æ¨¡å‹åˆæˆä¸€ç»„è¾“å…¥ï¼Œä½¿å…¶åœ¨é¢„è®­ç»ƒæ¨¡å‹å¤„è¯±å¯¼çš„æ¢¯åº¦æ–¹å‘ä¸å¯¹åº”ä»»åŠ¡å‘é‡å¯¹é½ï¼›å…ˆé€šè¿‡æ¢¯åº¦åŒ¹é…åœ¨è¾“å…¥-è¡¨ç¤ºç©ºé—´æ„é€ FDAsï¼Œå†ç”¨å®ƒä»¬å¯¹é¢„è®­ç»ƒæˆ–å‚æ•°åˆå¹¶åçš„æ¨¡å‹è¿›è¡Œè¡¨ç¤ºå¯¹é½ä¼˜åŒ–ï¼Œä»è€Œæ¨¡æ‹Ÿå¤šä»»åŠ¡è”åˆè®­ç»ƒï¼Œå¹¶å¯ä¸TA/TSV/WUDIç­‰å‚æ•°æ³•äº’è¡¥ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ Adaptive Functional Dual Anchors for Robust Post-hoc Mergingï¼šå¼•å…¥ä¸ç¡®å®šæ€§ä¸æŸå¤±åœ°å½¢æ„ŸçŸ¥çš„è‡ªé€‚åº”FDAæ„é€ ä¸é€‰æ‹©ï¼Œæå‡è·¨ä»»åŠ¡é²æ£’æ€§<br>â€¢ Beyond Linear Theory: Convergence and Generalization of Input-space Merging in Transformersï¼šå»ºç«‹éçº¿æ€§/å±‚çº§åŒ–ç½‘ç»œä¸‹FDAæ”¶æ•›ä¸æ³›åŒ–çš„ç†è®º<br>â€¢ Federated and Continual Model Merging via FDAsï¼šå°†FDAsæ‰©å±•è‡³è”é‚¦/æŒç»­å­¦ä¹ åœºæ™¯ï¼Œå®ç°æ— åŸå§‹æ•°æ®çš„ç¨³å®šå¢é‡åˆå¹¶ä¸é—å¿˜æŠ‘åˆ¶</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Sparser Block-Sparse Attention via Token Permutation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21270" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21270" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Scaling the context length of large language models (LLMs) offers significant<br>benefits but is computationally expensive. This expense stems primarily from<br>the self-attention mechanism, whose O(N^2) complexity with respect to<br>sequence length presents a major bottleneck for both memory and latency.<br>Fortunately, the attention matrix is often sparse, particularly for long<br>sequences, suggesting an opportunity for optimization. Block-sparse attention<br>has emerged as a promising solution that partitions sequences into blocks and<br>skips computation for a subset of these blocks. However, the effectiveness of<br>this method is highly dependent on the underlying attention patterns, which can<br>lead to sub-optimal block-level sparsity. For instance, important key tokens<br>for queries within a single block may be scattered across numerous other<br>blocks, leading to computational redundancy. In this work, we propose Permuted<br>Block-Sparse Attention (PBS-Attn), a plug-and-play method that<br>leverages the permutation properties of attention to increase block-level<br>sparsity and enhance the computational efficiency of LLM prefilling. We conduct<br>comprehensive experiments on challenging real-world long-context datasets,<br>demonstrating that PBS-Attn consistently outperforms existing block-sparse<br>attention methods in model accuracy and closely matches the full attention<br>baseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn<br>achieves an end-to-end speedup of up to 2.75times in long-context<br>prefilling, confirming its practical viability. Code available at<br>https://github.com/xinghaow99/pbs-attn</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ é•¿ä¸Šä¸‹æ–‡LLMçš„è‡ªæ³¨æ„åŠ›åœ¨åºåˆ—é•¿åº¦ä¸Šå‘ˆO(N^2)å¤æ‚åº¦ï¼Œprefillé˜¶æ®µçš„è®¡ç®—ä¸æ˜¾å­˜/å»¶è¿Ÿæˆä¸ºä¸»è¦ç“¶é¢ˆã€‚<br>â€¢ ç°æœ‰å—ç¨€ç–æ³¨æ„åŠ›ä¾èµ–å›ºå®šå—æ©ç ï¼›å½“é‡è¦keyåœ¨å…¨å±€åˆ†æ•£ï¼ˆå…¸å‹â€œç«–çº¿â€æ¨¡å¼ï¼‰æ—¶ï¼Œå•ä¸ªqueryå—éœ€è®¿é—®å¤§é‡keyå—ï¼Œå¯¼è‡´å—çº§ç¨€ç–æ€§å·®ã€å†—ä½™è®¡ç®—ä¸æ³¨æ„åŠ›è¦†ç›–ä¸è¶³ã€‚<br>â€¢ FlashAttentionä¸»è¦ç¼“è§£I/Oä¸å†…å­˜å ç”¨è€Œéè®¡ç®—é‡ï¼›ç›´æ¥å¯¹å…¨åºåˆ—åšå…¨å±€ç½®æ¢è™½å¯èƒ½æé«˜ç¨€ç–æ€§ï¼Œä½†ä¼šç ´åå› æœç»“æ„ï¼Œç¼ºä¹æ—¢ä¿å› æœåˆèƒ½ä¼˜åŒ–å—çº§ç¨€ç–æ€§çš„é‡æ’æœºåˆ¶ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•æœªç³»ç»Ÿåˆ©ç”¨æ³¨æ„åŠ›å¯¹é”®å€¼ç½®æ¢ä¸å˜ã€å¯¹æŸ¥è¯¢ç½®æ¢ç­‰å˜çš„æ€§è´¨æ¥åœ¨ä¸æ”¹å˜è¾“å‡ºçš„å‰æä¸‹é‡æ’tokenä»¥æå‡å—çº§ç¨€ç–åº¦ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºPBS-Attnï¼šå…ˆå¯¹åºåˆ—åšâ€œåˆ†æ®µå†…ç½®æ¢â€ï¼ˆæ®µé—´é¡ºåºä¸å˜ä»¥ä¿æŒå› æœï¼Œæ®µå†…å¯¹Qä¸K/Vç‹¬ç«‹é‡æ’ï¼‰ï¼Œå¹¶ç”¨æœ«å—æŸ¥è¯¢ä¼°è®¡çš„æ®µå†…keyé‡è¦åº¦è¿›è¡Œæ’åºä»¥èšé›†â€œç«–çº¿â€keyï¼›åœ¨ç½®æ¢åçš„å¸ƒå±€ä¸Šæ„é€ ç¨€ç–å—æ©ç å¹¶æ‰§è¡Œå—ç¨€ç–FlashAttentionï¼Œæœ€åå¯¹è¾“å‡ºåšé€†ç½®æ¢ä»¥æ¢å¤åŸé¡ºåºã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ å­¦ä¹ å‹åˆ†æ®µç½®æ¢ç”¨äºå› æœLLMï¼šç«¯åˆ°ç«¯å­¦ä¹ æ®µåˆ’åˆ†ä¸æ®µå†…/è·¨å¤´ç½®æ¢ç­–ç•¥ï¼Œå®ç°æœ€ä¼˜ç¨€ç–-ç²¾åº¦æƒè¡¡<br>â€¢ é¢å‘æµå¼Prefillçš„åœ¨çº¿ç½®æ¢ä¸KVç¼“å­˜ä¸€è‡´æ€§ï¼šåŠ¨æ€é‡è¦åº¦ä¼°è®¡ä¸è·¨chunkæŒç»­ç½®æ¢ï¼Œä¿æŒå› æœä¸ç¼“å­˜é«˜æ•ˆååŒ<br>â€¢ é¢å‘å¤šå¤´ä¸äº¤å‰æ³¨æ„çš„ç½®æ¢æ„ŸçŸ¥ç¨€ç–èŒƒå¼ï¼šæŒ‰å¤´/è·¨æ¨¡æ€çš„ç½®æ¢ä¸æ©ç è”åˆè®¾è®¡ï¼Œæ¨å¹¿åˆ°äº¤å‰æ³¨æ„ä¸å¤šæ¨¡æ€é•¿ä¸Šä¸‹æ–‡</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research<br> Suite</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21652" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21652" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>AI agents hold the potential to revolutionize scientific productivity by<br>automating literature reviews, replicating experiments, analyzing data, and<br>even proposing new directions of inquiry; indeed, there are now many such<br>agents, ranging from general-purpose "deep research" systems to specialized<br>science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of<br>these agents is critical for progress. Yet existing benchmarks fall short on<br>several fronts: they (1) fail to provide holistic, product-informed measures of<br>real-world use cases such as science research; (2) lack reproducible agent<br>tools necessary for a controlled comparison of core agentic capabilities; (3)<br>do not account for confounding variables such as model cost and tool access;<br>(4) do not provide standardized interfaces for quick agent prototyping and<br>evaluation; and (5) lack comprehensive baseline agents necessary to identify<br>true advances. In response, we define principles and tooling for more<br>rigorously benchmarking agents. Using these, we present AstaBench, a suite that<br>provides the first holistic measure of agentic ability to perform scientific<br>research, comprising 2400+ problems spanning the entire scientific discovery<br>process and multiple scientific domains, and including many problems inspired<br>by actual user requests to deployed Asta agents. Our suite comes with the first<br>scientific research environment with production-grade search tools that enable<br>controlled, reproducible evaluation, better accounting for confounders.<br>Alongside, we provide a comprehensive suite of nine science-optimized classes<br>of Asta agents and numerous baselines. Our extensive evaluation of 57 agents<br>across 22 agent classes reveals several interesting findings, most importantly<br>that despite meaningful progress on certain individual aspects, AI remains far<br>from solving the challenge of science research assistance.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç§‘å­¦AIä»£ç†éåœ°å¼€èŠ±ä½†ç¼ºå°‘å¯æ¯”æ€§å¼ºçš„æ•´ä½“è¯„æµ‹ï¼Œéš¾ä»¥åˆ¤æ–­è°æ›´é€‚åˆçœŸå®ç§‘ç ”åœºæ™¯ï¼ˆå›¾1ä¸è¡¨1ï¼Œç¬¬3é¡µï¼‰<br>â€¢ ç°æœ‰åŸºå‡†å¤šä¸çœŸå®äº§å“éœ€æ±‚è„±èŠ‚ï¼Œç¼ºå°‘æ¥è‡ªå®é™…ç”¨æˆ·é—®é¢˜çš„ä»»åŠ¡è®¾è®¡ï¼Œå¤–æ¨ä»·å€¼å¼±ï¼ˆè¡¨1ï¼Œç¬¬3é¡µï¼‰<br>â€¢ ç¼ºä¹æ ‡å‡†ã€å¯å¤ç°å®éªŒç¯å¢ƒä¸å·¥å…·ï¼Œå°¤å…¶ç¼ºå°‘å¯æ§çš„å¤§è§„æ¨¡æ–‡çŒ®æ£€ç´¢ï¼Œæ¯”è¾ƒç»“æœæ˜“å—ä¿¡æ¯é€šé“å·®å¼‚å¹²æ‰°ï¼ˆç¬¬6é¡µï¼‰<br>â€¢ æœªç³»ç»Ÿæ§åˆ¶æ··æ‚å› ç´ ï¼ˆæ¨ç†æˆæœ¬ã€å·¥å…·æƒé™ã€å¼€æ”¾æ€§ç­‰ï¼‰ï¼Œå®¹æ˜“â€œç”¨æ›´å¤šç®—åŠ›æ¢åˆ†æ•°â€ï¼Œéš¾ä»¥å…¬å¹³å¯¹æ¯”ï¼ˆç¬¬6-7é¡µï¼‰<br>â€¢ ä»»åŠ¡æ¥å£ä¸ç»Ÿä¸€ã€ä¸ä»£ç†æ¡†æ¶è€¦åˆï¼Œé€šç”¨ä»£ç†é›†æˆæˆæœ¬é«˜ã€å¤ç°æ€§å·®ï¼ˆç¬¬2-3é¡µï¼‰<br>â€¢ ç¼ºå°‘è¦†ç›–å¹¿æ³›æ¶æ„çš„æ ‡å‡†åŒ–åŸºçº¿ä»£ç†ï¼Œéš¾ä»¥è¯†åˆ«çœŸæ­£æ–¹æ³•å­¦è¿›æ­¥ï¼ˆè¡¨1ä¸ç¬¬7é¡µï¼‰</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºAstaBenchï¼šåŒ…å«è¦†ç›–æ–‡çŒ®ç†è§£ã€ä»£ç æ‰§è¡Œã€æ•°æ®åˆ†æåˆ°ç«¯åˆ°ç«¯å‘ç°çš„11é¡¹ç§‘å­¦ç ”ç©¶åŸºå‡†ï¼Œå¹¶é…å¥—Asta Environmentï¼ˆå¯æŒ‰æ—¥æœŸè£å‰ªçš„è¯­æ–™æ£€ç´¢å·¥å…·ä¸æ²™ç›’è®¡ç®—ç¬”è®°æœ¬ï¼‰ã€agent-evalè®¡è´¹ä¸æ’è¡Œæ¦œï¼Œç»Ÿä¸€åœ¨Inspect/MCPæ ‡å‡†æ¥å£ä¸‹è¿è¡Œã€‚ä»å·¥å…·ä¸æˆæœ¬ä¸¤ç«¯æ§å› ï¼Œè¾…ä»¥å‘å¸ƒ9ç±»Astaä¼˜åŒ–ä»£ç†ä¸å¤šåŸºçº¿ï¼ˆagent-baselinesï¼‰ï¼Œå®ç°å¯æ§ã€å¯å¤ç°ã€æˆæœ¬å¯æ¯”çš„ç«¯åˆ°ç«¯è¯„æµ‹ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ æˆæœ¬â€”æ€§èƒ½å¸•ç´¯æ‰˜æœ€ä¼˜çš„ç§‘å­¦ä»£ç†ç­–ç•¥å­¦ä¹ ï¼šåœ¨å—æ§å·¥å…·ä¸ç»Ÿä¸€è®¡è´¹ä¸‹ï¼Œå­¦ä¹ è·¨ä»»åŠ¡è‡ªé€‚åº”æ¨ç†/é‡è¯•/æŠ•ç¥¨ç­–ç•¥<br>â€¢ é¢å‘é•¿æ—¶ç¨‹ç§‘ç ”é¡¹ç›®çš„å¤šé˜¶æ®µä»£ç†ç¼–æ’ä¸è®°å¿†ï¼šæå‡è®¡åˆ’åˆ†è§£ã€è·¨ä¼šè¯è®°å¿†ä¸å¤±è´¥é‡è§„åˆ’èƒ½åŠ›<br>â€¢ ç§‘å­¦è¯„æµ‹ä¸­çš„LLM-as-Judgeä¸€è‡´æ€§ä¸æ ¡å‡†ï¼šæ„å»ºå¤šè£åˆ¤é›†æˆä¸åå·®æ ¡å‡†æ–¹æ³•ï¼Œæå‡å‘ç°ç±»ä»»åŠ¡åˆ¤åˆ†å¯é æ€§<br>â€¢ æ±¡æŸ“é²æ£’çš„æœ€æ–°ç§‘å­¦çŸ¥è¯†åŸºå‡†æ„å»ºï¼šåŸºäºæ—¥æœŸè£å‰ªä¸è¯æ®å¯è¿½æº¯ï¼Œç³»ç»Ÿç”Ÿæˆè®­ç»ƒå¤–/æ—¶åºå¤–ä»»åŠ¡<br>â€¢ å¯æ§æ£€ç´¢å¯¹RAGä»£ç†å› æœå½±å“è¯„ä¼°ï¼šåœ¨æ ‡å‡†åŒ–è¯­æ–™ä¸æ¥å£ä¸‹åˆ†ç¦»â€œä¿¡æ¯å¯è¾¾æ€§â€ä¸â€œæ¨¡å‹èƒ½åŠ›â€çš„è´¡çŒ®</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">PhysWorld: From Real Videos to World Models of Deformable Objects via<br> Physics-Aware Demonstration Synthesis</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21447" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21447" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Interactive world models that simulate object dynamics are crucial for<br>robotics, VR, and AR. However, it remains a significant challenge to learn<br>physics-consistent dynamics models from limited real-world video data,<br>especially for deformable objects with spatially-varying physical properties.<br>To overcome the challenge of data scarcity, we propose PhysWorld, a novel<br>framework that utilizes a simulator to synthesize physically plausible and<br>diverse demonstrations to learn efficient world models. Specifically, we first<br>construct a physics-consistent digital twin within MPM simulator via<br>constitutive model selection and global-to-local optimization of physical<br>properties. Subsequently, we apply part-aware perturbations to the physical<br>properties and generate various motion patterns for the digital twin,<br>synthesizing extensive and diverse demonstrations. Finally, using these<br>demonstrations, we train a lightweight GNN-based world model that is embedded<br>with physical properties. The real video can be used to further refine the<br>physical properties. PhysWorld achieves accurate and fast future predictions<br>for various deformable objects, and also generalizes well to novel<br>interactions. Experiments show that PhysWorld has competitive performance while<br>enabling inference speeds 47 times faster than the recent state-of-the-art<br>method, i.e., PhysTwin.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šå¦‚ä½•ä»çŸ­æ—¶é•¿ã€ç¨€ç¼ºçš„çœŸå®äº¤äº’è§†é¢‘ä¸­å­¦ä¹ æ—¢å‡†ç¡®åˆé«˜æ•ˆçš„å¯å½¢å˜ç‰©ä½“ä¸–ç•Œæ¨¡å‹ï¼Œç”¨äºæœºå™¨äººã€VR/ARçš„å®æ—¶é¢„æµ‹ä¸è§„åˆ’ï¼ˆè§ç¬¬1é¡µ-ç¬¬2é¡µï¼‰ã€‚<br>â€¢ é‡è¦æ€§ï¼šå¯å½¢å˜ç‰©ä½“å¹¿æ³›å­˜åœ¨äºå®é™…æ“ä½œåœºæ™¯ï¼Œè¦æ±‚æ¨¡å‹æ—¢ç‰©ç†ä¸€è‡´åˆèƒ½å®æ—¶æ¨ç†ï¼›é«˜ä¿çœŸæ¨¡æ‹Ÿå™¨è™½å‡†ç¡®ä½†éš¾ä»¥æ»¡è¶³å®æ—¶æ€§ï¼Œçº¯å­¦ä¹ æ–¹æ³•è™½å¿«ä½†ä¾èµ–å¤§é‡æ•°æ®ä¸”ä¸çœŸå®ç‰©ç†å­˜åœ¨åŸŸé—´å·®è·ï¼ˆç¬¬2é¡µï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼š<br> - å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡ç²’å­/ç½‘æ ¼/ç‚¹äº‘æ•°æ®ï¼Œæ¨¡æ‹Ÿæ•°æ®å¸¸ä¸ç°å®ä¸ä¸€è‡´ï¼Œä¸”éš¾é€‚é…ç©ºé—´éå‡åŒ€ç‰©æ€§ï¼ˆç¬¬2é¡µï¼‰ã€‚<br> - ç‰©ç†æ¨¡æ‹Ÿæ–¹æ³•ï¼ˆå¦‚Mass-Spring/MPMï¼‰è™½é€¼çœŸä½†æ¨ç†æ…¢ï¼Œéš¾ç”¨äºå®æ—¶ï¼ˆç¬¬2-3é¡µï¼‰ã€‚<br> - ä»£è¡¨æ€§å·¥ä½œAdaptiGraphä½¿ç”¨å…¨å±€ç‰©æ€§ã€æ•°æ®åˆæˆæ¬ ç‰©ç†ä¸€è‡´ï¼ŒPhysTwinåŸºäºå¼¹ç°§-è´¨ç‚¹ä¸”æ¨ç†æ…¢ï¼›éš¾å¤„ç†ç©ºé—´å¼‚è´¨ç‰©æ€§ä¸å¤šæ ·äº¤äº’ï¼ˆç¬¬2é¡µã€ç¬¬7é¡µè¡¨1æ˜¾ç¤ºå…¶FPS=17ï¼Œæœ¬æ–‡GNNè¾¾799ï¼Œå¿«çº¦47Ã—ï¼‰ã€‚<br>â€¢ æ•°æ®ç“¶é¢ˆï¼šå•æ¡çœŸå®è§†é¢‘æä¾›çš„è½¨è¿¹å•ä¸€ï¼Œç›´æ¥è®­ç»ƒGNNæ˜“è¿‡æ‹Ÿåˆä¸åˆ†å¸ƒå¤–é€€åŒ–ï¼ˆç¬¬6é¡µã€è¡¨6ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>å…ˆç”¨MPMæ„å»ºâ€œæ•°å­—å­ªç”Ÿâ€ï¼šå€ŸåŠ©VLMè‡ªåŠ¨é€‰æ‹©æœ¬æ„æ¨¡å‹ï¼Œå¹¶ä»¥â€œå…¨å±€åˆ°å±€éƒ¨â€çš„å¯å¾®ç‰©æ€§ä¼˜åŒ–å¯¹é½çœŸå®è§†é¢‘ï¼›å†é€šè¿‡æ›²ç‡å—é™Bezierè½¨è¿¹ä¸ä¸‰æ®µå¼é€Ÿåº¦ï¼ˆVMP-Genï¼‰å’ŒåŸºäºéƒ¨ä»¶è¯­ä¹‰çš„åæ–¹å·®æ‰°åŠ¨ï¼ˆP3-Pertï¼‰åˆæˆå¤šæ ·ç‰©ç†ä¸€è‡´æ¼”ç¤ºï¼Œç”¨äºè®­ç»ƒåµŒå…¥ç©ºé—´å¼‚è´¨ç‰©æ€§çš„è½»é‡GNNï¼Œå¹¶ç”¨çœŸå®è§†é¢‘å¯¹GNNä¸­çš„ç‰©æ€§å†å¾®è°ƒï¼Œæœ€ç»ˆå®ç°å¿«é€Ÿã€å‡†ç¡®ã€å¯æ³›åŒ–çš„åŠ¨ä½œæ¡ä»¶é¢„æµ‹ï¼ˆç¬¬3-6é¡µï¼Œå›¾1ï¼Œç¬¬7é¡µè¡¨1ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ æ¦‚ç‡åŒ–PhysWorldï¼šä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„ç‰©æ€§ä¼°è®¡ä¸GNNä¸–ç•Œæ¨¡å‹â€”â€”ä»¥è´å¶æ–¯ç‰©æ€§ä¸åˆ†å¸ƒå¼æ‰°åŠ¨æå‡é•¿æ—¶é¢„æµ‹é²æ£’æ€§ä¸æ³›åŒ–<br>â€¢ åœ¨çº¿è‡ªé€‚åº”çš„æ¨¡æ‹Ÿå™¨â†’GNNè’¸é¦ï¼šé—­ç¯ä¸»åŠ¨äº¤äº’ä¸ä¿¡æ¯å¢ç›Šé©±åŠ¨çš„æ¼”ç¤ºåˆæˆï¼ŒæŒç»­ç¼©å°ä»¿çœŸ-ç°å®åŸŸé—´å·®è·ç”¨äºå®æ—¶æœºå™¨äººè§„åˆ’<br>â€¢ é¢å‘å¤æ‚æ‹“æ‰‘ä¸å¤šä½“æ¥è§¦çš„PhysWorldï¼šæ”¯æŒæ–­è£‚/ç²˜å¼¹/æµå›ºè€¦åˆä¸å¤šç‰©ä½“/å·¥å…·äº¤äº’çš„æœ¬æ„é€‰æ‹©ä¸å±€éƒ¨ç‰©æ€§è¯†åˆ«ï¼Œæå‡é€šç”¨æ€§</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ARC-Encoder: learning compressed text representations for large language<br> models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20535" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20535" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent techniques such as retrieval-augmented generation or chain-of-thought<br>reasoning have led to longer contexts and increased inference costs. Context<br>compression techniques can reduce these costs, but the most effective<br>approaches require fine-tuning the target model or even modifying its<br>architecture. This can degrade its general abilities when not used for this<br>specific purpose. Here we explore an alternative approach: an encoder that<br>compresses the context into continuous representations which replace token<br>embeddings in decoder LLMs. First, we perform a systematic study of training<br>strategies and architecture choices for the encoder. Our findings led to the<br>design of an Adaptable text Representations Compressor, named ARC-Encoder,<br>which outputs x-times fewer continuous representations (typically<br>x!in!{4,8}) than text tokens. We evaluate ARC-Encoder across a variety<br>of LLM usage scenarios, ranging from in-context learning to context window<br>extension, on both instruct and base decoders. Results show that ARC-Encoder<br>achieves state-of-the-art performance on several benchmarks while improving<br>computational efficiency at inference. Finally, we demonstrate that our models<br>can be adapted to multiple decoders simultaneously, allowing a single encoder<br>to generalize across different decoder LLMs. This makes ARC-Encoder a flexible<br>and efficient solution for portable encoders that work seamlessly with multiple<br>LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder<br>, fine-tuning dataset and pretrained models are available at<br>https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šLLMåœ¨RAG/CoTç­‰åœºæ™¯ä¸‹éœ€è¦å¤„ç†æ›´é•¿ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´æ³¨æ„åŠ›è®¡ç®—å‘ˆå¹³æ–¹å¤æ‚åº¦ã€æ¨ç†æˆæœ¬æ¿€å¢ï¼Œä¸”ä¿¡æ¯è¢«ç¨€é‡Šæˆ–è¶…è¿‡ä¸Šä¸‹æ–‡çª—å£é™åˆ¶è€ŒæŸä¼¤èƒ½åŠ›ï¼ˆç¬¬1â€“2é¡µï¼‰<br>â€¢ ç°æœ‰å±€é™ï¼šç¡¬å‹ç¼©ï¼ˆåˆ è¯/æ‘˜è¦ï¼‰è™½æ¨¡å‹æ— å…³ä½†å‹ç¼©ç‡æœ‰é™ã€è¯­ä¹‰æµå¤±ï¼›è½¯å‹ç¼©ï¼ˆgist/memory tokensï¼‰è™½å‹ç¼©ç‡é«˜ï¼Œä½†é€šå¸¸éœ€æ”¹é€ æˆ–å¾®è°ƒè§£ç å™¨ï¼Œç ´åé€šç”¨æ€§ä¸å¯ç§»æ¤æ€§ï¼ˆç¬¬1â€“2é¡µï¼‰<br>â€¢ å›ºå®šè®°å¿†tokenæ•°é‡éš¾ä»¥é€‚é…ä¸åŒé•¿åº¦è¾“å…¥ï¼Œéš¾ä»¥åœ¨å„ç§åºåˆ—è§„æ¨¡ä¸Šä¿æŒç¨³å®šå‹ç¼©æ¯”ï¼›éœ€è¦ä¸€ç§æŒ‰å›ºå®šå‹ç¼©å› å­å¯¹ä»»æ„é•¿åº¦åºåˆ—ç¨³å®šå‹ç¼©çš„æ–¹æ³•ï¼ˆç¬¬3é¡µï¼‰<br>â€¢ ç›®æ ‡éœ€æ±‚ï¼šåœ¨ä¸ä¿®æ”¹è§£ç å™¨çš„å‰æä¸‹ï¼Œä»¥è¿ç»­è¡¨ç¤ºæ›¿ä»£åŸå§‹æ–‡æœ¬tokenï¼Œä¿æŒå°‘æ ·æœ¬ICLèƒ½åŠ›ä¸ä»»åŠ¡æ³›åŒ–ï¼ŒåŒæ—¶å¯çµæ´»è®¾å®š4Ã—/8Ã—ç­‰å‹ç¼©æ¯”ï¼Œå¹¶æ”¯æŒå¤šè§£ç å™¨å¤ç”¨ï¼ˆæ‘˜è¦ã€ç¬¬1é¡µï¼‰<br>â€¢ æ•ˆç‡ä¸å¯è½åœ°æ€§ï¼šéœ€å…¼é¡¾æ¨ç†åŠ é€Ÿä¸å¯é¢„è®¡ç®—å­˜å‚¨ã€‚è®ºæ–‡æ˜¾ç¤º4Ã—å‹ç¼©åœ¨prefillé˜¶æ®µçº¦1.8Ã—åŠ é€Ÿä¸”æ€§èƒ½æ¥è¿‘åŸæ–‡ä¸Šä¸‹æ–‡ï¼ˆè¡¨1ï¼Œç¬¬6é¡µï¼›é™„å½•B.1ï¼‰ï¼Œå¹¶èƒ½é€šè¿‡é‡åŒ–/PQå°†å…¨ç»´åŸºå‹ç¼©è¡¨ç¤ºå­˜å‚¨åˆ°â‰ˆ80GB/20GBé‡çº§ï¼ˆå›¾4ï¼Œç¬¬9é¡µï¼‰</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºARC-Encoderï¼šä½¿ç”¨å»å› æœçš„LLMç¼–ç å™¨åœ¨è‡ªæ³¨æ„åŠ›æœ€åä¸€å±‚ä»…å¯¹æŸ¥è¯¢Qåšåˆ†ç»„å¹³å‡æ± åŒ–ï¼ˆå›ºå®šæ± åŒ–å› å­ï¼‰ï¼Œä¿ç•™K/Vä¸å˜ï¼Œäº§å‡ºæ›´å°‘çš„è¿ç»­è¡¨ç¤ºï¼Œå¹¶ç»ä¸¤å±‚çº¿æ€§MLPæŠ•å½±åˆ°ç›®æ ‡è§£ç å™¨éšè—ç»´ï¼Œç›´æ¥æ›¿æ¢å…¶è¾“å…¥åµŒå…¥ï¼Œè§£ç å™¨å®Œå…¨å†»ç»“ï¼ˆè§å›¾1ï¼Œç¬¬3é¡µï¼‰ã€‚è®­ç»ƒé‡‡ç”¨â€œé‡æ„+ç»­å†™â€äº¤æ›¿é¢„è®­ç»ƒä¸ä»»åŠ¡å¾®è°ƒï¼Œé…åˆè§£ç å™¨ç‰¹å®šçš„å°å‹æŠ•å½±å¤´å®ç°å¤šè§£ç å™¨é€‚é…ï¼Œå¹¶é€šè¿‡å¹¶è¡Œåˆ†å—å‹ç¼©å®ç°é•¿ä¸Šä¸‹æ–‡æ‰©å±•ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è·¨å®¶æ—é€šç”¨æ½œç©ºé—´çš„ARC-Encoderï¼šåŸºäºå¯¹æ¯”/å¯¹é½å­¦ä¹ è®­ç»ƒè§£ç å™¨æ— å…³çš„å…±äº«è¡¨ç¤ºï¼Œå†ä»¥æå°å¤´é€‚é…å¤šç§è§£ç å™¨ï¼Œæå‡å¯ç§»æ¤æ€§ä¸é›¶æ ·æœ¬è¿ç§»<br>â€¢ å†…å®¹ä¸ä»»åŠ¡è‡ªé€‚åº”çš„æ³¨æ„åŠ›æ± åŒ–å‹ç¼©ï¼šå­¦ä¹ å¼é‡è¦æ€§å¼•å¯¼çš„åŠ¨æ€æ± åŒ–å› å­ä¸å±‚ä½ç‚¹é€‰æ‹©ï¼Œè”åˆç¨€ç–æ³¨æ„ä¸å¯è§£é‡Šä¿¡å·å®ç°æŒ‰éœ€å‹ç¼©<br>â€¢ å¹¶è¡Œåˆ†å—é•¿æ–‡å‹ç¼©ä¸­çš„è·¨å—ä¸€è‡´æ€§ä¸æ£€ç´¢èåˆï¼šèåˆè·¨å—æŒ‡ä»£/ä¸»é¢˜ä¸€è‡´æ€§å»ºæ¨¡ä¸æ£€ç´¢ä¿¡å·ï¼Œæå‡32k+é•¿æ–‡é—®ç­”ä¸æ‘˜è¦çš„å…¨å±€æ¨ç†èƒ½åŠ›</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Document Understanding, Measurement, and Manipulation Using Category<br> Theory</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.21553" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.21553" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We apply category theory to extract multimodal document structure which leads<br>us to develop information theoretic measures, content summarization and<br>extension, and self-supervised improvement of large pretrained models. We first<br>develop a mathematical representation of a document as a category of<br>question-answer pairs. Second, we develop an orthogonalization procedure to<br>divide the information contained in one or more documents into non-overlapping<br>pieces. The structures extracted in the first and second steps lead us to<br>develop methods to measure and enumerate the information contained in a<br>document. We also build on those steps to develop new summarization techniques,<br>as well as to develop a solution to a new problem viz. exegesis resulting in an<br>extension of the original document. Our question-answer pair methodology<br>enables a novel rate distortion analysis of summarization techniques. We<br>implement our techniques using large pretrained models, and we propose a<br>multimodal extension of our overall mathematical framework. Finally, we develop<br>a novel self-supervised method using RLVR to improve large pretrained models<br>using consistency constraints such as composability and closure under certain<br>operations that stem naturally from our category theoretic framework.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ å…³é”®é—®é¢˜ï¼šç¼ºä¹å¯¹æ–‡æ¡£â€œè¯­ä¹‰å±‚â€çš„å¯æ“ä½œæ•°å­¦è¡¨ç¤ºï¼Œç°æœ‰å·¥ä½œå¤šåœç•™åœ¨é¦™å†œå¼ç¬¦å·ç»Ÿè®¡æˆ–ç»éªŒå‹NLPç®¡çº¿ï¼Œéš¾ä»¥åœ¨ç»Ÿä¸€æ¡†æ¶ä¸­åº¦é‡ã€æ¯”è¾ƒä¸æ“æ§è¯­ä¹‰ä¿¡æ¯ï¼ˆå›¾2çš„æ­£äº¤QAæœ‰å‘æ— ç¯å›¾ç¤ºä¾‹è§ç¬¬8é¡µï¼›å¤šæ¨¡æ€æ€»ä½“æ¡†æ¶è§ç¬¬18é¡µå›¾5ï¼‰ã€‚<br>â€¢ é‡è¦æ€§ï¼šéœ€è¦å¯¹æ–‡æ¡£è¿›è¡Œç³»ç»Ÿçš„æµ‹é‡ï¼ˆä¿¡æ¯é‡ã€ç†µã€äº’ä¿¡æ¯ã€ä¿¡æ¯å¢ç›Šç­‰ï¼‰ä¸å˜æ¢ï¼ˆæ‘˜è¦ä¸â€œé‡Šç»å¼â€æ‰©å±•ï¼‰ï¼Œä»¥æ”¯æ’‘æ£€ç´¢ã€å¯¹é½ã€ç”Ÿæˆå’ŒçŸ¥è¯†èåˆç­‰å…³é”®åº”ç”¨ï¼Œä½†ç›®å‰ç¼ºä¹é—­åˆäºæ“ä½œçš„ç»“æ„ä¸åº¦é‡ï¼ˆç¬¬9-11é¡µå¼•å…¥â€œæ‘˜è¦/é‡Šç»â€å¯¹å¶ä¸æ ¼ç»“æ„ï¼‰ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™ï¼šæ‘˜è¦å¤šä¸ºå¯å‘å¼æˆ–é»‘ç®±ï¼Œä¸å…·å¯ç»„åˆæ€§ä¸å°é—­æ€§ï¼Œéš¾ä»¥è¿›è¡Œç‡å¤±çœŸåˆ†æï¼›è·¨æ–‡æ¡£ç›¸ä¼¼åº¦å¤šä¾èµ–å‘é‡ç›¸ä¼¼ï¼Œç¼ºå°‘åŸºäºå¯å›ç­”é—®é¢˜é›†åˆçš„ç²¾ç»†å¯¹é½ï¼›ä¿®è¾ç»“æ„è§£æå—é™äºæŠ½å–å¼æˆ–ä¼ ç»ŸRSTå·¥å…·ï¼Œéš¾ä»¥ä¸ä¿¡æ¯åº¦é‡è”åŠ¨ï¼›æ¨¡å‹æ”¹è¿›å¸¸ä¾èµ–äººç±»åé¦ˆï¼Œç¼ºä¹å¯éªŒè¯ã€å¯è‡ªåŠ¨åŒ–çš„è‡ªç›‘ç£çº¦æŸï¼ˆç¬¬13é¡µæå‡ºç”¨èŒƒç•´çº¦æŸåšRLVRï¼‰ã€‚<br>â€¢ å¤šæ¨¡æ€æŒ‘æˆ˜ï¼šå¦‚ä½•å°†é«˜å±‚è¯­ä¹‰ä¸ä½å±‚è¯æ®ï¼ˆå›¾åƒ/éŸ³é¢‘/è§†é¢‘/ä¼ æ„Ÿï¼‰åœ¨ç»Ÿä¸€ç»“æ„ä¸­å¯¹é½å¹¶åº¦é‡ä¸€è‡´æ€§ï¼Œç°æœ‰æ¡†æ¶æ™®éç¼ºå°‘å¯è¯æ˜çš„èåˆæœºåˆ¶ï¼ˆç¬¬14-15é¡µã€å¤šæ¨¡æ€æ‰©å±•ä¸æ¦‚ç‡èŒƒç•´ï¼‰ã€‚<br>â€¢ ç†è®ºç©ºç¼ºï¼šç¼ºä¹å¯¹â€œè¯­ä¹‰å‹ç¼©â€çš„ç‡å¤±çœŸæ›²çº¿ä¸ä¸‹ç•Œåˆ†æï¼Œä»¥åŠå¯¹ä¿¡æ¯ç†µ/å¯†åº¦åœ¨è¯­ä¹‰ç©ºé—´çš„å¯åŠ æ€§ã€å†—ä½™æ€§ä¸å¯ä¼°è®¡æ€§ç ”ç©¶ï¼ˆç¬¬14é¡µå›¾3ä¸ºæ“ä½œæ€§ç‡å¤±çœŸæ›²çº¿ç¤ºä¾‹ï¼›ç¬¬12-13é¡µæå‡ºå†…å®¹ç†µä¸é“¾ç»“æ„ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>å°†æ–‡æ¡£è¡¨ç¤ºä¸ºç”±é—®é¢˜-ç­”æ¡ˆå¯¹ç­‰ä»·ç±»æ„æˆçš„èŒƒç•´ï¼Œå®šä¹‰åŸºäºå¯å›ç­”é—®é¢˜é›†åˆçš„Jaccardå¼è·ç¦»ï¼Œå¹¶é€šè¿‡LLMé©±åŠ¨çš„åˆ†è§£ä¸æ­£äº¤åŒ–è·å¾—åŸå­QAé›†åˆåŠå…¶ä¸ä¿®è¾ç»“æ„çš„æ˜ å°„ï¼Œè¿›è€Œæ„é€ æ ¼æ¥å®ç°æ‘˜è¦ï¼ˆå‹ç¼©ï¼‰ä¸é‡Šç»ï¼ˆæ‰©å±•ï¼‰ï¼Œå¹¶åœ¨è¯¥ç»“æ„ä¸Šå®šä¹‰ä¿¡æ¯åº¦é‡ä¸ç‡å¤±çœŸåˆ†æï¼›åŒæ—¶æŠŠèŒƒç•´å¯ç»„åˆæ€§ä¸é—­åŒ…ç­‰æ€§è´¨è½¬åŒ–ä¸ºRLVRå¯éªŒè¯å¥–åŠ±ï¼Œç”¨äºè‡ªç›‘ç£æå‡å¤§æ¨¡å‹ä¸€è‡´æ€§ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ æ‘˜è¦çš„ç‡å¤±çœŸç†è®ºï¼šæ–‡æ¡£è¯­ä¹‰å‹ç¼©çš„ä¸‹ç•Œã€ä¸Šç•Œä¸å¯å®ç°ç®—æ³•â€”â€”å»ºç«‹R(D)ç†è®ºã€æ„é€ æœ€ä¼˜/æ¬¡ä¼˜æ‘˜è¦ç­–ç•¥å¹¶ç»™å‡ºå®éªŒæ›²çº¿ï¼ˆå‚ç…§ç¬¬14é¡µå›¾3ï¼‰ã€‚<br>â€¢ åŸºäºèŒƒç•´çº¦æŸçš„RLVRè‡ªç›‘ç£å­¦ä¹ ï¼šä»å¯ç»„åˆæ€§ä¸é—­åŒ…åˆ°ä¸€è‡´æ€§æå‡â€”â€”ç³»ç»Ÿç”Ÿæˆå¯éªŒè¯çº¦æŸï¼Œæ£€éªŒå¯¹LLMæ¨ç†ç¨³å®šæ€§ä¸å¯¹é½æ€§èƒ½çš„æ”¹è¿›ï¼ˆç¬¬13é¡µï¼‰ã€‚<br>â€¢ å¤šæ¨¡æ€èŒƒç•´ä¸Sheafä¸€è‡´æ€§çš„é‡Šç»æ‰©å±•ï¼šè·¨æ–‡æ¡£èåˆä¸çŸ›ç›¾æ£€æµ‹â€”â€”ä»¥Sheafä¸€è‡´æ€§ä¿è¯æ‰©å±•çš„å…¨å±€åè°ƒï¼Œåœ¨å›¾æ–‡éŸ³è§†é¢‘è¯æ®ä¸‹å®ç°å¯è¯æ˜çš„ä¸€è‡´æ€§ä¸å†²çªå®šä½ï¼ˆç¬¬14-16é¡µã€18é¡µå›¾5ï¼‰ã€‚</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">Taming Modality Entanglement in Continual Audio-Visual Segmentation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.17234" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.17234" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recently, significant progress has been made in multi-modal continual<br>learning, aiming to learn new tasks sequentially in multi-modal settings while<br>preserving performance on previously learned ones. However, existing methods<br>mainly focus on coarse-grained tasks, with limitations in addressing modality<br>entanglement in fine-grained continual learning settings. To bridge this gap,<br>we introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to<br>continuously segment new classes guided by audio. Through comprehensive<br>analysis, two critical challenges are identified: 1) multi-modal semantic<br>drift, where a sounding objects is labeled as background in sequential tasks;<br>2) co-occurrence confusion, where frequent co-occurring classes tend to be<br>confused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework<br>is designed to address these challenges. Specifically, for multi-modal semantic<br>drift, a Multi-modal Sample Selection (MSS) strategy is proposed to select<br>samples with high modal consistency for rehearsal. Meanwhile, for co-occurence<br>confusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed,<br>allowing for the increase of rehearsal sample frequency of those confusable<br>classes during training process. Moreover, we construct three audio-visual<br>incremental scenarios to verify effectiveness of our method. Comprehensive<br>experiments demonstrate that our method significantly outperforms single-modal<br>continual learning methods.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>â€¢ ç ”ç©¶é—®é¢˜ï¼šé¦–æ¬¡å°†éŸ³é¢‘-è§†è§‰åˆ†å‰²ï¼ˆAVSï¼‰æ‹“å±•åˆ°æŒç»­å­¦ä¹ åœºæ™¯ï¼Œæå‡ºæŒç»­éŸ³é¢‘-è§†è§‰åˆ†å‰²ï¼ˆCAVSï¼‰ï¼Œè¦æ±‚åœ¨é¡ºåºä»»åŠ¡ä¸­å®ç°åƒç´ çº§åˆ†å‰²å¹¶ä¿æŒå·²å­¦ç±»ä¸é—å¿˜ã€‚<br>â€¢ é‡è¦æ€§ï¼šçœŸå®åº”ç”¨ï¼ˆå¦‚å…·èº«æ™ºèƒ½ã€ç¯å¢ƒæ„ŸçŸ¥ï¼‰éœ€è¦ä»¥å…¨å±€éŸ³é¢‘çº¿ç´¢ç²¾ç¡®å¼•å¯¼å±€éƒ¨è§†è§‰åƒç´ ï¼Œä¸”åœ¨æ•°æ®æµä¸­ä¸æ–­å¼•å…¥æ–°ç±»è€Œä¸ä¸¢å¤±æ—§ç±»çŸ¥è¯†ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™1ï¼šå•æ¨¡æ€æŒç»­è¯­ä¹‰åˆ†å‰²æ–¹æ³•ç›´æ¥è¿ç§»åˆ°å¤šæ¨¡æ€æ—¶éš¾ä»¥ç»´æŠ¤è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§ï¼Œæ˜“å‘ç”Ÿç¾éš¾æ€§é—å¿˜ã€‚<br>â€¢ ç°æœ‰æ–¹æ³•å±€é™2ï¼šå·²æœ‰å¤šæ¨¡æ€æŒç»­å­¦ä¹ å¤šèšç„¦äºåˆ†ç±»/åˆ†ç¦»ç­‰ç²—ç²’åº¦ä»»åŠ¡ï¼Œç¼ºä¹åƒç´ çº§ç»†ç²’åº¦å»ºæ¨¡ï¼›è€Œé™æ€AVSæ–¹æ³•ä¸é€‚ç”¨äºæŒç»­å­¦ä¹ è®¾ç½®ã€‚<br>â€¢ æ ¸å¿ƒæŒ‘æˆ˜1ï¼šå¤šæ¨¡æ€è¯­ä¹‰æ¼‚ç§»â€”â€”æ—§ç±»åœ¨åç»­ä»»åŠ¡è¢«æ ‡ä¸ºèƒŒæ™¯ï¼Œå¯¼è‡´éŸ³/è§†é”™è¯¯å¯¹é½ï¼ˆå¦‚â€œé¼“-èƒŒæ™¯â€ï¼‰ï¼ŒåŠ å‰§è·¨æ¨¡æ€çŸ¥è¯†é—å¿˜ã€‚<br>â€¢ æ ¸å¿ƒæŒ‘æˆ˜2ï¼šå…±ç°æ··æ·†â€”â€”é«˜é¢‘å…±ç°ç±»åˆ«ï¼ˆå¦‚â€œå‰ä»–-äººâ€ï¼‰åœ¨å­¦ä¹ æ–°ç±»åç›¸äº’è¯¯åˆ¤ï¼Œæ¨¡æ€çº ç¼ å¢å¼ºï¼Œå½±å“æ—§ç±»ä¿æŒã€‚<br>â€¢ å¤ä¹ éš¾ç‚¹ï¼šåœ¨å—é™å†…å­˜ä¸‹ï¼Œä¼ ç»Ÿæ ·æœ¬å¤ä¹ è‹¥æœªç­›é™¤â€œæ¼‚ç§»æ ·æœ¬â€ï¼Œä¼šæ¶åŒ–å¯¹é½ï¼›å®ä¾‹å¤ä¹ ï¼ˆå¦‚EIRï¼‰éš¾ä¿è¯éŸ³é¢‘ä¸è§†è§‰å†…å®¹å¯¹é½ï¼Œå¤ä¹ è´¨é‡æ¬ ä½³ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>æå‡ºç¢°æ’é©±åŠ¨çš„å¤šæ¨¡æ€å¤ä¹ æ¡†æ¶CMRï¼šé€šè¿‡å¤šæ¨¡æ€æ ·æœ¬é€‰æ‹©ï¼ˆMSSï¼‰ä»¥mIoUå¢ç›ŠÎ”ç­›é€‰è·¨æ¨¡æ€ä¸€è‡´æ€§é«˜çš„æ ·æœ¬å…¥åº“ç¼“è§£è¯­ä¹‰æ¼‚ç§»ï¼›å¹¶ä»¥åŸºäºç¢°æ’çš„æ ·æœ¬å¤ä¹ ï¼ˆCSRï¼‰ç»Ÿè®¡æ—§æ¨¡å‹ä¸å½“å‰çœŸå€¼çš„â€œç¢°æ’â€é¢‘æ¬¡ï¼ŒæŒ‰ç±»é‡é‡‡æ ·è®°å¿†ä»¥å¼ºåŒ–æ˜“æ··ç±»åˆ«å¤ä¹ ï¼Œé™ä½å…±ç°æ··æ·†ä¸é—å¿˜ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>â€¢ è·¨ç›®æ ‡è§£è€¦çš„å¤šç›®æ ‡æŒç»­éŸ³è§†åˆ†å‰²ï¼šå°†å¤šç›®æ ‡æ ·æœ¬åˆ‡åˆ†ä¸ºä¼ªå•ç›®æ ‡ç‰‡æ®µä»¥æå‡å¤ä¹ è´¨é‡ä¸å»çº ç¼ èƒ½åŠ›<br>â€¢ ç”Ÿæˆå¯¹é½é©±åŠ¨çš„æ— æ ·æœ¬æŒç»­éŸ³è§†åˆ†å‰²ï¼šç”¨æ¡ä»¶ç”Ÿæˆæ¨¡å‹åˆæˆè¯­ä¹‰å¯¹é½çš„éŸ³é¢‘-è§†é¢‘å¯¹è¿›è¡Œä¼ªå¤ä¹ ä»¥çªç ´å†…å­˜é™åˆ¶<br>â€¢ å¼€æ”¾è¯æ±‡æŒç»­éŸ³è§†åˆ†å‰²ä¸è¯­è¨€å…ˆéªŒå¯¹é½ï¼šå¼•å…¥æ–‡æœ¬/è¯­è¨€å…ˆéªŒä¸ç±»åˆ«æ‰©å±•ä»¥å®ç°æœªçŸ¥ç±»å¢é‡ä¸è·¨æ¨¡æ€å¯¹é½ç»Ÿä¸€</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 6</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-10-27 04:01:31 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 6;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>