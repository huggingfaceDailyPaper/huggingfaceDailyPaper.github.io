<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - October 22, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            font-weight: 300;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #2c3e50;
            margin-bottom: 15px;
            line-height: 1.3;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.3em;
            display: flex;
            align-items: center;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 20px;
            border-radius: 6px;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 20px;
            border-radius: 6px;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 20px;
            border-radius: 6px;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 20px;
            border-radius: 6px;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">October 22, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.19779" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.19779" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, a goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target model's knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, a novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes a reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of a draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15\%). The code is publicly available at https://github.com/yuezhouhu/adaspec.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>论文关注加速大模型推理的Speculative Decoding（SD）中，草稿模型与目标模型对齐不充分导致接受率（α）偏低、速度收益受限的问题。现有做法多用在所有token上最小化前向KL的知识蒸馏，但这与SD真正目标——最大化被目标模型一次性接受的token比例——并不一致，且会把小模型有限容量浪费在难以学习、无助于提升接受率的token上。由于草稿模型容量受限，传统全量蒸馏还易出现收敛困难与性能瓶颈。因此，需要一种面向SD场景、兼顾容量约束与接受率目标的选择性蒸馏策略。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>AdaSPEC提出“两阶段、选择性蒸馏”的技术路线：先用目标模型对草稿模型的副本训练得到参考模型，再用参考模型筛除难以拟合的token，仅在“更可学”的token上蒸馏草稿模型。具体做法是对每个token计算以目标为教师的KL损失Lref与Ldraft，构造ΔL=Ldraft−Lref并选取ΔL较大的top-k% token（参考模型能学好而草稿模型尚未学会的部分）参与损失，优化前向KL以强化对可学token的对齐。该策略将小模型容量集中用于最能提升接受率的区域，避免在“硬样本”上过度消耗容量。论文还给出端到端算法、k的选择（常用0.4）与实现细节，易于与现有SD框架集成。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">📊</span>实验结果</h3>
            <div class="results">
                <p>在Pythia-31M→1.4B与CodeGen-350M→Phi-2两组配置、GSM8K/Alpaca/MBPP/CNN-DM/XSUM五项任务上，AdaSPEC在3轮与最优轮次两种设置下均显著提升接受率α，最高绝对提升达约15%（如MBPP在Pythia配置的最优轮次由49.88%到65.12%）。分布层面，AdaSPEC相较DistillSpec呈现更高的正logit间隔与更低的token级KL（图2），显示对齐度与预测置信度同步提升。实际墙钟评测中（vLLM，A100），AdaSPEC带来约10–20%生成加速；与更先进的EAGLE集成也获得+7.45% tokens/s与小幅精度提升。消融显示：选取top-40% token显著优于bottom-40%；前向KL优于RKL/TVD；较小k（0.2–0.4）通常更佳；方法可扩展到更大模型对（如Qwen2.5-0.5B→32B）并在混合任务训练中更少遗忘。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>可进一步设计更自适应的筛选机制，例如基于不确定性、难度建模或动态k的样本/token级调度，并探索“端到端联合”训练参考与草稿模型。目标函数上，可直接优化与SD更一致的指标（如接受率、block效率）或引入多目标权衡质量与速度。体系结构层面，结合树/多步验证的先进SD（EAGLE、Draft&Verify等）与系统层优化（缓存、量化/LoRA）形成正交增益。任务与泛化角度，可研究跨任务/领域课程学习与混合数据稳健性，分析不同规模差下的上限与失效模式，并探索跨家族/跨分词器的一致性蒸馏。最后，可引入更丰富反馈（如拒绝原因、校验置信）驱动“哪些token值得学”的闭环选择。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20579" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20579" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Most video reasoning models only generate textual reasoning traces without indicating when and where key evidence appears. Recent models such as OpenAI-o3 have sparked wide interest in evidence-centered reasoning for images, yet extending this ability to videos is more challenging, as it requires joint temporal tracking and spatial localization across dynamic scenes. We introduce Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal evidence into video reasoning, and carefully collect training data and design training strategies to address the aforementioned challenges. The model highlights key timestamps, objects, and bounding boxes alongside its answers, allowing reasoning to be grounded in concrete visual observations. To enable this functionality, we first curate and build two high-quality datasets, STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed temporal and spatial annotations, since most existing datasets offer either temporal spans for videos or spatial boxes on images, lacking unified spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start reinforcement learning strategy with multiple specially designed rewards that jointly encourage answer accuracy, temporal alignment, and spatial precision. On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance, raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent improvements are also observed on a broad range of video understanding benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond accuracy, the reasoning traces produced by Open-o3 Video also provide valuable signals for test-time scaling, enabling confidence-aware verification and improving answer reliability.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>论文针对视频推理中“只会给文字理由、不知道证据何时何地出现”的问题。作者指出，现有方法要么仅给文本CoT、要么只做时间或空间单侧定位，难以在动态场景中实现精确的时空对齐（见第2页图1）。根因在于缺乏统一的时空标注与可验证的推理轨迹，以及训练中的时空耦合与奖励稀疏（第2–3页）。该问题关系到视频推理的可解释性、可验证性与可靠性，亦呼应V-STAR等基准对“what–when–where”的综合评估需求（第7–8页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出Open-o3 Video：单模型、非agent框架，生成带时间戳与框选区域的显式时空证据，并将其嵌入推理链（第5页图3）。数据方面构建两套语料STGR-CoT-30k与STGR-RL-36k，含5.9k高质量时空样本，管线包括Gemini 2.5 Pro初注、框过滤与一致性校验，覆盖时/空/时空/通用QA多分布（第4页图2）。训练采用“两阶段”：先SFT冷启动学会结构化、可对齐输出，再用GSPO进行RL，复合奖励由答案准确、思考（时间+空间）和格式三部分组成；其中时间项用自适应时间近邻（σ退火）缓解稀疏并逐步收紧，空间项用时间门控（|Δt|≤τ才计IoU）确保时空一致（第6–7页）。实现上统一采样16帧并注入绝对时间戳，GSPO替代GRPO以稳定长链优化（第6页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">📊</span>实验结果</h3>
            <div class="results">
                <p>在V-STAR上，Open-o3 Video取得SOTA：相对Qwen2.5-VL-7B，mAM提升+14.4%、mLGM+24.2%，并超过GPT-4o（第8页表1）。具体维度：What准确率61.0；When（tIoU）在Chain1/2为24.5/24.0；Where（vIoU）在Chain1/2为25.4/6.0，均显著优于基线（第8页表1）。在VideoMME、WorldSense、VideoMMMU与TVGBench也全面提升，尤以长视频(+4.1)与时序定位mIoU(+4.5)突出（第9页表2）。消融显示SFT+RL(GSPO)最佳；自适应时间近邻与时间门控分别带来稳定且可信的时空奖励，去除将显著下降（第10页表4、表3）；高质量时空数据是关键驱动（第10页表5）。基于时空证据的信心加权投票优于多数投票，在WorldSense与VideoMMMU各+1.0（第16与19页图6、表7）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>数据层面：扩充更长、更复杂场景与小目标的高质量时空标注，覆盖多域与更细粒度事件（第17页A.7）。模型/训练：引入音频与语音以多模态对齐；探索更强的跨步推理与记忆机制；优化奖励设计（如层级/因果约束）与更稳健的序列级RL。推理与验证：进一步发展基于证据的自检与测试时标度（如多样化采样、证据一致性评分、跨链顺序一致性）。系统化扩展：从“想与帧”走向“想与轨迹/片段”，结合跟踪与多对象关系建模，或与轻量工具/agent柔性结合以处理超长视频与复杂任务。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.19600" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.19600" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In the quest for scientific progress, communicating research is as vital as the discovery itself. Yet, researchers are often sidetracked by the manual, repetitive chore of building project webpages to make their dense papers accessible. While automation has tackled static slides and posters, the dynamic, interactive nature of webpages has remained an unaddressed challenge. To bridge this gap, we reframe the problem, arguing that the solution lies not in a single command, but in a collaborative, hierarchical process. We introduce AutoPage, a novel multi-agent system that embodies this philosophy. AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline from narrative planning to multimodal content generation and interactive rendering. To combat AI hallucination, dedicated "Checker" agents verify each step against the source paper, while optional human checkpoints ensure the final product aligns perfectly with the author's vision, transforming the system from a mere tool into a powerful collaborative assistant. To rigorously validate our approach, we also construct PageBench, the first benchmark for this new task. Experiments show AutoPage not only generates high-quality, visually appealing pages but does so with remarkable efficiency in under 15 minutes for less than \0.1. Code and dataset will be released at https://mqleet.github.io/AutoPage_ProjectPage/{Webpage}$.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>论文聚焦“论文到项目网页”的自动化生成问题：研究者目前需要手工搭建项目页，耗时且质量不一。现有自动化多集中于静态载体（海报、幻灯、视频），难以处理网页的可滚动结构与交互元素，端到端LLM还常出现布局不合理、内容幻觉、缺乏人类反馈的问题（图1，第1页）。项目网页已成为研究传播与可复现性的关键入口，因此亟需既高质量又高效率、可交互且可控的生成方案。作者据此提出以人机协作、分层粗到细的思路替代单次端到端生成。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>作者提出AutoPage——多智能体、人机协作的三阶段流水线：1）叙事规划与结构化：用MinerU与Docling将PDF解析为Markdown与“资产库”，经内容规划器生成网页大纲并设有“Checker”校验（图2，第4页）。2）多模态内容生成：遵循“先文本后视觉”，先由文本生成器产出网页段落，再由视觉生成器从资产库精确挑选图表并进行一致性检查，支持可选的人类微调。3）交互式渲染：基于带标签的模板库做模板匹配与整合，由HTML生成器输出HTML/CSS/JS，并用HTML Checker做版式与可视完整性检查，允许最终样式指令微调。作者同时构建PageBench基准（1500+人类项目页，测试集100、模板库87），提出包含可读性、语义保真、压缩感知信息准确性与VLM-as-Judge视觉三维度的评测协议（第5–6页）。关键贡献在于：多代理分阶段协作、双重校验防幻觉、可选人类回路增强对齐，以及首个面向该任务的系统化基准与指标体系。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">📊</span>实验结果</h3>
            <div class="results">
                <p>AutoPage在多种底模上稳定提升端到端方法：如GPT‑4o‑mini基线上，美学得分2.71→2.95、布局与整体性2.08→2.38、视觉要素准确性2.96→3.08、语义保真0.554→0.621，压缩感知信息准确性1.786→1.941（表1，第6页）。在Gemini‑2.5‑Flash上，视觉要素2.82→3.13，语义保真0.684→0.742，压缩感知指标1.276→1.591（表1）。对弱底模提升更显著，显著缩小不同底模间的性能鸿沟（第7页）。用户偏好实验中，AutoPage平均得分7.16/10，高于Grok4‑fast(6.93)与Gemini2.5‑Flash(6.79)（图3，第7页）。消融显示校验器至关重要：移除全部校验后视觉要素精准度3.13→2.75，美学2.69→1.90，布局2.15→1.60（表2，第11页）；效率上，典型生成4–20分钟、成本$0.06–$0.20，使用Gemini‑2.5‑Flash可在15分钟内低于$0.1（第8页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>进一步方向包括：1）更强的人类偏好对齐与交互编辑（如基于RLAIF/人类指令的样式与叙事个性化），以及多语种、多领域扩展。2）学习型模板检索/生成与自适应布局（从模板库过渡到可学习的布局规划器），增强动态可视化与可访问性（无障碍、移动端适配）。3）更稳健的事实校验：将图表与源PDF页级锚点对齐，强化公式/表格解析与跨段落证据聚合，减少幻觉。4）评测深化：引入真实用户任务完成度与可用性指标，完善VLM‑as‑Judge的标定与一致性检验，并扩充PageBench到更多学科与更复杂交互。5）系统层面优化：自治代理的成本/时延优化、离线/本地化推理与增量更新，以支持规模化与隐私场景。</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 1</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>← 上一页</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button></span>
            <button id="next-btn" onclick="changePage(1)">下一页 →</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-10-24 08:51:51 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 1;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
</body>
</html>