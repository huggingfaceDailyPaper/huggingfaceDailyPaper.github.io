<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 06, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 06, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Diffusion Language Models are Super Data Learners</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.03276" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.03276" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Under strictly controlled pre-training settings, we observe a Crossover: when unique data is limited, diffusion language models (DLMs) consistently surpass autoregressive (AR) models by training for more epochs. The crossover shifts later with more or higher-quality data, earlier with larger models, and persists across dense and sparse architectures. We attribute the gains to three compounding factors: (1) any-order modeling, (2) super-dense compute from iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation; input or parameter noise improves AR under data constraint but cannot close the gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B unique Python tokens overtakes an AR coder trained with strictly matched settings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag and > 33% on MMLU using only 1B tokens, without any special tricks, just by repeating standard pre-training data. We also show that rising validation cross-entropy does not imply degraded downstream performance in this regime.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Data scarcity is becoming the primary bottleneck: high‚Äëquality unique tokens are limited while compute keeps scaling; we need models that extract more signal per unique token.<br>‚Ä¢ Autoregressive (AR) LMs have strong compute efficiency but limited data potential: causal inductive bias, rapid saturation/overfitting under data repetition, and weaker gains when unique data is scarce.<br>‚Ä¢ Lack of controlled comparisons: prior work did not systematically match AR vs. diffusion under identical pre‚Äëtraining settings to quantify when/why diffusion overtakes AR (the ‚Äúcrossover‚Äù).<br>‚Ä¢ Unclear diagnostics: rising validation cross‚Äëentropy during multi‚Äëepoch training is often misread as degraded capability; the paper shows downstream performance can keep improving.<br>‚Ä¢ Sparse vs. dense trade‚Äëoffs are poorly understood in data‚Äëbound regimes: MoE AR models degrade under repetition, while diffusion benefits from ‚Äúsuper‚Äëdense‚Äù compute but at higher cost.<br>‚Ä¢ Need to disentangle factors behind diffusion‚Äôs advantage: any‚Äëorder modeling, super‚Äëdense compute (iterative bidirectional denoising), and built‚Äëin Monte Carlo augmentation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Under strictly controlled, matched pre‚Äëtraining, the paper trains masked/absorbing discrete diffusion language models and AR baselines across unique‚Äëdata budgets, data quality, model size, and sparsity, and performs ablations (AR input masking, dropout) to isolate factors behind diffusion‚Äôs gains. It scales to a 1.7B code model (‚âà1.5T‚Äëtoken compute on 10B unique tokens) and analyzes validation‚Äëloss vs downstream accuracy to characterize the crossover and compute/data trade‚Äëoffs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Compute-Efficient Diffusion LMs: Algorithms for Training and Inference that Preserve Data Potential: Develop schedules, denoising weights, and parallel decoding to cut FLOPs while retaining diffusion‚Äôs data efficiency.<br>‚Ä¢ Data-Constrained Scaling Laws for Diffusion and Hybrids: Derive compute/parameter/data optima and crossover timing predictors across unique‚Äëdata budgets, model sizes, and qualities.<br>‚Ä¢ Any-Order‚ÄìAutoregressive Hybrids via Blockwise Diffusion with Teacher Forcing: Interpolate between AR and diffusion to balance compute efficiency and data potential under repetition.<br>‚Ä¢ Beyond Validation Cross-Entropy: Reliable Early-Stop and Progress Signals Under Heavy Data Reuse: Design diagnostics that track relative likelihoods and downstream gains during multi‚Äëepoch training.<br>‚Ä¢ KV-Cache Analogues for Diffusion: System Designs for Low-Latency Iterative Denoising: Create caching/scheduling primitives to parallelize diffusion decoding and reduce end‚Äëto‚Äëend latency.<br>‚Ä¢ MoE-Diffusion at Scale: Balancing Activated FLOPs and Total Parameters in Data-Bound Regimes: Study sparse expert routing, stability, and FLOPs/parameter trade‚Äëoffs for DLMs.<br>‚Ä¢ Safe Super-Dense Training: Memorization, Contamination, and Privacy Audits for Repeated-Data Diffusion Pretraining: Build auditing and regularization pipelines tailored to heavy reuse.<br>‚Ä¢ Evaluation Protocols and Crossover Timing in Generative Benchmarks: Standardizing Shot Settings and Decoding to Avoid Artifacts: Quantify how 0‚Äë/few‚Äëshot choices shift observed crossovers.<br>‚Ä¢ Monte Carlo Augmentation Theory for Masked Diffusion: Bias‚ÄìVariance Tradeoffs and Sample Complexity: Formalize how corruption distributions expand the learning signal vs. AR.<br>‚Ä¢ Retrieval-Augmented Diffusion Models: Scaling Knowledge with Trillion-Token Datastores: Fuse iterative denoising with retrieval to boost accuracy without more unique pretrain tokens.<br>‚Ä¢ RL and Preference Optimization for Diffusion LMs in Data-Limited Settings: Scale alignment methods tailored to iterative, any‚Äëorder generation.<br>‚Ä¢ Multilingual, Multimodal, and Long-Context Diffusion Pretraining: Assess whether data efficiency and crossovers persist beyond English text and standard lengths.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.03334" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.03334" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Due to the lack of effective cross-modal modeling, existing open-source audio-video generation methods often exhibit compromised lip synchronization and insufficient semantic consistency. To mitigate these drawbacks, we propose UniAVGen, a unified framework for joint audio and video generation. UniAVGen is anchored in a dual-branch joint synthesis architecture, incorporating two parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which enables bidirectional, temporally aligned cross-attention, thus ensuring precise spatiotemporal synchronization and semantic consistency. Furthermore, this cross-modal interaction is augmented by a Face-Aware Modulation module, which dynamically prioritizes salient regions in the interaction process. To enhance generative fidelity during inference, we additionally introduce Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly amplifies cross-modal correlation signals. Notably, UniAVGen's robust joint synthesis design enables seamless unification of pivotal audio-video tasks within a single model, such as joint audio-video generation and continuation, video-to-audio dubbing, and audio-driven video synthesis. Comprehensive experiments validate that, with far fewer training samples (1.3M vs. 30.1M), UniAVGen delivers overall advantages in audio-video synchronization, timbre consistency, and emotion consistency.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Open-source audio-video generation suffers from poor lip synchronization and weak semantic consistency (timbre/emotion) due to insufficient cross-modal modeling.<br>‚Ä¢ Two-stage, decoupled pipelines (audio‚Üívideo or video‚Üíaudio) block real-time cross-modal interplay, making alignment complex and often suboptimal.<br>‚Ä¢ Existing end-to-end approaches often handle only ambient sounds or exhibit weak audio-visual alignment, failing to deliver natural human speech with fine-grained temporal sync.<br>‚Ä¢ There is no unified, open-source framework that robustly supports multiple tasks (joint AV generation/continuation, video-to-audio dubbing, audio-driven video) with strong generalization, including out-of-domain cases.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>UniAVGen uses a symmetric dual-branch Diffusion Transformer (audio/video) with an Asymmetric Cross-Modal Interaction module‚Äîaudio-to-video via contextual audio windows and video-to-audio via temporally interpolated video context‚Äîaugmented by Face-Aware Modulation and Modality-Aware Classifier-Free Guidance to strengthen spatiotemporal alignment and fidelity across tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Real-Time UniAVGen: Low-Latency Streaming Audio-Video Co-Generation: Extend the architecture and MA-CFG for incremental, streaming inference with stable online cross-modal synchronization.<br>‚Ä¢ Disentangled Emotion and Timbre Control for Joint AV Diffusion: Learn controllable latent factors to independently manipulate emotion intensity, prosody, and timbre while preserving lip-sync and identity.<br>‚Ä¢ Self-Supervised Face-Aware Modulation Without Explicit Masks: Replace supervised face-mask guidance with self-supervised saliency/attention objectives to reduce dependency on face annotations and improve generalization.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.03001" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.03001" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite recent progress in using Large Language Models (LLMs) for automatically generating 3D scenes, generated scenes often lack realistic spatial layouts and object attributes found in real-world environments. As this problem stems from insufficiently detailed, coarse-grained instructions, advancing 3D scene synthesis guided by more detailed, fine-grained instructions that reflect real-world environments becomes crucial. Without such realistic scenes, training embodied agents in unrealistic environments can lead them to learn priors that diverge significantly from real-world physics and semantics, degrading their performance when deployed. Thus, verifying the alignment between the fine-grained instruction and the generated scene is essential for effective learning. However, current evaluation methods, such as CLIPScore and vision-language models (VLMs), often fail to reliably assess such alignment. This shortcoming arises primarily from their shallow understanding of 3D scenes, which often leads to improperly grounded scene components. To address this, we introduce LEGO-Eval, an evaluation framework equipped with diverse tools designed to explicitly ground scene components, enabling more accurate alignment assessments. We also present LEGO-Bench, a benchmark of detailed instructions that specify complex layouts and attributes of real-world environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with LEGO-Bench reveals significant limitations in current generation methods. Across all evaluated approaches, success rates reached at most 10% in generating scenes that fully align with fine-grained instructions.</p>
        </div>
    
        <div class="no-analysis">
            <p>No detailed analysis available for this paper.</p>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02818" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02818" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Tabular data remain the predominant format for real-world applications. Yet, developing effective neural models for tabular data remains challenging due to heterogeneous feature types and complex interactions occurring at multiple scales. Recent advances in tabular in-context learning (ICL), such as TabPFN and TabICL, have achieved state-of-the-art performance comparable to gradient-boosted trees (GBTs) without task-specific fine-tuning. However, current architectures exhibit key limitations: (1) single-scale feature processing that overlooks hierarchical dependencies, (2) dense attention with quadratic scaling in table width, and (3) strictly sequential component processing that prevents iterative representation refinement and cross-component communication. To address these challenges, we introduce Orion-MSP, a tabular ICL architecture featuring three key innovations: (1) multi-scale processing to capture hierarchical feature interactions; (2) block-sparse attention combining windowed, global, and random patterns for scalable efficiency and long-range connectivity; and (3) a Perceiver-style memory enabling safe bidirectional information flow across components. Across diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance while scaling effectively to high-dimensional tables, establishing a new standard for efficient tabular in-context learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-MSP .</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing tabular ICL models process features at a single scale, missing hierarchical dependencies across features, groups, and blocks (pp. 2‚Äì3).<br>‚Ä¢ Dense row-wise attention scales quadratically with feature count O(m^2), making wide real-world tables (>100 features) computationally impractical (pp. 2‚Äì3, 9‚Äì10).<br>‚Ä¢ Strictly sequential pipelines (column ‚Üí row ‚Üí ICL) prevent iterative refinement and cross-component communication; downstream insights cannot improve upstream representations (pp. 2‚Äì3).<br>‚Ä¢ LLM-based table serialization suffers from limited context windows and unreliable numerical reasoning, constraining few-shot performance on tables (p. 2).<br>‚Ä¢ Practical need: tabular data dominate real-world tasks (healthcare, finance); models must be efficient, scalable, and ICL-safe (pp. 1‚Äì3).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Orion-MSP introduces multi-scale row-wise processing with block-sparse attention (windowed, global, and random links) and a Perceiver-style memory that enables training-only writes and read-only refinement for all samples to preserve ICL safety. Concretely, it combines Set Transformer column embeddings, hierarchical scales (e.g., 1/4/16) with sparse masks (Figure 2, p. 7), and a split-masked ICL head, reducing feature complexity from O(m^2) to near-linear while allowing safe cross-component information flow (Figure 1, p. 4; Algorithm 2, p. 12).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ AutoScale-TabICL: Adaptive Multi-Scale Selection and Dynamic Sparsity Scheduling for Tabular In-Context Learning: Learn to choose scales and sparsity patterns per dataset to balance accuracy and compute.<br>‚Ä¢ SafeWrite: Formal Guarantees and Adaptive Policies for Perceiver Memory in ICL: Tighten the theory and design data-driven write/read gates that maximize utility without test leakage.<br>‚Ä¢ GroupFormer: Learnable Feature Grouping via Differentiable Clustering for Ultra-Wide Tables: Replace/augment PMA grouping with end-to-end learnable grouping that scales to 10k+ features.<br>‚Ä¢ Tabular-RAG: Retrieval-Augmented In-Context Learning Across Datasets: Retrieve similar training contexts from a corpus to populate memory and improve zero-shot generalization.<br>‚Ä¢ Explainable-Orion: Interpreting Multi-Scale Sparse Attention in Tabular Models: Provide feature-group and scale-level attributions to enhance transparency and debugging.<br>‚Ä¢ Orion-Reg: Extending Multi-Scale Sparse ICL to Regression and Multi-Task Settings: Generalize the architecture and loss design to continuous targets and joint tasks.<br>‚Ä¢ HashICL: Sublinear Attention via Hashing and Sketching for Extremely Wide Feature Spaces: Combine LSH/sketching with block-sparse attention to achieve sublinear complexity in m.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02802" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02802" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Tabular foundation models represent a growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TabTune, a unified library that standardizes the complete workflow for tabular foundation models through a single interface. TabTune provides consistent access to seven state-of-the-art models supporting multiple adaptation strategies, including zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). The framework automates model-aware preprocessing, manages architectural heterogeneity internally, and integrates evaluation modules for performance, calibration, and fairness. Designed for extensibility and reproducibility, TabTune enables consistent benchmarking of adaptation strategies of tabular foundation models. The library is open source and available at https://github.com/Lexsi-Labs/TabTune .</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Diverse, model-specific preprocessing (encodings, normalization, missing-value handling) forces bespoke pipelines and hinders reproducibility.<br>‚Ä¢ Fragmented APIs and inconsistent adaptation protocols (zero-shot, SFT, meta-learning, PEFT) make cross-model experimentation laborious and error-prone.<br>‚Ä¢ Lack of standardized, deployment-oriented evaluation for calibration, fairness, and efficiency obscures trustworthiness and real-world readiness.<br>‚Ä¢ Model selection complexity across data regimes (size, feature width, imbalance) and resource constraints leaves practitioners without clear guidance.<br>‚Ä¢ Benchmarking gaps and non-unified splits/metrics impede fair, reproducible comparisons of tabular foundation models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TabTune is a modular, scikit-learn-compatible pipeline that abstracts model-aware preprocessing (DataProcessor), unifies adaptation strategies (zero-shot, supervised and meta-learning fine-tuning, and PEFT via LoRA in TuningManager), and integrates calibrated/fair evaluation and a reproducible TabularLeaderboard. It standardizes heterogeneous TFMs behind a single fit‚Äìpredict‚Äìevaluate interface to enable consistent benchmarking and deployment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ PEFT for Bayesian Tabular Models: Stabilizing LoRA for TabPFN at Scale: Architect and evaluate PEFT schemes that are natively compatible with TabPFN to retain calibration while enabling efficient adaptation on larger datasets.<br>‚Ä¢ Calibration-Aware Fine-Tuning for TFMs: Jointly optimize accuracy and reliability using calibration losses, temperature scaling, and post-hoc recalibration integrated into TabTune‚Äôs training loops.<br>‚Ä¢ Fairness-Preserving In-Context Learning on Tables: Develop group-aware episode sampling and adapter regularization to reduce SPD/EOD/EOpD gaps without sacrificing accuracy.<br>‚Ä¢ TabTune-TS: Extending TFMs to Regression, Multi-Label, and Time Series: Generalize the API, preprocessing, and metrics to new task types with calibration and fairness diagnostics for each.<br>‚Ä¢ Robust TFMs Under Distribution Shift: A Shifted-Tab Benchmark: Add synthetic and real shifts/corruptions to benchmark robustness, uncertainty, and recovery strategies within TabTune.<br>‚Ä¢ AutoTFM: Automated Model and Strategy Selection in TabTune: Learn policies that pick the TFM and adaptation mode (zero-shot vs SFT vs PEFT vs meta-learning) given dataset descriptors and resource budgets.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01294" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01294" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>A deep understanding of kinematic structures and movable components is essential for enabling robots to manipulate objects and model their own articulated forms. Such understanding is captured through articulated objects, which are essential for tasks such as physical simulation, motion planning, and policy learning. However, creating these models, particularly for objects with high degrees of freedom (DoF), remains a significant challenge. Existing methods typically rely on motion sequences or strong assumptions from hand-curated datasets, which hinders scalability. In this paper, we introduce Kinematify, an automated framework that synthesizes articulated objects directly from arbitrary RGB images or textual descriptions. Our method addresses two core challenges: (i) inferring kinematic topologies for high-DoF objects and (ii) estimating joint parameters from static geometry. To achieve this, we combine MCTS search for structural inference with geometry-driven optimization for joint reasoning, producing physically consistent and functionally valid descriptions. We evaluate Kinematify on diverse inputs from both synthetic and real-world environments, demonstrating improvements in registration and kinematic topology accuracy over prior work.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Robots need accurate kinematic topology and joint parameters to enable simulation, motion planning, and policy learning, but creating high-DoF articulated models is labor-intensive and error-prone<br>‚Ä¢ Existing geometry-first methods require 4D sequences or controlled multi-scan capture, limiting scalability and deployment from casual inputs<br>‚Ä¢ Program-synthesis/LLM approaches often yield functionally plausible but geometrically imprecise models and struggle with multi-branch, high-DoF structures<br>‚Ä¢ Robot self-modeling methods typically assume access to motion/sensor data and are biased toward serial-chain topologies<br>‚Ä¢ There is a lack of open-vocabulary pipelines that infer both kinematic topology and joint parameters directly from static RGB images or text without pre-defined priors</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Kinematify generates a segmented 3D mesh from a single RGB image or text, builds an SDF-based contact graph, and infers a directed kinematic tree via MCTS with rewards for structural regularity, static stability, contact strength, symmetry, and hierarchy. It then classifies joint types using a vision-language model and optimizes axes/pivots with a Distance-Weighted Contact-Aware Virtual Linkage (DW-CAVL) SDF objective to enforce contact consistency and avoid collisions, exporting the result to URDF.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Contact Confidence for Robust Kinematic Inference: Estimate per-contact reliability and jointly refine part segmentation and the contact graph to improve topology recovery under noisy geometry<br>‚Ä¢ Adaptive Symmetry-Aware MCTS for Complex Articulations: Learn or schedule reward weights (e.g., symmetry vs. contact) to mitigate decorative/false contacts and better handle large symmetric clusters<br>‚Ä¢ Multi-Modal Kinematics: Unifying Static Geometry with Sparse Motion Cues: Integrate optional monocular or sparse motion observations with static SDF cues to further disambiguate joints and parents<br>‚Ä¢ Beyond Trees: Graph-Based Articulation with Closed Kinematic Loops and Compliance: Extend topology inference and optimization to mechanisms with loops, parallel linkages, compliant joints, and deformables<br>‚Ä¢ End-to-End Differentiable Articulation Synthesis from Images: Make the segmentation‚Äìcontact‚ÄìMCTS‚ÄìDW-CAVL stack differentiable to enable learning from data and simulation-in-the-loop feedback<br>‚Ä¢ Uncertainty-Aware URDF Synthesis for Safe Planning: Quantify and propagate uncertainty over topology and joint parameters, enabling risk-aware planners and active information gathering<br>‚Ä¢ Articulation Parameter Identification with Dynamics and Materials: Jointly infer inertias, joint limits, friction, damping, and stiffness from geometry and limited interaction data<br>‚Ä¢ Interactive Kinematify: Human-in-the-Loop Corrections at Scale: Design lightweight editing and active query mechanisms to correct ambiguous attachments/joints with minimal user input<br>‚Ä¢ Benchmarking Open-Vocabulary High-DoF Articulation: Create standardized datasets, metrics, and protocols spanning images-to-URDF for multi-branch, high-DoF objects and robots</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.03146" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.03146" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As reasoning models scale rapidly, the essential role of multimodality in human cognition has come into sharp relief, driving a growing need to probe vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either overemphasize textual reasoning or fall short of systematically capturing vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs insufficiently assessed. To address this limitation, we introduce MME-CC (Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded benchmark that organizes 11 representative reasoning tasks into three fundamental categories of visual information: spatial, geometric, and knowledge-based reasoning, and provides fine-grained analyses of MLLMs' cognitive capacity across these dimensions. Based on MME-CC, we conduct extensive experiments over 16 representative MLLMs. Our study reveals that closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs. 30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak (less than or equal to 30%). We further identify common error patterns, including orientation mistakes, fragile cross-view identity persistence, and poor adherence to counterfactual instructions, and observe that Chain-of-Thought typically follows a three-stage process (extract -> reason -> verify) with heavy reliance on visual extraction. We hope this work catalyzes a shift toward treating the cognitive capacity of MLLMs as central to both evaluation and model design.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing multimodal benchmarks either overemphasize text-based reasoning or lack a systematic taxonomy for vision-centric cognitive behaviors, leaving MLLMs‚Äô visual cognitive capacity insufficiently assessed (see Abstract and Introduction; Table 3 on page 5 contrasts vision-based settings).<br>‚Ä¢ There is a need for language-independent, vision-grounded evaluation that reduces textual shortcuts and probes core visual cognition‚Äîespecially spatial and geometric reasoning‚Äîwhere current models are notably weak (‚â§30% in both categories; Table 4 on page 6), with overall best accuracy only 42.66%.<br>‚Ä¢ Current evaluations rarely diagnose why models fail; recurring errors include orientation/reference-frame mistakes, fragile cross-view identity persistence, and poor adherence to counterfactual instructions, and CoT behaviors are under-characterized (Section 4 and Figure 3 on page 7), limiting targeted model improvement.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces MME-CC, a vision-grounded benchmark of 1,173 human-curated questions across 11 tasks organized into Spatial, Geometric, and Visual Knowledge reasoning, built via a human-in-the-loop pipeline (definition ‚Üí data acquisition ‚Üí post-processing ‚Üí model-based filtering) and evaluated with an LLM-as-a-judge. It provides fine-grained diagnostics (per-category results, error taxonomy, CoT analyses) to surface vision-centric cognitive behaviors while minimizing textual shortcuts.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Cross-View Consistent Spatial Reasoning for MLLMs via Reference-Frame Alignment and Entity Tracking: Develop modules (e.g., scene graphs/pose-aware memory) that maintain orientations and object identities across views to reduce orientation and deduplication errors.<br>‚Ä¢ Instruction-Faithful Vision-Language Models with Counterfactual Reasoning: Train and decode models to reliably follow counterfactual and adversarial (sandbagging) instructions, separating visual grounding from instruction conditioning to prevent literal-overriding failures.<br>‚Ä¢ Visual-Grounded Chain-of-Thought with Iterative Evidence Verification: Integrate structured describe-then-reason prompting and visual re-scanning checkpoints into CoT to stabilize reasoning, curb excessive self-interruptions, and improve planning tasks (e.g., maze, Unblock Me).</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LiveTradeBench: Seeking Real-World Alpha with Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.03628" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.03628" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Static, single-turn LLM benchmarks don‚Äôt test sequential decision-making under live uncertainty; markets evolve independently, with delayed and noisy feedback.<br>‚Ä¢ Existing interactive agent environments are largely deterministic and controllable, enabling tree search but failing to capture autonomous, stochastic market dynamics that require adaptation.<br>‚Ä¢ Offline backtesting suffers from information leakage and cannot reflect real-time volatility/regime shifts, leading to overestimated performance and poor generalization.<br>‚Ä¢ Most LLM trading agents use low-level single-asset buy/sell actions, neglecting portfolio-level allocation, cross-asset reasoning, and risk management.<br>‚Ä¢ Lack of a low-cost, live evaluation framework that streams real data and supports consistent, end-to-end assessment of LLM trading competence.<br>‚Ä¢ Evaluations are usually confined to one market; there‚Äôs a need to test transfer and robustness across structurally distinct markets (e.g., U.S. stocks vs. Polymarket).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>LiveTradeBench is a live, multi-market portfolio-management benchmark that streams real-time prices and news and frames trading as allocation over a probability simplex (POMDP), mapping allocations to executable rebalancing across U.S. stocks and Polymarket. A ReAct-style LLM agent with tool use and short-term memory converts observations (positions, prices, news) into portfolio weights, enabling apples-to-apples evaluation with risk/return metrics over 50 live trading days.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Cost-Aware LiveTradeBench: Incorporating Transaction Costs, Liquidity, and Slippage into Live LLM Trading Evaluation: Add fees, spreads, and slippage models to assess robustness of allocation policies under realistic frictions.<br>‚Ä¢ Long-Context Financial Agents with Hierarchical Memory for Live Portfolio Management: Extend observation horizons and richer news ingestion (full articles, more markets) with hierarchical/long-term memory for temporal reasoning.<br>‚Ä¢ Reinforcement- and Multi-Agent-Enhanced LLM Traders in Live Markets: Augment ReAct agents with specialized tools, RL fine-tuning, and multi-agent roles to improve decision quality, risk control, and market interaction modeling.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02309" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02309" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We revisit test-time scaling for language model reasoning and ask a fundamental question: at equal token budget and compute, is it better to run multiple independent chains in parallel, or to run fewer chains that iteratively refine through sequential steps? Through comprehensive evaluation across 5 state-of-the-art open source models and 3 challenging reasoning benchmarks, we find that sequential scaling where chains explicitly build upon previous attempts consistently outperforms the dominant parallel self-consistency paradigm in 95.6% of configurations with gains in accuracy upto 46.7%. Further, we introduce inverse-entropy weighted voting, a novel training-free method to further boost the accuracy of sequential scaling. By weighing answers in proportion to the inverse entropy of their reasoning chains, we increase our success rate over parallel majority and establish it as the optimal test-time scaling strategy. Our findings fundamentally challenge the parallel reasoning orthodoxy that has dominated test-time scaling since Wang et al.'s self-consistency decoding (Wang et al., 2022), positioning sequential refinement as the robust default for modern LLM reasoning and necessitating a paradigm shift in how we approach inference-time optimization.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Parallel self-consistency dominates test-time scaling, but it is unclear (under matched compute) whether running many independent chains is actually better than iteratively refining fewer chains.<br>‚Ä¢ Parallel chains cannot self-correct or accumulate context across attempts, limiting error correction, verification, and focused compute allocation on hard substeps.<br>‚Ä¢ Majority voting treats all chains equally and ignores model confidence; there is no principled, training-free aggregation of reasoning chains based on uncertainty.<br>‚Ä¢ Prior sequential methods are narrow, often require fine-tuning or bespoke architectures, and rarely compare to parallel baselines under strictly matched token budgets.<br>‚Ä¢ Practitioners lack guidance on optimal chain length and how accuracy scales with token budgets across models and tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A training-free sequential refinement framework where each step explicitly builds on previous reasoning, combined with inverse-entropy weighted voting that aggregates answers using token-level logprob entropies to weight higher-confidence chains more. Experiments enforce strictly matched token budgets vs parallel self-consistency and evaluate across multiple open-source models and reasoning benchmarks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Entropy-Gated Hybrid Reasoning: Dynamically combine parallel exploration with sequential refinement using entropy thresholds to branch or merge chains under a fixed compute/latency budget.<br>‚Ä¢ Width-vs-Depth Laws for Inference-Time Compute: A theoretical framework characterizing when sequential refinement outperforms parallel self-consistency and how gains scale with token budgets.<br>‚Ä¢ Sequential Scaling for Multimodal and Code Reasoning: Extend iterative refinement and entropy-weighted aggregation to vision-language and program synthesis/repair tasks.<br>‚Ä¢ Adaptive Chain-Length Selection via Online Confidence: Learn policies that stop or continue refinement based on real-time entropy signals to maximize accuracy per token.<br>‚Ä¢ Latency-Aware Sequential Inference in Production: Architectures that interleave planning and execution (e.g., micro-batched or staggered steps) to retain sequential gains with bounded wall-clock time.<br>‚Ä¢ Beyond Entropy: Robust Uncertainty Aggregation for Chain Voting: Evaluate and fuse alternative confidence signals (calibration, mutual information, variance across continuations) to improve aggregation.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.03718" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.03718" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Collaborative dialogue relies on participants incrementally establishing common ground, yet in asymmetric settings they may believe they agree while referring to different entities. We introduce a perspectivist annotation scheme for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures speaker and addressee grounded interpretations for each reference expression, enabling us to trace how understanding emerges, diverges, and repairs over time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k annotated reference expressions with reliability estimates and analyze the resulting understanding states. The results show that full misunderstandings are rare once lexical variants are unified, but multiplicity discrepancies systematically induce divergences, revealing how apparent grounding can mask referential misalignment. Our framework provides both a resource and an analytic lens for studying grounded misunderstanding and for evaluating (V)LLMs' capacity to model perspective-dependent grounding in collaborative dialogue.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Apparent grounding can mask divergent referents in asymmetric dialogue (e.g., MapTask), so interlocutors may believe they agree while talking about different entities‚Äîhindering coordination and task success.<br>‚Ä¢ Existing MapTask landmark IDs/annotations obscure misalignment: they conflate multiple same-named instances and separate lexical variants, making it hard to tell whether speaker and addressee truly align on a referent.<br>‚Ä¢ Most reference-resolution and benchmarking frameworks treat grounding as a single end state and do not capture speaker vs. addressee personal interpretations or how they evolve over turns.<br>‚Ä¢ Symbolic dialogue-state models exist but are difficult to integrate into scalable, schema-constrained, reference-level annotation suitable for LLM-assisted pipelines.<br>‚Ä¢ The field lacks a resource and analytic lens to quantify understanding states and transitions (aligned, pending, misunderstood) and to evaluate (V)LLMs‚Äô perspective-dependent grounding in collaborative dialogue.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes a perspectivist, reference-expression-level annotation scheme with unified landmark IDs (disambiguating giver/follower and multiplicity) and a five-attribute decision cascade (is_quantificational, is_specified, is_accommodated, is_grounded, is_imagined), operationalized via a JSON-schema-constrained GPT-5 pipeline over MapTask transactions to annotate both speaker-intended and addressee-interpreted referents and derive understanding-state transitions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Benchmarking Perspective-Dependent Grounding in (V)LLMs with MapTask-Perspectivist Annotations: Build an evaluation suite that tests incremental grounding, alignment detection, and repair under asymmetric information.<br>‚Ä¢ From Landmarks to Paths: Fine-Grained Spatial Reasoning for Grounded Dialogue: Extend annotations beyond landmark IDs to encode directions, distances, and route-level spatial relations.<br>‚Ä¢ Hearing Without Seeing: Evaluating Omniscience and Perspective Bias in LLM/VLM Dialogue Understanding: Compare participant vs. overhearer vs. analyzer settings to measure perspective-taking and bias in detecting misalignment.<br>‚Ä¢ Multimodal Grounding Cues: Incorporating Prosody and Gaze into Perspectivist Reference Annotation: Add audio/visual signals to improve detection of accommodation, grounding, and repair acts.<br>‚Ä¢ Human‚ÄìLLM Agreement at Scale for Perspectivist Grounding: A Reliability Study: Conduct multi-annotator IAA studies and compare schema-constrained LLM outputs across models and prompts.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02358" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02358" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Query augmentation makes queries more meaningful by appending further information to the queries to find relevant documents. Current studies have proposed Large Language Model (LLM)-based embedders, which learn representation for embedding and generation for query augmentation in a multi-task manner by leveraging the generative capabilities of LLM. During inference, these jointly trained embedders have conducted query augmentation followed by embedding, showing effective results. However, augmenting every query leads to substantial embedding latency and query augmentation can be detrimental to performance for some queries. Also, previous methods have not been explored in multimodal environments. To tackle these problems, we propose M-Solomon, a universal multimodal embedder that can adaptively determine when to augment queries. Our approach first divides the queries of the training datasets into two groups at the dataset level. One includes queries that require augmentation and the other includes queries that do not. Then, we introduces a synthesis process that generates appropriate augmentations for queries that require them by leveraging a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation. Through this step, M-Solomon can conduct query augmentation only when necessary by learning to generate synthetic augmentations with the prefix /augment for queries that demand them and to generate the simple string /embed for others. Experimental results showed that M-Solomon not only surpassed the baseline without augmentation by a large margin but also outperformed the baseline that always used augmentation, providing much faster embedding latency.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Always augmenting every query substantially increases embedding latency and compute, harming inference-time efficiency.<br>‚Ä¢ Query augmentation can misinterpret or hallucinate details for some queries, degrading retrieval accuracy; a one-size-fits-all policy is unsafe.<br>‚Ä¢ Existing LLM-based embedders lack an adaptive mechanism to decide when to augment and have not been validated broadly in multimodal settings, limiting effectiveness and generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>M‚ÄëSolomon is a universal multimodal embedder that first generates a control token (/augment or /embed) to decide whether to produce answer‚Äëstyle augmentation, then embeds the (augmented or original) query in a single forward pass. It is trained with joint objectives‚Äîautoregressive loss for control-token+augmentation generation and contrastive loss for retrieval‚Äîusing teacher‚Äësynthesized augmentations for datasets identified via a pilot study as benefiting from augmentation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Query-Level Adaptive Augmentation for Multimodal Retrieval: Learn per-query augmentation decisions (replacing dataset-level splitting) using uncertainty estimation, calibration, or a lightweight policy network.<br>‚Ä¢ Reasoning-Aware Augmentation for Retrieval: Introduce a third mode that triggers reasoning-based augmentation for reasoning-intensive benchmarks (e.g., BRIGHT, RAR-b) and analyze accuracy‚Äìlatency trade-offs.<br>‚Ä¢ Cost-Aware Token-Budgeted Augmentation via Reinforcement Learning: Jointly optimize when to augment and how long the augmentation should be under latency/throughput constraints without sacrificing retrieval quality.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 4</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-06 23:05:07 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 4;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>