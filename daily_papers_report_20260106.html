<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - January 06, 2026</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">January 06, 2026</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.20578" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.20578" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLMs often produce confident but incorrect answers and struggle to assess their own correctness, limiting reliability, safety, and compute-aware deployment<br>â€¢ Existing approaches (text-based self-critique, token-probability confidence, multi-sample consistency, external judges/reward models) either correlate weakly with true correctness, degrade on long/compositional tasks, or impose substantial inference and supervision costs<br>â€¢ It remains unclear whether correctness can be predicted intrinsically from the modelâ€™s internal generation dynamics, which could enable early failure detection and low-overhead self-verification</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Gnosis passively decodes correctness signals from an LLMâ€™s hidden states and attention patterns, compressing internal traces into fixed-budget, length-agnostic descriptors and predicting correctness via a lightweight 5M-parameter module, all while keeping the backbone frozen and adding negligible inference cost.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Self-Aware Decoding: Real-Time Compute Control Using Internal Correctness Signals: Leverage Gnosis-style descriptors to modulate generation depth, sampling, and verification on-the-fly for efficient, failure-aware decoding<br>â€¢ Multimodal Gnosis: Internal Failure Prediction in Vision-Language and Tool-Use Models: Extend intrinsic self-verification to multimodal architectures by reading cross-modal attention and hidden-state circuits to anticipate hallucinations and tool errors<br>â€¢ Correctness-Guided Reasoning: Steering Chain-of-Thought with Internal Reliability Signals: Integrate correctness predictors into reasoning loops to prune failing trajectories, adjust step granularity, and guide search without external judges</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02204" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02204" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Unified multimodal systems struggle to combine LLM-level reasoning with high-fidelity visual generation; diffusion models lack inherent logical reasoning and in-context learning, while multimodal LLMs are often perception-only.<br>â€¢ Pure autoregressive visual generation using raster-scan next-token prediction is prohibitively slow at high resolutions due to quadratic sequence lengths, making interactive 1024Ã—1024 image synthesis impractical.<br>â€¢ Reconstruction-oriented VQ tokenizers produce visual codes with low semantic density, limiting multimodal understanding and alignment with textual representations.<br>â€¢ ARâ€“diffusion hybrid architectures rely on separate representations, introducing re-encoding overheads for interleaved tasks and constraining deep multimodal integration.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>NextFlow is a unified decoder-only autoregressive transformer that uses next-token modeling for text and next-scale hierarchical prediction for images, powered by a dual-codebook tokenizer to balance semantic richness and pixel fidelity. A robust training pipeline with prefix-tuned GRPO stabilizes multi-scale AR generation and aligns objectives, with an optional diffusion decoder for detail refinement.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Hierarchical Autoregressive Video Generation via Next-Scale Temporal Modeling: Extend next-scale prediction to temporal hierarchies for efficient high-fidelity video synthesis with coherent motion and structure.<br>â€¢ Learning Adaptive Scale Schedules for Content-Aware Next-Scale Generation: Replace predefined scale schedules with learned, data-driven policies that adapt hierarchy depth and transitions to scene complexity and target resolution.<br>â€¢ Prefix-Tuned Multimodal RL for Fine-Grained Editing and Safety Alignment: Generalize prefix-focused GRPO to multi-objective reward functions for precise editing control, preference alignment, and safety across interleaved textâ€“image tasks.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">K-EXAONE Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.01739" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.01739" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Build frontier-level LLM performance under Koreaâ€™s constrained AI infrastructure (limited data centers/AI chips), requiring cost-efficient scaling<br>â€¢ Reduce training/inference cost at 100B+ scales where dense models are prohibitive, while maintaining high capability<br>â€¢ Enable efficient ultraâ€“long-context processing (up to 256K tokens) without full global attentionâ€™s memory/compute overhead<br>â€¢ Expand multilingual coverage beyond prior EXAONE (EN/KO/ES) to DE/JA/VI and mitigate data imbalance for consistent cross-language performance<br>â€¢ Improve token efficiency and downstream performance with a better tokenizer for multilingual, STEM, and code domains<br>â€¢ Stabilize and utilize MoE training at scale (routing stability, avoidance of capacity-based token dropping) and improve decoding throughput</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>K-EXAONE is a 236B-parameter MoE LLM (top-8-of-128 plus a shared expert; ~23B active) with dropless, sequence-level load-balanced routing, hybrid global/sliding-window attention (SWA window 128) for 256K contexts, and an MTP auxiliary module that enables self-drafting for ~1.5Ã— faster decoding. It augments performance via a 150k SuperBPE tokenizer (superword units, NFC normalization), thinking-augmented multilingual pretraining (including DE/JA/VI), and FP8 training with stability features (QK Norm, SWA-only RoPE).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Dropless Sequence-Level Routing at Trillion Scale: Generalizing MoE Stability and Efficiency: Extend and analyze dropless, sequence-level load-balanced routing to trillion-parameter MoE models, quantifying convergence, utilization, and latency trade-offs.<br>â€¢ Adaptive Hybrid Attention for 256K+ Contexts: Input-Conditioned GA/SWA Scheduling: Learn to dynamically allocate global vs. sliding-window attention and window sizes per layer/query to optimize KV-cache usage and quality under ultra-long contexts.<br>â€¢ Cross-Lingual Reasoning Transfer with Document-Grounded Thinking Trajectories: Develop principled pipelines for generating and aligning step-by-step reasoning traces across languages, especially low-resource ones, to improve multilingual reasoning fidelity.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.01425" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.01425" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Video face swapping must inject a source identity while preserving target video attributes and temporal consistency; frame-by-frame IFS causes flicker, jitter, and identity/attribute drift.<br>â€¢ Existing diffusion-based VFS methods improve coherence but lag behind state-of-the-art IFS in identity similarity and attribute preservation due to weak or missing explicit supervision.<br>â€¢ Entanglement between identity, expression, and motion makes multi-modal condition injection difficult; current approaches lack robust mechanisms to decouple and fuse these signals.<br>â€¢ Preserving fine-grained facial dynamics (expressions, lip-sync) and backgrounds under challenging scenarios (large pose, occlusion, animation, small faces) remains hard.<br>â€¢ The field lacks comprehensive, diverse benchmarks to objectively evaluate identity coherence, attribute preservation, and realism across varied conditions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes SyncID-Pipe to pretrain an Identity-Anchored Video Synthesizer (First-Last-Frame DiT with adaptive pose attention) and, combined with an IFS model, construct bidirectional ID quadruplets for explicit supervision. Building on this paired data, DreamID-Vâ€” a DiT-based VFS frameworkâ€”uses Modality-Aware Conditioning to inject spatio-temporal context, structural (pose) guidance, and identity embeddings, augmented by Synthetic-to-Real curriculum learning and Identity-Coherence reinforcement learning to improve realism and consistency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Streaming DreamID-V: Real-Time Diffusion Transformers for Temporally Stable Face Swapping: Develop causal/efficient DiT architectures and incremental conditioning to achieve low-latency, flicker-free VFS on streaming inputs.<br>â€¢ 3D-Aware Modality Conditioning for Robust Video Face Swapping: Integrate explicit 3D face/body models and differentiable rendering into the conditioning pipeline to better disentangle identity/expression and handle extreme poses/occlusions.<br>â€¢ Self-Supervised Synthetic-to-Real Adaptation for Video Face Swapping: Reduce dependence on IFS-supervised quadruplets via cycle-consistency, pseudo-labeling, and domain adaptation to learn from large-scale unlabeled real videos.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02256" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02256" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Asynchronous policy conflicts across VAR timesteps due to drastic fluctuations in parallel token-grid sizes, causing unstable RL training, slow convergence, and suboptimal alignment.<br>â€¢ Existing GRPO/bandit-style RL assumes homogeneous, sequential actions with terminal-only rewards, leading to poor spatiotemporal credit assignment, gradient imbalance (high-resolution steps dominate), and interference across tokens in VAR.<br>â€¢ Absence of a systematic RL recipe for text-to-image VAR, where RL operates with far fewer samples than pretraining, exacerbating instability and hindering robust alignment on text rendering tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Enhance GRPO for VAR with three components: (1) Value as Middle Return (VMR) to inject structure-preserving intermediate rewards and provide low-variance guidance to early steps; (2) Per-Action Normalization Weighting (PANW) to reweight per-step losses by inverse token-grid size, balancing gradients across scales; and (3) Mask Propagation (MP) to gate rewards and gradients to causally relevant spatiotemporal regions, improving credit assignment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Optimal Intermediate Rewards for Hierarchical VAR RL: Automatically place and calibrate intermediate returns (VMR) via meta-learning to minimize variance while preserving family-optimality.<br>â€¢ Neural Credit Maps: End-to-End Learned Mask Propagation for VAR Alignment: Replace heuristic MP with learnable modules that predict spatiotemporal credit masks using supervision from reward gradients and causal attribution.<br>â€¢ Token-Count-Aware RL: Theory and Algorithms for Heterogeneous Parallel Action Spaces: Develop generalized weighting and KL regularization schemes beyond PANW with formal guarantees for variable-size, parallel action sets across AR, diffusion, and VAR.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">GARDO: Reinforcing Diffusion Models without Reward Hacking</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24138" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24138" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ RL fine-tuning of diffusion models relies on imperfect proxy rewards (model-based or rule-based), leading to reward hacking where proxy scores rise but visual quality drops<br>â€¢ KL regularization to a static reference policy curbs hacking but harms sample efficiency as the penalty can dominate updates and constrain learning<br>â€¢ Static reference models are suboptimal and impede exploration, preventing discovery of genuinely better behaviors<br>â€¢ RL fine-tuning often collapses diversity (mode collapse), reducing coverage of prompts and visual modes</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>GARDO introduces gated KL regularization that penalizes only high-uncertainty samples (identified via reward-model ensemble disagreement), an adaptive reference policy that periodically updates to the current policy to preserve exploration and efficiency, and diversity-aware optimization that amplifies advantages for high-quality, diverse samples without destabilizing training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Beyond-GARDO: Bayesian Reward Uncertainty Estimation for Robust Diffusion RL: Replace ensemble-based disagreement with calibrated Bayesian/MC-dropout uncertainty to improve gating precision and reduce compute<br>â€¢ Learning to Gate: Meta-Learned Regularization Schedulers for Reward-Hacking Mitigation: Train a small controller to learn when and how much to regularize per-sample based on features of prompts, latents, and reward signals<br>â€¢ Theory of Selective KL: Convergence and Safety Guarantees for Gated Regularization in Policy-Gradient Diffusion: Provide theoretical analysis of stability, bias, and sample complexity under selective KL and adaptive anchors<br>â€¢ Distributionally Robust Diffusion RL with OOD-Aware Rewards: Integrate OOD detection and distributionally robust objectives to prevent reward exploitation under distribution shift beyond ensemble disagreement<br>â€¢ Cross-Modal GARDO: Adaptive Regularization for Video and Audio Generative Models: Extend the gated/adaptive/diversity framework to spatiotemporal and audio domains with task-specific diversity metrics<br>â€¢ Human-in-the-Loop Adaptive Anchors: Updating Reference Policies via Active Preference Queries: Periodically adapt the reference model using targeted human preference data where reward uncertainty is highest</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">VINO: A Unified Visual Generator with Interleaved OmniModal Context</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02358" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02358" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Visual generation pipelines are fragmented, with separate models for text-to-image, text-to-video, and editing, hindering scalability and unified deployment.<br>â€¢ Heterogeneous instruction formats (long descriptive captions vs short imperative edits) are hard to reconcile within one generator, leading to unstable training and inconsistent outputs.<br>â€¢ Current models struggle to reliably disentangle and prioritize multimodal signals (text, images, videos), causing semantic conflicts, inconsistent conditioning, and weak instruction following.<br>â€¢ Identity preservation and attribute leakage remain problematic in complex, multi-reference scenarios across both static (images) and dynamic (videos) content.<br>â€¢ Multimodal LLMs provide unified perception but still depend on external diffusion decoders, lacking an end-to-end unified high-resolution visual generator.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>VINO couples a visionâ€“language model with a Multimodal Diffusion Transformer, using interleaved conditioning tokens, learnable query tokens, and a token-boundary mechanism that ties VLM semantics to injected VAE latents for consistent multi-reference grounding and stable multimodal conditioning. A progressive training curriculum expands a video base model to a unified multi-task generator, aligning on long captions before mixing short instructions to enable both image and video generation and editing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Interleaved Token-Routing for Prioritized Multimodal Conditioning in Unified Generators: Design adaptive routing/attention policies that dynamically weight text, image, and video cues to resolve conflicts and improve instruction fidelity.<br>â€¢ Curriculum Learning for Unified Visual Editing: From Narratives to Atomic Edits: Explore staged curricula and task mixtures that transition from long-form captions to fine-grained edits, optimizing learnable query tokens for precise local modifications.<br>â€¢ OmniModal Boundary Tokens for 3D and Audio-Visual Generation: Extend the token-boundary mechanism to 3D and audio streams, enabling coherent identity and attribute preservation across image, video, 3D, and audio modalities.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02281" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02281" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Enabling infinite-horizon, online 3D geometry understanding without GPU memory overflow, which is critical for AR, robotics, and embodied AI but infeasible with batch/offline models.<br>â€¢ Avoiding catastrophic drift and forgetting in long sequences caused by compressing history into small hidden states in RNN-style streaming methods.<br>â€¢ Preventing unbounded KV cache growth in causal transformer-based streaming models, which leads to prohibitive memory and latency.<br>â€¢ Designing pruning that remains compatible with FlashAttention, since attention-weight-based importance requires materializing the attention matrix and breaks hardware-optimized kernels.<br>â€¢ Exploiting massive token redundancy in contiguous trajectories by selectively retaining truly informative historical context.<br>â€¢ Filling the evaluation gap with a continuous, ultra-long benchmark to rigorously test stability over thousands of frames.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>InfiniteVGGT introduces a training-free, FlashAttention-compatible rolling memory for causal transformers that prunes the KV cache using key-space diversity via negative cosine similarity, preserves the first-frame anchor tokens, and applies layer/head-wise adaptive budgets with TopK retention to keep memory bounded while maintaining long-term context.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learned Diversity-Aware Memory Pruning for Streaming 3D Transformers: Replace heuristic cosine-similarity pruning with a learned importance predictor and differentiable budget scheduling trained end-to-end for improved stability and completeness.<br>â€¢ Multimodal Rolling Memory: Integrating RGB, LiDAR, and IMU for Infinite-Horizon 3D Perception: Extend the rolling KV cache across modalities with cross-attentive fusion to enhance robustness in challenging, long-duration environments.<br>â€¢ Long3D++: A Benchmark for Dynamic, Outdoor, and Million-Frame Continuous 3D Reconstruction: Scale Long3D to dynamic scenes and million-frame sequences with standardized protocols to stress-test drift, completeness, and real-time performance.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02346" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02346" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are 2times to 7times larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Test-time scaling (TTS) improves reasoning but is compute-intensive, especially when generating many parallel chains; there is a need to shift the accuracyâ€“cost frontier.<br>â€¢ Pretraining gains are slowing due to extreme compute and limited high-quality data; compact models require efficient SFT/RL to unlock reasoning capabilities.<br>â€¢ Existing SOTA reasoning systems often rely on larger (8Bâ€“32B) models with poor token/inference efficiency; practical workloads need fast, memory-efficient long-context inference under large parallel batches.<br>â€¢ RLVR training can be unstable (policy collapse, zero-advantage batches, long outputs) and SFT with heterogeneous sequence lengths suffers from data-parallel token imbalance, degrading optimization.<br>â€¢ Data curation/mixing (teacher diversity, difficulty-aware weighting, rollout multiplicity) is critical yet under-optimized, limiting transfer and generalization across domains.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Falcon-H1R-7B uses a hybrid Transformerâ€“Mamba architecture for fast, memory-efficient long-context inference, coupled with curated, math-dominant long-CoT SFT and GRPO-based RLVR enhanced by stability/efficiency tweaks (TIS, CE-on-positives, online backfill+cache, domain-specific rewards, balanced DP token normalization). It integrates DeepConf-based test-time scaling to dynamically prune low-confidence chains, improving accuracy while reducing token usage and compute.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Confidence-Calibrated Test-Time Scaling for Hybrid Reasoning Models: Learn per-query calibration to tune DeepConf pruning/aggregation, optimizing accuracyâ€“token trade-offs under varying difficulty and budgets.<br>â€¢ Balanced Token Normalization for Long-Context SFT: A theoretical and empirical study of data-parallel balanced per-token loss, with convergence analysis and extensions to multi-turn/tool-use training.<br>â€¢ Cache-Efficient GRPO-RLVR for Long Chains: Systematically evaluate online sampling/backfill with bounded generation caches to stabilize RL with very long responses, across math, code, and science domains.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Recursive Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24601" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24601" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLMs have limited context windows and exhibit context rot, causing rapid performance degradation as input length and task complexity increase.<br>â€¢ Many real-world, long-horizon tasks require dense access to 10M+ token inputsâ€”far beyond current model windowsâ€”without losing critical details.<br>â€¢ Existing long-context methods (summarization/compaction, retrieval/tool-use, generic recursive agents) either discard necessary information, are task-specific, or still require fitting inputs into the LMâ€™s window, limiting scalability and cost-effectiveness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Recursive Language Models (RLMs) treat the prompt as an external object in a Python REPL, letting the LM write code to peek, slice, and decompose the input and recursively call sub-LMs on targeted snippetsâ€”an out-of-core inference strategy that scales to arbitrarily long contexts. The root LM orchestrates programmatic inspection and recursive subcalls, avoiding feeding the entire prompt into the network while preserving a standard LM interface.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Information-Density-Aware Recursive Scheduling for RLMs: Learn policies that adapt chunking, caching, and recursion depth to task-specific information-density and complexity profiles.<br>â€¢ Learned Controllers for Programmatic Decomposition in RLMs: Train meta-controllers that generate optimal REPL code and subcall plans under accuracyâ€“latencyâ€“cost constraints.<br>â€¢ Secure and Verifiable RLM Sandboxes: Develop hardened execution environments with provenance tracking, attestations, and formal checks to mitigate prompt/code injection and ensure correctness of recursive computation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02356" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02356" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Precise, language-guided object-level geometric transformations (translate/rotate/resize) are hard; existing text-based editors mainly alter style/appearance and fail at spatial control.<br>â€¢ High-quality paired supervision for spatial manipulation is scarce and costly; SFT with pixel-level losses entangles objects and backgrounds, limiting geometry-aware learning.<br>â€¢ Drag-based 2D/3D approaches require manual handles or complex 3D lifting/rendering, demanding expertise and reducing fidelity, thus not user-friendly for natural-language editing.<br>â€¢ RL for diffusion editing is computationally heavy due to dense rollouts where many denoising steps are uninformative, hindering efficiency and scalability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Talk2Move uses GRPO-based reinforcement learning over diffusion trajectories with stochastic perturbations and object-centric spatial rewards to align translations, rotations, and scaling with text instructions, while off-policy step importance and active sampling (early exit) focus training on informative denoising steps. A data-efficient pipeline auto-generates imageâ€“instruction pairs and performs a small SFT cold start using synthesized videos/images on a QwenImageEdit backbone.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Talk2Move-3D: Language-Guided Geometric Object Transformation with Multi-View Consistency: Extend to 3D scenes and multi-view inputs, enforcing geometric and photometric consistency across views for object-level transforms.<br>â€¢ Self-Supervised Spatial Reward Learning for Text-Guided Editing: Learn object-centric spatial rewards from weak signals (cycle consistency, contrastive spatial relations, physics priors) to eliminate reliance on paired data and SFT cold start.<br>â€¢ Hierarchical GRPO for Multi-Object and Sequential Scene Rearrangement: Create a hierarchical RL planner to compose and schedule multiple object transformations from complex language instructions, modeling constraints and inter-object interactions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Confidence Estimation for LLMs in Multi-turn Interactions</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02179" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02179" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new "Hinter-Guesser" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Confidence estimation research overwhelmingly targets single-turn QA, leaving the dynamic, evolving nature of multi-turn dialogue underexplored<br>â€¢ Downstream agentic and human-in-the-loop systems need per-turn reliable confidence to decide when to ask clarifying questions, invoke tools, or commit to actions<br>â€¢ Existing methods lack calibration and monotonicity as context accumulates, and current benchmarks/metrics (episodic tasks, unnormalized ECE) misrepresent multi-turn confidence behavior<br>â€¢ Varying dialogue lengths and mixed information quality (real hints vs filler) complicate fair evaluation and mask whether confidence increases reflect true information gain</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces a controlled multi-turn evaluation framework with two desiderataâ€”per-turn calibration and monotonic confidenceâ€”operationalized via InfoECE (length-normalized ECE by information level) and Kendallâ€™s Ï„ for monotonicity, supported by the Hinterâ€“Guesser paradigm and adapted incremental QA datasets; it proposes P(SUFFICIENT), a logit-based probe that comparatively improves calibration and monotonicity and better distinguishes genuine information gains from conversational filler.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Monotonicity-Regularized Fine-Tuning for Multi-turn Confidence: Train LLMs with objectives that directly penalize non-monotonic confidence across information levels while optimizing per-level calibration<br>â€¢ Information-Gain-Aware Confidence Estimation in Dialogue: Model and incorporate turn-level information gain signals to adjust confidence, ensuring robustness to filler content and non-informative turns<br>â€¢ Contextual Post-hoc Calibration for Multi-turn LLMs: Develop calibration methods (e.g., temperature/Platt scaling) conditioned on dialogue context and normalized information levels to improve InfoECE without degrading Kendallâ€™s Ï„</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.01046" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.01046" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Causal attention in decoder-only LLMs creates information asymmetry, leaving early tokens unaware of later context and degrading semantic embeddings.<br>â€¢ Next-token prediction biases the final token toward generation rather than semantic compression, making mean/last-token pooling suboptimal.<br>â€¢ Existing training-free fixes rely on input-level modifications (prompting, input repetition, token prepending) that cause quadratic cost, lost-in-the-middle effects, or unpredictable out-of-vocabulary token behavior.<br>â€¢ Contrastive fine-tuning methods yield strong embeddings but require substantial compute and data, limiting scalability across rapidly evolving LLM backbones.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>KV-Embedding re-routes the final tokenâ€™s Key-Value states at selected layers as an internal prefix, granting all tokens access to a compressed global summary within a single forward pass; layers are chosen automatically via intrinsic dimensionality, and a prompt mitigates prediction bias.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive KV-Rerouting for Long-Context LLMs: Dynamically select rerouting anchors and prefix length based on sequence properties to improve performance on 8Kâ€“32K token contexts and streaming scenarios.<br>â€¢ Learning-Free Bidirectional Context via Multi-Anchor Prefix Synthesis: Combine KV states from multiple positions/heads to synthesize richer internal prefixes that approximate bidirectional context without any training.<br>â€¢ Cross-Architecture Intrinsic Dimensionality Maps for Model-Agnostic Embeddings: Systematically characterize intrinsic dimensionality across LLM families to refine universal layer selection rules and theoretical guarantees for training-free embeddings.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">CPPO: Contrastive Perception for Vision Language Policy Optimization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.00501" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.00501" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ VLMs underperform on multimodal reasoning because final-answer RL rewards do not disentangle perception (vision grounding) from reasoning, leading to suboptimal optimization.<br>â€¢ Existing perception-aware methods require forced separation of perception and reasoning with special tokens or external LLM judges/CoT supervision, introducing overhead, scalability issues, and reward hacking risks.<br>â€¢ Prior unsupervised approaches (e.g., PAPO) apply KL divergence uniformly across all tokens/rollouts, which is unbounded and can cause reward collapse, over-regularization of reasoning tokens, and reinforcement of wrong perception tokens.<br>â€¢ Lack of a scalable, unsupervised mechanism to reliably identify perception tokens within natural generation without disrupting the modelâ€™s reasoning flow.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>CPPO augments GRPO with entropy-based detection of perception tokens via entropy shifts under information-removing image perturbations, then applies a token-level InfoNCE Contrastive Perception Loss using the original image as anchor, an information-preserving perturbation as positive, and an information-removing perturbation as negative. The contrastive loss is gated to correct rollouts and applied only to detected perception tokens, avoiding extra models and CoT supervision.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Perturbation Curriculum for CPPO: Learn a curriculum that dynamically selects information-preserving/removing perturbations to maximize perception token signal and training stability.<br>â€¢ Adversarial Perception Token Discovery via Learned Perturbation Generators: Train a perturbation generator to produce targeted image corruptions that more precisely expose vision-dependent tokens and improve token selection robustness.<br>â€¢ Temporal CPPO: Contrastive Perception Optimization for Video-Language Reasoning: Extend CPPO to spatiotemporal inputs to disentangle perception across frames and optimize temporal grounding in video question answering.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02267" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02267" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Real-world HMR datasets provide imperfect SMPL/SMPL-X annotations derived from fitting, introducing systematic biases that limit model accuracy.<br>â€¢ Annotated multi-view data are scarce, causing multi-view methods to underperform and generalize poorly across datasets compared to single-view approaches.<br>â€¢ Synthetic data offer pixel-perfect supervision but suffer from a significant synthetic-to-real domain gap, especially for regression-based mesh recovery.<br>â€¢ Existing dense correspondence and diffusion-based HMR methods are largely single-view and deterministic, lacking multi-view geometric consistency, uncertainty quantification, and fine-grained handling of low-resolution hands/partial views.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DiffProxy trains a Stable Diffusion 2.1â€“based proxy generator (frozen UNet with trainable text/image/cross-modality and epipolar attentions plus T2I-Adapter and DINOv2 conditioning) on synthetic multi-view data to produce multi-view consistent, pixel-aligned SMPL-X dense proxies (segmentation and UV) with a hand refinement pass. It then fits SMPL-X via uncertainty-weighted reprojection optimization using test-time stochastic sampling to estimate per-pixel reliability, achieving zero-shot generalization from synthetic-only training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ TempoDiffProxy: Temporally Consistent Diffusion Proxies for Video-Based Human Mesh Recovery: Extend DiffProxy with temporal attention and motion priors to enforce cross-frame consistency and reduce jitter in dynamic, multi-view videos.<br>â€¢ MultiHuman-DiffProxy: Diffusion-Generated Dense Proxies for Multi-Person, Occlusion-Heavy Scenes: Generalize the framework to multi-subject settings with instance-aware correspondence, cross-view association, and occlusion reasoning.<br>â€¢ SelfTrain-DiffProxy: Uncertainty-Guided Real-World Adaptation via Pseudo-Labels: Use DiffProxyâ€™s per-pixel uncertainty to curate and weight pseudo-labels from unlabeled real images for self-training and domain adaptation without human annotations.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.01836" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.01836" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing safety evaluations focus on universal harms (e.g., toxicity, violence) and fail to assess organization-specific allowlist/denylist policy compliance.<br>â€¢ There is no standardized, reproducible protocol for measuring adherence to organization-defined rules; teams often rely on ad hoc, manual prompt checks.<br>â€¢ Fixed benchmarks cannot capture the diversity and continual evolution of policies across domains and organizations.<br>â€¢ Current LLMs exhibit a critical asymmetryâ€”high accuracy on allowed tasks but poor robustness in enforcing prohibitions, especially under adversarial promptsâ€”posing risks in regulated, high-stakes deployments.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>COMPASS takes an organizationâ€™s allowlist and denylist, automatically generates base and adversarial edge-case queries, runs them against the target chatbot, and uses an LLM judge to label responses as aligned or misaligned with policies. The framework stress-tests refusal behavior with adversarial transformations (e.g., jailbreak-style phrasing and indirect references) to measure routine compliance and robustness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Robust Denylist Enforcement via Adversarial Counterfactual Training: Use COMPASS-generated edge cases to train guardrails or fine-tune models that improve refusal consistency without harming allowlist performance.<br>â€¢ Dynamic Policy Tracking for Continual Organization-Specific Alignment: Develop methods that automatically ingest evolving policies and perform continual evaluation and adaptation to maintain compliance over time.<br>â€¢ Multi-Judge and Formal Criteria for Reliable Policy Compliance Assessment: Combine multi-LLM judges with formalized policy rules to reduce evaluation noise and improve the reliability and auditability of compliance judgments.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23035" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23035" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Pseudo-label drift from confirmation bias in semi-supervised remote sensing segmentation causes error accumulation and unstable optimization, especially under scarce labels.<br>â€¢ Existing GAN-, consistency-, and pseudo-label-based methods rely on self-generated supervision without strong external guidance, making them fragile in complex RS scenes and prone to reinforcing early mistakes.<br>â€¢ Homogeneous co-training/teacherâ€“student architectures produce highly correlated errors and struggle to break confirmation bias.<br>â€¢ RS segmentation demands both global semantic context and fine-grained local boundaries; single-prior models fail to balance these complementary cues.<br>â€¢ Pixel-level annotations are costly; effective use of abundant unlabeled RS data is essential.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Co2S is a heterogeneous dual-student framework that couples a CLIP-initialized ViT (global, language-aligned priors) with a DINOv3-initialized ViT (local, self-supervised priors), guided by explicit-implicit semantic co-guidance (text-embedding queries for CLIP, learnable queries for DINOv3) and trained with UniMatch-style dual-stream consistency. A confidence-aware, pixel-wise stability loss performs global-local collaborative fusion by letting the higher-confidence student supervise the other, mitigating pseudo-label drift and improving semantic correctness and boundary precision.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Open-Vocabulary Co-Guided Semi-Supervised RS Segmentation: Integrate adaptive prompt engineering and large language models to generalize Co2S beyond fixed taxonomies.<br>â€¢ Multi-Modal Co-Fusion for Semi-Supervised RS (RGBâ€“SARâ€“LiDAR): Extend the heterogeneous dual-student to fuse complementary priors across modalities with modality-aware stability arbitration.<br>â€¢ Active Co-Guidance: Confidence-Aware Query and Label Budget Allocation for RS Segmentation: Combine Co2S with active learning to select pixels/tiles and class queries that maximally reduce pseudo-label drift.<br>â€¢ Calibrated Semi-Supervised RS via Uncertainty-Aware Co-Fusion: Replace fixed thresholds with calibrated uncertainty modeling to adapt stability loss and improve pseudo-label reliability across domains.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.01426" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.01426" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of large, executable, and high-quality SWE datasets with validated trajectories; existing corpora miss realistic environments and are vulnerable to Git-history leakage, undermining genuine problem-solving.<br>â€¢ Existing training paradigms (mid-training, RL, hybrids) are compute- and infrastructure-heavy, hyperparameter-sensitive, and constrained by scarce executable instances, raising the question of how far a lightweight SFT-only approach can go.<br>â€¢ Standard SFT treats all tokens equally, inadvertently learning from erroneous actions; there is a need for error-aware supervision and curricula that avoid reinforcing mistakes.<br>â€¢ Test-time scaling strategies are underexplored: how to allocate compute between longer interaction horizons and parallel rollouts, and how to design effective verifiers for selection.<br>â€¢ Importance: Building reproducible, efficient, and high-performing SWE agents for repository-scale issue resolution demands long-horizon reasoning, robust tool use, and multi-turn interactionâ€”benefiting both research and open-source deployment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SWE-Lego is a lightweight SFT-only framework that combines a hybrid executable dataset (32k real + synthetic instances, 18k validated trajectories), refined SFT with step-level error masking and difficulty-based curriculum, and test-time scaling via sequential turns and parallel rollouts with generative verifiers. This recipe achieves state-of-the-art performance among open-source models of similar size on SWE-bench Verified, without Git-history hacks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Error-Aware SFT for SWE Agents: Develop fine-grained, dynamic error detection and masking using execution traces and semantic signals, improving beyond step-level masking.<br>â€¢ Learning to Scale at Test Time: Compute-Aware Parallelâ€“Sequential Scheduling for SWE Agents: Train policies that automatically allocate compute between interaction length and parallel rollouts, guided by learned generative verifiers.<br>â€¢ SWE-Lego++: Cross-Language Executable Corpus with Robust Anti-Leakage Guarantees: Expand the dataset beyond Python to multi-language repositories and strengthen automated detection/prevention of solution leakage (e.g., Git hacking), enabling broader generalization.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.01576" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.01576" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Evaluating novelty is critical yet challenging in peer review, as reviewers must assess submissions against a vast, rapidly evolving literature. This report presents OpenNovelty, an LLM-powered agentic system for transparent, evidence-based novelty analysis. The system operates through four phases: (1) extracting the core task and contribution claims to generate retrieval queries; (2) retrieving relevant prior work based on extracted queries via semantic search engine; (3) constructing a hierarchical taxonomy of core-task-related work and performing contribution-level full-text comparisons against each contribution; and (4) synthesizing all analyses into a structured novelty report with explicit citations and evidence snippets. Unlike naive LLM-based approaches, OpenNovelty grounds all assessments in retrieved real papers, ensuring verifiable judgments. We deploy our system on 500+ ICLR 2026 submissions with all reports publicly available on our website, and preliminary analysis suggests it can identify relevant prior work, including closely related papers that authors may overlook. OpenNovelty aims to empower the research community with a scalable tool that promotes fair, consistent, and evidence-backed peer review.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Reviewers must assess novelty against a vast, rapidly evolving literature, making accurate novelty evaluation time-consuming and error-prone.<br>â€¢ Peer review lacks transparent, evidence-backed novelty judgments, leading to inconsistency and limited scalability.<br>â€¢ Authors and reviewers may overlook closely related prior work, risking inflated novelty claims and unfair decisions.<br>â€¢ Existing LLM-based approaches often rely on unguided generation without grounding in real papers, resulting in unverifiable assessments.<br>â€¢ There is a need for standardized, reproducible novelty analysis with explicit citations and evidence snippets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>OpenNovelty is an LLM-powered agentic pipeline that extracts core task and contribution claims to form queries, retrieves relevant prior work via semantic search, builds a hierarchical taxonomy, performs contribution-level full-text comparisons, and synthesizes a structured novelty report with explicit citations and evidence. All judgments are grounded in retrieved papers, enabling verifiable, transparent assessments.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Benchmarking Verifiable Novelty Assessment: A Gold-Standard Dataset and Metrics for Evaluating Evidence-Grounded Novelty Judgments.<br>â€¢ Citation- and Graph-Aware Retrieval for Scholarly Novelty Agents: Integrating citation networks and knowledge graphs to improve coverage and relevance of prior work retrieval.<br>â€¢ Human-in-the-Loop OpenNovelty: Interactive Workflows for Authors and Reviewers to refine claims, queries, and judgments with structured evidence feedback.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.00863" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.00863" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of a physically grounded, reversible bridge between matter (vibration, hierarchy) and music; existing sonification is largely symbolic and nonâ€“structure-preserving.<br>â€¢ Need a generative principle that balances coherence and adaptability; current methods overvalue perfection or randomness and underuse selective imperfection (mid-defect, mid-entropy regimes).<br>â€¢ Conventional AI mostly interpolates within training distributions and lacks long-range coherence and human-like structural signatures; requires agentic, collective models to invent.<br>â€¢ Difficulty connecting "deep time" across scales (from femtosecond molecular motions to cultural evolution) on a single perceptual plane.<br>â€¢ De novo protein/material design demands representations that encode physics, hierarchy, and reversibility; many ML workflows lack such physical grounding.<br>â€¢ Absence of quantitative cross-domain metrics linking musical scale structure (entropy/defects) to material analogs (e.g., Hallâ€“Petch optimum).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Materiomusic: reversible, physics-grounded mappings that translate vibrational structures in matter (proteins, webs, flames) into musical architectures and back, preserving relational structure; operationalized by exhaustive 12-TET scale census to quantify selective imperfection and by agentic swarm-based AI that composes outputs with human-like network signatures (small-worldness, modular integration, long-range coherence).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Agentic Materiomusic for De Novo Protein Design Under Selective Imperfection: Develop swarm-based composition-to-sequence pipelines that intentionally introduce mid-level defects to optimize foldability and function.<br>â€¢ Entropyâ€“Defect Universality Beyond 12-TET: A Cross-Cultural and Microtonal Census: Test the mid-entropy, mid-defect corridor across non-Western and microtonal systems to assess universality of selective imperfection.<br>â€¢ Physics-Grounded Sonification for Structural Health Monitoring: Real-time, reversible sonification of hierarchical materials and architectures (e.g., webs, lattices) to detect tension shifts, defects, and fracture through listening.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21472" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21472" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multi-annotator medical image segmentation is an important research problem, but requires annotated datasets that are expensive to collect. Dermoscopic skin lesion imaging allows human experts and AI systems to observe morphological structures otherwise not discernable from regular clinical photographs. However, currently there are no large-scale publicly available multi-annotator skin lesion segmentation (SLS) datasets with annotator-labels for dermoscopic skin lesion imaging. We introduce ISIC MultiAnnot++, a large public multi-annotator skin lesion segmentation dataset for images from the ISIC Archive. The final dataset contains 17,684 segmentation masks spanning 14,967 dermoscopic images, where 2,394 dermoscopic images have 2-5 segmentations per image, making it the largest publicly available SLS dataset. Further, metadata about the segmentation, including the annotators' skill level and segmentation tool, is included, enabling research on topics such as annotator-specific preference modeling for segmentation and annotator metadata analysis. We provide an analysis on the characteristics of this dataset, curated data partitions, and consensus segmentation masks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of large-scale, publicly available multi-annotator dermoscopic skin lesion segmentation datasets with annotator labels; existing datasets are single-annotator or very small (e.g., ISIC 2019-Seg)<br>â€¢ Multi-annotator labels are expensive to collect yet crucial for studying annotator disagreement, consensus formation, and label noise, which impact model robustness and reliability<br>â€¢ Absence of annotator metadata (e.g., skill level, segmentation tool) limits analysis of annotator-specific biases and the development of annotator-aware models<br>â€¢ Need for curated data partitions and consensus masks to enable reproducible benchmarking and standardized evaluation in dermoscopic segmentation</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Curate and release IMA++: a large dermoscopic multi-annotator segmentation dataset from the ISIC Archive with 17,684 masks across 14,967 images, including 2â€“5 segmentations for 2,394 images. The dataset includes annotator labels and metadata (skill level, tool), curated splits, and consensus masks, alongside a characterization analysis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Annotator-Aware Segmentation: Modeling Skill and Tool Effects in Dermoscopic Labeling: Build models that condition on annotator metadata to predict annotator-specific and consensus masks, reducing bias and improving reliability<br>â€¢ Learning from Disagreement: Uncertainty and Consensus Estimation for Skin Lesion Segmentation: Leverage multi-annotator masks to quantify uncertainty, derive probabilistic boundaries, and optimize consensus aggregation strategies<br>â€¢ Benchmarking Foundation Models on IMA++: Multi-annotator Robustness and Generalization: Evaluate and fine-tune vision foundation models on IMA++ to assess sensitivity to label variability and improve generalization under annotator disagreement</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02315" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02315" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on https://github.com/Sk-2103/Prithvi-CAFE{Prithvi-CAFE Github}</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Geo-Foundation Models (GFMs) underperform the baseline U-Net on flood inundation mapping (e.g., Sen1Floods11), indicating difficulty capturing critical local nuances.<br>â€¢ Existing GFMs (notably Prithvi) are limited to six input spectral bands, restricting multi-channel/multi-modal fusion essential for flood mapping.<br>â€¢ Full fine-tuning of large GFMs is computationally costly; parameter-efficient adaptation is needed to make them practical.<br>â€¢ Current architectures lack effective fusion of long-range transformer context with fine-grained CNN spatial detail.<br>â€¢ There is a need to generalize GFMs to real-world datasets beyond benchmarks and improve held-out/geographically novel performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>CAFE splits inputs into complementary streams: a Prithvi transformer encoder fine-tuned via lightweight per-block adapters on six bands, and a parallel CNN residual branch enhanced by Convolutional Attention Modules for remaining channels; multi-level transformer features are aligned with FPN and fused with CNN outputs using a Multi-Scale Multi-Level Feature Attention Fusion module, followed by a UperNet decoder for segmentation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ CAFE-Temporal: Spatiotemporal Fusion for Flood Dynamics â€” Extend CAFE with temporal encoders (e.g., video transformers) to model flood evolution from SAR-optical time series and improve event generalization.<br>â€¢ CAFE-AutoChannel: Adaptive Channel Selection and Routing for Multimodal EO â€” Learn scene-dependent channel partitioning and dynamic routing into transformer vs. CNN branches to maximize complementary information.<br>â€¢ Uncertainty-aware CAFE: Probabilistic Flood Segmentation with Calibration â€” Integrate Bayesian layers or deep ensembles to quantify aleatoric/epistemic uncertainty and provide calibrated risk maps for decision support.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2601.02314" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2601.02314" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While Chain-of-Thought (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are faithful generative drivers of the model's output or merely post-hoc rationalizations. We introduce Project Ariadne, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs hard interventions (do-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the Causal Sensitivity (Ï†) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent Faithfulness Gap. We define and detect a widespread failure mode termed Causal Decoupling, where agents exhibit a violation density (Ï) of up to 0.77 in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as "Reasoning Theater" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Faithfulness Gap: Chain-of-Thought (CoT) traces often act as post-hoc narratives rather than causally driving model outputs.<br>â€¢ Limitations of existing evaluations: Plausibility and textual-similarity metrics cannot test causal linkage and miss â€œReasoning Theater.â€<br>â€¢ Safety risk in high-stakes use: Decisions may be governed by latent parametric priors while stated logic contradicts conclusions.<br>â€¢ Lack of formal auditing/metrics: No standard causal framework or metrics (e.g., Ï†, Ï) to quantify whether intermediate reasoning actually controls final answers.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Project Ariadne models the reasoning process as a Structural Causal Model and performs hard counterfactual do-interventions on intermediate steps (logic flips, fact reversals, premise negations), re-executes the agent, and computes Causal Sensitivity Ï† = 1 âˆ’ S(a, a*). It flags Causal Decoupling when strong interventions leave answers semantically unchanged and reports aggregate metrics (Expected Faithfulness, Violation Density) across tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Path-Specific Causal Audits for LLM Reasoning: Extend Ariadne to multi-node and path-specific interventions to map minimal causal sets that flip decisions and quantify pathway-level effects.<br>â€¢ Training LLM Agents for Causal Faithfulness with Ï•-Guided Optimization: Use the Causal Sensitivity score as a reward in RLHF/DPO to penalize decoupled traces and align reasoning with outputs.<br>â€¢ Do More Thinking or More Theater? Evaluating Test-Time Compute for Causal Faithfulness: Benchmark System-2/test-time compute models to test whether increased thinking reduces violation density (Ï) versus producing longer post-hoc explanations.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22877" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22877" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Text-to-image diffusion models may generate harmful or copyrighted content, motivating research on concept erasure. However, existing approaches primarily focus on erasing concepts from text prompts, overlooking other input modalities that are increasingly critical in real-world applications such as image editing and personalized generation. These modalities can become attack surfaces, where erased concepts re-emerge despite defenses. To bridge this gap, we introduce M-ErasureBench, a novel multimodal evaluation framework that systematically benchmarks concept erasure methods across three input modalities: text prompts, learned embeddings, and inverted latents. For the latter two, we evaluate both white-box and black-box access, yielding five evaluation scenarios. Our analysis shows that existing methods achieve strong erasure performance against text prompts but largely fail under learned embeddings and inverted latents, with Concept Reproduction Rate (CRR) exceeding 90% in the white-box setting. To address these vulnerabilities, we propose IRECE (Inference-time Robustness Enhancement for Concept Erasure), a plug-and-play module that localizes target concepts via cross-attention and perturbs the associated latents during denoising. Experiments demonstrate that IRECE consistently restores robustness, reducing CRR by up to 40% under the most challenging white-box latent inversion scenario, while preserving visual quality. To the best of our knowledge, M-ErasureBench provides the first comprehensive benchmark of concept erasure beyond text prompts. Together with IRECE, our benchmark offers practical safeguards for building more reliable protective generative models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing concept erasure methods focus on text prompts and overlook multimodal inputs (learned embeddings, inverted latents) commonly used in personalization and image editing.<br>â€¢ Erased concepts frequently re-emerge via non-textual pathways, with Concept Reproduction Rate exceeding 90% in white-box settings, indicating current approaches disrupt textâ€“image alignment rather than truly removing concepts.<br>â€¢ There is no comprehensive benchmark or retraining-free safeguard to assess and improve robustness of concept erasure against multimodal, white-/black-box attack surfaces in diffusion models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces M-ErasureBench, a multimodal evaluation framework that tests concept erasure across text prompts, learned embeddings, and inverted latents under white-/black-box access, and proposes IRECE, a plug-and-play inference-time module that localizes target concepts via cross-attention and selectively perturbs associated latents during denoising to restore robustness without retraining.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Towards Certified Multimodal Concept Erasure in Diffusion Models: Develop formal robustness guarantees and certification procedures that bound CRR across text, embedding, and latent inputs under white-/black-box adversaries.<br>â€¢ Unified Erasure Policies via Cross-Attention Attribution and Latent Control: Learn a generalizable, modality-agnostic erasure policy that combines attention-based localization with adaptive latent perturbations, balancing robustness and visual quality.<br>â€¢ Erasure-Aware Personalization Pipelines for Diffusion Models: Extend benchmarking and defenses to personalization methods (e.g., Textual Inversion, DreamBooth) by co-designing embedding learning and inference-time safeguards that respect erased concepts.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 8</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2026-01-07 02:28:29 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 8;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>