<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 18, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 18, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">P1: Mastering Physics Olympiads with Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13612" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13612" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Move LLMs from rubric-fitting puzzle solving to science-grade reasoning that stands up to physical reality, with physics as the sharpest test.<br>‚Ä¢ Open-source models lag behind closed-source systems on Olympiad-level physics; lack accessible high-performing physics reasoners.<br>‚Ä¢ Prevailing training paradigms favor supervised fine-tuning/prompting, which is insufficient for deep multi-step derivations and symbol-to-reality grounding in physics.<br>‚Ä¢ Absence of specialized agentic frameworks to scaffold complex physics problem-solving and coordinate tool use.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce P1, a family of open-source physics reasoning LLMs trained entirely via reinforcement learning on Olympiad-grade tasks; augment with PhysicsMinions, an agentic framework that orchestrates multi-step solution strategies and tool use to achieve state-of-the-art competition performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond Olympiads: Reinforcement Learning for Science-Grade Reasoning Across STEM: Extend RL-only training from physics to chemistry, engineering, and biology with grounded, task-specific benchmarks.<br>‚Ä¢ PhysicsMinions 2.0: A General Agentic Framework for Multi-Modal Scientific Reasoning: Integrate diagrams, data tables, and simulators into the agent, enabling tool-augmented, multimodal problem-solving.<br>‚Ä¢ From Text to Experiment: Benchmarks and Training Loops that Bind Symbolic Reasoning to Physical Reality: Create evaluation suites and RL environments where solutions are validated via numerical simulation or experimental feedback.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.12609" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.12609" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Scaling omnimodal LLMs efficiently without 1T+ token budgets, while maintaining strong understanding, reasoning, and generation across modalities.<br>‚Ä¢ Poor cross-modality spatio-temporal alignment in self-attention limits video and audiovisual reasoning in existing models.<br>‚Ä¢ Unstable and suboptimal multimodal RL/SFT pipelines (hallucination, weak reasoning); need progressive, balanced training and stable reinforcement methods.<br>‚Ä¢ Inefficient expert utilization and load imbalance in conventional MoE; need dynamic capacity and modality-aware routing to balance compute vs. capability.<br>‚Ä¢ Scarcity and mismatch of high-quality, balanced multimodal data and lacking mechanisms to let language condition image/speech generation in a unified model.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce a dynamic-capacity MoE with shared, modality-routed, and null experts plus Omni-Modality 3D RoPE to align tokens across space and time. Train via progressive cross-modal SFT enhanced by iterative GSPO-DPO reinforcement on ~75B curated multimodal tokens, with special image/speech generation tokens enabling text-conditioned synthesis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Streaming Omni-MoE: Latency-Aware Expert Routing for Real-Time Audio‚ÄìVisual‚ÄìText Interaction: Extend dynamic-capacity routing and 3D RoPE to streaming inputs with latency-aware scheduling and token-level expert activation.<br>‚Ä¢ 4D Omni-Alignment: Self-Supervised Positional Encoding for Frequency‚ÄìSpace‚ÄìTime Fusion: Generalize 3D RoPE to include audio frequency and learn cross-modality alignment via self-supervised objectives.<br>‚Ä¢ Multi-Objective GSPO-DPO: Safe and Controllable Reinforcement Learning for Omnimodal Generation: Combine GSPO-DPO with multi-objective constraints to improve safety, controllability, and reasoning across text, image, and speech outputs.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11793" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11793" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Open-source research agents lag behind proprietary systems in deep, tool-augmented research capability, leaving gaps in performance, transparency, and reproducibility.<br>‚Ä¢ Prior work largely scales only model size or context length; it neglects interaction depth, and isolated test-time chain scaling can degrade reasoning without environment feedback.<br>‚Ä¢ Many open-weight LLMs release weights without full agentic toolchains/frameworks, limiting end-to-end research workflows (planning, browsing, coding, synthesis).<br>‚Ä¢ Existing agents are constrained by short contexts and few tool calls, and by inefficient context management that retains all tool outputs, causing overflow and brittle long-horizon reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MiroThinker v1.0 adds interactive scaling as a third axis by training a ReAct-style agent with reinforcement learning to sustain deep agent‚Äìenvironment loops (up to 600 tool calls) within a 256K context, alongside model/context scaling (8B/30B/72B). It provides a modular tool suite (Linux sandbox for code/commands, file I/O, Google search, LLM-assisted scraping), plus recency-aware context retention and result truncation, trained on the MiroVerse dataset (MultiDocQA and agentic trajectory synthesis) to enable robust, evidence-grounded long-horizon research.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Interaction Scaling Laws for Research Agents: Systematically characterize how accuracy and efficiency scale with interaction depth across benchmarks and tools to guide resource allocation.<br>‚Ä¢ Dynamic Context Retention and External Memory for Long-Horizon Tool Use: Learn task-aware retention policies and integrate persistent memory modules beyond the context window for better fidelity and efficiency.<br>‚Ä¢ Safety-Aware Reinforcement Learning for Web-Scale Research Agents: Incorporate veracity checks, access controls, and adversarial training to ensure robust, secure, and leakage-resistant behavior over hundreds of interactions.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13254" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13254" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies "expert" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLM training and re-training to add/boost capabilities is compute- and time-intensive; a cheaper post-training path is needed.<br>‚Ä¢ Existing model souping typically uses uniform averaging and ad hoc candidate selection, which is suboptimal.<br>‚Ä¢ Benchmark categories often have low or negative inter-correlations; uniform soups dilute category-specific expertise instead of exploiting complementarity.<br>‚Ä¢ Models exhibit inconsistent performance across categories; a method is needed to improve robustness and consistency across tasks.<br>‚Ä¢ Prior automatic merging approaches are not category-aware for LLM benchmarks (e.g., tool use, math, multilingual) and leave performance on the table.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SoCE performs category-aware souping by identifying weakly correlated benchmark category clusters, selecting the top expert checkpoint per category from a candidate pool, and then computing a non-uniform weighted average of expert model weights. Weights are chosen via a grid search over the simplex to maximize aggregate benchmark performance, yielding robust gains over uniform soups and individual models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Category-Aware Soup Weights via Differentiable Validation Objectives: Replace grid search with end-to-end or black-box optimization to learn weights under multi-objective constraints and regularization.<br>‚Ä¢ Prompt-Conditioned Model Souping for Instance-Level Expert Routing: Make soup weights dynamic at inference time based on prompt/category detection to fuse experts per input.<br>‚Ä¢ Cross-Architecture Soups: Merging Heterogeneous LLMs with Representation Alignment: Extend SoCE to fuse different base architectures via layer-wise alignment/adapters.<br>‚Ä¢ Shapley-Guided Expert Selection for Model Souping: Use cooperative game-theoretic attributions to prioritize experts and set weights efficiently while preserving fairness and complementarity.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Part-X-MLLM: Part-aware 3D Multimodal Large Language Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13647" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13647" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Fragmented 3D understanding and editing workflows lack a unified, language-native interface; existing models struggle to produce executable, part-aware plans that can drive downstream geometry modules.<br>‚Ä¢ Many 3D MLLMs and generative methods rely on 2D/multi-view proxies and output unstructured text, resulting in weak grounding to RGB point clouds and limited control over parts for compositional generation and localized editing.<br>‚Ä¢ Tight coupling between symbolic planning and geometric synthesis reduces interoperability; a decoupled approach is needed so a single planner can consistently control diverse geometry engines and diffusion pipelines.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A native 3D multimodal LLM that, given an RGB point cloud and text, autoregressively emits a single structured token program (part-level bounding boxes, semantic descriptions, edit commands) in an executable grammar. A dual-encoder disentangles structure from semantics, and the resulting plan drives compatible geometry/diffusion backends for part-aware generation, grounded Q&A, and localized editing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Universal 3D Part Grammar: Learning Cross-Modal, Cross-Engine Executable Programs: Extend the grammar and training to meshes, voxels, NeRFs, and CAD, improving compatibility and compositional composure across geometry engines.<br>‚Ä¢ Real-Time Differentiable Part-Aware Editing: Integrating Planner with Differentiable Geometry Backends: Couple the symbolic planner with differentiable synthesis to enable gradient-based refinement and interactive, low-latency editing.<br>‚Ä¢ Self-Supervised Part-Centric Pretraining from Unlabeled RGB Point Clouds: Develop masked program modeling and contrastive objectives to learn part structure and semantics at scale without manual labels.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.09611" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.09611" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Sequential, autoregressive (AR) ‚Äúthinking-aware‚Äù pipelines suffer error propagation and semantic drift, which can paradoxically degrade image fidelity on complex instructions.<br>‚Ä¢ Existing benchmarks evaluate only the final image vs. the prompt, lacking measures of reasoning‚Äìimage output alignment, making failures hard to diagnose.<br>‚Ä¢ Standard SFT/RL optimize final outcomes only, providing coarse supervision that cannot enforce stepwise, trajectory-level cross-modal consistency.<br>‚Ä¢ Multimodal models struggle with complex, knowledge-intensive edits/generation, leading to hallucinations and misidentification when reasoning is vague or misaligned.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MMaDA-Parallel is a purely discrete diffusion-based multimodal framework that generates text and images in parallel with bidirectional attention at every denoising step to prevent AR error propagation and enforce cross-modal grounding. It is trained via supervised fine-tuning on thinking-aware quadruplets and further optimized with Parallel Reinforcement Learning (ParaRL), which applies semantic rewards along the trajectory to strengthen reasoning‚Äìimage alignment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive ParaRL: Self-supervised Trajectory Rewards for Parallel Multimodal Diffusion: Learn reward models and confidence-weighted stepwise signals without labeled reasoning traces to improve alignment.<br>‚Ä¢ Knowledge-Grounded Parallel Diffusion: Integrating Structured World Models into Thinking-Aware Editing: Fuse knowledge graphs or retrieval into the parallel denoising process to stabilize reasoning on complex, factual instructions.<br>‚Ä¢ Beyond Images: Video and 3D Thinking-Aware Parallel Diffusion for Temporally Consistent Editing: Extend parallel text‚Äìvisual diffusion and trajectory-level alignment to video/3D, enforcing cross-frame semantic consistency and reasoning continuity.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11653" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11653" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ RAG pipelines critically depend on rerankers to filter coarse retrieval results, but current LLM-based reranking paradigms have structural flaws.<br>‚Ä¢ Pointwise reranking suffers from the Ranking Myopia Trap‚Äîno global context or inter-document comparison‚Äîleading to suboptimal global orders.<br>‚Ä¢ Listwise reranking faces List Rigidity‚Äîfixed-length inputs, context window limits, poor scalability to large candidate sets, and higher latency (often via sliding windows).<br>‚Ä¢ Pairwise approaches capture relative order but incur O(N^2) complexity, making them impractical at scale.<br>‚Ä¢ High-quality labeled data for training LLM rerankers is scarce; existing supervision often lacks both absolute score fidelity and globally consistent rankings.<br>‚Ä¢ Training objectives often misalign with ranking metrics and ignore cross-group score distribution consistency, limiting robustness and generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>GroupRank introduces groupwise scoring: the model jointly ingests a query and a small group of candidate documents, performs within-group comparisons, and outputs per-document relevance scores‚Äîcombining pointwise flexibility with listwise comparative awareness. It is trained with GRPO using a heterogeneous reward that blends ranking metrics (e.g., NDCG, Recall) and a distributional term to align score distributions across groups, and leverages a synthetic data pipeline that fuses LLM-based pointwise scoring and listwise ranking to create high-quality supervision for both retriever and reranker.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive GroupRank: Dynamic Group Partitioning for Scalable RAG Reranking: Learn to choose group sizes and partitioning strategies per query to optimize the accuracy‚Äìlatency trade-off and maximize comparative signal.<br>‚Ä¢ Calibrated Heterogeneous Rewards for Groupwise RL Reranking: Incorporate uncertainty calibration, exposure fairness, and diversity-aware terms into the reward to improve robustness and cross-group score alignment.<br>‚Ä¢ End-to-End Retriever‚ÄìGroupRank Co-Optimization with Synthetic-to-Real Transfer: Jointly train retriever and groupwise reranker with iterative synthetic data refinement and online feedback to bridge domain shift and enhance generalization.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13704" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13704" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing video generation benchmarks emphasize visual fidelity, temporal smoothness, and prompt adherence but lack rigorous evaluation of higher-order reasoning.<br>‚Ä¢ It remains unclear whether image-to-video (I2V) models can perform multi-step, chain-of-frames reasoning comparable to LLMs across structural, spatial, symbolic, and action-planning tasks.<br>‚Ä¢ There is no standardized, diverse, and hierarchical task suite (with difficulty levels) to systematically assess and compare video reasoning capabilities.<br>‚Ä¢ Open-source models show latent reasoning potential but are constrained by limited training scale and data diversity, impeding fair assessment and progress.<br>‚Ä¢ Enhancing reasoning typically requires costly data collection and supervised finetuning; lightweight, training-free test-time methods to boost reasoning are missing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce TiViBench, a hierarchical I2V benchmark spanning four reasoning dimensions (24 tasks, three difficulty levels) to systematically evaluate chain-of-frames reasoning. Propose VideoTPO, a training-free test-time preference optimization that uses multi-pass generation plus LLM self-analysis to refine prompts and select better candidates, improving reasoning without extra training, data, or reward models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ TiViRM: Learning Reward Models for Video Reasoning on TiViBench: Train video-specific evaluators from human/synthetic preferences to enable RLHF/DPO-style optimization of I2V reasoning.<br>‚Ä¢ Think-in-Video Scaling Laws: Model, Data, and Compute Trade-offs on TiViBench: Empirically quantify how scaling factors and data diversity affect reasoning across the four dimensions and 24 tasks.<br>‚Ä¢ CoT-to-CoF Reasoners: Integrating Language Chains-of-Thought with Chain-of-Frames Video Planning: Couple LLM planning with I2V generation in a closed loop, using textual rationales to guide stepwise frame synthesis and action execution.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13648" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13648" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Most 3D generation focuses on visual geometry/appearance and omits key physical and articulation attributes (e.g., density, absolute scale, joint limits), making assets unusable in physics simulators.<br>‚Ä¢ Retrieval-based articulated methods attach motions to existing meshes, limiting novelty and generalization to in-the-wild images and providing incomplete articulation info.<br>‚Ä¢ Prior physical/deformation works often assume homogeneous materials or omit essential physical parameters, reducing realism.<br>‚Ä¢ Existing physical 3D generators (e.g., PhysXGen) do not produce plug-and-play assets for standard simulators (URDF/XML), hindering downstream robotics/embodied AI use.<br>‚Ä¢ VLM token budgets make explicit geometry prediction impractical; prior compressions introduce special tokens/new tokenizers, complicating fine-tuning and deployment.<br>‚Ä¢ Physical 3D datasets lack diversity and comprehensive annotations, restricting generalization across everyday object categories.<br>‚Ä¢ There is growing demand for sim-ready assets that enable contact-rich robotic policy learning and interactive simulation directly from single real-world images.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>PhysX-Anything uses a unified VLM (Qwen2.5) with a multi-round dialogue to predict global physical/kinematic descriptions and per-part coarse geometry encoded by a 32^3 voxel run-length representation that compresses geometry tokens by 193√ó without special tokens. A controllable flow transformer refines fine-grained geometry, and a format decoder outputs part-level meshes plus URDF/XML for plug-and-play simulation; training leverages the newly curated PhysX-Mobility dataset.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond-32^3: High-Resolution Token-Efficient Geometry for VLM-Based Physical 3D Generation: Hierarchical, sparse tokenization to scale geometric fidelity while staying within VLM token budgets.<br>‚Ä¢ Video-to-Sim: Learning Physical Attributes and Articulations from In-the-Wild Video: Use multi-view/motion cues to recover absolute scale, materials, and joint parameters for sim-ready assets.<br>‚Ä¢ PhysX-Anything-Soft: Generating Deformable, Multi-Material Sim-Ready 3D Assets: Extend to soft bodies and heterogeneous materials with validated FEM/mass‚Äìspring parameters.<br>‚Ä¢ SceneX-Anything: Single-Image Generation of Multi-Object, Interacting Scenes with Consistent Physics: Produce URDF scene graphs with inter-object constraints and contact properties.<br>‚Ä¢ AutoPhys-Calib: Uncertainty-Aware Calibration of Physical Parameters via Simulation-in-the-Loop: Quantify and reduce uncertainty by optimizing against behavioral metrics in simulators.<br>‚Ä¢ Robot-in-the-Loop Asset Generation: Closed-Loop Co-Training of Generators and Control Policies Using Simulation Feedback: Jointly improve asset realism and task performance through RL-driven guidance.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">UFO^3: Weaving the Digital Agent Galaxy</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11332" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11332" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO^3, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO^3 models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence. We evaluate UFO^3 on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO^3 achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO^3 achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Single-device agent confinement leads to siloed intelligence and blocks use of heterogeneous resources (GPU servers, desktop apps, mobile sensors).<br>‚Ä¢ Lack of reliable, low-latency cross-device coordination makes workflows brittle, manual, and latency-prone under network variability.<br>‚Ä¢ Absence of dynamic, adaptive workflow modeling prevents asynchronous parallelism and responsive fault recovery; agents default to sequential execution.<br>‚Ä¢ Difficulty safely extending and integrating new device agents across OS/hardware due to missing unified protocol and concurrency/safety guarantees.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>UFO3 models each request as a mutable distributed DAG (TaskConstellation) whose subtasks are scheduled asynchronously across heterogeneous devices by a Constellation Orchestrator with safe assignment, consistency checks, and dynamic graph rewrites as results stream. A unified Agent Interaction Protocol (AIP) over persistent channels and template-driven MCP-enabled device agents provide reliable dispatch/streaming, capability discovery, and easy integration for resilient cross-device execution.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning-Guided Constellation Optimization for Cross-Device Agents: Train scheduling and graph-rewrite policies that minimize latency and failures by predicting device capabilities and network conditions.<br>‚Ä¢ Privacy- and Policy-Aware AIP for Enterprise Agent Fabrics: Extend AIP with formal data-access policies, auditability, and differential privacy to orchestrate sensitive workflows across organizational boundaries.<br>‚Ä¢ Formal Verification of Dynamic DAG Orchestration at Scale: Develop compositional proofs and runtime monitors to guarantee safety, liveness, and consistency under concurrent edits and failures in large constellations.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.12710" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.12710" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce EvoSynth, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing automated red-teaming largely selects, combines, or refines known strategies and prompts, lacking the ability to autonomously invent new attack mechanisms.<br>‚Ä¢ Prompt-level search/optimization struggles as defenses harden, failing to uncover diverse, higher-complexity vulnerabilities.<br>‚Ä¢ Most systems adjust text prompts rather than the underlying attack logic, missing code-level self-correction and evolution.<br>‚Ä¢ There is a need for realistic black-box evaluation that exercises full commercial safety stacks (input/output filters) via public APIs.<br>‚Ä¢ Benchmarking is fragmented; a unified, strong baseline suite is needed to reliably measure progress and diversity of attacks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>EvoSynth is a multi-agent, black-box evolutionary framework that synthesizes executable, code-based jailbreak algorithms and iteratively self-corrects them at the code level using feedback from a judge and the target model. A Reconnaissance Agent proposes attack categories/concepts, an Algorithm Creation Agent generates and evolves attack code until validated, and an Exploitation Agent selects and executes algorithms via a contextual bandit policy to maximize success.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Co-Evolutionary Defense Synthesis Against EvoSynth-Style Attacks: Automate the generation and evaluation of adaptive defenses in a black-box arms race with evolving code-based attacks.<br>‚Ä¢ Interpretable Evolution of Code-Based Jailbreak Algorithms: Derive mechanistic taxonomies and causal explanations for evolved attack programs to improve auditing and safety insights.<br>‚Ä¢ Transferability and Robustness of Evolved Jailbreak Methods Across Safety Stacks: Systematically study cross-model/generalization properties and design attack-agnostic mitigations.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Back to Basics: Let Denoising Generative Models Denoise</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13720" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13720" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Today's denoising diffusion models do not "denoise" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than "Just image Transformers", or JiT, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Modern diffusion models commonly predict noise or velocity instead of clean images, misaligning with the original denoising objective and potentially hindering performance.<br>‚Ä¢ Under the manifold assumption, clean images lie on a low-dimensional manifold while noise/velocity are off-manifold; predicting off-manifold targets demands high capacity and can fail in high-dimensional settings (e.g., high-resolution generation).<br>‚Ä¢ Prior work treats x/œµ/v predictions as equivalent via loss reweighting, overlooking that the choice of prediction target fundamentally changes the learning problem and network capacity needs.<br>‚Ä¢ Existing Transformer-based generative pipelines often rely on tokenizers, pre-training, or extra losses; there is a need for a self-contained, simple approach that works directly on raw pixels.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Adopt direct clean-image (x) prediction with appropriately reformulated loss weighting, and implement a simple, large-patch, pixel-level Transformer (‚ÄúJust image Transformers‚Äù, JiT) for diffusion that uses no tokenizer, no pre-training, and no extra losses.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Manifold-Aware Diffusion Training: Capacity-adaptive schedules and architectures that explicitly exploit low-dimensional manifolds to stabilize high-resolution generation.<br>‚Ä¢ Beyond Images: Let Denoising Generative Models Denoise in Audio and Video: Extending x-prediction and JiT to raw audio/video to test manifold-based denoising across modalities.<br>‚Ä¢ Theory of Target Selection in Diffusion: Capacity and Generalization Trade-offs Between x-, œµ-, and v-Prediction: Formal analysis of when each target is optimal under manifold and capacity constraints.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13655" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13655" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at https://github.com/allenai/olmoearth_pretrain{https://github.com/allenai/olmoearth_pretrain}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Earth observation data is spatial, temporal, and highly multimodal, making random-masking SSL tasks too easy due to redundancy across space, time, and modalities.<br>‚Ä¢ Existing self-supervised paradigms trade off stability and representation quality: MAE is stable but yields weaker features, while latent MIM/I-JEPA often suffer training instability and representation collapse.<br>‚Ä¢ Contrastive reconstruction across all tokens introduces many "easy" negatives because tokens come from different modalities/distributions, diluting the training signal.<br>‚Ä¢ Foundation models are complex and expensive to train/deploy, slowing adoption for real-world nonprofit and environmental applications.<br>‚Ä¢ Evaluation practices are inconsistent across remote sensing models, hindering fair comparison and objective progress tracking.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>OlmoEarth uses a ViT encoder‚Äìdecoder trained with Latent MIM Lite: fixed random linear projections define latent targets for masked patches, combined with modality-aware masking (bandset-level encode/decode selection) and bandset-only patch discrimination to stabilize training and improve representations. A pooled instance contrastive loss aligns global embeddings across modalities/timesteps, and supervised map layers are integrated as decode-only targets in the same latent token space.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning-Stable Latent Targets for Multimodal EO: From Frozen Random Projections to Structured Dictionaries: Replace fixed random projections with learned, orthogonal or sparse dictionaries under stability constraints to further boost representation quality while preventing collapse.<br>‚Ä¢ Adaptive Modality-Aware Masking for Redundant Spatio-Temporal Data: Learn a sample- and modality-conditioned masking policy (e.g., via RL or meta-learning) that targets redundancy and cloud/seasonal patterns to maximize task difficulty and efficiency.<br>‚Ä¢ Continual OlmoEarth: Streaming Pretraining and Deployment for Real-Time Earth Observation: Develop lifelong, streaming pretraining with active learning and drift detection to keep models up-to-date and robust for NGO operations under evolving landscapes and labels.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13646" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13646" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-G√∂del Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Fixed agent scaffolds and static action spaces limit adaptability; manually engineering optimal scaffolds is infeasible given the vast design space.<br>‚Ä¢ Existing self-improving agents rely on costly offline evolution (e.g., DGM ~ $22k/run) and often overfit to specific benchmarks/LLMs, reducing generalization.<br>‚Ä¢ There is a need for a live, low-overhead, LLM-agnostic mechanism that can create task-specific tools during runtime to improve effectiveness without test-time scaling or extra pipelines.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Live-SWE-agent starts from a minimal bash-only scaffold and uses a step-reflection prompt to decide, at each step, whether to create, revise, and execute custom tools (executable scripts) as first-class actions during issue solving. This live, on-the-fly self-evolution is LLM-agnostic, requires no offline training, and interleaves tool refinement with problem-solving.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond Tools: Live Self-Evolution of Agent Policies and Scaffolds: Extend on-the-fly evolution to prompts, workflows, memory modules, and decision policies, and study generalization across tasks and LLMs.<br>‚Ä¢ Tool Memory: Transfer and Retrieval of Live‚ÄëSynthesized Tools Across SWE Tasks: Build a persistent tool store with retrieval/adaptation so agents can reuse, specialize, and compose previously created tools to cut cost and latency.<br>‚Ä¢ Safe Self‚ÄëModification: Formal Verification and Sandboxing for Live Evolving SWE Agents: Integrate static/dynamic analysis, capability controls, and formal checks to ensure synthesized tools are correct, secure, and do not harm the environment.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Genomic Next-Token Predictors are In-Context Learners</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.12797" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.12797" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training? To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Determine whether emergent in-context learning (ICL) is unique to human language or a modality-agnostic consequence of large-scale next-token predictive training.<br>‚Ä¢ Create a controlled, cross-domain evaluation framework that fairly compares ICL across linguistic and genomic sequences under a limited four-token vocabulary, avoiding representational artifacts.<br>‚Ä¢ Address the lack of large, autoregressive genomic models and prior reliance on meta-ICL (explicit in-context objectives), by testing organically emergent ICL in a state-of-the-art genomic model (Evo2).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Design parallel bitstring program-synthesis tasks and render them in two modality-specific encodings (genomic nucleotides and linguistic digits), then evaluate Evo2 and Qwen3 under matched k-shot in-context prompts with greedy decoding and exact-match accuracy to analyze scaling trends in ICL.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Scaling Laws for Emergent In-Context Learning in Genomic Transformers: Quantify how model size, data scale/diversity, and context length influence ICL performance and compare scaling exponents to language models.<br>‚Ä¢ Mechanistic Circuits of Genomic ICL: Probing Pattern Induction in Evo2: Use mechanistic interpretability to identify attention pathways and subcircuits that implement copying, local transforms, and global statistics in genomic ICL.<br>‚Ä¢ Unified Cross-Modality ICL Benchmarks Beyond Nucleotide Alphabets: Extend the framework to proteins and other biological or symbolic sequences with larger alphabets to test generality, robustness, and transfer of ICL across modalities.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.12997" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.12997" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Web agents lack persistent cross-session memory, so they cannot learn from past successes and failures across tasks.<br>‚Ä¢ Agents exhibit repetitive errors (e.g., revisiting links, stalling at login gates, triggering CAPTCHAs), wasting steps and reducing robustness and sample efficiency.<br>‚Ä¢ Existing fixes (backtracking, progress rewards) focus on single-episode mitigation and do not provide long-term episodic memory or reflection.<br>‚Ä¢ Limited LLM context windows prevent recalling prior trajectories; history compression and episodic replay are underused in web navigation.<br>‚Ä¢ Retraining or reinforcement fine-tuning is costly and slow; a model-agnostic, plug-in approach that improves without retraining is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>WebCoach augments any web agent with a retrieval-augmented coaching loop backed by persistent cross-session memory. It condenses trajectories into episodic summaries, stores them in an external memory, and at runtime retrieves relevant experiences (by similarity and recency) to inject targeted advice via hooks, enabling continual improvement without retraining.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning When and How to Coach: End-to-End Optimization of Intervention Policies for Memory-Augmented Web Agents: Train policies to decide optimal timing and content of coaching, balancing helpful guidance with minimal disruption.<br>‚Ä¢ Scalable Episodic Memory for Web Agents: Adaptive Condensation, Pruning, and Hierarchical Retrieval: Develop algorithms for memory abstraction, deduplication, and tiered retrieval to keep EMS efficient and effective at scale.<br>‚Ä¢ Federated and Personalized WebCoach: Shared Cross-Session Memory Across Agents with Conflict Resolution and Domain Adaptation: Enable multi-agent, cross-domain memory sharing while learning personalized advice and resolving conflicting experiences.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.09809" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.09809" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at https://github.com/kdafnis/STS.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Zero-shot VLMs degrade substantially under out-of-distribution (OOD) domain shifts, requiring per-sample, label-free adaptation.<br>‚Ä¢ Existing test-time prompt tuning (e.g., TPT) backpropagates through large encoders, incurring high latency and memory overhead during inference.<br>‚Ä¢ Parameter-efficient fine-tuning (e.g., LoRA/TTL) still modifies internal architectures, violating black-box constraints and limiting applicability to proprietary/fixed models.<br>‚Ä¢ Training-free, memory-bank methods assume well-distributed test streams, consume substantial memory, and can be brittle to nonstationary or biased arrival orders.<br>‚Ä¢ Prior latent-space methods (e.g., TPS) learn unconstrained per-class shifts that may be less semantically grounded and can underperform prompt tuning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Spectrum-Aware Test-Time Steering (STS) computes an SVD of initial text prototypes to form a low-dimensional semantic basis and learns, per test sample, a small coefficient vector that induces a shared shift of all class prototypes in that subspace. The coefficients are optimized by minimizing marginal entropy over confident augmented views, without backpropagating through frozen encoders, enabling lightweight, black-box adaptation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Nonlinear Spectrum-Aware Steering via Kernel or Hypernetwork Subspaces: Replace linear SVD subspaces with kernelized or learned nonlinear manifolds to capture complex domain shifts.<br>‚Ä¢ Joint Text-and-Visual Latent Steering for Test-Time Alignment in VLMs: Simultaneously steer text prototypes and visual embeddings to improve cross-modal alignment under shift.<br>‚Ä¢ Adaptive Rank and Online Basis Refinement for Subspace Steering: Learn the subspace rank and update the spectral basis episodically or across samples while preserving black-box operation.<br>‚Ä¢ Latent Augmentation for Encoder-Free Test-Time Adaptation: Generate diverse latent views to reduce or eliminate repeated image-encoder passes and cut compute.<br>‚Ä¢ Confidence-Calibrated Entropy Objectives for Robust Episodic TTA: Explore tempered entropy, margin-based, or uncertainty-aware objectives to stabilize adaptation on hard domains (e.g., satellite imagery).<br>‚Ä¢ Subspace Fusion from Prompt Ensembles for Robust Generalization: Construct a unified steering basis from multiple prompt templates/sources to broaden semantic coverage and improve OOD robustness.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13714" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13714" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only 6K unlabeled images and 0.02% additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over 11 benchmarks, UnSAMv2 improves NoC_{90} (5.69 rightarrow 4.75), 1-IoU (58.0 rightarrow 73.1), and AR_{1000} (49.6 rightarrow 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ SAM/SAM-2 lack explicit, continuous granularity control; a single point yields up to three discrete masks, forcing manual selection and extra prompts.<br>‚Ä¢ Supervised training on SA-1B ties objectness to human annotation bias, discourages hierarchical reasoning, and lacks explicit part‚Äìinstance correspondences; dense labels across granularities are costly.<br>‚Ä¢ Ambiguity arises when the same prompt matches multiple plausible scales (part vs. whole), reducing efficiency and interpretability; a data-driven, unsupervised way to discover and control granularity is needed across images and videos.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>UnSAMv2 constructs hierarchical pseudo-labels from unlabeled images via MaskCut-based instance discovery and DINO feature‚Äìdriven bottom-up merging, then assigns each mask a continuous, relative granularity score. It augments SAM-2 with a lightweight Fourier-based granularity encoder and a granularity-aware mask token, enabling a single point plus scalar to produce the desired mask, trained by finetuning only the decoder (LoRA) on 6k unlabeled images.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Granularity-Controllable Video Segmentation with Learned Temporal Hierarchies: Extend UnSAMv2 with trainable memory and temporal objectives so granularity-conditioned masks remain consistent across frames.<br>‚Ä¢ Context-Relative Granularity Beyond Area: Learning Semantic and Topological Cues for Scale Control: Replace purely area-based granularity assignment with context-aware measures (edges, semantics, part‚Äìwhole graphs) to better align scales with scene structure.<br>‚Ä¢ Multimodal Granularity Control via Text and Depth Prompts for 2D/3D Segmentation: Fuse language and depth prompts with the granularity scalar to control part/instance segmentation across images and 3D scenes.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11407" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11407" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloom's level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of large-scale, high-quality microscopy multimodal datasets for training MLLMs, which limits scientific reasoning on microscopy data.<br>‚Ä¢ Existing benchmarks (e.g., MicroVQA with 1,042 questions and 255 images) are too small for supervised fine-tuning and mainly serve evaluation; broader suites like ¬µBench have shallow scientific depth.<br>‚Ä¢ Agent-generated supervision is prone to errors and hallucinations, with no robust mechanism to enforce cross-modal consistency among image, caption, and QA.<br>‚Ä¢ Microscopy MLLM work has focused on hardware interfacing rather than reasoning; datasets rarely align with research workflows (EU/HG/EP) or offer higher Bloom-level difficulty.<br>‚Ä¢ Need principled data construction and quality control to enable smaller open-source MLLMs (~4B) to reach competitive microscopy reasoning performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MicroVQA++ builds a large microscopy VQA corpus via a three-stage pipeline: bootstrap grounded QAs from expert-validated figure‚Äìcaption pairs in BIOMEDICA, filter inconsistencies using HiCQA-Graph that fuses NLI entailment, CLIP-based vision‚Äìlanguage alignment, and agent signals, then convert to MCQs with an MLLM agent and human screening to produce quality-controlled train/test splits.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ HiCQA-Graph++: End-to-end Cross-Modal Consistency Pretraining for Microscopy VQA ‚Äî Extend the heterogeneous image‚Äìcaption‚ÄìQA graph with learnable consistency scores and pretraining objectives to further reduce hallucinations and improve downstream reasoning.<br>‚Ä¢ Causal MicroVQA: Counterfactual and Interventional Data Augmentation for Microscopy Reasoning ‚Äî Generate and evaluate counterfactual images/captions and interventions to train MLLMs on causal, not just correlational, scientific reasoning.<br>‚Ä¢ Active MicroVQA Curation: Human-in-the-Loop Difficulty Calibration and Uncertainty-Guided Sampling ‚Äî Use model uncertainty and Bloom-level estimators to prioritize annotation and refine MCQs, improving dataset hardness and reliability efficiently.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Dynamic Reflections: Probing Video Representations with Text Alignment</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.02767" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.02767" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types. While significant progress has been made in aligning images with text, the temporal nature of video data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data provided at test time, especially when using state-of-the-art video encoders. We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations. Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to general-purpose video representation and understanding. Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data. Project page can be found at https://video-prh.github.io/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Cross-modal alignment research has largely focused on static images and text, leaving the temporal video domain underexplored.<br>‚Ä¢ Existing alignment scores on static modalities are low and hard to interpret, with uncertainty whether low values arise from model limits or impoverished test-time inputs.<br>‚Ä¢ There is no robust zero-shot probe for evaluating video encoders; current evaluations rely on costly, task-specific training.<br>‚Ä¢ The relationship between video‚Äìtext semantic alignment and performance on both semantic and non-semantic downstream tasks is unclear.<br>‚Ä¢ How temporal reasoning capabilities correlate with cross-modal alignment, and where models fail on hard negatives, remains insufficiently understood.<br>‚Ä¢ Lack of principles (scaling laws) to predict alignment improvements from richer test-time visual/text inputs without retraining.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Adapt mutual k-NN alignment to video‚Äìtext by comparing nearest-neighbor structures between video embeddings aggregated over multiple frames and text embeddings aggregated over multiple captions, selecting optimal intermediate layers. Propose parametric test-time scaling laws that predict alignment gains as the number of frames and captions increases, and validate across 121 video and language encoders while correlating alignment with downstream performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Test-Time Scaling Laws for Video‚ÄìText Alignment: Theory and Predictive Bounds: Derive analytical models and bounds relating mutual k-NN alignment to frames, captions, and dataset size; validate across datasets and encoders.<br>‚Ä¢ Zero-Shot Model Selection for Video Understanding via Alignment Probes: Build calibrated alignment-based scores that reliably predict performance on diverse semantic and non-semantic video tasks, enabling training-free model selection.<br>‚Ä¢ Temporal Hard-Negative Benchmarks for Evaluating Video‚ÄìLanguage Alignment and Reasoning: Construct challenging datasets with temporally perturbed or confounded clips to stress-test temporal reasoning and link alignment failures to specific temporal phenomena.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07577" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07577" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Centralized RAG architectures incur high data collection/integration/management costs and pose privacy/governance risks at scale.<br>‚Ä¢ Decentralized settings feature heterogeneous, often unreliable sources (outdated/misinformation/incomplete), which degrade retrieval accuracy and answer quality.<br>‚Ä¢ Existing decentralized RAG approaches commonly assume fully reliable sources or rely on complex training/validator protocols, lacking a lightweight inference-time mechanism to assess and prioritize source reliability.<br>‚Ä¢ Centralized score management creates a single point of failure and enables manipulation; transparent, tamper-proof reliability records are needed to maintain trust.<br>‚Ä¢ Systems must provide auditable, actionable feedback to data owners while preserving decentralization and scalability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes dRAG: a decentralized RAG system that computes sentence-level importance (via MC-Shapley or information-theoretic V-entropy/RORA) to update per-source reliability and usefulness, then uses usefulness for source sampling and reliability for reranking. Reliability scores and updates are secured with Ethereum smart contracts and ECDSA-signed, batched on-chain logs, providing verifiable, tamper-proof management without a central authority.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Reliability-Aware Reranking for Decentralized RAG under Partial Coverage: Learn a query-conditioned gate that decides when to apply reliability-based reranking versus pure relevance to handle non-overlapping, document-level pollution.<br>‚Ä¢ Convergence Analysis of Blockchain-Secured Source Reliability in dRAG: Theoretical and empirical study of how query diversity, source count/quality, and feedback regimes affect convergence speed and performance ceilings of reliability scores.<br>‚Ä¢ Zero-Knowledge Auditing for Privacy-Preserving Reliability Updates in Decentralized RAG: Integrate ZK proofs to validate query-state and contribution without revealing sensitive content, maintaining verifiability while enhancing privacy and reducing on-chain data.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">OpenUS: A Fully Open-Source Foundation Model for Ultrasound Image Analysis via Self-Adaptive Masked Contrastive Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.11510" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.11510" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Ultrasound (US) is one of the most widely used medical imaging modalities, thanks to its low cost, portability, real-time feedback, and absence of ionizing radiation. However, US image interpretation remains highly operator-dependent and varies significantly across anatomical regions, acquisition protocols, and device types. These variations, along with unique challenges such as speckle, low contrast, and limited standardized annotations, hinder the development of generalizable, label-efficient ultrasound AI models. In this paper, we propose OpenUS, the first reproducible, open-source ultrasound foundation model built on a large collection of public data. OpenUS employs a vision Mamba backbone, capturing both local and global long-range dependencies across the image. To extract rich features during pre-training, we introduce a novel self-adaptive masking framework that combines contrastive learning with masked image modeling. This strategy integrates the teacher's attention map with student reconstruction loss, adaptively refining clinically-relevant masking to enhance pre-training effectiveness. OpenUS also applies a dynamic learning schedule to progressively adjust the difficulty of the pre-training process. To develop the foundation model, we compile the largest to-date public ultrasound dataset comprising over 308K images from 42 publicly available datasets, covering diverse anatomical regions, institutions, imaging devices, and disease types. Our pre-trained OpenUS model can be easily adapted to specific downstream tasks by serving as a backbone for label-efficient fine-tuning. Code is available at https://github.com/XZheng0427/OpenUS.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of reproducible, open-source ultrasound (US) foundation models; prior works pre-train on proprietary data, hindering transparency and benchmarking.<br>‚Ä¢ US imaging exhibits strong domain shifts (operator dependence, probe/device/protocol variability), speckle noise, and low contrast, making generalization and label efficiency difficult.<br>‚Ä¢ Scarcity of standardized annotations in US limits supervised training; a label-efficient pre-training approach is needed.<br>‚Ä¢ Existing SSL methods have limitations: contrastive learning emphasizes global semantics over dense pixel details, while random/attention-only MIM can miss clinically relevant regions; natural-image FMs transfer poorly due to different imaging physics.<br>‚Ä¢ Need for a backbone that captures both local and global long-range dependencies tailored to US characteristics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>OpenUS introduces a global-local masked contrastive learning framework on a Vision Mamba backbone, coupled with a self-adaptive masking strategy that fuses teacher attention with student reconstruction loss via an Adaptive Learning Priority score and dynamic easy-to-hard scheduling. Pre-trained on 308K images from 42 public US datasets, it learns clinically relevant, dense representations and serves as a label-efficient backbone for downstream classification and segmentation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ OpenUS-Video: Temporal Self-Adaptive Masked Contrastive Learning for Ultrasound Sequences: Extend ALP-guided masking to video with temporal consistency and motion-aware reconstruction for cine US.<br>‚Ä¢ Domain-Adaptive OpenUS: Continual Self-Adaptive Masking Across Devices and Protocols: Integrate domain adaptation and continual learning to dynamically recalibrate masking and representations for cross-device/probe variability.<br>‚Ä¢ Clinically Guided OpenUS: Weak Labels and Probe Metadata in Self-Adaptive Masking: Incorporate weak supervision (landmarks, contours) and acquisition metadata to further focus masks on diagnostically salient regions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Instella: Fully Open Language Models with Stellar Performance</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.10628" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.10628" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Most high-performing LLMs are closed or only open-weight, withholding data and training recipes, which blocks transparency, reproducibility, and contamination auditing.<br>‚Ä¢ The community lacks a fully open, competitive 3B-parameter model that is efficient in token usage and accessible to researchers with modest compute.<br>‚Ä¢ There is a gap in transparent long-context models; real applications require 100K+ token windows, yet open recipes and data for such models are scarce.<br>‚Ä¢ Reasoning-focused small models with reinforcement learning are largely not reproducible due to proprietary datasets or undisclosed RL pipelines.<br>‚Ä¢ Standardized, openly released evaluation and training pipelines are needed to enable fair benchmarking and systematic study of data/training choices.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Instella is a fully open 3B transformer trained via two-stage pretraining (4.07T general-domain, then 57B reasoning-heavy with synthetic math), seed-averaged weight ensembling, and instruction alignment using 2.3M SFT examples plus DPO. Variants extend to 128K context via continued pretraining and synthetic long-context SFT (Instella-Long) and improve mathematical reasoning with multi-stage GRPO on open datasets (Instella-Math).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Open-Source Weight Ensembling Strategies for Robust Small LLMs: Systematically compare checkpoint averaging and advanced merge methods across seeds to quantify stability, generalization, and domain transfer.<br>‚Ä¢ Scaling Transparent Long-Context Modeling to 1M Tokens with Minimal Compute: Explore RoPE scaling, efficient attention, and curriculum schedules with fully open long-context datasets and recipes.<br>‚Ä¢ Beyond Math: Fully Open Reinforcement Learning for General-Purpose Reasoning at 3B Scale: Extend GRPO with open verifiers and multi-domain tasks (logic, coding, tool use) to study cross-domain reasoning emergence.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 8</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-18 23:07:04 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 8;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>