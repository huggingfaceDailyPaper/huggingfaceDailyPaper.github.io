<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 26, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 26, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.17592" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.17592" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve. Our system provides modular implementations of key components: MAP-Elites quality-diversity algorithms, asynchronous DAG-based evaluation pipelines, LLM-driven mutation operators with insight generation and bidirectional lineage tracking, and flexible multi-island evolutionary strategies. In order to assess reproducibility and validate our implementation we evaluate GigaEvo on challenging problems from the AlphaEvolve paper: Heilbronn triangle placement, circle packing in squares, and high-dimensional kissing numbers. The framework emphasizes modularity, concurrency, and ease of experimentation, enabling rapid prototyping through declarative configuration. We provide detailed descriptions of system architecture, implementation decisions, and experimental methodology to support further research in LLM driven evolutionary methods. The GigaEvo framework and all experimental code are available at https://github.com/AIRI-Institute/gigaevo-core.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Reproducibility gap in LLM-guided evolutionary frameworks like AlphaEvolve due to high-level descriptions and unspecified implementation details<br>â€¢ Need for a modular, concurrent, and extensible system to experiment with hybrid LLMâ€“evolutionary strategies at scale<br>â€¢ Practical challenges in reliable LLM-based code mutation (diff generation failures) and integrating rich context (insights, lineage) into mutation<br>â€¢ Requirement to validate and benchmark on hard optimization problems (e.g., geometry and sphere packing) while maintaining solution diversity via quality-diversity search</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>GigaEvo is a modular open-source framework that combines a Redis-backed program archive with an asyncio DAG execution pipeline, a MAP-Elites quality-diversity engine (single/multi-island with migration), and a LangGraph-based LLM mutation operator enriched by structured insights and bidirectional lineage; it supports rewrite/diff modes, multi-model routing, and Hydra-based declarative configuration with a directory-driven problem interface.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Multi-Island MAP-Elites for LLM-Guided Optimization: Design dynamic behavior spaces and migration policies that adjust to search progress to improve diversity and convergence<br>â€¢ Learning Mutation Policies: Reinforcement Fine-Tuning and Model Routing for Robust Rewrite/Diff Generation: Optimize LLM mutation strategies and routing across models to reduce failures and increase fitness gains<br>â€¢ Measuring the Value of Insight: Ablation and Causal Analysis of Bidirectional Lineage Tracking in Evolutionary Search: Quantify how insights and lineage analyses affect search efficiency and solution quality<br>â€¢ Generalizing GigaEvo Beyond Geometry: Applying Hybrid Validation and Constraint Handling to Real-World ML and Systems Optimization Tasks: Extend the framework to practical domains with complex constraints and datasets</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19320" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19320" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Preserve first-frame identity while achieving precise pose-driven motion control in human image animation<br>â€¢ Resolve real-world spatio-temporal misalignments between reference image and driving pose (spatial proportion/structure gaps; temporal movement type/amplitude gaps)<br>â€¢ Eliminate start-gap discontinuities (abrupt jump from reference to first pose) that cause artifacts and incoherence<br>â€¢ Overcome limitations of dominant R2V methods that relax alignment, leading to identity drift, visual artifacts, and temporal instability under mismatched inputs<br>â€¢ Address scarcity and difficulty of I2V approaches, which require tight cross-condition alignment and often demand heavy training resources</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SteadyDancer reframes human animation as an I2V problem with first-frame preservation, introducing a Condition-Reconciliation Mechanism (channel-wise fusion, parameter-efficient injection via LoRA, and temporal/global augmentations) and Synergistic Pose Modulation (spatial refiner, temporal coherence, and frame-wise attention alignment with hierarchical aggregation), trained via a staged pipeline (action supervision, condition-decoupled distillation, and pose-simulation for start-gap mitigation) to achieve harmonized identity fidelity and motion control with fewer resources.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Start-Gap Aware Animation: Learning Motion Ramps for Smooth First-Frame Transitions: Learn explicit transition policies that synthesize adaptive motion warm-ups from reference to target pose under varying pose gaps.<br>â€¢ Multi-Condition SteadyDancer: Integrating 3D Geometry and Camera Motion for Robust Human-Object Interactions: Fuse SMPL/depth and camera parameters into the reconciliation pipeline to improve interaction realism and controllability.<br>â€¢ Pose-Robust I2V: Self-Supervised Denoising and Uncertainty Modeling for Noisy/Incomplete Pose Guidance: Introduce pose uncertainty estimation and self-supervised correction to stabilize animation under imperfect pose inputs.<br>â€¢ Efficient SteadyDancer: Token/Layer Pruning and Knowledge Distillation for Real-Time On-Device Animation: Compress the DiT and condition modules while preserving first-frame fidelity and motion precision for edge deployment.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MedSAM3: Delving into Segment Anything with Medical Concepts</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19046" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19046" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Specialist medical segmentation models lack generalizability and require extensive, time-consuming manual annotation for new tasks and modalities.<br>â€¢ Existing SAM-based medical models depend on geometric prompts, which are laborious for complex anatomy and fail to capture clinicians' semantic intent.<br>â€¢ Prior text-guided segmentation approaches are constrained by fixed vocabularies, limiting open-vocabulary generalization to nuanced clinical concepts.<br>â€¢ Current tools are isolated from agentic ecosystems and do not leverage multimodal LLMs for reasoning and iterative refinement.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MedSAM-3 fine-tunes the SAM 3 architecture on medical images paired with semantic concept labels to enable open-vocabulary Promptable Concept Segmentation via text prompts across modalities and videos. It further introduces a MedSAM-3 Agent that integrates multimodal LLMs to perform concept reasoning and iterative segmentation refinement in an agent-in-the-loop workflow.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Ontology-Guided MedSAM3: Aligning Open-Vocabulary Segmentation with Clinical Ontologies â€” Map prompts to SNOMED/RadLex to standardize concept grounding, improve precision, and enable interoperable downstream analytics.<br>â€¢ Federated MedSAM3 for Privacy-Preserving Cross-Institutional Concept Segmentation â€” Use federated learning with differential privacy to expand generalization across hospitals without sharing raw data.<br>â€¢ Uncertainty-Aware Agentic MedSAM3: Calibrated Concept Segmentation with Clinician-in-the-Loop â€” Integrate uncertainty estimation and active learning in the agent workflow to prioritize review of ambiguous regions and reduce annotation burden.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19900" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19900" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0/Agent0-VL{this https URL}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Vision-language agents depend on human-annotated preferences and external rewards, which are biased, sparse/incomplete, and cap scalability and capability.<br>â€¢ Text-only self-evaluation cannot reliably verify complex multi-step visual, geometric, and numeric reasoning, leading to evaluation hallucinations and language shortcuts.<br>â€¢ Existing self-rewarding methods lack tool-grounded process feedback and selective self-repair, leaving reasoning trajectories unverified and brittle.<br>â€¢ Training tool-integrated RL agents is hard under multimodal partial observability and compositional reasoning; stable alignment between reasoning and evaluation distributions is needed.<br>â€¢ There is a need for zero-external-reward continual improvement to reduce human supervision while increasing robustness and generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Agent0-VL unifies a Solver and a Verifier within a single LVLM to perform tool-integrated multi-turn reasoning and tool-grounded generative verification with confidence-gated self-repair, producing dense process rewards. These roles interact in a Self-Evolving Reasoning Cycle optimized via GRPO using self-generated process and outcome rewards, aligning reasoning and verification distributions for zero-external-reward continual improvement.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Beyond Text: Multi-Tool Orchestration for Visionâ€“Language Agents: Design a meta-controller that schedules diverse tools (simulation, symbolic solvers, OCR, graph analyzers) for reasoning and verification, learning tool selection and ordering under uncertainty.<br>â€¢ Convergence and Robustness of Self-Evolving Solverâ€“Verifier Loops: Provide theoretical analysis and guarantees for stability, calibration, and hallucination bounds in tool-grounded self-rewarding RL, including GRPO dynamics and KL regularization across roles.<br>â€¢ Domain-Adaptive Agent0-VL for Scientific and Medical Imaging: Extend the framework with domain-specific tools (e.g., radiomics, microscopy quantification) and curricula to achieve annotation-light self-evolution and trustworthy verification in specialized visual sciences.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20635" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20635" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Unified many-to-many image generation/editing is underexplored: models must handle variable numbers of inputs/outputs while keeping strong cross-image consistency and producing highly dynamic scenes.<br>â€¢ Existing paradigms fall short: image-only diffusion models lack temporal/semantic coherence; token-centric any-to-any models struggle with generation quality and instruction following; video-based models are biased toward smooth, contiguous clips, limiting discontinuous, high-dynamics outputs.<br>â€¢ Repurposing video priors for image sets introduces positional ambiguity between discrete frames and continuous video; a minimally invasive adaptation plus diverse, well-curated data/training is needed to preserve motion priors while injecting image-task diversity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>iMontage repurposes a pretrained video diffusion backbone (MMDiT + 3D VAE + LLM text encoder) by treating inputs/outputs as pseudo-frames and introducing a Marginal RoPE headâ€“tail temporal indexing to separate input and output slots, enabling variable-length many-to-many generation. A task-agnostic prompt interface and two-phase, temporally diverse data curation/training preserve motion priors while teaching broad image manipulation and high-dynamic content.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Marginal RoPE++: Adaptive Temporal Slotting for Unified Imageâ€“Video Generation: Learn task-aware temporal index allocation to seamlessly switch between videos and image sets and control inter-frame dynamics.<br>â€¢ Discontinuity-Augmented Video Pretraining for Many-to-Many Generators: Enrich pretraining with hard cuts, large viewpoint/subject jumps, and storyboard-like sequences to strengthen priors for highly dynamic, discontinuous content.<br>â€¢ Benchmarks and Metrics for Many-to-Many Image Generation: Consistency, Dynamics, and Instruction Fidelity: Create public datasets and protocols for storyboard, multi-view, and multi-object composition with metrics for cross-image consistency, dynamic range, and instruction following.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20561" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20561" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Unified multimodal models lack demonstrable synergy where language understanding genuinely guides visual generation<br>â€¢ Existing evaluations conflate failure modes (knowledge deficit, reasoning deficit, understanding-to-generation transfer failure), preventing fine-grained attribution<br>â€¢ Risk of data leakage in current benchmarks undermines reliability, allowing memorization to masquerade as reasoning<br>â€¢ Absence of controlled, decoupled analysis hinders identification of architectural/training strategies that bridge understanding and generation<br>â€¢ Unclear whether and how Chain-of-Thought (CoT) and knowledge retrieval can be leveraged in generative pathways</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes UniSandbox, a decoupled evaluation framework with controlled synthetic, leak-proof datasets that isolate Knowledge vs. Reasoning to attribute the understandingâ€“generation gap, and studies interventions such as explicit CoT, self-training to internalize implicit reasoning, and analysis of knowledge transfer and query-based architecturesâ€™ latent CoT-like behavior.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Planning-Guided Unified Models: Designing Explicit Interfaces from Understanding CoT to Generative Decoders: Build architectures that expose and route structured CoT/plans from the understanding module into the generator to systematically reduce transfer failures.<br>â€¢ Self-Training for Implicit Multimodal Reasoning: Scaling and Generalizing CoT-Internalization for Generation: Develop large-scale self-training regimes that distill explicit CoT into generators so they perform implicit reasoning without CoT at inference.<br>â€¢ Regularizing Latent CoT in Query-Based Decoders for Knowledge Transfer: Introduce objectives that surface, align, and exploit latent CoT-like dynamics in query-based architectures to improve retrieval and use of newly learned knowledge during generation.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20102" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20102" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Full self-attention has quadratic complexity, making efficient long-context training and inference in LLMs computationally prohibitive.<br>â€¢ Training-free sparse attention (Full-Sparse) often causes substantial performance degradation because it drops large attention mass and poorly approximates full attention across layers.<br>â€¢ Native sparse-attention training (Sparse-Sparse) paradoxically learns less sparse attention due to gradient update deficiencyâ€”excluded low-ranked KV receive no gradientsâ€”leading to high attention entropy, weak suppression of irrelevant tokens, poor full-attention inference, and limited long-context extrapolation; a unified approach is needed to maintain gradient flow and increase inherent sparsity while supporting flexible sparsity budgets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SSA alternates training between full and block-sparse attention streams and enforces per-layer bidirectional alignment (sparsity and commitment losses) between their attention outputs, preserving gradients to all tokens while explicitly encouraging a more selective, inherently sparse attention distribution for robust performance under both sparse and full inference.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive SSA: Layer-wise Scheduling and Alignment for Optimal Sparsity: Learn per-layer routing probabilities and alignment weights based on attention entropy/importance to further improve sparsity and accuracy.<br>â€¢ Token-Level SSA: Fine-Grained Sparse Attention with Bidirectional Alignment: Extend SSA from block-sparse to token-sparse selection and alignment, analyzing trade-offs and system optimizations for near-quadratic efficiency.<br>â€¢ SSA for Long-Context Extrapolation: Theoretical and Empirical Analysis of Sink Attention Mitigation: Formalize SSAâ€™s mitigation of attention sinks, derive training curricula for extreme contexts, and validate extrapolation gains across million-token settings.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">GigaWorld-0: World Models as Data Engine to Empower Embodied AI</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19861" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19861" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Embodied AI faces a severe data bottleneck: collecting diverse, labeled real-world interaction data (textures, lighting, layouts, viewpoints) is costly, hurting generalization.<br>â€¢ Sim-to-real gap: conventional simulators and stylization methods lack photorealism and physical plausibility, leading to poor transfer to real robots.<br>â€¢ Existing video generators scale parameters but offer limited fine-grained control (appearance/camera/action), weak multi-view coherence, and insufficient geometric/physical grounding; inference is slow and training is compute-heavy.<br>â€¢ 3D reconstruction pipelines typically require dense multi-view captures and are not tightly coupled with physics; robot/object physical parameters are unknown and hard to identify; deformable objects are poorly modeled.<br>â€¢ Cross-embodiment use of human egocentric videos is impeded by appearance and kinematic gaps between human hands and robot arms.<br>â€¢ Viewpoint shifts break policy robustness, while collecting multi-view annotations is expensive.<br>â€¢ Large video foundation models demand high memory/compute; efficient FP8 training/inference and sparse attention are underutilized.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>GigaWorld-0 is a unified world-model data engine that couples a controllable video generator (MoE flow-matching DiT with sparse attention, FP8, and denoising-step distillation) with a 3D pipeline (3DGS-based FG/BG generation, differentiable system identification, and executable motion planning) to synthesize photorealistic, geometrically consistent, physically plausible, instruction-aligned interaction data. Post-trained modules for appearance transfer, view transfer, and human-to-robot mimic transfer, plus IDM-based action labeling and an FP8-efficient GigaTrain stack, enable scalable, diverse data for robust VLA training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ World Models as Interactive Policy Environments for Model-Based RL: Transform the data engine into a closed-loop simulator for safe exploration, planning, and on-policy learning.<br>â€¢ World Models as Policy Co-Designers for Task Decomposition and Action Synthesis: Use generative priors to propose subgoals and action sketches that bootstrap VLA/RL policies.<br>â€¢ Self-Improving World Models via Robot-in-the-Loop Data Flywheels: Continually refine generation, physics, and control by ingesting real rollouts and active, quality-aware data selection.<br>â€¢ Cross-Embodiment Learning from In-the-Wild Human Videos with Physical Alignment: Generalize mimic transfer to diverse morphologies using kinematic constraints and physics-aware correspondence.<br>â€¢ Deformable-Object-Centric World Models from Monocular Video: Learn feedforward soft-body dynamics and manipulation priors for cloth, rope, and fluids within the unified data engine.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Soft Adaptive Policy Optimization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20347" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20347" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ High-variance token-level importance ratios in RL fine-tuning (amplified in Mixture-of-Experts and long responses) cause unstable policy updates.<br>â€¢ Hard clipping in GRPO/GSPO is brittle: tight bands suppress learning signals; loose bands admit noisy off-policy gradients; GSPO can discard entire sequences due to a few outlier tokens.<br>â€¢ Existing methods are either sequence-level coherent or token-level adaptive, but not both; this hurts sample efficiency when sequences contain mixed on-/off-policy tokens.<br>â€¢ Negative-token gradients diffuse across large vocabularies, increasing instability; current algorithms lack mechanisms to differentially regulate positive vs. negative updates.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SAPO replaces hard clipping with a temperature-controlled soft gate on token importance ratios, yielding a smooth trust region that attenuates off-policy updates without zeroing gradients. It uses asymmetric temperatures (Ï„neg > Ï„pos) to dampen high-variance negative-token gradients and, under small-step/low-dispersion conditions, reduces to a coherent sequence-level soft gate while retaining token-level adaptivity for heterogeneous sequences.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Temperature Scheduling for Soft Policy Optimization in MoE LLMs: Learn per-token/sequence temperatures from off-policyness and routing dispersion to balance stability and exploration.<br>â€¢ Routing-Aware Soft Gates for Stable RL in Mixture-of-Experts Models: Integrate expert routing signals into SAPOâ€™s gating to selectively down-weight tokens from unstable experts and improve MoE robustness.<br>â€¢ Theory of Soft Trust Regions in Group-Based RL for LLMs: Provide convergence and stability guarantees, variance bounds, and connections to KL-constrained optimization for SAPO-style gates.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20123" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20123" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Video diffusion transformers fail to generalize beyond their training sequence length (video length extrapolation), limiting practical long-form generation.<br>â€¢ Two core failure modes emerge at longer horizons: model-specific periodic content repetition and universal quality degradation (blurred spatial details, frozen dynamics).<br>â€¢ Existing approaches focus on positional encodings to curb repetition, overlook quality degradation, and achieve only limited extrapolation (â‰ˆ2Ã—), lacking a fundamental attention-level solution.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>UltraViCo is a training-free, plug-and-play technique that applies a constant decay to attention weights for tokens beyond the training window, suppressing attention dispersion and the periodic patterns induced by positional encodings. This preserves learned attention structure, jointly mitigating repetition and quality degradation to push extrapolation from 2Ã— to 4Ã— across models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive UltraViCo: Learning Content-Aware Attention Decay for Long-Horizon Video Generation: Learn per-layer, per-head decay schedules conditioned on content and extrapolation ratio to optimize long-range fidelity.<br>â€¢ AttnDispersion Regularization: Training Objectives to Immunize Video Diffusion Transformers Against Long-Horizon Degradation: Introduce training-time regularizers or curricula that constrain attention dispersion, reducing reliance on post-hoc decay at inference.<br>â€¢ UltraViCo-S: Extending Attention Decay to Spatial and Cross-Modal Extrapolation in Generative Transformers: Generalize the decay mechanism to spatial resolution, audio-visual sequences, and 3D/scene generation for robust extrapolation across modalities.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20211" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20211" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Generative models have excelled in RGB synthesis, but real-world applications require RGBA manipulation. This has led to a fragmented landscape: specialized, single-task models handle alpha but lack versatility, while unified multi-task frameworks are confined to the RGB domain. To bridge this critical gap, we propose OmniAlpha, the first unified, multi-task generative framework for sequence-to-sequence RGBA image generation and editing. Its architecture features MSRoPE-BiL, a novel RoPE method with a bi-directionally extendable layer axis for its Diffusion Transformer (DiT) backbone, enabling the concurrent processing of multiple input and target RGBA layers. To power this framework, we introduce AlphaLayers, a new dataset of 1,000 high-quality, multi-layer triplets, built via a novel automated synthesis and filter pipeline. Jointly training OmniAlpha on this dataset across a comprehensive suite of 21 diverse tasks, extensive experiments demonstrate that our unified approach consistently outperforms strong, specialized baselines. Most notably, OmniAlpha achieves a dramatic 84.8% relative reduction in SAD for mask-free matting on AIM-500 and wins over 90% of human preferences in layer-conditioned completion. Our work proves that a unified, multi-task model can learn a superior shared representation for RGBA, paving the way for more powerful, layer-aware generative systems.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Fragmented RGBA tooling: specialized, single-task models (matting, layer decomposition, object removal, RGBA T2I) cannot generalize or share representations across tasks.<br>â€¢ Alpha-blind unification: existing unified multi-task frameworks operate only in RGB; their architectures, autoencoders, and conditioning mechanisms do not handle transparency or multi-layer workflows.<br>â€¢ Practical need: real-world VFX/design require accurate alpha for fine details and semi-transparent materials, and flexible multi-layer compositing beyond opaque RGB.<br>â€¢ Missing mechanism for concurrent multi-layer processing: diffusion transformers lack a principled way to index and jointly process multiple input/target RGBA layers.<br>â€¢ Data scarcity: absence of high-quality, aligned foreground/background/composite triplets with masks and captions to support joint multi-task RGBA training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>OmniAlpha is a latent diffusion transformer formulated as sequence-to-sequence RGBA generation/editing, combining an alpha-aware VAE adapted from RGB via opaque initialization with MSRoPE-BiLâ€”a bi-directionally extendable layer-axis rotary embeddingâ€”to process multiple input and target layers concurrently. Joint training on the curated AlphaLayers dataset across 21 tasks unifies T2I, layer-conditioned completion, matting, object removal, and layer decomposition.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ OmniAlpha-Video: Unified Multi-Task RGBA Video Generation with Temporal Layer RoPE: Extend MSRoPE-BiL with a time axis for temporally consistent alpha and multi-layer video tasks (video matting, temporal object removal, layer-aware completion).<br>â€¢ Scaling AlphaLayers: Web-Scale Synthetic and Real RGBA Triplets for Robust Multi-Task Training: Expand the automated pipeline and consistency filtering to millions of triplets; study scaling laws and domain diversity effects on RGBA multitask performance.<br>â€¢ Interactive OmniAlpha: RLHF for Human-Preferred Layer-Aware Editing: Incorporate preference optimization and interactive feedback loops to improve controllability and semantic alignment in layer-conditioned completion and matting.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19827" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19827" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present ReDirector, a novel camera-controlled video retake generation method for dynamically captured variable-length videos. In particular, we rectify a common misuse of RoPE in previous works by aligning the spatiotemporal positions of the input video and the target retake. Moreover, we introduce Rotary Camera Encoding (RoCE), a camera-conditioned RoPE phase shift that captures and integrates multi-view relationships within and across the input and target videos. By integrating camera conditions into RoPE, our method generalizes to out-of-distribution camera trajectories and video lengths, yielding improved dynamic object localization and static background preservation. Extensive experiments further demonstrate significant improvements in camera controllability, geometric consistency, and video quality across various trajectories and lengths.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Camera-controlled video retake generation struggles with variable-length inputs and dynamic camera motion, leading to degraded performance outside fixed-length, low-motion regimes.<br>â€¢ Warping-based methods depend on external geometry (depth/point tracking) and heuristic scale alignment, propagating artifacts and failing to disentangle dynamic objects from static backgrounds.<br>â€¢ Implicit approaches often fuse signals via naive concatenation/addition and typically encode only target extrinsics, resulting in coarse multi-view conditioning and poor generalization to out-of-distribution trajectories and lengths.<br>â€¢ Prior misuse of RoPE (absolute positions or partial axes) prevents spatiotemporal alignment between input and target videos, undermining length-agnostic conditioning.<br>â€¢ There is a need to seamlessly integrate camera conditions with positional encodings to encode multi-view relationships within and across videos, improving geometric consistency, dynamic object localization, and background preservation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>ReDirector fine-tunes a camera-controlled video generative model using shared 3D RoPE for both input and target videos and introduces Rotary Camera Encoding (RoCE)â€”a camera-conditioned phase shift integrated into RoPE plus SO(2)-based geometry-aware attentionâ€”to achieve multi-view-consistent, any-length video retakes under dynamic motion.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Self-Supervised RoCE: Learning Camera-Conditioned Rotary Encoding Without Poses: Train phase shifts from multi-view consistency signals to remove dependence on explicit camera pose estimation.<br>â€¢ Depth-Aware RoCE: Joint Pixel- and Camera-Level Encoding for Occlusion Reasoning: Fuse PlÃ¼cker-ray camera encodings with learned depth/visibility cues to handle thin structures and occlusions.<br>â€¢ Adaptive Phase Scheduling for Ultra-Long Video Retakes: Curriculum and dynamic phase modulation to maintain stability and geometric consistency over thousands of frames.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">HunyuanOCR Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19575" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19575" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters. HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow "OCR expert models" and inefficient "General VLMs". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks. HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Pipeline-based OCR systems suffer from error propagation across cascaded modules and high development/maintenance overhead, especially in complex layouts and long-text documents.<br>â€¢ General-purpose VLMs show strong OCR ability but are too large and resource-intensive for low-latency, cost-sensitive, or on-device deployment.<br>â€¢ Existing OCR-specific VLMs often depend on separate layout analysis, limiting end-to-end optimization and robustness on intricate layouts and lengthy sequences.<br>â€¢ There is a need for a unified, single-pass model that covers spotting, parsing, IE, VQA, and text image translation with consistent, structured outputs.<br>â€¢ High-quality, application-aligned training data and validated reinforcement learning strategies for OCR tasks are lacking, hindering reliability in hard cases (e.g., tables, formulas, multilingual, low-quality scans).<br>â€¢ Practical deployment requires lightweight models with efficient serving (e.g., vLLM) while maintaining SOTA performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>HunyuanOCR is a 1B-parameter end-to-end OCR VLM that links a native-resolution ViT to a 0.5B LLM via an adaptive MLP token-pooling connector and XD-RoPE (separating text/height/width/time) to unify spotting, parsing, IE, VQA, and translation in a single pass. It is trained on ~200M curated/synthetic multilingual samples with four-stage pretraining (alignmentâ†’multimodalâ†’long-contextâ†’SFT) and online RL (GRPO) with task-specific rewards to boost accuracy and robustness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Reward Shaping for OCR VLMs: From Heuristics to Learned Metrics: Develop learned, differentiable reward models for RL that capture parsing fidelity, layout correctness, and translation adequacy/fluency to further improve end-to-end OCR.<br>â€¢ Streaming Long-Document OCR with Memory-Enhanced VLMs: Extend context beyond 32k via retrieval/memory and streaming decoding to handle multi-page documents and books with stable reading-order control.<br>â€¢ On-Device HunyuanOCR: Quantization, Distillation, and Adaptive Token Pooling: Combine post-training quantization, task-aware distillation, and dynamic visual token selection to achieve real-time edge deployment without sacrificing quality.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VQ-VA World: Towards High-Quality Visual Question-Visual Answering</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20573" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20573" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Open-source models lack the ability to perform Visual Questionâ€“Visual Answering (VQ-VA), while proprietary systems (e.g., GPT-Image, NanoBanana) already exhibit this capability.<br>â€¢ Predominant training data emphasize predefined, pixel-level edits and prompts, underrepresenting free-form generation that requires world knowledge and multi-step reasoning.<br>â€¢ There is no large-scale, high-quality, real-image VQ-VA corpus or systematic benchmark; existing datasets/benchmarks often rely on synthetic images and reward pixel-level edits, making it hard to measure semantic reasoning and knowledge use.<br>â€¢ Data scarcity leads open models to misinterpret questions and fail to synthesize contextually coherent visual answers, creating a substantial performance gap.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces VQ-VA World, a web-scale agentic data-construction pipeline that mines interleaved web documents to retrieve semantically related image pairs, generates and filters free-form VQ-VA questions, rewrites them for diversity, and adds reasoning tracesâ€”yielding 1.8M high-quality samples used to fine-tune LightFusion. It also releases IntelligentBench, a human-curated VQ-VA benchmark with VLM-validated automatic judging to systematically evaluate knowledge and reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Scaling Open VQ-VA Pretraining with Interleaved Web and Video Data: Extend the agentic pipeline to video and temporally grounded tasks, building multimodal corpora for temporal, causal, and multi-step visual answering.<br>â€¢ Reasoning-Aware Architectures for Visual Questionâ€“Visual Answering: Integrate explicit planning, retrieval-augmented knowledge modules, and chain-of-thought distillation to improve semantic reasoning and instruction following in VQ-VA.<br>â€¢ Evaluating VQ-VA Beyond Pixel Similarity: Human-Calibrated Semantic Scoring and Safety Audits: Develop robust, human-validated evaluation metrics and protocols that assess semantic correctness, knowledge use, and safety/fairness in image answers.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20462" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20462" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Causal, robust long-horizon video generation is needed for streaming, interactive applications, and world models, but dominant diffusion approaches denoise frames in parallel and are inherently non-causal.<br>â€¢ Diffusion training is not end-to-end (noise schedules, per-step supervision), incurs high computational cost, and suffers exposure bias and trainâ€“test mismatch in autoregressive rollouts.<br>â€¢ Normalizing flows offer end-to-end maximum likelihood, exact likelihood evaluation, and invertibility, yet prior NF video models lacked capacity and practical sampling, leaving their viability for high-quality video unclear.<br>â€¢ Existing denoising strategies for noise-augmented training are inadequate: 3D VAE decoder fine-tuning breaks temporal consistency, and flow-based score denoising is noisy and non-causal.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>STARFlow-V is a globalâ€“local autoregressive normalizing flow in a spatiotemporal latent space (3D causal VAE): a deep causal Transformer flow models inter-frame context while per-frame shallow flows handle local structure, trained end-to-end via exact MLE. It adds flow-score matchingâ€”a lightweight, one-frame-look-ahead causal denoiserâ€”and accelerates sampling with block-wise Jacobi iteration and pipelined decoding, supporting T2V, I2V, and V2V via the modelâ€™s invertibility.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Scaling Causal Normalizing Flows for Long-Horizon World Models: Study scaling laws, memory designs, and curriculum strategies to extend STARFlow-V to minute-long coherent rollouts with stable likelihood training.<br>â€¢ Flow-Score Denoisers: From One-Frame Look-Ahead to Multi-Scale Temporal Consistency: Design causal denoisers with multi-frame, multi-scale cues and improved objectives to enhance motion fidelity while preserving streamability.<br>â€¢ Parallelizable Inversion for Autoregressive Flows in Video: Theory and Systems: Establish convergence guarantees for block-wise Jacobi, and develop hardware-aware pipeline schedulers to reach real-time high-resolution generation with error bounds.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20415" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20415" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our dataset and code will be released at https://github.com/LongHZ140516/MajutsuCity.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Balancing stylistic diversity with fine-grained, object-level controllability in city-scale 3D generation<br>â€¢ Overcoming geometric artifacts, multi-view inconsistency, and poor editability in implicit/neural scene representations<br>â€¢ Breaking the Retrieve-and-Place limitation of procedural mesh pipelines constrained by fixed asset libraries<br>â€¢ Addressing the lack of text-aligned multimodal datasets (layouts/heights, 3D assets, PBR materials/skyboxes) for language-grounded city synthesis<br>â€¢ Establishing reliable, domain-specific evaluation metrics for structural consistency, material realism, and lighting/atmosphere in 3D city scenes<br>â€¢ Enabling post-generation, natural languageâ€“driven interactive editing at the object level</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A unified four-stage language-to-city pipeline: LLM-based scene design, LongCLIP-guided diffusion for semantic layouts and ControlNet for height maps, shape-constrained instance-level 3D asset synthesis with seamless PBR materials/skyboxes, and explicit mesh assembly into an editable city. MajutsuAgent adds five atomic language-driven edits, MajutsuDataset supplies text-aligned training data, and a VLM-based AQS/RDR evaluation assesses structure, richness, materials, and lighting.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Constraint-Aware Language-to-City Planning: Prompt validation and automatic repair to prevent contradictory spatial instructions and eliminate cascade errors in layout-to-scene generation<br>â€¢ Topology-Preserving Building Generation with Hybrid 2Dâ€“3D Constraints: A method combining image and point-cloud guidance to model complex, irregular architectures while guaranteeing geometric validity and editability<br>â€¢ Sim-Aware Urban World Models: Integrating physics, traffic, and crowd simulation into language-driven city generation to produce interaction-ready scenes for robotics, VR, and game engines</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Fara-7B: An Efficient Agentic Model for Computer Use</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19663" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19663" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Scarcity of large, high-quality computer-use (CUA) trajectories; human annotation is slow and expensive<br>â€¢ Existing synthetic pipelines are brittle on dynamic websites and lack robust, generic verification, leading to low yield and high cost<br>â€¢ DOM/accessibility-treeâ€“dependent agents struggle to generalize across diverse UIs; need robust pixel-based grounding<br>â€¢ Current CUA systems rely on large frontier models with high token costs, limiting low-latency, privacy-preserving on-device deployment<br>â€¢ Benchmarks underrepresent realistic, multi-step web tasks; evaluation does not capture tail scenarios<br>â€¢ Safety and reliability gaps: agents must avoid irreversible or sensitive actions (critical points) and recover from loops/errors</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>They introduce FaraGen, a multi-agent synthetic data engine (task proposal, task solving, and multi-verifier filtering) that produces diverse, verified web trajectories at ~$1/task, and distill these into Fara-7B, a 7B pixel-in, action-out CUA that operates from screenshots (no DOM at inference), predicts coordinates, and runs efficiently on-device; they also release WebTailBench to cover underrepresented real-world tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Crossing the Critical Point: Safe End-to-End Web Agency via Sandboxed Transactions and Credential Simulation: Enable agents to safely execute beyond critical points using mock payment/identity sandboxes and explicit user-approval protocols while preserving training fidelity.<br>â€¢ AutoVerifier: Learning Calibrated Multimodal Judges to Reduce Hallucinations and Verification Error in CUA Trajectories: Train data-driven, uncertainty-aware multimodal verifiers to improve agreement with humans and cut false positives/negatives across task types.<br>â€¢ FaraLoop: Self-Improving Pixel-to-Action Agents via On-Policy Data Generation and Active Domain Targeting: Iteratively bootstrap Fara-7B as both solver and verifier to lower data costs, target hard/undercovered domains, and enhance long-horizon robustness.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19430" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19430" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing embodied task planners ignore operations research (OR) scheduling, executing subtasks sequentially without exploiting parallelizable steps and waiting times, leading to inefficient completion.<br>â€¢ Prior methods rarely provide explicit 3D spatial grounding for each action step, often reducing 3D tasks to text QA and making plans non-actionable for navigation/manipulation.<br>â€¢ The field lacks a large-scale benchmark that couples OR-aware efficiency optimization with 3D grounding, leaving current 3D MLLMs unevaluated and undertrained for efficiency-centric, spatially grounded planning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes ORS3D and the ORS3D-60K dataset, and introduces GRANTâ€”an embodied MLLM that uses a scheduling token to call an external dynamic-programming solver (knapsack-style) for optimal parallel scheduling and a grounding token to localize target objects in point clouds, producing efficiency-aware, 3D-grounded step-wise plans.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Differentiable Scheduling: Integrating Optimization Solvers into 3D MLLMs: Embed the OR solver within the model to enable end-to-end, gradient-based learning of scheduling with uncertainty and learned constraints.<br>â€¢ From Sim to Real: Deploying ORS3D Schedulers on Physical Robots in Dynamic Homes: Validate GRANT on real robots, addressing perception noise, stochastic durations, and online re-planning in changing environments.<br>â€¢ Generalized Parallel Task Scheduling with Resource and Deadline Constraints in 3D Scenes: Extend the scheduler to handle multiple overlapping parallelizable subtasks, shared resources, deadlines, and stochastic time models, with learned subtask-type inference from raw sensory inputs.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19773" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19773" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to "think with images", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ VLMs struggle with multi-step â€œthinking with images,â€ as text-only reasoning built on static visual embeddings and shallow cross-modal alignment fails to capture fine-grained structures, spatial relationships, and quantitative dependencies in real scenes (hurting complex VQA performance).<br>â€¢ Both proprietary and open-source VLMs lack effective selection, invocation, and coordination of visual tools; naive tool augmentation often degrades accuracy, highlighting the need for agentic strategies that interleave reasoning with tool execution.<br>â€¢ There is no unified, scalable training environment for tool-integrated visual reasoning that offers standardized tool APIs, executable interaction loops, verifiable feedback, and efficient trajectory logging; existing works are narrow (single-tool, task-specific) and prior agent gyms are largely text-only or limited to games/embodiment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces VISTA-Gym, a scalable agentic RL environment with a unified interface to 26 visual tools, executable interaction loops, and verifiable feedback, spanning 7 tasks across 13 datasets. Using VISTA-Gym, it trains VISTA-R1 end-to-end via multi-turn trajectory sampling and reinforcement learning to learn dynamic tool selection, invocation, and coordination interleaved with reasoning for complex visual question answering.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Curriculum Learning for Tool-Integrated Visual Reasoning at Scale: Design progressive curricula in VISTA-Gym that stage tools and task complexity to stabilize training and improve sample efficiency for agentic RL.<br>â€¢ Autonomous Tool Discovery and Composition in VLM Agents: Move beyond fixed tool sets by enabling agents to discover, adapt, and compose tools into macro-actions, learning when and how to create new tool-use routines.<br>â€¢ Uncertainty- and Feedback-Aware Reward Shaping for Visual Tool Use: Integrate uncertainty estimation and tool feedback signals to craft robust reward functions that reduce hallucinations, penalize mis-invocations, and better guide multi-step visual reasoning.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MagicWorld: Interactive Geometry-driven Video World Exploration</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.18886" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.18886" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing interactive video world models underutilize the link between user actions and underlying 3D geometry, causing structural instability under viewpoint changes.<br>â€¢ Models tend to forget historical context during multi-step interactions, leading to error accumulation and semantic/structural drift over time.<br>â€¢ There is a need to explore dynamic scenes from a single image via continuous actions while maintaining structural and temporal consistency, which current methods struggle to achieve.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MagicWorld integrates 3D geometric priors and memory retrieval by introducing AG3D, which builds action-driven point clouds from the first frame of each interaction to constrain viewpoint transitions, and HCR, which retrieves and conditions on relevant historical frames during autoregressive synthesis to reduce drift and preserve consistency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ NeRF-Enhanced MagicWorld: Replace point clouds with action-conditioned neural radiance fields to achieve dense geometry, better occlusion handling, and stronger 3D consistency.<br>â€¢ Language-Driven MagicWorld: Unify keyboard actions with natural language instructions, grounding text in geometry-aware controls for flexible interactive scene exploration.<br>â€¢ Memory-Augmented MagicWorld: Develop hierarchical episodic retrieval and long-horizon memory mechanisms to maintain coherence across extended multi-step interactions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20562" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20562" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While recent video generation models have achieved significant visual fidelity, they often suffer from the lack of explicit physical controllability and plausibility. To address this, some recent studies attempted to guide the video generation with physics-based rendering. However, these methods face inherent challenges in accurately modeling complex physical properties and effectively control ling the resulting physical behavior over extended temporal sequences. In this work, we introduce PhysChoreo, a novel framework that can generate videos with diverse controllability and physical realism from a single image. Our method consists of two stages: first, it estimates the static initial physical properties of all objects in the image through part-aware physical property reconstruction. Then, through temporally instructed and physically editable simulation, it synthesizes high-quality videos with rich dynamic behaviors and physical realism. Experimental results show that PhysChoreo can generate videos with rich behaviors and physical realism, outperforming state-of-the-art methods on multiple evaluation metrics.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Current video generation achieves visual fidelity but lacks explicit physical controllability and plausibility, often imitating motion without causal physical reasoning.<br>â€¢ Prior physics-guided methods struggle to accurately reconstruct complex, per-part material fields from a single image, leading to coarse or incorrect dynamics.<br>â€¢ Existing simulators or pipelines offer limited temporal control (manual, static parameters), producing short, monotonous dynamics and relying on multi-view reconstructions or high-quality 3D assets.<br>â€¢ There is no unified, fine-grained alignment between text semantics, object parts, and continuous physical quantities, limiting interpretability and fine-grained edits.<br>â€¢ Applications demand physically realistic yet editable dynamics (e.g., counterfactual effects, cinematic control) over longer horizons.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>PhysChoreo is a two-stage framework that reconstructs a per-point, part-aware material field from a single image using soft assignment and hierarchical cross-attention (supervised by physics-aware losses), then performs physics-editable MPM/rigid simulation with temporal interventions and conditions a video diffusion model on the simulated trajectories to generate controllable, physically consistent videos.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ End-to-End Differentiable Physics-Conditioned Video Generation: Jointly train material inference, simulation, and video synthesis through differentiable physics to close the loop from pixels to edits.<br>â€¢ Probabilistic Part-Aware Material Inference from a Single Image: Model uncertainty over material fields (E, Î½, Ï) with Bayesian or diffusion inference to support robust control under ambiguity.<br>â€¢ Fluidâ€“Solidâ€“Rigid Coupled PhysChoreo for Complex Interactions: Extend simulation/editing to fluids, plasticity, fracture, and frictional contact for richer, real-world interactions.<br>â€¢ Open-Vocabulary Textâ€“Partâ€“Physics Learning with Verified Measurements: Build a large, human-verified dataset linking free-form text, parts, and measured physical properties to reduce reliance on LLM annotations.<br>â€¢ Online Closed-Loop Physical Video Editing via Control Policies: Learn policies (e.g., RL or MPC) that adapt physics edits in real time based on intermediate simulation/video feedback.<br>â€¢ Multi-View and Temporal Material Reconstruction from In-the-Wild Videos: Fuse multi-frame cues to refine part segmentation and material fields, improving long-horizon consistency.<br>â€¢ Perceptual and Physics Metrics for Long-Horizon Plausibility: Design benchmarks and metrics that quantify physical realism, instruction following, and stability over extended sequences.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19111" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19111" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Diffusion-based local edits produce highly realistic, subtle manipulations that evade whole-image AIGC classifiers.<br>â€¢ Existing benchmarks emphasize binary image-level detection, lacking pixel-level localization of diffusion edits and model attribution.<br>â€¢ Traditional Image Forgery Localization datasets focus on copy-move/splicing and rarely cover modern diffusion-based editing.<br>â€¢ Real-world scenarios involve multi-turn, sequential edits by different generators, which current datasets/methods do not evaluate.<br>â€¢ There is a need to assess robustness to common post-processing (e.g., resizing, JPEG), where baseline methods are fragile.<br>â€¢ Detectors must generalize across unseen diffusion models; current benchmarks provide limited coverage of generators and edit types.<br>â€¢ Practical applications (trust, misinformation, copyright) require fine-grained localization plus identification of the editing model, not just classification.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce DiffSeg30k, a 30k-image benchmark with pixel-level masks and per-region model attribution for up to three sequential diffusion-based edits, created via a VLM-guided pipeline (Qwen2.5-VL + Grounded-SAM) that selects meaningful regions and generates context-aware prompts for localized inpainting across eight diffusion models. Benchmark FCN, SegFormer, and DeepLabv3+ on binary and semantic segmentation to assess multi-turn localization, robustness to distortions, and cross-generator generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Robust Segmentation for Post-Processed AIGC: Training and architectures to withstand resizing, compression, and photometric distortions in localized edit detection.<br>â€¢ Cross-Generator Generalization via Domain-Invariant Forensics: Learn generator-agnostic cues that transfer across unseen diffusion models and edit types.<br>â€¢ Multi-Turn Edit Disentanglement and Chronology Estimation: Jointly localize edits, attribute models, and infer the temporal order of sequential modifications.<br>â€¢ VLM-in-the-Loop Forensic Supervision: Use vision-language models to generate hard negatives and targeted prompts for curriculum learning in edit localization.<br>â€¢ Unified Localization-and-Attribution Transformers: End-to-end models that segment edited regions and directly predict the responsible diffusion model per instance.<br>â€¢ Self-Supervised Pretraining on Synthetic Edits: Leverage large-scale automatically edited data to pretrain forensic segmentation backbones without manual labels.<br>â€¢ From Images to Video: Spatiotemporal Localization and Attribution of Diffusion-Based Edits in Video Streams.<br>â€¢ Human-Centric AIGC Editing Forensics: Specialized benchmarks and methods for sensitive human/face-centric localized edits with privacy-aware evaluation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.18734" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.18734" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical "City-District-Grid" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a "produce-refine-evaluate" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing single diffusion-model approaches struggle to generate city-scale scenes that are both personalized and infinitely extensible<br>â€¢ Manual construction of large, diverse urban environments (varying heights, styles, and layouts) is prohibitively labor-intensive<br>â€¢ Lack of top-down planning and compositional reasoning in prior methods leads to incoherent global layouts and weak district-level semantics<br>â€¢ Absence of user-interactive, evolution-aware mechanisms to expand cities while preserving spatial coherence<br>â€¢ Insufficient benchmarks and multidimensional metrics to evaluate semantics, geometry, texture, and layout quality comprehensively</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Yoâ€™City is an agentic framework that leverages off-the-shelf large models to plan a hierarchical Cityâ€“Districtâ€“Grid layout (Global Planner and Local Designer), then performs grid-level generation via a self-critic â€œproduceâ€“refineâ€“evaluateâ€ isometric image loop followed by image-to-3D synthesis. It further enables user-interactive, relationship-guided expansion using scene-graphâ€“based distance- and semantics-aware layout optimization for spatially coherent growth.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Multi-Agent Urban Generators for Real-Time City Evolution: Integrate simulation feedback (traffic, population, services) to continuously adapt layouts and assets during generative expansion<br>â€¢ Physics-Aware Yoâ€™City: Embedding Environmental and Mobility Constraints in Generative Urban Design: Incorporate sunlight, wind, noise, structural, and traffic models to constrain and guide planning and 3D synthesis<br>â€¢ Cross-Modal CityGPT: Joint Text, Sketch, GIS, and Image Guidance for Controlled 3D Urban Generation: Fuse heterogeneous inputs to improve controllability and realism across Cityâ€“Districtâ€“Grid levels</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Cognitive Foundations for Reasoning and Their Manifestation in LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16660" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16660" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. To understand this gap, we synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning reasoning invariants, meta-cognitive controls, representations for organizing reasoning & knowledge, and transformation operations. We introduce a fine-grained evaluation framework and conduct the first large-scale empirical analysis of 192K traces from 18 models across text, vision, and audio, complemented by 54 human think-aloud traces, which we make publicly available. We find that models under-utilize cognitive elements correlated with success, narrowing to rigid sequential processing on ill-structured problems where diverse representations and meta-cognitive monitoring are critical. Human traces show more abstraction and conceptual processing, while models default to surface-level enumeration. Meta-analysis of 1.6K LLM reasoning papers reveals the research community concentrates on easily quantifiable elements (sequential organization: 55%, decomposition: 60%) but neglecting meta-cognitive controls (self-awareness: 16%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 66.7% on complex problems. By establishing a shared vocabulary between cognitive science and LLM research, our framework enables systematic diagnosis of reasoning failures and principled development of models that reason through robust cognitive mechanisms rather than spurious shortcuts, while providing tools to test theories of human cognition at scale.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLMs often solve complex tasks yet fail on simpler variants, indicating reliance on mechanisms unlike human reasoning.<br>â€¢ Lack of a shared, fine-grained cognitive taxonomy to diagnose reasoning processes across models and humans.<br>â€¢ Existing methods overemphasize sequential decomposition (e.g., chain-of-thought) and neglect meta-cognitive controls linked to success.<br>â€¢ Limited empirical, cross-modal, large-scale analysis of how cognitive elements manifest in model reasoning traces.<br>â€¢ Models possess successful behavioral repertoires but fail to deploy them spontaneously, especially on ill-structured problems.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper synthesizes cognitive science into a 28-element taxonomy and builds a fine-grained evaluation framework to annotate and analyze 192K model traces and human think-alouds across modalities, complemented by a meta-analysis of 1.6K papers. It uses these insights to design automatic test-time reasoning guidance that scaffolds effective cognitive structures, improving performance by up to 66.7% on complex problems.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Training-Time Cognitive Scaffolding for LLMs: Integrating the 28-element taxonomy into pretraining/finetuning objectives to internalize meta-cognitive controls and diverse representations.<br>â€¢ Adaptive Meta-Cognitive Controllers for Ill-Structured Problems: Learning policies that detect poorly structured tasks and trigger monitoring, strategy shifts, and self-evaluation.<br>â€¢ Representation Diversity Induction in LLMs: Methods to encourage abstraction, conceptual grouping, and alternative problem framings over surface-level enumeration.<br>â€¢ Multimodal Meta-Cognition Benchmarks: A standardized suite evaluating meta-cognitive elements (self-awareness, monitoring, planning) across text, vision, and audio.<br>â€¢ Automatic Cognitive Repertoire Deployment: Reinforcement learning and programmatic guidance to decide when and how to switch reasoning strategies during inference.<br>â€¢ Humanâ€“LLM Cognitive Alignment Studies: Controlled experiments comparing think-alouds and model traces to validate the taxonomy and identify training interventions for alignment.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">Unified all-atom molecule generation with neural fields</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.15906" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.15906" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Generative models for structure-based drug design are often limited to a specific modality, restricting their broader applicability. To address this challenge, we introduce FuncBind, a framework based on computer vision to generate target-conditioned, all-atom molecules across atomic systems. FuncBind uses neural fields to represent molecules as continuous atomic densities and employs score-based generative models with modern architectures adapted from the computer vision literature. This modality-agnostic representation allows a single unified model to be trained on diverse atomic systems, from small to large molecules, and handle variable atom/residue counts, including non-canonical amino acids. FuncBind achieves competitive in silico performance in generating small molecules, macrocyclic peptides, and antibody complementarity-determining region loops, conditioned on target structures. FuncBind also generated in vitro novel antibody binders via de novo redesign of the complementarity-determining region H3 loop of two chosen co-crystal structures. As a final contribution, we introduce a new dataset and benchmark for structure-conditioned macrocyclic peptide generation. The code is available at https://github.com/prescient-design/funcbind.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing structure-based generative models are siloed by modality (small molecules vs peptides vs proteins), limiting transferability and practical use at multi-modality interfaces.<br>â€¢ Common representations trade off scalability and expressivity: point clouds complicate use of modern vision architectures and variable-size handling; raw voxels are memory-heavy and do not scale to larger all-atom systems.<br>â€¢ Protein/peptide generators often operate at residue-level and depend on sequence databases/inverse folding, making all-atom design and inclusion of non-canonical amino acids difficult; current cyclic peptide methods largely cannot handle non-canonical chemistries.<br>â€¢ There is no unified, target-conditioned, all-atom generator trainable across modalities and a lack of benchmarks/datasets for macrocyclic peptide generation.<br>â€¢ Diffusion-based samplers can be slow and complex; there is a need for efficient, modality-agnostic training/sampling strategies in a unified framework.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>FuncBind represents molecules as continuous all-atom neural fields and learns spatial latent feature maps via a VAE; a target-conditioned 3D U-Net denoiser (used with diffusion or walkâ€“jump sampling) generates binder latents given a protein target and modality, which are decoded into atomic-density fields and postprocessed to recover discrete molecules (including non-canonical amino acids).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Cross-Modality Transfer in Unified Neural-Field Generators: Quantify and exploit transfer learning across modalities (e.g., nucleic acids, glycans) to improve data efficiency and generalization in unified all-atom models.<br>â€¢ SE(3)-Equivariant Latent Neural Fields for All-Atom Generation: Integrate rotational/translational equivariance into the latent denoiser and/or neural-field decoder to reduce augmentation needs and improve sample efficiency.<br>â€¢ End-to-End Discrete Reconstruction from Neural Fields: Learn differentiable bond and residue/monomer typing jointly with the decoder to replace rule-based postprocessing and better support diverse non-canonical chemistries.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20647" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20647" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Text-to-video models produce low-diversity outputs for a single prompt, collapsing to narrow styles, motions, and scene layouts.<br>â€¢ Existing diversity methods (designed for images) are impractical for videoâ€”requiring expensive test-time optimization, memory-heavy latent caches, access to full training data, or architectural changesâ€”and they overlook temporal/cinematic dimensions.<br>â€¢ Users rely on prompt engineering and exhaustive seed/guidance sweeps; guidance (e.g., CFG) reduces stochasticity, making diversity inconsistent, compute-intensive, and hard to control.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DPP-GRPO performs set-level policy optimization by combining a Determinantal Point Process (to impose diminishing returns on similar samples) with Group Relative Policy Optimization (to provide groupwise feedback), yielding a plug-and-play, model-agnostic policy that generates diverse yet faithful video sets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive DPP-GRPO for Controllable Diversity in Text-to-Video: Introduce user-controllable axes (camera motion, object trajectory, scene composition) with adaptive kernels to target specific diversity dimensions.<br>â€¢ Learning Diversity-Aware Reward Models for Video Generation: Replace hand-crafted DPP similarity with learned, multimodal reward models that capture temporal coherence and cinematic language.<br>â€¢ Scaling DPP-GRPO to Long-Form Narrative Video: Extend set-level diversity optimization to multi-shot, story-consistent generation, balancing scene diversity with narrative continuity.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Concept-Aware Batch Sampling Improves Language-Image Pretraining</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20643" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20643" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Prevailing data curation for VLM pretraining is offline and concept-agnostic, yielding static subsets that are hard to repurpose, accelerate data depletion, and can propagate biases from black-box filters.<br>â€¢ Per-sample quality filtering ignores concept-level distributions; IID mini-batches over-represent frequent concepts and under-train rare ones, hurting long-tailed zero-shot classification.<br>â€¢ Different downstream tasks (e.g., classification vs retrieval) prefer different concept distributions, yet existing pipelines lack task-adaptive, online control of batch composition.<br>â€¢ Open, reproducible alternatives to proprietary online curation are scarce; practitioners lack transparent levers to shape concept distributions during training.<br>â€¢ Web data issues (link rot, noisy alt-texts) motivate concept grounding and concept-aware recaptioning to improve supervision quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Build DataConcept (128M imageâ€“text pairs) with concept tags, bounding boxes, confidence scores, and concept-aware recaptions, then perform online Concept-Aware Batch Sampling (CABS) that scores samples by concept metadata to form task-specific sub-batches. Two instantiations: CABS-DM balances concept coverage via a gain function combining under-representation and rarity; CABS-FM prioritizes samples with high concept multiplicity for retrieval.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Curriculum CABS: Dynamic Score Scheduling for Balanced Classificationâ€“Retrieval Pretraining: adapt the sampling score over training to first emphasize single-object diversity then shift to multi-object complexity.<br>â€¢ Self-Supervised Concept Discovery for CABS without Expensive Annotations: learn concept banks and tags from self-supervised region features, reducing reliance on external detectors.<br>â€¢ Task-Aware Multi-Objective Batch Sampling via Reinforcement Learning: learn a sampling policy that optimizes a weighted mix of classification and retrieval validation metrics online.<br>â€¢ Fairness-Aware Concept-Aware Sampling to Mitigate Bias Propagation in VLMs: integrate sensitive-attribute proxies and parity constraints into CABS to balance representation while preserving utility.<br>â€¢ Multilingual and Multimodal CABS for Videoâ€“Audioâ€“Text Pretraining: extend concept-aware sampling to temporal and acoustic concepts and multilingual captions for broader generalization.<br>â€¢ Adaptive Filter Ratios and Superbatch Sizing for Efficient Online Curation: automatically tune f and super-batch size based on optimization dynamics and compute budget to improve throughputâ€“accuracy trade-offs.<br>â€¢ Concept-Aware Fine-Tuning: Applying CABS to Downstream Data Selection: use concept-targeted batches during task-specific fine-tuning to align with deployment distributions.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20250" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20250" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Obtaining the precise 3D motion of a table tennis ball from standard monocular videos is a challenging problem, as existing methods trained on synthetic data struggle to generalize to the noisy, imperfect ball and table detections of the real world. This is primarily due to the inherent lack of 3D ground truth trajectories and spin annotations for real-world video. To overcome this, we propose a novel two-stage pipeline that divides the problem into a front-end perception task and a back-end 2D-to-3D uplifting task. This separation allows us to train the front-end components with abundant 2D supervision from our newly created TTHQ dataset, while the back-end uplifting network is trained exclusively on physically-correct synthetic data. We specifically re-engineer the uplifting model to be robust to common real-world artifacts, such as missing detections and varying frame rates. By integrating a ball detector and a table keypoint detector, our approach transforms a proof-of-concept uplifting method into a practical, robust, and high-performing end-to-end application for 3D table tennis trajectory and spin analysis.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Recovering accurate 3D ball trajectory and spin from standard monocular broadcast video is difficult due to fast motion, occlusions, motion blur, lighting changes, and diverse camera viewpoints.<br>â€¢ Models trained on clean synthetic inputs often fail to generalize to real-world noise: imperfect/missing 2D detections and varying frame rates.<br>â€¢ Real-world videos lack 3D ground-truth trajectories and spin labels, blocking direct supervised training; spin is only indirectly observable without specialized hardware.<br>â€¢ Prior uplifting methods are proof-of-concepts that ignore front-end perception, lacking robustness to missed detections, occlusions, and detector errors.<br>â€¢ Physics-fitting or calibration-heavy monocular approaches are brittle, relying on event identification and explicit camera calibration, and are sensitive to video quality.<br>â€¢ Existing datasets (e.g., TTST, Blurball) lack sufficient resolution, scale, and comprehensive annotations (2D ball, table keypoints, spin) to train and evaluate a practical end-to-end system.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A two-stage pipeline: a front-end Segformer++ heatmap-based ball detector (with 3-frame input) and a table keypoint detector plus robust temporal filtering produce clean 2D ball tracks and 13 table keypoints. A re-engineered 2D-to-3D uplifting network, trained solely on physically-correct synthetic data, embeds these inputs to predict metric 3D trajectory and initial spin, robust to missing detections and varying frame rates, achieving zero-shot transfer to real videos; training of front-end uses the new high-resolution, richly annotated TTHQ dataset.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Physics-Consistent Self-Training for Monocular 3D Table Tennis in the Wild: Leverage pseudo-3D labels and physics constraints to adapt the uplifting network to real footage without 3D ground truth.<br>â€¢ Handling Moving Cameras: Joint Camera Calibration and 3D Uplifting from Broadcast Table Tennis: Integrate dynamic camera motion estimation from table keypoints with trajectory/spin prediction for non-static broadcasts.<br>â€¢ Uncertainty-Aware Real-Time 3D Trajectory and Spin Estimation for Coaching Applications: Add probabilistic modeling to quantify uncertainty under occlusions and optimize the pipeline for low-latency, on-device inference.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.17943" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.17943" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educating, a domain that demands external professional knowledge integration and rigorous step-wise reasoning, existing approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its Plan-Do-Study-Act philosophy into a self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions, visual guides, audio narrations, and interactive references. To support evaluation, we construct SciVBench, a benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena. Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-of-the-art video agents on the benchmark, establishing a new paradigm for the community.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing video MLLMs cannot effectively leverage external resources and tools, limiting their ability to handle complex scientific tasks.<br>â€¢ Scientific video understanding requires external professional knowledge, rigorous step-wise reasoning, and complex planningâ€”current systems struggle to produce effective plans and cannot self-evolve based on execution feedback.<br>â€¢ LLM hallucinations and instability undermine reliability and safety, making educational guidance from videos error-prone.<br>â€¢ There is no dedicated benchmark for scientific-phenomenon video analysis, hindering systematic evaluation and progress.<br>â€¢ Current systems rarely generate comprehensive, engaging multimodal educational materials (text, visuals, audio, interactive) tailored to specific scientific processes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SciEducator is a Deming Cycle (Planâ€“Doâ€“Studyâ€“Act)-driven multi-agent system that iteratively plans, executes, evaluates, and refines tool-integrated workflows for scientific video comprehension using an LLM-based planner/evaluator, external knowledge retrieval, and a solution pool. It then produces multimodal educational outputsâ€”textual instructions, visual guides, audio narrations, and interactive e-bookletsâ€”by aggregating procedures, safety notes, equipment images, and shopping links.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Deming-Agent for Real-Time Lab Assistance: Extend SciEducator to real-time monitoring and step-wise guidance in lab settings, with safety verification and sensor/tool integration.<br>â€¢ Science-Video Reasoning with Physics Engines: Couple multi-agent planning with differentiable simulators to validate, predict, and refine scientific interpretations from videos.<br>â€¢ Personalized Multimodal Science Tutoring: Build adaptive e-booklet and narration generation driven by learner models, proficiency estimation, and engagement feedback.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.18394" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.18394" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing. We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date. We analyze how context, question type, and external knowledge affect accuracy and calibration, and how adding factual news context modifies belief formation and failure modes. Our results show that forecasting ability is highly variable as it depends on what, and how, we ask.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLMsâ€™ real-world forecasting ability on events beyond their training cutoff is underexplored, yet critical for high-stakes decision-making where predictive reliability and calibration matter.<br>â€¢ Existing benchmarks (e.g., Metaculus AI Forecasting Benchmark, ForecastBench) emphasize aggregate performance/profit and tool use but lack granular analysis of how domain structure, question type, and prompt framing drive success or failure.<br>â€¢ The impact of adding factual news context on model belief formation and failure modes (e.g., rumour overweighting, definition drift, recency bias) is unknown, limiting interpretability and safe deployment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Compare reasoning-optimized and standard LLM families on post-cutoff real-world forecasting questions, scoring outputs with accuracy and Brier metrics. Vary prompt framing and inject factual news context to analyze changes in calibration, belief formation, and failure modes across domains.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Calibrating LLM Forecasts via Context Hygiene: Filter and weight external news by source credibility and novelty to reduce rumour overweighting and recency bias, improving calibration under proper scoring rules.<br>â€¢ A Taxonomy of Forecasting Question Types for LLMs: Define and benchmark domain-specific question classes and operational definitions to curb definition drift and reveal structure-dependent performance.<br>â€¢ Adaptive Prompting and Retrieval for Robust LLM Forecasting: Learn a meta-policy that selects prompt frames and retrieval depth per domain/question attributes to optimize accuracy and calibration.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 10</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-26 23:11:18 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 10;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>