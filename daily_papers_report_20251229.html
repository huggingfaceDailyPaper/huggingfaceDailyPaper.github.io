<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - December 29, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">December 29, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.17504" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.17504" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Video object insertion (VOI) lacks robust 4D scene understanding, making scale, pose, and placement across camera motion unreliable.<br>‚Ä¢ Existing methods struggle with occlusions and visibility changes, causing objects to float, bleed through, or overwrite scene content.<br>‚Ä¢ Inpainting models edit only within masks and fail to model illumination/shadow changes around the object; mask-free generators sacrifice user controllability.<br>‚Ä¢ There is no supervised dataset pairing object-removed/present videos with clean reference object images to teach illumination- and shadow-aware insertion.<br>‚Ä¢ Practical applications (virtual product placement, film post-production) require production-quality temporal coherence, geometry alignment, and lighting consistency not met by current research/commercial tools.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A two-stage pipeline: (1) 4D-aware mask generation via single-video 4D reconstruction, user-controlled 3D placement, scene-flow-based propagation, and camera-aligned reprojection to obtain temporally coherent, occlusion-aware masks; (2) diffusion-based video synthesis anchored by high-fidelity first-frame inpainting and LoRA fine-tuning on the new ROSE++ triplets to jointly render the object and local illumination/shadow changes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ End-to-End 4D-Aware VOI Without External Geometry: Train a unified model that jointly infers geometry-aware masks and synthesizes insertion, removing reliance on stitched pretrained 4D modules.<br>‚Ä¢ LightStage-VOI: Inverse-Rendering Guided Video Object Insertion: Integrate environment map estimation and material/BRDF recovery to produce physically consistent shadows, reflections, and global illumination.<br>‚Ä¢ DynInsert: Physics- and Contact-Aware Insertion of Articulated/Deformable Objects: Extend from static to dynamic inserted objects that move with and interact physically with scene elements via learned scene flow and differentiable contact.<br>‚Ä¢ AutoPoseScale: Metric-Depth Calibrated Object Placement from a Single Reference Image: Infer real-world scale and pose automatically from metric depth and scene priors to reduce manual placement effort.<br>‚Ä¢ Multi-Object, Multi-Shot VOI with Cross-Scene Consistency: Maintain identity, lighting, and geometry across multiple camera shots and objects using shot-graph constraints and 4D scene graphs.<br>‚Ä¢ Self-Supervised ROSE++-XL: Web-Scale Mining of Object-Removed/Present Pairs: Build a larger training corpus with automatic object retrieval and illumination supervision to further improve generalization.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.17220" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.17220" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Humans understand long and complex texts by relying on a holistic semantic representation of the content. This global view helps organize prior knowledge, interpret new information, and integrate evidence dispersed across a document, as revealed by the Mindscape-Aware Capability of humans in psychology. Current Retrieval-Augmented Generation (RAG) systems lack such guidance and therefore struggle with long-context tasks. In this paper, we propose Mindscape-Aware RAG (MiA-RAG), the first approach that equips LLM-based RAG systems with explicit global context awareness. MiA-RAG builds a mindscape through hierarchical summarization and conditions both retrieval and generation on this global semantic representation. This enables the retriever to form enriched query embeddings and the generator to reason over retrieved evidence within a coherent global context. We evaluate MiA-RAG across diverse long-context and bilingual benchmarks for evidence-based understanding and global sense-making. It consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ RAG lacks global semantic awareness, so retrieval and generation rely on local evidence and struggle to align dispersed clues in long documents; the paper aims to add a human-like global ‚Äúmindscape‚Äù to guide understanding, retrieval, and reasoning (Figure 1, p.1).<br>‚Ä¢ Existing context-aware embeddings mainly enrich chunk representations with local context and do not inject global semantics into the query; generators also lack access to global context, creating a retriever‚Äìgenerator asymmetry that wastes correct retrieval (Sec. 3.3‚Äì3.4; Table 5, p.5).<br>‚Ä¢ Long-window LLMs are costly/inefficient for very long inputs; MiA-RAG achieves higher accuracy with far fewer tokens (Table 8, p.14; Helmet results show MiA-Gen-14B + MiA-Emb with ~3‚Äì13k tokens outperforming GPT-4o at 128k‚Äì2M tokens).<br>‚Ä¢ Current long-context systems underperform on global sense-making tasks (beyond locating local facts); they need mechanisms to synthesize evidence coherently across a document (Sec. 5.3; Table 9, p.14).<br>‚Ä¢ Lack of fine-grained supervision linking questions to supporting evidence in long narratives; the paper builds silver chunk/node annotations to train and evaluate retrieval (Algorithm 1, p.12; Table 1, p.4).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MiA-RAG builds a hierarchical global summary (the mindscape) and conditions both retrieval and generation on it: MiA-Emb integrates summary+query via residual fusion and contrastive training over silver chunk/node evidence, and MiA-Gen is SFT to reason over retrieved chunks within the same mindscape. This symmetric conditioning yields enriched query embeddings, selective retrieval, and integrative reasoning that outperform larger vanilla RAG systems (Table 2, p.5).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Dynamic Mindscape for Evolving Corpora: Incrementally update and version global summaries as documents change, with stability‚Äìplasticity control and low-latency refresh.<br>‚Ä¢ Multimodal Mindscape-RAG: Extend the mindscape to images, tables, and code, learning unified global abstractions that guide retrieval across modalities.<br>‚Ä¢ Personalized Mindscape Memory for Assistants: Build user- and project-level mindscapes with privacy-preserving summarization and adaptive retrieval for long-term workflows.<br>‚Ä¢ End-to-End Mindscape‚ÄìRetriever‚ÄìGenerator Training: Jointly optimize summarization, embedding, and generation using alignment signals like MCEA and task rewards.<br>‚Ä¢ Mindscape Quality Estimation and Repair: Train critics to detect misleading or low-coverage summaries and trigger targeted re-summarization or augmentation.<br>‚Ä¢ Corpus-Level Mindscapes for Cross-Document QA: Construct graph-of-summaries spanning many documents and integrate with GraphRAG for global sense-making at collection scale.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MAI-UI Technical Report: Real-World Centric Foundation GUI Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22047" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22047" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of native agent‚Äìuser interaction: real instructions are ambiguous/underspecified; existing agents rarely ask clarifying questions or seek consent, harming reliability in practical use.<br>‚Ä¢ Limits of UI-only operation: long, brittle multi-step UI sequences amplify error propagation and cannot access tasks better handled via APIs; agents need tool-use (e.g., MCP) to compress workflows and expand capability.<br>‚Ä¢ No practical deployment architecture: cloud-only raises privacy, latency, and cost concerns; device-only is capacity-limited; missing is a native device‚Äìcloud collaboration with privacy-aware routing and seamless handoff.<br>‚Ä¢ Brittleness in dynamic environments: models trained on static logs overfit to layouts and fail with pop-ups, app updates, or unexpected dialogs; robust online learning in dynamic, stateful GUIs is missing.<br>‚Ä¢ Grounding data quality/diversity gaps: open datasets contain noisy instructions and narrow perspectives; agents need multi-perspective instruction-as-reasoning to generalize across complex screens.<br>‚Ä¢ Lack of scalable RL infrastructure: stateful GUI envs are hard to parallelize; current systems do not support large-scale, long-horizon, asynchronous rollouts for agent training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MAI-UI trains foundation GUI agents with an instruction-as-reasoning SFT+RL pipeline, a self-evolving trajectory data loop that adds ask_user and mcp_call actions, and a native device‚Äìcloud collaboration (local agent+monitor, cloud agent, unified trajectory memory) for privacy-aware routing. A scalable online RL stack (containerized Android envs, asynchronous rollouts, hybrid TP/PP/CP parallelism, GRPO with curriculum/repetition-penalty/replay) improves robustness on long-horizon, dynamic tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Proactive Consent-Aware GUI Agents via Formalized User-Interaction Policies: Formalize when/how agents ask users for missing details and consent, with verifiable safeguards and minimal-interruption strategies.<br>‚Ä¢ Provably Private Device‚ÄìCloud Routing for GUI Agents: Design information-theoretic or differential-privacy guarantees for on-device/cloud handoffs and sensitive-content detection.<br>‚Ä¢ Continual Online RL for Non-Stationary Mobile GUIs: Lifelong, test-time adaptive agents that handle app updates, new layouts, and emergent pop-ups without catastrophic forgetting.<br>‚Ä¢ Autonomous MCP Tool Discovery and Schema Induction for Mobile Agents: Let agents self-discover tools/APIs, infer schemas, and learn when to switch from UI manipulation to API calls.<br>‚Ä¢ Unified Cross-Platform Foundation GUI Agent for Desktop‚ÄìWeb‚ÄìMobile: A single agent that generalizes across OSes and form factors with adaptive grounding and action abstractions.<br>‚Ä¢ Instruction-as-Reasoning 2.0: Learning Perspective Selection and Synthesis from Human Traces: Learn to choose and compose appearance/function/location/intent perspectives from human interaction data.<br>‚Ä¢ On-Device Foundation GUI Models under 2B Parameters via Distillation and Sparse Adaptation: Distill large agents into small, privacy-preserving on-device models with LoRA/sparsity and latency-aware training.<br>‚Ä¢ Robust Trajectory Monitoring and Recovery with Self-Explainable Error Summaries: Train monitors that detect deviations early, generate actionable error summaries, and plan recovery subroutines.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21675" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21675" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ MLLMs excel at semantic-level tasks but lack robust perceptual-level understanding of aesthetics, quality, structure, and texture, leading to unstable or inconsistent judgments.<br>‚Ä¢ No unified, hierarchical benchmark exists that jointly covers IAA, IQA, and especially the underexplored ISTA; prior work lacks a coherent definition and fine-grained criteria for structure/texture.<br>‚Ä¢ Existing datasets and evaluations are siloed (only scoring or only QA), often saturated, and fail to comprehensively test perceptual reasoning across tasks and domains.<br>‚Ä¢ General-purpose MLLMs struggle with reliable numeric rating (score regression) and cross-domain transfer, while specialized models don‚Äôt generalize beyond their niche.<br>‚Ä¢ Lack of plug-and-play perceptual reward models hinders aligning text-to-image generation with human-perceived aesthetics, fidelity, and structural richness.<br>‚Ä¢ Perceptual cues are critical to human judgment in content creation, enhancement, and generative alignment; closing this gap is essential for practical, human-centered visual AI.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces UniPercept-Bench, a hierarchical (Domain‚ÄìCategory‚ÄìCriterion) benchmark spanning IAA, IQA, and ISTA with both Visual Rating and VQA, built via an MLLM-assisted (generation + reject sampling) and human-refined pipeline, including structured ISTA annotations and a computed ISTA-10K score. It further trains UniPercept‚Äîa baseline MLLM‚Äîvia Domain-Adaptive Pre-Training (~800K perceptual samples) and Task-Aligned RL (GRPO) using a binary VQA reward and an Adaptive Gaussian Soft Reward for ratings, enabling robust cross-domain perceptual scoring/reasoning and serving as a plug-and-play reward/metric for T2I.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ UniPercept-Video: Unified Perceptual-Level Understanding for Video Aesthetics, Quality, and Structure‚Äîextend the taxonomy, VR/VQA tasks, and rewards to temporal distortions, motion blur, and dynamic textures.<br>‚Ä¢ Culture-Aware Aesthetic Alignment: Modeling Cross-Regional Preferences for IAA‚Äîincorporate diverse cultural annotations and priors to personalize and debias aesthetic judgments.<br>‚Ä¢ Perceptual Chain-of-Thought: Interpretable Reasoning for Fine-Grained Visual Judgments‚Äîtrain models to produce structured perceptual rationales and attributions to stabilize ratings and improve QA fidelity.<br>‚Ä¢ Differentiable UniPercept Loss: End-to-End Perceptual Optimization for Generative Models‚Äîturn UniPercept rewards into differentiable surrogates/critics for direct training of diffusion/flow models.<br>‚Ä¢ Physically-Grounded ISTA: Building a Large-Scale Dataset with Real Material Scans and 3D Geometry‚Äîground structure/texture labels in measured BRDFs/geometry to boost realism and generalization.<br>‚Ä¢ Preference-Conditioned Perceptual Scoring: Personalizing IAA/IQA/ISTA via Few-Shot and RLHF‚Äîadapt perceptual outputs to user or domain profiles while preserving cross-domain consistency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ProEdit: Inversion-based Editing From Prompts Done Right</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22118" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22118" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Global attention feature injection over-injects source image information, causing the model to prioritize source attributes over the target prompt and fail on attribute edits (pose, number, color)<br>‚Ä¢ Initializing sampling from inverted source latents imposes a strong source prior, drawing trajectories back to the source distribution and hindering large edits (e.g., color changes)<br>‚Ä¢ Existing attention-control methods often require selecting/tuning specific heads/layers/blocks, and suffer from a trade-off between background preservation and edit strength<br>‚Ä¢ There is a need for a training-free, plug-and-play method that preserves non-edited regions while enabling accurate attribute edits across both images and videos with few sampling steps</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ProEdit is a training-free, plug-and-play editing approach that combines KV-mix (masked mixing of source/target K,V attention features) with Latents-Shift (AdaIN-style regional perturbation of inverted noise) to reduce source over-injection while preserving structure. It identifies edit regions via attention-derived masks, fully injects source features in non-edited areas, mixes K,V in edited areas with strength Œ¥, and shifts the latent distribution in edited regions with blend Œ≤ to follow target prompts.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive KV-Mix Scheduling for Flow Transformers: Learn per-layer, per-timestep Œ¥ and injection schedules to balance edit strength and background preservation automatically<br>‚Ä¢ Learned Region Masks from Multimodal Cues for Prompt-Guided Editing: Replace attention-only masks with a train-free or lightweight learned mask predictor that fuses text, attention, and image semantics for finer edit localization<br>‚Ä¢ Temporal-ProEdit: Motion-Aware KV-Mix and Latent Shift for Long Video Editing: Introduce optical-flow/feature-tracking guided masks and temporally consistent Œ¥/Œ≤ schedules for long, complex videos<br>‚Ä¢ 3D-Consistent ProEdit for Multi-View and NeRF Scene Editing: Extend KV-mix and Latents-Shift to enforce multi-view and geometry consistency for 3D/NeRF or multi-camera editing<br>‚Ä¢ AutoTune ProEdit: Instance-Level Policy Optimization of Injection and Shift Hyperparameters: Use black-box optimization or RL to auto-tune Œ¥, Œ≤, and injection steps per input for optimal edit quality and preservation</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">TimeBill: Time-Budgeted Inference for Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21859" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21859" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLMs are increasingly used in time-critical systems where responses must meet hard or firm deadlines, but autoregressive generation makes end-to-end latency uncertain and tightly coupled to unknown response length (page 1; Fig. 1; Sec. 3.1).<br>‚Ä¢ Existing efficient inference methods either compress offline (quantization/pruning) and cannot adapt at runtime, or use fixed KV-cache eviction ratios that fail across diverse tasks/time budgets‚Äîcausing overruns or degraded response quality (pages 1‚Äì2).<br>‚Ä¢ Prior response-length predictors are coarse-grained (bucket classifiers with few classes) or limited by context length (BERT-based), and existing ML-based latency predictors are less interpretable and not ideal for online use‚Äîhindering accurate, hardware-aware time estimation (Sec. 2.1).<br>‚Ä¢ There is no runtime mechanism that maps predicted execution time to LLM decoding configurations (e.g., KV eviction) to maximize response performance while provably meeting a time budget, especially under WCET considerations (Sec. 3.2; Sec. 5.1).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TimeBill combines an SLM-based fine-grained response-length classifier with a FLOPs-guided, profiled execution-time estimator to predict per-request WCET, then analytically computes the optimal KV-cache eviction ratio that satisfies a given time budget while minimizing performance loss. It runs prediction in parallel with prefill and uses prompt compression to hide predictor latency, enabling per-request adaptive, time-budgeted inference (Fig. 2; Eq. 11).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Multi-Knob Time-Budgeted Control for LLM Serving: Co-optimize KV eviction, KV quantization, early-exit, and draft‚Äìverify policies under deadlines with online optimization/control.<br>‚Ä¢ Uncertainty-Calibrated WCET for LLMs via Distributional Length Prediction: Replace fixed pessimistic factors with calibrated predictive intervals (e.g., conformal) to tighten WCET while maintaining safety.<br>‚Ä¢ Deadline-Aware Multi-Tenant Scheduling with TimeBill Signals: Integrate per-request WCET and optimal Œ± into cluster schedulers for admission control, placement, and goodput maximization under mixed SLAs.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22120" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22120" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ VLMs often fail to ground answers in fine-grained visual evidence (e.g., thin polylines, small symbols), causing evidence-mismatched VQA predictions.<br>‚Ä¢ Existing cue-based methods are shape-rigid (rectangular crops/masks) and miss irregular or fragmented evidence.<br>‚Ä¢ Tool- or domain-specific solutions generalize poorly across datasets, layouts, and image types.<br>‚Ä¢ Generating intermediate visual cues at inference adds computational overhead and risks cascading errors.<br>‚Ä¢ Models exploit text-only shortcuts; they can answer from the question alone without relying on critical pixels.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>BiPS trains VLMs with bidirectional KL constraints in GRPO: minimize KL between the original image policy and an evidence-preserving view to encourage coverage of supporting regions, and maximize KL between the original and an evidence-ablated view to break text-only shortcuts and enforce fine-grained visual reliance. The paired views are programmatically generated by editing chart rendering code (with LLM-assisted reformulation and difficulty filtering), yielding precise preserve/ablate supervision without inference-time tools.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ BiPS in the Wild: Weakly-Supervised Perceptual Shaping for Natural Images and Video: Extend preserve/ablate view generation beyond charts using segmentation/tracking and weak supervision to validate BiPS on photos, diagrams, and videos.<br>‚Ä¢ Learning to Edit Evidence: An Autonomous View-Editor for Preserve/Ablate Generation: Train a learned editor (RL/verification-guided) to synthesize evidence-preserving/ablated views without executable code or human rules.<br>‚Ä¢ Multi-Granular and Feature-Space BiPS: Joint Pixel- and Representation-Level Perceptual Shaping: Integrate coarse-to-fine pixel edits with feature-level KL shaping to improve robustness, stability, and data efficiency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21643" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21643" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Weather modeling requires both accurate prediction and mechanistic interpretation, yet existing methods treat these goals in isolation, separating generation from understanding. To address this gap, we present Omni-Weather, the first multimodal foundation model that unifies weather generation and understanding within a single architecture. Omni-Weather integrates a radar encoder for weather generation tasks, followed by unified processing using a shared self-attention mechanism. Moreover, we construct a Chain-of-Thought dataset for causal reasoning in weather generation, enabling interpretable outputs and improved perceptual quality. Extensive experiments show Omni-Weather achieves state-of-the-art performance in both weather generation and understanding. Our findings further indicate that generative and understanding tasks in the weather domain can mutually enhance each other. Omni-Weather also demonstrates the feasibility and value of unifying weather generation and understanding.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Fragmented modeling: Generation (nowcasting/inversion) and understanding (diagnostics/QA) are handled by disjoint models, preventing shared representations and cross-task benefits (see Fig. 2, p.2).<br>‚Ä¢ Lack of interpretability: Generative weather models are black boxes and rarely justify forecasts; actionable decisions for extremes require causal, mechanistic explanations beyond pixel scores.<br>‚Ä¢ Capability gap: MLLMs can describe weather but cannot synthesize physical radar fields; generative models can predict fields but cannot assess or explain them‚Äîno single model does both.<br>‚Ä¢ Evaluation limitations: Pixel-level metrics miss semantic/structural quality; there is a need to unify physical accuracy with perceptual/diagnostic assessment aligned with expert practice.<br>‚Ä¢ Untapped complementarity: Training tasks separately wastes mutual supervision signals; unified learning can improve both sides (demonstrated by joint U+G gains, Table 2, p.9).<br>‚Ä¢ Practical coverage: Regions lacking radar benefit from satellite-to-radar inversion; unifying cross-modal generation with understanding helps deployable, explainable monitoring.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Omni-Weather is a unified multimodal foundation model with a shared self-attention backbone that, given a task prompt, routes modality-specific encoders (text, image, satellite generation encoder, and an EarthFormer-based radar sequence encoder) to dual decoders: a VAE decoder for field generation and a text decoder for explanatory reports. It introduces a weather-tailored Chain-of-Thought dataset and uses CoT in training and inference to provide causal reasoning that improves interpretability and perceptual fidelity while jointly training generation and understanding for mutual gains.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Omni-Weather-Long: Extending Unified Generation‚ÄìUnderstanding to Medium-Range and Global Forecasts: Scale the shared backbone to multi-day/global settings (e.g., cyclone tracks), preserving interpretable CoT reasoning under longer temporal horizons.<br>‚Ä¢ Physically Consistent CoT: Causal Reasoning with Hard Physics Constraints for Weather Generation: Embed conservation laws and dynamical priors into CoT and decoding to enforce physically valid, explainable forecasts.<br>‚Ä¢ General-VAE Bridge for Scientific Domains: Adapting General-Domain VAEs to Meteorological Fields: Develop alignment/adapters so unified models can plug in off-the-shelf VAEs without losing scientific fidelity.<br>‚Ä¢ Any-to-Any Weather Fusion: Unifying Radar, Satellite, NWP, and Lightning in a Single Omni-Modal Generator‚ÄìAnalyst: Expand inputs/outputs to multi-sensor any-to-any translation with shared reasoning over heterogeneous evidence.<br>‚Ä¢ Real-Time Decision Support with Explainable Nowcasting: From Causal Narratives to Risk and Action: Link CoT explanations to hazard/risk metrics and recommended actions for operational forecasting and emergency response.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.18745" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.18745" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The ability for AI agents to "think with images" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Open multimodal agents struggle to "think with images"‚Äîthey lack robust multi-step reasoning that interleaves perception and logic across high-density visuals (maps, composite charts), which is critical for real-world tasks like document analysis and navigation (page 1‚Äì2).<br>‚Ä¢ Existing benchmarks overemphasize object-level perception in natural images and single-region lookups, offering short reasoning chains and failing to drive progress in complex visual reasoning; hence a harder benchmark is needed (page 2‚Äì4).<br>‚Ä¢ Current visual search is narrow: it targets discrete objects rather than generalized, relational, fuzzy, or conceptual regions specified in free-form language, misaligned with how users describe attention targets (page 2).<br>‚Ä¢ Monolithic single-agent MLLMs conflate heavy perception and reasoning loads, leading to optimization and credit-assignment difficulties and weaker performance than proprietary systems (page 4‚Äì5); open models also tend to reply too concisely (page 5).<br>‚Ä¢ There is no standardized way to evaluate or leverage interleaved multi-region evidence gathering; even frontier models like OpenAI o3 achieve only 40.8% on the proposed benchmark, underscoring the gap (page 1‚Äì3).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>INSIGHT-O3 is a two-agent framework where a high-level visual reasoning agent (vReasoner) delegates free-form region descriptions to a reinforcement-learned visual searcher (InSight-o3-vS) that performs generalized visual search via iterative zoom/crop to localize relational/fuzzy regions and return evidence. Complemented by the O3-BENCH benchmark, this plug-and-play vSearcher significantly boosts diverse vReasoners on multi-hop, high-density visual tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Joint Credit Assignment for Co-Training vReasoner and vSearcher in Interleaved Visual Reasoning: Develop multi-agent RL algorithms that stabilize non-stationary training and assign credit across reasoning‚Äìsearch turns for end-to-end performance gains.<br>‚Ä¢ Spatiotemporal InSight-o3: Generalized Visual Search for Video and 3D Scenes: Extend generalized visual search to temporal and 3D settings, enabling multi-hop reasoning over motion, occlusion, and depth-aware relations.<br>‚Ä¢ Document-Scale Multimodal Planning with Cross-Page Evidence Aggregation: Scale INSIGHT-O3 to multi-image documents (reports, manuals) with index-aware search, cross-page linking, and tool-augmented reasoning (OCR, table parsing, routing).</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">SWE-RM: Execution-free Feedback For Software Engineering Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21919" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21919" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Execution-based feedback (unit tests) is sparse, binary, and often unreliable due to limited test coverage and noisy/over-specific tests, restricting scalable TTS and RL.<br>‚Ä¢ Test-time scaling (TTS) alone is an inadequate proxy for verifier quality‚Äîverifiers with similar TTS can behave very differently in RL; missing dimensions are discrimination and calibration.<br>‚Ä¢ Existing execution-free verifiers are underexplored, typically short-context (‚âà32k), trained on small/limited datasets and policy distributions, and lack robust calibration.<br>‚Ä¢ RL for SWE suffers from sparse 0/1 rewards, sandbox/compute bottlenecks, and reward noise; poorly calibrated reward models destabilize training.<br>‚Ä¢ There is a need for a versatile verifier usable across both TTS and RL that provides fine-grained, reliable feedback over entire trajectories, including very long contexts.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SWE-RM trains a long-context (up to 256k) MoE reward model as a generative classifier over full agent trajectories, producing a calibrated continuous score from YES/NO logits and evaluated by a three-metric recipe (TTS, AUC, ECE). Through large-scale ablations (data scaling to ~100k samples, 2:1 positive:negative ratio, mixed on/off-policy and multi-source data), the model is used for both TTS ranking and as hybrid execution-free+execution-based reward in RL.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning to Calibrate Execution-Free Verifiers for SWE RL: Post-hoc and in-training calibration methods (e.g., temperature scaling, isotonic, focal objectives) to reduce ECE and stabilize RL.<br>‚Ä¢ Hybrid Verifier Scheduling for Code Agents: A learned policy to adaptively weight execution-free and execution-based feedback per instance and training phase.<br>‚Ä¢ Million-Token Reward Modeling for Multi-Repo SWE Tasks: Scaling long-context reward models beyond 256k to handle multi-file, multi-repo, and long-horizon trajectories.<br>‚Ä¢ Data-Centric Reward Modeling via Automatic Mining and Debiasing: Pipelines to mine, denoise, and rebalance SWE trajectories, optimizing positive/negative ratios and reducing label noise.<br>‚Ä¢ Online Reward-Model Adaptation in Agentic RL: Continual and on-policy updates of the reward model to track policy shift and mitigate OOD drift during training.<br>‚Ä¢ Interpretable Reward Models for Software Engineering Agents: Attribution and critique generation to explain reward decisions and detect spurious correlations.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SVBench: Evaluation of Video Generation Models on Social Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21507" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21507" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent text-to-video generation models exhibit remarkable progress in visual realism, motion fidelity, and text-video alignment, yet they remain fundamentally limited in their ability to generate socially coherent behavior. Unlike humans, who effortlessly infer intentions, beliefs, emotions, and social norms from brief visual cues, current models tend to render literal scenes without capturing the underlying causal or psychological logic. To systematically evaluate this gap, we introduce the first benchmark for social reasoning in video generation. Grounded in findings from developmental and social psychology, our benchmark organizes thirty classic social cognition paradigms into seven core dimensions, including mental-state inference, goal-directed action, joint attention, social coordination, prosocial behavior, social norms, and multi-agent strategy. To operationalize these paradigms, we develop a fully training-free agent-based pipeline that (i) distills the reasoning mechanism of each experiment, (ii) synthesizes diverse video-ready scenarios, (iii) enforces conceptual neutrality and difficulty control through cue-based critique, and (iv) evaluates generated videos using a high-capacity VLM judge across five interpretable dimensions of social reasoning. Using this framework, we conduct the first large-scale study across seven state-of-the-art video generation systems. Our results reveal substantial performance gaps: while modern models excel in surface-level plausibility, they systematically fail in intention recognition, belief reasoning, joint attention, and prosocial inference.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Gap in social reasoning: Current video generators render visually plausible scenes but fail to capture intentions, beliefs, emotions, norms, and causal social logic‚Äîproducing literal actions without why they happen (pp. 1‚Äì2, Fig. 1a).<br>‚Ä¢ Lack of benchmarks: Existing evaluations (e.g., VBench, EvalCrafter, Morpheus) emphasize visual quality, motion, and physics, not social coherence; there is no standardized testbed for social reasoning in generation (p. 2).<br>‚Ä¢ Evaluation challenges: Social interactions admit many valid realizations; continuous scoring is noisy and human annotation is costly‚Äîrequiring a robust, scalable, training-free evaluation protocol (pp. 4‚Äì5).<br>‚Ä¢ Prompt bias and difficulty: Uncontrolled prompts leak answers or include interpretive language; tasks lack calibrated difficulty via social cue manipulation (gaze, pointing, occlusion), confounding assessment (p. 4, Fig. 2).<br>‚Ä¢ Practical importance: Human-like video synthesis needs mental-state inference, joint attention, prosocial responses, and norm compliance; findings show large gaps in SOTA models across these dimensions (Table 2).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A training-free, four-agent pipeline maps 30 classic social cognition paradigms across seven dimensions into difficulty-controlled, conceptually neutral video prompts and evaluates generations via a VLM judge on five binary dimensions (Core Paradigm, Faithfulness, Social Coherence, Cue Effectiveness, Plausibility) (Fig. 2; Table 1).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Long-Video SVBench: Benchmarking Extended Social Reasoning in Multi-Event Generative Videos: Extend SVBench to long-horizon tasks (e.g., deception, delayed gratification, multi-stage coordination) requiring multi-event causal chains and memory.<br>‚Ä¢ Socially Coherent Video Generation via RL from VLM/Human Feedback: Optimize generators directly on SVBench‚Äôs five binary dimensions using preference learning and RL to improve social logic under sparse cues.<br>‚Ä¢ Causal Mental-State Modules for Video Diffusion/Transformers: Integrate explicit belief/goal inference and joint-attention controllers into video backbones to ground actions in latent social causality.<br>‚Ä¢ Culture-Aware Social Norm Generation and Evaluation: Expand norms (proxemics, queuing, etiquette) across cultures and contexts to test generalization and bias in social behavior synthesis.<br>‚Ä¢ Calibrated VLM Judges for Social Reasoning with Uncertainty: Develop evaluators aligned to human thresholds, with reliability estimation and counterfactual probing to reduce scoring noise and bias.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SlideTailor: Personalized Presentation Slide Generation for Scientific Papers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.20292" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.20292" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ One-size-fits-all slide generators ignore subjective user preferences (narrative flow, emphasis, aesthetics), often yielding misaligned, low-utility slides.<br>‚Ä¢ Users find it hard to articulate detailed preferences; leveraging natural artifacts (a prior paper‚Äìslides pair and a .pptx template) is needed but underused or handled only for layout by existing methods.<br>‚Ä¢ Current systems struggle to jointly align content structure and visual style, causing poor coverage/flow, template misuse, blank space, and missing figures/tables.<br>‚Ä¢ Slides are rarely planned together with the presenter‚Äôs speech; the lack of speech-aware planning hurts coherence and downstream uses like automatic presentation videos.<br>‚Ä¢ The field lacks a dedicated benchmark and interpretable metrics for assessing preference alignment and overall quality, making evaluation ad hoc.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SlideTailor is an agentic pipeline that distills implicit content preferences from a paper‚Äìslides example and aesthetic preferences from a .pptx template into a structured profile, then performs preference-guided paper reorganization, slide outlining with a chain-of-speech, template-aware layout selection, and programmatic .pptx editing. This produces fully editable, personalized slides that align both content and visual style.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ End-to-End Multimodal Preference Learning for Personalized Slide Generation: Train a unified VLM/LLM to jointly learn preference extraction, planning, and rendering from large-scale paper‚Äìslides corpora, surpassing training-free pipelines.<br>‚Ä¢ HAPE: Human-Aligned Preference Evaluation for Personalized Presentations: Design fine-grained, human-calibrated metrics and multi-judge protocols to assess aesthetic fidelity and content structure while mitigating MLLM self-bias.<br>‚Ä¢ Beyond Academia: Cross-Domain Preference-Guided Presentation Generation: Generalize the framework to business, education, and marketing, studying domain transfer, preference harmonization, and robustness across diverse templates.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21980" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21980" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This paper presents a new state-of-the-art algorithm for exact 3times3 matrix multiplication over general non-commutative rings, achieving a rank-23 scheme with only 58 scalar additions. This improves the previous best additive complexity of 60 additions without a change of basis. The result was discovered through an automated search combining ternary-restricted flip-graph exploration with greedy intersection reduction for common subexpression elimination. The resulting scheme uses only coefficients from {-1, 0, 1}, ensuring both efficiency and portability across arbitrary fields. The total scalar operation count is reduced from 83 to 81.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Practical need to reduce additive complexity in rank-23 3√ó3 matrix multiplication, since additions/subtractions materially impact real-world performance in BLAS-like libraries and repeated small-matrix workloads (e.g., graphics, scientific computing).<br>‚Ä¢ Existing best schemes still incur more additions: 60 additions without change of basis (prior state of the art), 61 with change of basis, 62 in other recent work, and 98 in Laderman‚Äôs original scheme‚Äîleaving clear room for improvement.<br>‚Ä¢ Many search approaches work over Z2 and then lift coefficients, which complicates correctness over general (including non-commutative) rings and harms portability; there is a need for ternary-coefficient ({‚àí1,0,1}) schemes valid over arbitrary fields/rings.<br>‚Ä¢ The search space is vast and riddled with local optima; existing methods lack effective mechanisms to both navigate flips under ternary constraints and aggressively eliminate additions via global common-subexpression reuse.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>An automated three-phase search that performs a random walk on a ternary-restricted flip graph (with a plus operator to escape local optima while preserving correctness) and then applies greedy intersection reduction (global CSE across U, V, W) to minimize additions. This yields a rank-23 scheme using only {‚àí1,0,1} coefficients with 58 additions (total scalar ops 81) without change of basis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Toward Additive Optimality for Rank-23 3√ó3 Multiplication: Lower Bounds and Tightness Proofs: Establish theoretical lower bounds on additive complexity and determine whether 58 additions is optimal.<br>‚Ä¢ Ternary Flip-Graph Search Beyond 3√ó3: Low-Addition Schemes for Other Small Matrix Formats: Extend the ternary flip-graph + greedy CSE methodology to formats such as 2√ó3, 3√ó4, and 4√ó4, comparing ranks and additive costs.<br>‚Ä¢ From Algorithm to Library: Integrating the 58-Addition 3√ó3 Scheme into BLAS and Benchmarking Across Architectures: Implement, vectorize, and benchmark the scheme on CPUs/GPUs to quantify end-to-end performance gains and portability.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.21625" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.21625" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large reasoning models (LRMs) are typically trained using reinforcement learning with verifiable reward (RLVR) to enhance their reasoning abilities. In this paradigm, policies are updated using both positive and negative self-generated rollouts, which correspond to distinct sample polarities. In this paper, we provide a systematic investigation into how these sample polarities affect RLVR training dynamics and behaviors. We find that positive samples sharpen existing correct reasoning patterns, while negative samples encourage exploration of new reasoning paths. We further explore how adjusting the advantage values of positive and negative samples at both the sample level and the token level affects RLVR training. Based on these insights, we propose an Adaptive and Asymmetric token-level Advantage shaping method for Policy Optimization, namely A3PO, that more precisely allocates advantage signals to key tokens across different polarities. Experiments across five reasoning benchmarks demonstrate the effectiveness of our approach.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ The roles and training dynamics of positive vs. negative rollouts in RL with verifiable rewards (RLVR) are underexplored, leaving open how each polarity affects entropy, exploration/exploitation, response length, reward progression, and generalization.<br>‚Ä¢ Existing methods often treat polarities symmetrically or only at the sample level, lacking fine-grained, polarity-aware token-level credit assignment; prior analyses are limited in datasets/models, reducing generalizability of conclusions.<br>‚Ä¢ RLVR training suffers from imbalanced exploration (entropy collapse or over-cautious exploration), reward hacking and mojibake when optimizing only one polarity, and a trade-off between fast reward gains and broad exploration.<br>‚Ä¢ Negative-sample emphasis can amplify training‚Äìinference mismatch (probability gaps between engines), destabilizing training; a stable approach is needed to harness negative samples without collapse.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A3PO (Adaptive and Asymmetric token-level Advantage shaping for Policy Optimization) boosts advantages for low-probability tokens in positive rollouts and high-probability tokens in negative rollouts early in training, then smoothly decays these boosts to a standard regime. Built on DAPO, it applies polarity-aware token thresholds and time-decayed scaling to maintain exploration, prevent premature entropy collapse, and improve validation generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Polarity-Aware RLVR for Vision‚ÄìLanguage Models: Extend adaptive, asymmetric token-level advantage shaping to multimodal tokens and align entropy/exploration across text and vision streams for robust VLM reasoning.<br>‚Ä¢ Mitigating Training‚ÄìInference Mismatch in RLVR: Design calibration, adaptive clipping, and importance-sampling strategies targeted at negative rollouts to reduce engine probability gaps and guarantee stable policy updates.<br>‚Ä¢ Curriculum Scheduling of Sample Polarity and Token Credit: Learn a closed-loop scheduler that dynamically tunes positive/negative ratios and token categories (by entropy/probability) based on online signals to balance sharpening and discovery.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 5</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-12-29 23:09:06 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 5;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>