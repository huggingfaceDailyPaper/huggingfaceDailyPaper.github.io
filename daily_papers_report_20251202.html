<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - December 02, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">December 02, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.18538" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.18538" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Fragmented understanding of the end-to-end lifecycle of code LLMs (from data curation and pre-training to alignment, RL, and agents), leading to suboptimal design choices.<br>‚Ä¢ Misalignment between academic benchmarks and real-world software engineering needs (code correctness, security, large-repository context, and workflow integration).<br>‚Ä¢ Unclear trade-offs among general vs. code-specialized LLMs, model architectures, training tasks, and scaling/hyperparameter decisions.<br>‚Ä¢ Lack of standardized, verifiable evaluation and post-training strategies (SFT, preference optimization, RL) that reliably improve code quality and robustness.<br>‚Ä¢ Safety gaps in data provenance/licensing/privacy, adversarial robustness, bias, and effective red-teaming for code LLMs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A comprehensive, experimentally grounded guide that systematizes tasks, datasets, metrics, and training recipes across pre-training, supervised fine-tuning, reinforcement learning (including verifiable rewards), and agentic workflows; it benchmarks general and code-specialized LLMs and probes scaling laws, framework choices, hyperparameters, architectures, and data to distill practical trade-offs for real-world deployment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ RepoBench: A Repository-Scale, Long-Context Benchmark for Real-World Software Engineering: Evaluate end-to-end correctness, security, maintainability, and workflow integration under multi-file, multi-agent, and tool-augmented settings.<br>‚Ä¢ Scaling RL with Verifiable Rewards for Code LLMs: A Large-Scale Study of Reward Shaping and Quality-Oriented Objectives: Train code models across diverse tasks using executable, verifiable rewards with principled shaping and quality metrics to improve reliability.<br>‚Ä¢ Safety Alignment for Code LLMs via Multi-Stage Preference Optimization and Red-Teaming Pipelines: Combine curated safety datasets, SFT, advanced preference optimization, RL-based safety alignment, and adversarial evaluations to mitigate privacy, license, bias, and vulnerability risks.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LongVT: Incentivizing "Thinking with Long Videos" via Native Tool Calling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20785" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20785" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables "Thinking with Long Videos" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Long-form video reasoning is prone to hallucinations because evidence is sparse and dispersed over time, making textual Chain-of-Thought alone unreliable.<br>‚Ä¢ Existing R1-style pipelines are language-centric and use uniform frame sampling, often missing decisive moments and failing to adaptively localize evidence.<br>‚Ä¢ LMMs have limited context windows, preventing efficient processing of hours-long videos without targeted retrieval.<br>‚Ä¢ Current RL for video reasoning typically optimizes answer-only or time-only objectives, misaligning answer correctness with temporal grounding.<br>‚Ä¢ The open-source ecosystem lacks fine-grained, tool-augmented training data and evaluation benchmarks for ‚Äúvideo segment-in-a-haystack‚Äù reasoning; many datasets are multiple-choice and weakly grounded.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>LongVT introduces interleaved Multimodal Chain-of-Tool-Thought (iMCoTT) that lets an LMM propose time windows, call a native crop_video tool to resample fine-grained frames, self-reflect, and iterate until answers are visually grounded. It uses a three-stage training pipeline‚Äîcold-start SFT with tool-augmented data (VideoSIAH), agentic RL with a joint answer-format-IoU reward (GRPO), and reinforcement fine-tuning with self-distilled high-quality rollouts‚Äîplus a new VideoSIAH(-Eval) data suite for training and evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Audio-Visual LongVT: Joint Tool-Calling for Multimodal Evidence in Long Videos: Extend iMCoTT to invoke audio tools (ASR, audio cropping) alongside video cropping for stronger grounding under silent or noisy visual contexts.<br>‚Ä¢ Planning and Tool Composition for Long-Video Agents: Beyond Temporal Cropping: Learn policies that compose multiple native tools (temporal crop, tracking, OCR, subtitle retrieval) with hierarchical planning for complex, cross-shot reasoning.<br>‚Ä¢ Self-Play LongVT: Synthetic iMCoTT Generation for Data-Efficient Segment-in-a-Haystack Learning: Reduce reliance on curated supervision by using self-play, trajectory bootstrapping, and verifier-driven filtering to generate high-quality tool-augmented traces at scale.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01816" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01816" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Reliance on static, single-frame T2I training/evaluation leads to overfitting to pattern matching and semantic fusion, preventing models from learning causal-temporal dynamics.<br>‚Ä¢ Existing benchmarks emphasize aesthetics and semantic alignment but lack event-level spatiotemporal causality assessment, limiting true world knowledge internalization.<br>‚Ä¢ Current models show a gap: specialized T2I excel in visuals but lack intrinsic world knowledge; unified models improve coherence yet still struggle with spatiotemporal consistency and lack holistic multi-frame metrics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce Envision, a benchmark for chained text-to-multi-image generation grounded in world knowledge and spatiotemporal causality, featuring 1,000 four-stage prompts across six domains, and Envision-Score‚Äîa holistic event-level metric integrating aesthetics, consistency, and physical plausibility.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Causal Memory Architectures for Unified Multi-Image and Video Generation: Design models with explicit temporal state, causal graphs, and object/attribute persistence to improve narrative coherence and spatiotemporal consistency.<br>‚Ä¢ Envision-Score++: Human-Aligned, Physics-Grounded Evaluation for Causal Event Generation: Extend the metric with human preference modeling, physics-based checks, and automatic causal chain verification for better alignment with human judgments.<br>‚Ä¢ Curriculum Learning from Semantic Anchoring to World Simulation for Multimodal Generative Models: Develop staged training curricula following the Envision-Vision to progressively build semantic, spatial, temporal, and simulation capabilities using multi-image sequences as intermediate representations.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">Stabilizing Reinforcement Learning with LLMs: Formulation and Practices</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01374" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01374" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Mismatch between sequence-level rewards and token-level optimization in RL for LLMs lacks a principled justification and often causes instability.<br>‚Ä¢ Direct optimization of sequence-level objectives is high-variance/intractable; value-based methods depend on unreliable value models at LLM scale.<br>‚Ä¢ Training‚Äìinference discrepancy (different engines/kernels) and policy staleness (off-policy/asynchronous updates) degrade the validity of policy gradient updates.<br>‚Ä¢ In MoE models, dynamic expert routing amplifies both discrepancies and invalidates token-level importance sampling, further destabilizing training.<br>‚Ä¢ Lack of a unifying theory explaining why techniques like IS correction, clipping, and Routing Replay help, and when they are necessary.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>They show that token-level REINFORCE with token-level importance sampling is a first-order approximation to the true sequence-level objective, valid when training‚Äìinference discrepancy and policy staleness are small, obtained by decomposing the IS ratio into these two factors. To enforce these conditions‚Äîespecially in MoE‚Äîthey introduce Routing Replay variants (R2/R3) that fix routed experts to reduce discrepancy/staleness, and use a minimalist baseline (MiniRL) with group-normalized rewards and decoupled PPO-style clipping to stabilize updates.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond First-Order: Second-Order Corrections for Token-Level Surrogates in LLM RL: Derive and test second-order/curvature-aware corrections and adaptive trust-region schedules that tighten the gap to the true sequence-level objective.<br>‚Ä¢ Bias-Reduced Routing Replay for MoE RL: Design unbiased or lower-bias replay schemes (e.g., stochastic routing reconstruction, control variates) that preserve first-order validity without altering the target policy.<br>‚Ä¢ Adaptive Staleness Control in Large-Scale LLM RL: Online estimation of policy staleness to adapt mini-batch reuse, learning rates, and clipping thresholds under varying off-policy degrees and asynchronous rollouts.<br>‚Ä¢ Discrepancy-Aware Engines for RL with LLMs: Build calibration/compensation layers and batch-invariant kernels to minimize training‚Äìinference drift, with real-time diagnostics and automatic mitigation.<br>‚Ä¢ Low-Variance Sequence-Level RL for LLMs: Develop variance-reduced estimators (e.g., stratified sampling, control variates, reparameterization) that directly optimize sequence-level rewards at scale.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">How Far Are We from Genuinely Useful Deep Research Agents?</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01948" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01948" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing evaluations validate DRAs mostly on QA or close-ended tasks, not on end-to-end research report synthesis that real users need.<br>‚Ä¢ Current open-ended benchmarks often rely on LLM-generated tasks, leading to misalignment with human demands and insufficient task complexity.<br>‚Ä¢ Evaluation is largely subjective and lacks fine-grained, standardized criteria for structure, analytical depth, and factual grounding, limiting practical utility.<br>‚Ä¢ There is no comprehensive, empirically grounded failure taxonomy to explain where DRAs break (retrieval, reasoning, generation), hindering targeted improvements.<br>‚Ä¢ In practice, DRAs struggle with evidence integration, verification, and reasoning-resilient planning, which undermines their adoption in real-world research workflows.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce FINDER, a human-curated benchmark of 100 research tasks with 419 checklist items that standardize report structure, analytical depth, and factual grounding. Build DEFT, a grounded 14-mode failure taxonomy across reasoning, retrieval, and generation, derived from ~1,000 DRA reports via human‚ÄìLLM co-annotation with inter-annotator reliability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Reasoning-Resilient Planning for Deep Research Agents: Develop and evaluate planning algorithms that maintain correctness under noisy evidence and changing contexts using FINDER and DEFT signals.<br>‚Ä¢ Trustworthy Evidence Integration and Verification in DRAs: Design multi-source corroboration, citation auditing, and fact-checking pipelines to reduce integration and verification failures identified by DEFT.<br>‚Ä¢ Checklist-Guided Automatic Grading of Research Reports: Convert FINDER‚Äôs structured checklists into reliable LLM-based or hybrid automatic scorers with validated agreement to human judgments.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.00425" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.00425" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent video diffusion models can synthesize visually compelling clips, yet often violate basic physical laws-objects float, accelerations drift, and collisions behave inconsistently-revealing a persistent gap between visual realism and physical realism. We propose NewtonRewards, the first physics-grounded post-training framework for video generation based on verifiable rewards. Instead of relying on human or VLM feedback, NewtonRewards extracts measurable proxies from generated videos using frozen utility models: optical flow serves as a proxy for velocity, while high-level appearance features serve as a proxy for mass. These proxies enable explicit enforcement of Newtonian structure through two complementary rewards: a Newtonian kinematic constraint enforcing constant-acceleration dynamics, and a mass conservation reward preventing trivial, degenerate solutions. We evaluate NewtonRewards on five Newtonian Motion Primitives (free fall, horizontal/parabolic throw, and ramp sliding down/up) using our newly constructed large-scale benchmark, NewtonBench-60K. Across all primitives in visual and physics metrics, NewtonRewards consistently improves physical plausibility, motion smoothness, and temporal coherence over prior post-training methods. It further maintains strong performance under out-of-distribution shifts in height, speed, and friction. Our results show that physics-grounded verifiable rewards offer a scalable path toward physics-aware video generation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Modern video generators often violate basic physics (e.g., objects float, accelerations drift, collisions are inconsistent), revealing a gap between visual and physical realism.<br>‚Ä¢ Existing post-training relies on human/VLM feedback or visual-feature alignment that cannot precisely verify compliance with Newton‚Äôs laws, thus improving appearance but not dynamics.<br>‚Ä¢ There is a lack of scalable, rule-based (verifiable) rewards and broad physics benchmarks; models struggle with OOD shifts and can exhibit reward hacking without proper physical constraints.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>NewtonRewards post-trains video diffusion models using verifiable physics-grounded rewards derived from measurable proxies: optical flow as a velocity surrogate and video embeddings as a mass proxy. It enforces a discrete constant-acceleration constraint and a mass-conservation term to fine-tune generators toward Newtonian-consistent motion across five primitives on the NewtonBench-60K dataset.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Verifiable Rewards for Momentum and Energy Conservation in Multi-Object Video Generation: Extend proxy-based constraints to handle collisions, momentum exchange, and energy conservation for interacting objects.<br>‚Ä¢ 3D Physics-Aware Video Post-Training via Geometry and Depth Proxies: Integrate depth, camera pose, and 3D scene geometry to enforce Newtonian dynamics under moving cameras and varying viewpoints.<br>‚Ä¢ Self-Supervised Physics Compliance from Real-World Footage Using Proxy Models: Learn physics-grounded constraints from large-scale real videos without simulation labels, leveraging optical flow, segmentation, and embeddings to generalize to OOD conditions.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20614" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20614" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Previous works have explored various customized generation tasks given a reference image, but they still face limitations in generating consistent fine-grained details. In this paper, our aim is to solve the inconsistency problem of generated images by applying a reference-guided post-editing approach and present our ImageCritic. We first construct a dataset of reference-degraded-target triplets obtained via VLM-based selection and explicit degradation, which effectively simulates the common inaccuracies or inconsistencies observed in existing generation models. Furthermore, building on a thorough examination of the model's attention mechanisms and intrinsic representations, we accordingly devise an attention alignment loss and a detail encoder to precisely rectify inconsistencies. ImageCritic can be integrated into an agent framework to automatically detect inconsistencies and correct them with multi-round and local editing in complex scenarios. Extensive experiments demonstrate that ImageCritic can effectively resolve detail-related issues in various customized generation scenarios, providing significant improvements over existing methods.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Fine-grained inconsistencies (texts, logos, small structures) between generated images and reference inputs persist in customized generation due to VAE encoding/decoding discrepancies and loss of shallow-layer information in decoder-only/DiT architectures.<br>‚Ä¢ Lack of high-quality datasets and benchmarks focused on detail-level consistency; existing datasets prioritize global subject identity over local fidelity.<br>‚Ä¢ Existing post-editing approaches (reference-based super-resolution and multi-image editing via MLLMs) fail to accurately localize and correct fine details and may exacerbate errors.<br>‚Ä¢ Attention coupling in DiT-based editing causes conflicting cues from input and reference branches, hindering precise region-wise correction.<br>‚Ä¢ Need for automated, scalable workflows to detect, localize, and iteratively fix inconsistencies in complex, real-world scenarios.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ImageCritic is a reference-guided post-editing framework for diffusion models that introduces an attention alignment loss and a detail encoder to decouple and align reference/input attentions and accurately localize fine-grained corrections, trained on a VLM-filtered reference‚Äìdegraded‚Äìtarget dataset and deployed via an agent chain for automated multi-round local editing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ VideoCritic: Temporally Consistent Reference-Guided Attention Alignment for Diffusion Video Editing: Extend attention alignment and detail encoding to video, enforcing temporal consistency across frames with motion-aware masks and sequence-level losses.<br>‚Ä¢ Mask-Free Consistency Correction via Pseudo-Attention Supervision in Diffusion Transformers: Replace explicit SAM-based masks with self-supervised or VLM-induced pseudo masks to guide attention alignment, reducing annotation overhead and improving scalability.<br>‚Ä¢ Plug-and-Play Consistency Modules for Diverse Generative Backbones: Generalize the detail encoder and attention alignment loss across UNet-, DiT-, and decoder-only architectures, evaluating cross-model transfer and robustness for text/logo fidelity.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20649" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20649" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce infty-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish infty-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that infty-RoPE consistently surpasses previous autoregressive models in overall VBench scores.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Finite temporal horizon from 3D-RoPE‚Äôs absolute indexing and short-clip training, causing attention degradation beyond the model‚Äôs seen positions.<br>‚Ä¢ Slow prompt responsiveness and weak fine-grained action control during long rollouts due to stale, large KV caches.<br>‚Ä¢ Inability to realize discontinuous cinematic transitions (scene cuts, flashbacks) within a single autoregressive generation stream.<br>‚Ä¢ Existing autoregressive methods rely on costly retraining/long self-rollouts, suffer train‚Äìtest mismatch or exposure bias, and still remain constrained by positional strategies and memory footprints.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Infinity-RoPE is a training-free, plug-and-play inference framework combining Block-Relativistic RoPE (moving local temporal frame with backward rotation and semanticization beyond horizon), KV Flush (cache reset to only a global sink and the last frame for instant action responsiveness), and RoPE Cut (controlled discontinuous RoPE offsets for cinematic multi-cut transitions), enabling infinite-horizon, controllable video generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Relativistic Positional Encodings for Long-Video Diffusion: Train end-to-end models that internalize Block-Relativistic RoPE and semanticization to improve stability and quality beyond seen horizons.<br>‚Ä¢ Adaptive KV Management for Instant Action Control in Streaming Video: Develop learned or attention-driven KV gating/refresh policies that optimize the trade-off between prompt responsiveness and temporal continuity across diverse scenes.<br>‚Ä¢ Cinematic Transition Planning with RoPE Cut Schedules: Use reinforcement learning or sequence planning to automatically select cut timings and offsets that maximize narrative coherence while preserving identity and motion consistency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.02014" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.02014" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Native unified multimodal models struggle to balance image/video understanding and generation within one architecture.<br>‚Ä¢ Decoupled visual representations cause format mismatches (spatial/temporal compression and channel size), require MoE-style routing, add parameters, and create representation conflicts that hurt both efficiency and quality.<br>‚Ä¢ Existing unified designs often bias toward a single task: single-encoder approaches favor either understanding or generation; late-fusion (e.g., Show-o2) yields semantic-heavy features that degrade generation fidelity.<br>‚Ä¢ Discrete visual tokens (VQ-VAE/codebooks) lose information and limit high-fidelity generation; continuous latent spaces are preferred but underutilized for unified UMMs.<br>‚Ä¢ Current training recipes do not fully exploit potential synergy‚Äîunderstanding and generation often interfere rather than mutually enhance.<br>‚Ä¢ Efficient handling of video tokens is challenging due to long sequences and temporal structure.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TUNA builds a unified continuous visual representation by cascading a 3D causal VAE encoder with a modified SigLIP2 representation encoder (1√ó1 patch embed over VAE latents), producing visual tokens that are fused with text and processed by an LLM decoder. Understanding uses an autoregressive LM head, while generation/editing use a flow-matching head on noised visual tokens; a three-stage training pipeline (rep+flow pretrain, end-to-end continue pretrain, SFT) aligns the unified representation and unlocks cross-task synergy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Scaling Unified Visual Tokenizers for Native UMMs: Evaluate larger and multi-scale representation encoders and hierarchical fusion over VAE latents to further improve balance between understanding and generation.<br>‚Ä¢ Audio-Image-Video Unified Representation via Cascaded Encoders: Extend TUNA‚Äôs cascaded VAE‚Üírepresentation-encoder design to audio/speech latents for fully tri-modal native UMMs.<br>‚Ä¢ Temporal-Adaptive Unified Tokens for Long-Video UMMs: Introduce sparse/windowed/streaming attention and dynamic frame grouping to handle long-form video understanding and generation efficiently.<br>‚Ä¢ Curriculum and Timestep Scheduling for Flow-Matched UMMs: Learn data- and objective-aware timestep curricula and conditioning strategies to improve generation quality and stability.<br>‚Ä¢ Self-Supervised Multitask Pretraining for Unified Representations: Jointly pretrain with captioning, masked modeling, and reconstruction to strengthen semantics without sacrificing generative fidelity.<br>‚Ä¢ Cross-Model Distillation to Balance Semantics and Fidelity: Distill from strong diffusion generators and strong vision encoders into the unified tokenizer to optimize alignment with both regimes.<br>‚Ä¢ Safety- and Text-Robust Image/Text Rendering in Unified Generators: Add controllable decoding, safety filters, and robust text rendering objectives for reliable instruction following.<br>‚Ä¢ Parameter-Efficient Scaling of Native UMMs without MoE: Explore low-rank/adaptor tuning and shared modules to scale LLM decoders while keeping a compact unified visual stack.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">LFM2 Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.23404" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.23404" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present LFM2, a family of Liquid Foundation Models designed for efficient on-device deployment and strong task capabilities. Using hardware-in-the-loop architecture search under edge latency and memory constraints, we obtain a compact hybrid backbone that combines gated short convolutions with a small number of grouped query attention blocks, delivering up to 2x faster prefill and decode on CPUs compared to similarly sized models. The LFM2 family covers 350M-8.3B parameters, including dense models (350M, 700M, 1.2B, 2.6B) and a mixture-of-experts variant (8.3B total, 1.5B active), all with 32K context length. LFM2's training pipeline includes a tempered, decoupled Top-K knowledge distillation objective that avoids support mismatch; curriculum learning with difficulty-ordered data; and a three-stage post-training recipe of supervised fine-tuning, length-normalized preference optimization, and model merging. Pre-trained on 10-12T tokens, LFM2 models achieve strong results across diverse benchmarks; for example, LFM2-2.6B reaches 79.56% on IFEval and 82.41% on GSM8K. We further build multimodal and retrieval variants: LFM2-VL for vision-language tasks, LFM2-Audio for speech, and LFM2-ColBERT for retrieval. LFM2-VL supports tunable accuracy-latency tradeoffs via token-efficient visual processing, while LFM2-Audio separates audio input and output pathways to enable real-time speech-to-speech interaction competitive with models 3x larger. LFM2-ColBERT provides a low-latency encoder for queries and documents, enabling high-performance retrieval across multiple languages. All models are released with open weights and deployment packages for ExecuTorch, llama.cpp, and vLLM, making LFM2 a practical base for edge applications that need fast, memory-efficient inference and strong task capabilities.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Edge deployment requires strict TTFT, stable inter-token latency, and low peak memory on CPUs/NPUs, but most open models target larger scales and accelerator-heavy settings, leaving a gap for small edge-first models.<br>‚Ä¢ Existing efficient backbones (SSMs/linear attention mixed with conv or attention-heavy stacks) often worsen device-level latency/memory; prior searches rely on proxies (perplexity, cache size) rather than hardware-in-the-loop TTFT/latency/memory objectives.<br>‚Ä¢ Small-model distillation via Top-K teacher logits suffers from support mismatch and instability with temperature scaling; better KD objectives and curricula are needed to raise quality without prohibitive cost.<br>‚Ä¢ Edge applications need native multimodality (vision, audio) and retrieval with tunable accuracy‚Äìlatency trade-offs; current VL/audio/retrieval pipelines are typically too heavy for real-time on-device use.<br>‚Ä¢ Practical deployment lacks open, quantized weights and optimized packages for common edge runtimes; consistent CPU speedups versus baselines remain limited.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>LFM2 uses hardware-in-the-loop architecture search to select a minimal hybrid backbone‚Äîgated short depthwise convolutions interleaved with a few grouped-query attention layers‚Äîsubject to device-side latency and memory budgets. Models are trained on 10‚Äì12T tokens with a long-context mid-training phase and a decoupled, tempered Top-K distillation objective, followed by supervised fine-tuning, length-normalized preference optimization, and model merging; the family spans dense (350M‚Äì2.6B), MoE (8.3B total/1.5B active), and token-efficient VL, low-latency audio, and late-interaction retrieval variants with open, edge-ready deployments.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Minimal Hybrid Backbones for Edge LMs: A Cross-Device Evaluation of Conv+GQA vs SSM/Linear Attention: Systematically benchmark architectures under identical quantization/backends across CPUs/NPUs to quantify cache behavior, KV traffic, and quality‚Äìlatency‚Äìmemory trade-offs.<br>‚Ä¢ Decoupled Tempered Top-K Distillation: Theory, Scheduling, and Scaling Laws for Small On-Device Models: Formalize support-mismatch avoidance, study temperature/K schedules, and measure gains across multilingual/code corpora and model sizes.<br>‚Ä¢ Real-Time On-Device Speech-to-Speech with Unified Discrete Audio Tokenization: Design codecs, streaming decoders, and routing to further lower TTFT and inter-token latency while improving speech quality under tight CPU/NPU budgets.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.00590" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.00590" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3times fewer than AriGraph and <1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Open IE KGs extracted by LLMs are noisy, redundant, and inconsistent (synonyms, coreference, predicate variation), undermining precision, interpretability, and logical consistency<br>‚Ä¢ Existing LLM/RAG systems mostly use KGs as auxiliary scaffolds for text retrieval rather than as high-quality, standalone knowledge sources<br>‚Ä¢ Closed IE pipelines suffer from error accumulation across stages; end-to-end cIE models require costly retraining on scarce annotated data and are hard to adapt to new domains<br>‚Ä¢ Lack of ontology enforcement leads to type/domain‚Äìrange violations and fragmented graphs; integrating large ontologies into automated extraction is non-trivial<br>‚Ä¢ Practical constraints demand compact, well-connected KGs with low token budgets; prior approaches (e.g., GraphRAG, AriGraph) are token-heavy at build time</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Wikontic is a multi-stage, LLM-driven pipeline that extracts candidate triplets with qualifiers from text, aligns and validates them against a Wikidata-derived ontology (enforcing type and domain‚Äìrange constraints via P31/P279 hierarchy), and performs alias-aware entity normalization/deduplication using dense retrieval. The result is a compact, ontology-consistent KG stored with canonical labels and aliases and used directly for multi-hop QA.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond-Wikidata: Cross-Ontology Alignment for Ontology-Aware KG Construction: Extend Wikontic to integrate and reconcile multiple ontologies, enabling broader coverage and schema interoperability<br>‚Ä¢ Temporal and Qualifier-Rich Reasoning in Wikontic: Incorporate richer qualifiers (time, location, provenance) and temporal consistency checks to support time-sensitive reasoning<br>‚Ä¢ Multilingual Wikontic: Ontology-Aligned KG Construction from Non-English Text: Generalize extraction, typing, and linking across languages with cross-lingual embeddings and multilingual alias catalogs<br>‚Ä¢ Streaming Wikontic: Incremental KG Construction with Consistency Guarantees: Develop online updating and conflict resolution mechanisms for real-time, ontology-consistent KG maintenance</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Rectifying LLM Thought from Lens of Optimization</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01925" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01925" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Long chain-of-thought (CoT) LLMs often overthink, producing excessively long, redundant reasoning that increases latency and compute and can harm accuracy.<br>‚Ä¢ RLVR pipelines optimize only terminal correctness rewards, providing no process-level guidance; this leads to unstable optimization paths (oscillations, backtracking) that hinder convergence.<br>‚Ä¢ Existing fixes (length/information regularization or heuristic control of reasoning) either sacrifice accuracy or lack scalable, low-noise, process-level feedback during training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RePro models CoT as an optimization trajectory and defines a surrogate objective (average log-probability of the ground-truth answer conditioned on partial thoughts), then aggregates a magnitude (net improvement) and stability (Kendall-tau-like) score into a process-level reward computed on entropy-selected thought segments and integrated into RLVR (e.g., PPO/GRPO/REINFORCE++) via normalized advantages.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Proxy Objectives for Process Rewards in LLM Reasoning: Train domain-adaptive or self-supervised surrogate objectives to enable process-level rewards without gold answers and to better capture progress signals.<br>‚Ä¢ Adaptive Segment Selection for Process-Level Reward: From Entropy to Uncertainty Curves: Develop learned or Bayesian segment selection strategies that optimize signal-to-noise and compute by modeling uncertainty/difficulty over thought trajectories.<br>‚Ä¢ Theoretical Analysis of Process-Reward RLVR: Convergence, Stability, and Sample Efficiency: Provide formal guarantees and trade-off characterizations for combining outcome and process rewards, including optimal weighting of magnitude vs. stability and alpha scheduling.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.20549" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.20549" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive. Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking. In this work, we introduce Flash-DMD, a novel framework that enables fast convergence with distillation and joint RL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only 2.1% its training cost. Second, we introduce a joint training scheme where the model is fine-tuned with an RL objective while the timestep distillation training continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as a powerful regularizer, effectively stabilizing the RL training process and preventing policy collapse. Extensive experiments on score-based and flow matching models show that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in the few-step sampling regime, outperforming existing methods in visual quality, human preference, and text-image alignment metrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Diffusion models require many iterative steps, making sampling computationally expensive and slow for practical deployment.<br>‚Ä¢ Existing timestep distillation methods accelerate sampling but demand extensive training and often degrade image fidelity.<br>‚Ä¢ RL-based fine-tuning for aesthetics or user preference is unstable, prone to reward hacking, and can cause policy collapse.<br>‚Ä¢ There is a need for a unified training framework that achieves high-fidelity few-step generation efficiently while stabilizing RL optimization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Flash-DMD introduces an efficient timestep-aware distillation that produces high-fidelity few-step generators at ~2‚Äì3% of DMD2‚Äôs training cost, and a joint training scheme where RL fine-tuning runs concurrently with distillation so the distillation loss regularizes RL to prevent collapse and reward hacking.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Flash-DMD: Multi-Objective Joint Distillation-RL for Balancing Aesthetics, Realism, and Safety: Develop Pareto-aware optimization to trade off competing objectives with dynamic weighting.<br>‚Ä¢ Flash-DMD for Video and 3D Generative Models: Extend timestep-aware distillation and joint RL to temporally coherent video synthesis and 3D scene generation.<br>‚Ä¢ Theory of Joint Distillation-RL in Generative Modeling: Provide stability and convergence guarantees and derive principled schedules for combining losses and exploration.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01801" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01801" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting Q-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundations models to specialize into reliable real-world experts.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Generalist VLA policies are unreliable for millimeter-level, dexterous manipulation with deformable objects (e.g., shoelace threading).<br>‚Ä¢ Long-horizon robustness is weak‚Äîerrors compound across steps, especially when precision is required.<br>‚Ä¢ Human demonstrations in dexterous tasks are noisy and suboptimal; na√Øve behavior cloning learns hesitation and mistakes.<br>‚Ä¢ Mismatch between training (fixed-length action chunks) and deployment (smoothed, receding-horizon optimized actions) causes distribution shift and degraded control.<br>‚Ä¢ Standard regression/critics under sparse rewards are brittle; a robust task-progress signal is needed to filter and improve data quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>GR-RL applies a three-stage reinforcement-augmented pipeline: learn a distributional offline RL critic as a task-progress estimator to filter demonstrations, apply morphological symmetry (mirror) augmentation, and perform online RL by steering a latent noise predictor for the flow-based action policy to align training with deployment and improve long-horizon precision.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Distilling Online-Refined Specialists into Generalist VLA Backbones: Transfer the behaviors learned via online RL back into the base VLA to achieve both generality and reliability.<br>‚Ä¢ Credit Assignment in Latent Noise Spaces for Long-Horizon Dexterous Control: Design algorithms that better attribute sparse rewards in large latent action spaces to reduce behavior drift and stabilize online learning.<br>‚Ä¢ Optimization-in-the-Loop Imitation for VLA: Training with Trajectory Smoothing and Receding-Horizon Control: Integrate deployment-time motion optimization into the training objective to eliminate train‚Äìtest mismatch and improve precision.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01031" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01031" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-Language-Action models (VLAs) are becoming increasingly capable across diverse robotic tasks. However, their real-world deployment remains slow and inefficient: demonstration videos are often sped up by 5-10x to appear smooth, with noticeable action stalls and delayed reactions to environmental changes. Asynchronous inference offers a promising solution to achieve continuous and low-latency control by enabling robots to execute actions and perform inference simultaneously. However, because the robot and environment continue to evolve during inference, a temporal misalignment arises between the prediction and execution intervals. This leads to significant action instability, while existing methods either degrade accuracy or introduce runtime overhead to mitigate it. We propose VLASH, a general asynchronous inference framework for VLAs that delivers smooth, accurate, and fast reaction control without additional overhead or architectural changes. VLASH estimates the future execution-time state by rolling the robot state forward with the previously generated action chunk, thereby bridging the gap between prediction and execution. Experiments show that VLASH achieves up to 2.03x speedup and reduces reaction latency by up to 17.4x compared to synchronous inference while fully preserving the original accuracy. Moreover, it empowers VLAs to handle fast-reaction, high-precision tasks such as playing ping-pong and playing whack-a-mole, where traditional synchronous inference fails. Code is available at https://github.com/mit-han-lab/vlash</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Real-world VLA deployments suffer from action stalls and slow, discontinuous motion under synchronous inference, forcing sped-up demos to appear smooth.<br>‚Ä¢ Naive asynchronous inference introduces prediction‚Äìexecution temporal misalignment due to inference latency, causing unstable, inaccurate actions when executed on evolved robot/environment states.<br>‚Ä¢ Existing fixes (e.g., RTC inpainting, extra correction heads) add runtime overhead, require architectural changes, or complex multi-threaded redesigns, hindering adoption.<br>‚Ä¢ Current VLAs often under-utilize robot state, so simply feeding a future state at test time is insufficient without targeted training, limiting robust low-latency control.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VLASH performs future-state-aware asynchronous inference by rolling the robot state forward with the previously issued action chunk to estimate the execution-time state and conditioning the policy on it, eliminating prediction‚Äìexecution misalignment without extra overhead or architecture changes. It enforces state usage via a temporal-offset fine-tuning augmentation with shared-observation packing for efficiency and optionally applies action quantization at deployment to further accelerate motion.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Predicting the Future: Visual Foresight for Asynchronous VLAs: Learn to predict future visual tokens/observations alongside rolled robot state to fully align perception and action at execution time.<br>‚Ä¢ Delay-Adaptive VLASH: Online Estimation and Control Horizon Scheduling: Jointly estimate inference latency and adapt execution horizons/offsets and action quantization on-the-fly for different hardware loads and tasks.<br>‚Ä¢ Uncertainty-Aware Future-State Conditioning for Safe High-Speed Robotics: Incorporate dynamics and rollout uncertainty (e.g., ensembles or Bayesian heads) to generate risk-aware actions that remain stable under model and environment variance.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01342" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01342" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large-scale video-text pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicit world knowledge such as object motion, 3D geometry, and physical cues. In contrast, masked video modeling (MVM) directly exploits spatiotemporal structures but trails text-supervised methods on general tasks. We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encourages shortcut learning. To address these, we disentangle the traditional encoder-decoder design into an Encoder-Predictor-Decoder (EPD) framework, where the predictor acts as a latent world model, and propose InternVideo-Next, a two-stage pretraining scheme that builds a semantically consistent yet detail-preserving latent space for this world model. First, conventional linear decoder in pixel MVM enforces the predictor output latent to be linearly projected to, thus separable in pixel space, causing the conflict with semantic abstraction. Our Stage 1 proposes a conditional diffusion decoder and injects reliable image-level semantic priors to enhance semantics and convergence, thus bridging pixel-level fidelity with high-level semantic abstraction. Stage 2 further learns world knowledge by predicting frozen Stage 1 targets within this space, mitigating shortcut learning. Trained on public, unlabeled videos, InternVideo-Next achieves state-of-the-art results across benchmarks and provides a scalable path toward general video representation learning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Reliance on noisy, synthetic video‚Äìtext captions limits semantic coverage and overlooks implicit world knowledge such as motion, 3D geometry, and physical cues, introducing bias in video‚Äìtext pretraining.<br>‚Ä¢ Pixel-level masked video modeling (MVM) struggles with convergence and conflicts with semantic abstraction; linear decoders force predictor outputs to be linearly projectable to pixels, harming high-level semantics.<br>‚Ä¢ Latent prediction methods often learn shortcuts or drift semantically, losing low-level detail and failing to capture robust spatiotemporal dynamics, resulting in gaps on both appearance- and motion-intensive tasks.<br>‚Ä¢ Lack of a semantically consistent, detail-preserving latent space shared by encoder and predictor prevents forming a robust latent world model.<br>‚Ä¢ Need a scalable, unbiased, video-only pretraining path that bridges pixel fidelity and semantic abstraction while remaining multimodally friendly and competitive with video‚Äìtext models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>InternVideo-Next disentangles masked video modeling into an Encoder‚ÄìPredictor‚ÄìDecoder framework with two-stage pretraining: Stage 1 builds a semantically rich yet detail-preserving latent space via diffusion-based pixel reconstruction guided by image-model (SigLIP) semantic alignment and semantic-aware masking; Stage 2 performs frozen-teacher latent prediction with multi-block masking to learn robust spatiotemporal and causal dynamics while avoiding shortcut learning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Caption-Free Semantic Priors for Video Pretraining: Self-Distilling Semantics Without Image‚ÄìText Teachers: Replace external image-text teachers with self-supervised or bootstrapped semantic targets to remove dependency while retaining convergence and semantic richness.<br>‚Ä¢ Geometry- and Physics-Aware Latent World Models: Inject Explicit 3D and Physical Constraints into EPD Training: Incorporate multi-view consistency, depth/flow supervision, or differentiable physics to strengthen geometric and causal reasoning in the latent space.<br>‚Ä¢ Scaling Laws and Efficient Training for Video-Only Foundation Models: Data, Compute, and Architecture Trade-offs: Systematically study scaling behaviors across data quality/quantity, masking, predictor/decoder capacity, and diffusion steps to optimize efficiency and performance.<br>‚Ä¢ Multimodal Interactive World Models for Video-LLMs: Extending EPD to Audio and Language for Open-World Reasoning: Integrate audio and lightweight text alignment during pretraining and evaluate as a frozen backbone in video-chat and retrieval to enhance interactive multimodal understanding.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Flow Straighter and Faster: Efficient One-Step Generative Modeling via MeanFlow on Rectified Trajectories</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.23342" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.23342" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Flow-based generative models have recently demonstrated strong performance, yet sampling typically relies on expensive numerical integration of ordinary differential equations (ODEs). Rectified Flow enables one-step sampling by learning nearly straight probability paths, but achieving such straightness requires multiple computationally intensive reflow iterations. MeanFlow achieves one-step generation by directly modeling the average velocity over time; however, when trained on highly curved flows, it suffers from slow convergence and noisy supervision. To address these limitations, we propose Rectified MeanFlow, a framework that models the mean velocity field along the rectified trajectory using only a single reflow step. This eliminates the need for perfectly straightened trajectories while enabling efficient training. Furthermore, we introduce a simple yet effective truncation heuristic that aims to reduce residual curvature and further improve performance. Extensive experiments on ImageNet at 64, 256, and 512 resolutions show that Re-MeanFlow consistently outperforms prior one-step flow distillation and Rectified Flow methods in both sample quality and training efficiency. Code is available at https://github.com/Xinxi-Zhang/Re-MeanFlow.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ High sampling cost in flow/diffusion models due to highly curved generative trajectories, requiring many ODE steps for accurate sampling.<br>‚Ä¢ Rectified Flow needs repeated reflow iterations and cannot achieve perfectly straight trajectories in finite steps; one-step Euler is unreliable in edge cases even after a single reflow.<br>‚Ä¢ MeanFlow enables one-step sampling but trains poorly on highly curved paths due to noisy, unstable supervision and slow convergence.<br>‚Ä¢ Few-step/distillation paradigms often have high training compute and stability issues, with heavy dependence on high-end GPUs; a more efficient, robust pipeline is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Re-Meanflow learns the mean velocity field along a trajectory rectified by a single reflow step, stabilizing the MeanFlow objective without requiring perfectly straight paths, and adds a simple truncation heuristic that discards the top 10% couplings by ‚Ñì2 trajectory distance to mitigate residual curvature. This synergy yields efficient, high-quality one-step generation with reduced training compute.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Curvature-Aware Truncation for One-Step Flows: Learn data-driven criteria to dynamically identify and down-weight high-curvature couplings beyond fixed top-k filtering.<br>‚Ä¢ Multi-Interval Rectified MeanFlow: Model piecewise or weighted mean velocities over adaptively chosen time intervals to further improve accuracy while keeping one-step sampling.<br>‚Ä¢ Theoretical Guarantees for Rectified Mean Velocity Sampling: Derive error bounds and sufficient conditions under Lipschitz assumptions for one-step sampling with rectified mean fields.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.00722" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.00722" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Layer-wise KV retrieval during decoding introduces heavy synchronization and breaks compute‚Äìmemory overlap, causing up to 60% latency overhead.<br>‚Ä¢ Existing KV optimizations for long-context input rely on preprocessing and fully retaining newly generated KV, which is ill-suited for growing caches in reasoning and degrades accuracy/throughput (often worse than full attention).<br>‚Ä¢ Static offloading and I/O-bound CPU‚ÄìGPU transfers cannot adapt to sequence-length growth, leading to >80% performance drops with small length increases; adaptive memory management is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SpeContext co-designs an algorithm‚Äìsystem‚Äìcompiler stack that uses a distilled LM-based lightweight retrieval head (head-level attention, pruned to QK projections) to preselect important tokens before inference, enabling asynchronous KV prefetch with elastic loading and an adaptive memory model that maximizes GPU utilization as sequences grow.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning-to-Retrieve: Online Distillation of Lightweight Heads for Adaptive Long-Context Reasoning: Continually fine-tune the retrieval head to specific domains and prompts to improve alignment and accuracy under distribution shifts.<br>‚Ä¢ Distributed SpeContext: Cross-GPU KV Prefetch and Elastic Loading for Cluster-Scale LLM Serving: Extend the asynchronous dataflow and memory model to multi-GPU/multi-node settings with interconnect-aware sharding and prefetch.<br>‚Ä¢ Speculative Context Sparsity for Multimodal and RAG LLMs: Generalize DLM-guided retrieval to multimodal inputs and retrieval-augmented generation, selecting salient cross-modal tokens and document chunks.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">Accelerating Streaming Video Large Language Models via Hierarchical Token Compression</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.00891" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.00891" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose Streaming Token Compression (STC), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: STC-Cacher, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and STC-Pruner, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to 99\% of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by 24.5\% and 45.3\%.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Streaming VideoLLMs suffer from excessive latency because ViT must repeatedly encode temporally redundant frames, making real-time applications (e.g., live commentary, AR) impractical.<br>‚Ä¢ LLM pre-filling is burdened by inflated multi-frame visual token sequences, causing quadratic self-attention cost and large KV memory, further increasing end-to-end latency.<br>‚Ä¢ Existing compression methods either rely on offline/global video access or instruction-aware pruning, violating streaming causality; KV cache compression only helps decoding, and intra-ViT token merging (e.g., ToMe) degrades SVU performance.<br>‚Ä¢ There is a need for a causal, query-agnostic, plug-and-play approach that jointly reduces ViT compute and LLM sequence length while preserving accuracy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes Streaming Token Compression (STC), a hierarchical, plug-and-play framework with two modules: STC-Cacher reduces ViT cost by caching reference-frame features and selectively recomputing only dynamic tokens identified via Key-projection cosine similarity, while STC-Pruner shortens the LLM prefill sequence by dual-anchor (temporal and spatial) novelty scoring and pruning under causal, query-agnostic constraints.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Causally Adaptive Token Caching for Streaming ViTs: Train a lightweight policy to adjust cache interval and reuse ratio online based on scene dynamics to optimize accuracy‚Äìlatency trade-offs.<br>‚Ä¢ Query-Time Fusion for Streaming Token Compression: Integrate late-arriving instructions with gated re-weighting over retained tokens to recover task-relevant context without violating causal processing.<br>‚Ä¢ End-to-End Differentiable STC with Sparse Attention Backbones: Co-train STC with sparse/linear attention LLMs to jointly optimize visual encoding and prefilling for edge deployment under strict latency and memory budgets.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PromptBridge: Cross-Model Prompt Transfer for Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01420" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01420" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) underpin applications in code generation, mathematical reasoning, and agent-based workflows. In practice, systems access LLMs via commercial APIs or open-source deployments, and the model landscape (e.g., GPT, Claude, Llama) evolves rapidly. This rapid evolution forces frequent model switches driven by capability, cost, deployment constraints, and privacy. Yet prompts are highly model-sensitive: reusing a prompt engineered for one model on another often yields substantially worse performance than a prompt optimized for the target model. We term this phenomenon Model Drifting. Through extensive empirical analysis across diverse LLM configurations, we show that model drifting is both common and severe. To address this challenge, we introduce PromptBridge, a training-free framework that preserves prompt effectiveness under model switches, enabling cross-model prompt transfer without costly per-task or per-model re-optimization. PromptBridge requires only a small set of alignment tasks for calibration. It first applies Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to obtain task- and model-specific optimal prompts via iterative reflective refinement and quantitative evaluation. Using the resulting calibrated prompt pairs for the source and target models, PromptBridge learns a cross-model prompt mapping. At test time, i.e., for an unseen task, given a source-model prompt, this mapping directly produces an optimized prompt for the target model. Experiments in single-agent and multi-agent settings show that PromptBridge consistently improves downstream accuracy while reducing migration effort. The code will be available soon.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Model drifting: prompts tuned for one LLM often degrade when transferred to another, creating a transfer gap relative to the target model‚Äôs own optimal prompt.<br>‚Ä¢ Existing prompt optimization methods are largely task-specific, assume a fixed model, rely on proprietary models or soft prompts, and require costly per-task/per-model re-tuning with limited cross-model robustness.<br>‚Ä¢ Differences in model alignment, interfaces (e.g., role tags), tokenization, and training corpora make prompt libraries brittle across model families; there is a need for a training-free, data-light solution that preserves performance during model switches in single- and multi-agent systems.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>PromptBridge is a training-free framework that calibrates source‚Äìtarget prompt pairs on a small set of alignment tasks using Model-Adaptive Reflective Prompt Evolution (MAP-RPE), then learns a cross-model prompt mapping to transform a source prompt into a target-compatible prompt for unseen tasks. At test time, this mapping enables zero-shot prompt adaptation without re-optimization or task-specific evaluation data.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Continual PromptBridge: Online drift-aware calibration and mapping updates to maintain transfer performance under frequent LLM releases and evolving alignments.<br>‚Ä¢ Universal Prompt Embedding Space for Cross-Model Transfer: Learn a shared latent representation of prompts across models to enable compact anchor sets and more robust zero-shot mappings.<br>‚Ä¢ PromptBridge-Agents: Joint cross-model prompt transfer for multi-agent systems that co-adapts local and global agent prompts while coordinating inter-agent dependencies.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.00466" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.00466" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Test-time compute scaling has emerged as a powerful paradigm for enhancing mathematical reasoning in large language models (LLMs) by allocating additional computational resources during inference. However, current methods employ uniform resource distribution across all reasoning sub-problems, creating fundamental bottlenecks where challenging sub-problems receive insufficient attention while routine operations consume disproportionate resources. This uniform allocation creates performance bottlenecks where additional computational resources yield diminishing returns. Inspired by dual-process theory, we propose SCALE (Selective Resource Allocation), a framework that selectively allocates computational resources based on sub-problem difficulty. SCALE operates through four stages: (1) problem decomposition into sequential reasoning sub-problems, (2) difficulty assessment of each sub-problem to distinguish between routine operations and computationally challenging sub-problems, (3) selective processing mode assignment between System 1 for simple sub-problems and System 2 for complex ones, and (4) sequential execution with context propagation. By concentrating resources on challenging sub-problems while processing routine operations efficiently, SCALE achieves substantial performance improvements with superior resource utilization. Extensive experiments demonstrate that SCALE significantly outperforms uniform scaling baselines, achieving accuracy improvements of up to 13.75 percentage points (57.50% to 71.25% on AIME25) while reducing computational costs by 33%-53%, representing a major advance in test-time scaling that addresses fundamental limitations of current approaches.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Uniform test-time compute is allocated across all reasoning steps, causing overthinking and bottlenecks where challenging sub-problems lack sufficient attention while routine operations waste resources.<br>‚Ä¢ Existing adaptive methods adjust global reasoning length at the problem level but do not perform fine-grained, intra-problem resource allocation based on sub-problem difficulty.<br>‚Ä¢ Additional compute often yields diminishing returns under uniform allocation, limiting the effectiveness of test-time scaling on hard mathematical tasks.<br>‚Ä¢ Iterative and ensemble baselines (e.g., InftyThink, Majority Voting) incur large token costs without consistently outperforming targeted allocation strategies.<br>‚Ä¢ There is a need for a principled framework that concentrates compute on solution-critical sub-problems while processing routine steps efficiently, improving both accuracy and token efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SCALE is a four-stage, dual-process framework: decompose a math problem into sequential sub-problems, assess each sub-problem‚Äôs difficulty, route easy ones to fast System 1 and hard ones to deliberate System 2 based on a threshold, then execute sequentially with full context propagation‚Äîconcentrating compute on difficult steps while minimizing waste on routine operations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learned Sub-problem Router: Training a Neural Compute Allocator for Fine-grained Test-time Scaling: Replace prompt-based difficulty assessment with a trained router that predicts sub-problem difficulty and allocates variable compute budgets via supervised or reinforcement learning.<br>‚Ä¢ Continuous Compute Budgets: Token-level Resource Scheduling for Mathematical Reasoning: Extend binary System1/System2 routing to continuous, per-step token budgets using optimization or bandit methods to maximize accuracy under compute constraints.<br>‚Ä¢ SCALE Beyond Math: Selective Allocation for Code Generation and Theorem Proving: Generalize sub-problem decomposition, difficulty assessment, and selective routing to program synthesis and formal proofs, with domain-specific decomposition and verification mechanisms.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.17282" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.17282" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multilingual text-to-image (T2I) models have advanced rapidly in terms of visual realism and semantic alignment, and are now widely utilized. Yet outputs vary across cultural contexts: because language carries cultural connotations, images synthesized from multilingual prompts should preserve cross-lingual cultural consistency. We conduct a comprehensive analysis showing that current T2I models often produce culturally neutral or English-biased results under multilingual prompts. Analyses of two representative models indicate that the issue stems not from missing cultural knowledge but from insufficient activation of culture-related representations. We propose a probing method that localizes culture-sensitive signals to a small set of neurons in a few fixed layers. Guided by this finding, we introduce two complementary alignment strategies: (1) inference-time cultural activation that amplifies the identified neurons without backbone fine-tuned; and (2) layer-targeted cultural enhancement that updates only culturally relevant layers. Experiments on our CultureBench demonstrate consistent improvements over strong baselines in cultural consistency while preserving fidelity and diversity.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Multilingual T2I models often generate culturally neutral or English-biased images for non-English prompts, undermining cross-lingual cultural consistency<br>‚Ä¢ Current efforts focus on cross-lingual semantic alignment (encoders) rather than cross-cultural grounding in the visual generation process<br>‚Ä¢ Short or ‚Äúnoun-only‚Äù prompts fail to activate culture-specific knowledge, creating a cross-modal gap compared with LLMs/recommenders that localize responses<br>‚Ä¢ There is limited understanding of where culture-sensitive signals reside in diffusion models (layers/neurons), hindering targeted control<br>‚Ä¢ Existing approaches lack lightweight, plug-and-play interventions and standardized benchmarks to diagnose and improve cultural consistency across languages<br>‚Ä¢ Risk of conflating cultural typicality with stereotypes calls for methods that preserve fidelity/diversity while ensuring context-appropriate cultural cues</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Probe diffusion models to localize culture-sensitive representations by contrasting attention patterns between ‚Äúculture-style modifier + noun‚Äù and ‚Äúnoun-only‚Äù prompts and quantifying activation shifts with Top-K SAE, revealing sparse neurons in fixed layers. Then improve cultural consistency via two strategies: (1) zero-training inference-time amplification of identified neurons and (2) layer-targeted fine-tuning of culturally relevant layers; evaluation is enabled by the curated 15-country CultureBench and CultureVQA.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Causal Editing of Culture-Sensitive Neurons in Diffusion Models: Disentangle and robustly edit culture-related units to control cultural cues without affecting semantics or style fidelity<br>‚Ä¢ Prompt-Aware Cultural Augmentation for Multilingual T2I: An agent that auto-detects under-specified prompts and injects culturally appropriate modifiers conditioned on language and region<br>‚Ä¢ CultureBench++: Long-Tail Languages and Temporal Drift in Cross-Cultural T2I: Expand coverage to low-resource languages and study how cultural representations evolve over time and affect generation</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01949" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01949" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is a technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, a plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: a graph-structured pruning module that removes visually redundant tokens, and a query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT-7B, it achieves up to 6.8x prefill speedup and 10x FLOP reduction, while retaining 96.88% of the original performance.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Explosion of visual tokens in MLLMs leads to high memory footprint and latency due to quadratic attention, especially for high-resolution images and videos<br>‚Ä¢ Existing pruning paradigms either rely on unreliable attention scores (attention sink/drift) or ignore query semantics, causing loss of task-critical tokens<br>‚Ä¢ Many approaches require retraining or are backbone-specific, limiting plug-and-play deployment and compatibility with optimized attention backends<br>‚Ä¢ Need a scalable redundancy estimator that captures both local and long-range visual similarity without exhaustive computation</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Script is a training-free, model-agnostic pruning framework that couples graph-structured redundancy estimation (via cosine-similarity weighted even‚Äìodd bipartite graphs) with query-conditioned semantic selection using Determinantal Point Processes to retain diverse, query-relevant visual tokens. Together, these modules remove visually redundant tokens and preserve task-critical cues without retraining.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Query-Aware Pruning for Streaming Video: Extend Script with temporal graphs and online DPP to model and prune spatiotemporal redundancy in long video sequences<br>‚Ä¢ Learnable Graph-Structured Token Pruning: Add lightweight, task-aware learning of graph weights or selection policies to optimize redundancy estimation while maintaining minimal training overhead<br>‚Ä¢ Theoretical Guarantees for DPP-Based Pruning in MLLMs: Establish bounds on performance loss and diversity/relevance trade-offs for query-conditioned DPP selection<br>‚Ä¢ Hardware-Aware Multimodal Pruning with FlashAttention Co-Design: Jointly design pruning schedules and attention kernels to maximize throughput and memory efficiency on modern accelerators</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01707" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01707" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of a benchmark that integrates human gaze into streaming video understanding across past, present, and proactive tasks, leading to evaluations that diverge from real AR/embodied use-cases.<br>‚Ä¢ Difficulty of leveraging raw, noisy gaze in egocentric streams (moving viewpoints, unstable trajectories), with no robust fixation/scanpath modeling and spatio-temporal grounding for QA generation.<br>‚Ä¢ Existing MLLMs and gaze QA models struggle to causally integrate gaze over time, model user intention, and provide reliable proactive alerts, revealing fundamental limitations in gaze-conditioned temporal reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>STREAMGAZE introduces a semi-automatic gaze-guided pipeline that aligns egocentric videos with raw gaze via fixation extraction, FOV/out-of-FOV region-specific visual prompting, and scanpath construction to generate spatio-temporally grounded QA pairs; it defines a unified past‚Äìpresent‚Äìproactive task taxonomy and evaluates MLLMs with gaze-aware prompting to diagnose failure modes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Gaze-Attentive Streaming Transformers for Causal Video Reasoning: Fuse gaze as dynamic queries into long-horizon memory to track attention shifts and predict future actions in real time.<br>‚Ä¢ Uncertainty-Aware Fixation and Scanpath Estimation for Egocentric Streams: Probabilistic denoising and uncertainty modeling of gaze to jointly ground gaze and objects, improving temporal reasoning and proactive alerts.<br>‚Ä¢ Task-Aware Prompt and Policy Routing for Gaze-Guided MLLMs: Meta-controller that adaptively selects and blends textual, visual, and salience-map prompts and reasoning strategies per task to maximize performance across past, present, and proactive settings.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01030" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01030" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures. While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning. Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions. In this work, we propose Lotus-2, a two-stage deterministic framework for stable, accurate and fine-grained geometric dense prediction, aiming to provide an optimal adaption protocol to fully exploit the pre-trained generative priors. Specifically, in the first stage, the core predictor employs a single-step deterministic formulation with a clean-data objective and a lightweight local continuity module (LCM) to generate globally coherent structures without grid artifacts. In the second stage, the detail sharpener performs a constrained multi-step rectified-flow refinement within the manifold defined by the core predictor, enhancing fine-grained geometry through noise-free deterministic flow matching. Using only 59K training samples, less than 1% of existing large-scale datasets, Lotus-2 establishes new state-of-the-art results in monocular depth estimation and highly competitive surface normal prediction. These results demonstrate that diffusion models can serve as deterministic world priors, enabling high-quality geometric reasoning beyond traditional discriminative and generative paradigms.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Single-image geometric dense prediction is ill-posed; models must infer globally coherent, physically plausible structure beyond appearance cues.<br>‚Ä¢ Discriminative regressors are constrained by limited supervised data scale/quality/diversity and lack strong world priors, leading to poor OOD performance (e.g., transparency, reflections, low texture).<br>‚Ä¢ Directly reusing diffusion‚Äôs stochastic, multi-step generative formulation causes non-determinism, structural variance, error accumulation, and slow inference; test-time ensembling blurs details without fixing instability.<br>‚Ä¢ Multi-step flows are hard to optimize under scarce ground-truth geometry and accumulate errors; single-step fine-tunes often lose fine-grained details.<br>‚Ä¢ FLUX‚Äôs parameter-free Pack‚ÄìUnpack introduces grid artifacts, especially under single-step settings, harming local continuity and fidelity.<br>‚Ä¢ There is a need to exploit large generative models as deterministic world priors to achieve accurate, stable, and detailed predictions with minimal labeled data.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Lotus-2 is a two-stage deterministic framework that adapts FLUX rectified-flow priors for dense prediction: a single-step, clean-data core predictor with a lightweight local continuity module yields accurate, globally coherent geometry, and an optional constrained multi-step rectified-flow detail sharpener refines high-frequency details within the predictor‚Äôs manifold‚Äîboth stages are noise-free and efficient.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Deterministic World Priors for Video: Temporally Consistent Rectified-Flow for 4D Geometry: Extend Lotus-2 with temporal flows and consistency constraints to deliver stable, detail-rich depth/normals in videos.<br>‚Ä¢ Multi-Task Lotus: Joint Deterministic Flow for Depth, Normals, and Intrinsics: Unify multiple geometric modalities in a shared deterministic flow with cross-modal physical constraints to boost accuracy and robustness.<br>‚Ä¢ Metric Depth with Minimal Labels: Scale-Consistent Deterministic Flows via Weak and Self-Supervision: Augment Lotus-2 with weak cues (stereo, sparse LiDAR, vanishing points) and self-training to move from affine-invariant to metric depth at low annotation cost.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Asking like Socrates: Socrates helps VLMs understand remote sensing images</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.22396" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.22396" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent multimodal reasoning models, inspired by DeepSeek-R1, have significantly advanced vision-language systems. However, in remote sensing (RS) tasks, we observe widespread pseudo reasoning: models narrate the process of reasoning rather than genuinely reason toward the correct answer based on visual evidence. We attribute this to the Glance Effect, where a single, coarse perception of large-scale RS imagery results in incomplete understanding and reasoning based on linguistic self-consistency instead of visual evidence. To address this, we propose RS-EoT (Remote Sensing Evidence-of-Thought), a language-driven, iterative visual evidence-seeking paradigm. To instill this paradigm, we propose SocraticAgent, a self-play multi-agent system that synthesizes reasoning traces via alternating cycles of reasoning and visual inspection. To enhance and generalize these patterns, we propose a two-stage progressive RL strategy: first, RL on fine-grained Grounding tasks to enhance RS-EoT capabilities, followed by RL on RS VQA to generalize to broader understanding scenarios. Experiments show RS-EoT achieves state-of-the-art performance on multiple RS VQA and grounding benchmarks. Analyses reveal clear iterative cycles of reasoning and evidence seeking, confirming RS-EoT mitigates the Glance Effect and enables genuine evidence-grounded reasoning. Our code, data, and models are available at https://geox-lab.github.io/Asking_like_Socrates</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Multimodal models exhibit pseudo reasoning on remote sensing tasks: explicit "thinking" often narrates steps without grounding in visual evidence and can underperform the non-reasoning base model.<br>‚Ä¢ The "Glance Effect"‚Äîa single coarse pass over large-scale RS imagery‚Äîyields incomplete perception, pushing models toward linguistically self-consistent answers rather than evidence-based logic.<br>‚Ä¢ Remote sensing demands iterative evidence acquisition due to wide spatial extents, scale variations, and subtle cues; existing SFT‚ÄìRL multimodal frameworks assume static global perception and lack mechanisms for targeted, stepwise inspection.<br>‚Ä¢ RL on common RS VQA (mostly Yes/No) is prone to reward hacking, hindering stable training; direct distillation of traces from frontier models produces reasoning patterns misaligned with RS cognition.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RS-EoT is a language-driven, iterative reasoning‚Äìperception loop instilled via SocraticAgent self-play (text-only Reasoner + image-aware Perceiver) to synthesize evidence-grounded traces, then strengthened by a two-stage RL pipeline (IoU-based grounding RL and multiple-choice VQA RL with graded symmetric rewards) to produce RS-EoT-7B.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Socratic-EoT for Multi-Temporal Remote Sensing: Evidence Seeking Across Time: Extend iterative reasoning‚Äìperception to sequences for change detection, motion cues, and temporally grounded answers.<br>‚Ä¢ Geo-Conditioned RS-EoT: Using Sun Angle, Sensor, and Map Priors to Guide Evidence Seeking: Integrate metadata (sun azimuth, sensor, GIS priors) to inform questions and constrain targeted visual inspections.<br>‚Ä¢ Reward Modeling for Evidence-Grounded RS VQA: Beyond Multiple-Choice Reconstruction: Train visual verifiers and step-wise reward models to support open-ended answers and mitigate reward hacking without MC conversion.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Agentic Policy Optimization via Instruction-Policy Co-Evolution</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01945" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01945" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and explores the interaction with the environment. To bridge the gap, we introduce INSPO, a novel Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop. INSPO maintains a dynamic population of instruction candidates that are sampled with questions, where reward signals in RL loops are automatically attributed to each instruction, and low performers are periodically pruned. New instructions are generated and verified through an on-policy reflection mechanism, where an LLM-based optimizer analyzes past experience from a replay buffer and evolves more effective strategies given the current policy. We conduct extensive experiments on multi-turn retrieval and reasoning tasks, demonstrating that INSPO substantially outperforms strong baselines relying on static instructions. INSPO discovers innovative instructions that guide the agent toward more strategic reasoning paths, achieving substantial performance gains with only a marginal increase in computational overhead.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ RL with verifiable rewards (RLVR) relies on static, hand-crafted instructions that are often suboptimal and can become misaligned as the policy improves, constraining exploration and leading to local optima.<br>‚Ä¢ Existing automated prompt optimization methods are offline and do not incorporate on-policy, environment-driven feedback during RL, making them ill-suited for multi-turn, tool-integrated agents.<br>‚Ä¢ Instruction design is costly and highly sensitive; small changes can significantly affect trajectories and RL convergence, especially in multi-turn settings where instruction quality determines tool-use strategies and reward density.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>INSPO co-evolves instructions and policy by maintaining a dynamic instruction population with importance weights updated from rewards, sampling instructions via softmax during GRPO-based training, and periodically pruning low performers. New instructions are generated through on-policy reflection using an LLM optimizer over failure trajectories from a replay buffer, then verified on a proxy set before inclusion.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Uncertainty-Aware INSPO: Bayesian Weighting and Risk-Sensitive Instruction Selection: Introduce Bayesian weight updates and risk-sensitive sampling to better balance exploration/exploitation under reward uncertainty.<br>‚Ä¢ INSPO for Multi-Tool Ecosystems: Co-Evolving Instructions Across Heterogeneous APIs: Extend co-evolution to routing and coordination across multiple tools (search, code, calculators), optimizing instructions for tool selection and sequencing.<br>‚Ä¢ Learning the Instruction Proposer: Meta-RL for Automatic Prompt Generation in Co-Evolution: Replace the static LLM optimizer with a trainable proposer optimized via meta-RL or reinforcement learning from proxy rewards to improve instruction generation quality and stability.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">HiconAgent: History Context-aware Policy Optimization for GUI Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01763" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01763" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Graphical User Interface (GUI) agents require effective use of historical context to perform sequential navigation tasks. While incorporating past actions and observations can improve decision making, naive use of full history leads to excessive computational overhead and distraction from irrelevant information. To address this, we introduce HiconAgent, a GUI agent trained with History Context-aware Policy Optimization (HCPO) for efficient and effective utilization of historical information. HCPO optimizes history usage in both sampling and policy updates through two complementary components: (1) Dynamic Context Sampling (DCS) presents the agent with variable length histories during sampling, enabling adaptive use of the most relevant context; (2) Anchor-guided History Compression (AHC) refines the policy update phase with a dual branch strategy where the compressed branch removes history observations while keeping history actions as information flow anchors. The compressed and uncompressed branches are coupled through a history-enhanced alignment loss to enforce consistent history usage while maintaining efficiency. Experiments on mainstream GUI navigation benchmarks demonstrate strong performance. Despite being smaller, HiconAgent-3B outperforms GUI-R1-7B by +8.46 percent grounding accuracy and +11.32 percent step success rate on GUI-Odyssey, while achieving comparable results on AndroidControl and AITW with up to 2.47x computational speedup and 60 percent FLOPs reduction.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ GUI agents need to leverage historical context (past screenshots and actions), but fixed-length or action-only histories are often suboptimal, as history dependence varies across steps and tasks.<br>‚Ä¢ Naively using full visual history inflates sequence length and attention cost, causing significant computational overhead and distraction from irrelevant tokens.<br>‚Ä¢ Existing RL-based GUI agents largely ignore efficient, effective history utilization (typically dropping visual history), lacking mechanisms to retain critical cues while reducing redundancy; naive uniform sampling of history lengths can further degenerate training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>HCPO trains GUI agents to use history effectively and efficiently by combining Dynamic Context Sampling (variable-length histories with an exponential-biased schedule during rollouts) and Anchor-guided History Compression (drop history visuals after early fusion, retain action tokens as anchors, and align compressed and full-history branches via KL).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning to Predict History Length for GUI Agents: A policy or auxiliary module that predicts optimal per-step history window at inference, replacing sampling with deterministic, context-aware selection.<br>‚Ä¢ Adaptive Layer-wise History Fusion and Compression for Multimodal RL: Jointly learn the early-fusion depth and token-pruning policy to optimize the compute‚Äìaccuracy trade-off across models and tasks.<br>‚Ä¢ History-Aware Reward Shaping for GUI Navigation: Design rewards that explicitly measure temporal consistency and history usage (e.g., anchor-guided grounding continuity) to better guide RL training.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.00369" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.00369" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal a critical, overlooked factor in reconstruction degradation: the approximate noise error. This error stems from approximating the noise at step t with the prediction at step t-1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale œâ as a step-wise variable and derives a mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ DDIM inversion relies on approximating the noise at step t with the prediction at t‚àí1, causing a persistent noise-prediction discrepancy and cumulative trajectory drift.<br>‚Ä¢ Fixed classifier-free guidance (CFG) scales amplify this inversion error and break the near-bijection, forcing a trade-off between semantic fidelity and reconstruction accuracy.<br>‚Ä¢ Existing methods primarily compensate downstream (e.g., optimizing prompts/latents), adding complexity and compute, with fragile behavior and suboptimal background preservation.<br>‚Ä¢ Heuristic or fixed guidance schedules during inversion leave the root cause of error accumulation unaddressed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>POLARIS treats the CFG scale œâ as a per-timestep variable and derives a closed-form update via projection-orthogonal least squares on conditional/unconditional noise-change vectors to minimize the Euclidean noise discrepancy between consecutive steps, yielding a plug-and-play adaptive guidance schedule with negligible overhead.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Adaptive Guidance Policies for Diffusion Inversion: Replace analytic POLARIS updates with a learned policy (e.g., reinforcement learning) to optimize œâ under diverse data and model regimes.<br>‚Ä¢ POLARIS-Video: Temporally Consistent Adaptive Inversion for Video Diffusion Models: Extend POLARIS to sequences with temporal coherence constraints and joint scale scheduling.<br>‚Ä¢ Theoretical Bounds on Inversion Error under Dynamic CFG: Formal analysis of convergence, stability, and error propagation when using per-step adaptive guidance in diffusion inversion.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ORION: Teaching Language Models to Reason Efficiently in the Language of Thought</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.22891" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.22891" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose "thinking" tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Large Reasoning Models rely on long, verbose chains of thought that inflate latency, token cost, and can produce redundant or incoherent reasoning (overthinking).<br>‚Ä¢ RL-based reasoning training (e.g., R1-style RLVR) is computationally expensive and hardware-inefficient; even small (1.5B) models take days due to rollout generation.<br>‚Ä¢ Natural-language CoT is verbose and not always faithful; there is a need for a compact, symbolic representation aligned with human cognitive efficiency (Language of Thought).<br>‚Ä¢ Existing brevity controls (e.g., fixed length budgets, adaptive budgets, or pruning) impose rigid penalties that can force under- or over-reasoning and lack adaptive preference for the shortest correct solution.<br>‚Ä¢ Practical deployment of reasoning models (e.g., agentic systems) is hindered by communication and inference overhead from verbose traces; a Pareto-efficient accuracy‚Äìcompression trade-off is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ORION aligns LLMs to a symbolic reasoning language, Mentalese, via supervised fine-tuning and then applies verifier-based RL with Shorter Length Preference Optimization (SLPO), a GRPO-style objective that rewards shorter correct traces while permitting longer reasoning when necessary. This two-stage RLVR pipeline yields 4‚Äì16√ó compression and up to 5√ó lower latency while maintaining high accuracy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Latent Mentalese: Learning Compact Internal Reasoning without External Tokens: Train models to execute Mentalese-like steps in hidden states and surface only final answers, pushing compression beyond explicit trace generation.<br>‚Ä¢ Auto-Discovering Mentalese Operators via Program Synthesis and Meta-RL: Automatically learn the operator set and step semantics to improve generalization and interpretability across domains.<br>‚Ä¢ SLPO for Multimodal and Agentic Settings: Length-Aware Rewarding across Text, Code, and Tools: Extend SLPO to multimodal reasoning and tool use, optimizing end-to-end latency and bandwidth in deployed agents.<br>‚Ä¢ Verifier-Light SLPO: Robust Preference Optimization without Expensive External Checkers: Develop self-verification and probabilistic checkers to reduce verifier dependence and scale beyond math-specific domains.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-11">

    <div class="paper">
        <h2 class="paper-title">The Art of Scaling Test-Time Compute for Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.02008" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.02008" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Test-time scaling (TTS) -- the dynamic allocation of compute during inference -- is a promising direction for improving reasoning in large language models (LLMs). However, a systematic comparison of well-known TTS strategies under identical conditions is missing, and the influence of model type and problem difficulty on performance remains unclear. To address these gaps, we conduct the first large-scale study of TTS, spanning over thirty billion tokens generated using eight open-source LLMs (7B to 235B parameters), across four reasoning datasets. We observe three consistent trends: (1) no single TTS strategy universally dominates; (2) reasoning models exhibit distinct trace-quality patterns across problem difficulty and trace length, forming short-horizon and long-horizon categories; and (3) for a given model type, the optimal TTS performance scales monotonically with compute budget. Based on these insights, we provide a practical recipe for selecting the best TTS strategy, considering problem difficulty, model type, and compute budget, providing a practical guide to effective inference-time scaling.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of a systematic, controlled comparison of test-time scaling (TTS) strategies across the same models, datasets, and compute budgets<br>‚Ä¢ Unclear how model type (post-training method) and problem difficulty influence the effectiveness of TTS, leading to inconsistent outcomes<br>‚Ä¢ Existing sequential extensions can reinforce errors and degrade accuracy; no universally best TTS strategy or principled, model-aware selection recipe exists</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Conduct a large-scale, controlled evaluation (>30B tokens) of API-friendly TTS strategies‚Äîfirst-finish search (short-m@k), last-finish search, and beam search‚Äîacross eight LLMs (7B‚Äì235B) and four reasoning datasets. Analyze trace quality vs. length and difficulty to classify models into short-horizon vs. long-horizon types and derive a compute- and model-aware recipe (e.g., shortest for low compute, beam search for medium, majority voting for high) for choosing the optimal TTS strategy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Reasoning Horizons: Instance-Level Estimation and Adaptive TTS Allocation: Train predictors to estimate per-question horizon and difficulty, dynamically selecting strategy and budget (k, N, beam width) to maximize accuracy under constraints.<br>‚Ä¢ Horizon-Aware Post-Training for Stable Long-Trace Reasoning: Develop RL post-training methods beyond GRPO/GSPO that explicitly optimize trace stability and quality over long chains, mitigating length bias and improving long-horizon performance.<br>‚Ä¢ Meta-Schedulers for Compute-Optimal Test-Time Scaling: Build bandit/RL controllers that jointly choose TTS strategy and parameters conditioned on model family and task signals, unifying internal early-stopping with external parallel/search scaling.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01481" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01481" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Although prevailing camera-controlled video generation models can produce cinematic results, lifting them directly to the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos remains challenging, which is a pivotal capability for taming 4D worlds. Some works resort to data augmentation or test-time optimization, but these strategies are constrained by limited model generalization and scalability issues. To this end, we propose ChronosObserver, a training-free method including World State Hyperspace to represent the spatiotemporal constraints of a 4D world scene, and Hyperspace Guided Sampling to synchronize the diffusion sampling trajectories of multiple views using the hyperspace. Experimental results demonstrate that our method achieves high-fidelity and 3D-consistent time-synchronized multi-view videos generation without training or fine-tuning for diffusion models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Generating time-synchronized multi-view videos from a single monocular video while preserving 3D consistency and high fidelity remains unsolved.<br>‚Ä¢ Directly extending camera-controlled video generation to multiple views causes 3D inconsistencies due to independent sampling trajectories and weakly coupled conditionals across viewpoints.<br>‚Ä¢ Existing approaches rely on data augmentation with scarce multi-view dynamic data (limited generalization) or per-scene test-time optimization/fine-tuning (high computational cost, poor scalability).<br>‚Ä¢ A training-free mechanism is needed to impose explicit spatiotemporal constraints that couple cross-view sampling and enforce a unified 4D scene.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ChronosObserver builds a World State Hyperspace from monocular depth/pose (base static/dynamic TSDF states) and incrementally augments it with states extracted from generated views; Hyperspace Guided Sampling projects these states into target views and integrates their renderings/masks into latent denoising to synchronize diffusion trajectories across views. This training-free guidance yields 3D-consistent, time-synchronized multi-view videos for both internal- and external-view settings.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Uncertainty-Aware Hyperspace Guidance for 4D Video: Model and weight state projections by learned uncertainty to handle noisy depth/pose and occlusions during sampling.<br>‚Ä¢ End-to-End Trainable Hyperspace Diffusion for Multi-View Video: Jointly learn depth/pose, hyperspace construction, and diffusion guidance to reduce reliance on external estimators and boost fidelity.<br>‚Ä¢ Real-Time Hyperspace-Guided Multi-View Synthesis: Develop faster projection/fusion and lightweight guidance to enable interactive, low-latency multi-view generation.<br>‚Ä¢ Memory-Augmented Hyperspace for Long-Horizon 4D Consistency: Introduce temporal memory and pruning strategies to maintain coherent geometry over long videos and shot transitions.<br>‚Ä¢ Neural Field‚ÄìAugmented Hyperspace Sampling: Integrate neural radiance fields or 3D Gaussians with hyperspace constraints to further improve geometry, occlusion handling, and view consistency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01191" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01191" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Specialized clinical AI assistants are rapidly entering medical practice, often framed as safer or more reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) against three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using a 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench (clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in completeness, communication quality, context awareness, and systems-based safety reasoning. These findings reveal that tools marketed for clinical decision support may often lag behind frontier LLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Widespread adoption of clinical AI assistants without independent, quantitative evaluation creates an evidence gap between marketing claims and real-world performance.<br>‚Ä¢ Unclear whether specialist tools (often leveraging domain-specific training or RAG) truly outperform frontier generalist LLMs that benefit from larger training corpora and advanced alignment.<br>‚Ä¢ Potential patient safety risks due to deficits in completeness, communication quality, context awareness, and systems-based safety reasoning; need transparent, standardized benchmarking to guide deployment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Construct a 1,000-item medical mini-benchmark by sampling 500 MedQA (knowledge) and 500 single-turn HealthBench (expert-alignment) prompts, then compare two clinical tools (OpenEvidence, UpToDate Expert AI) to three generalist LLMs (GPT-5, Gemini 3 Pro, Claude Sonnet 4.5). HealthBench responses are graded across accuracy, completeness, communication quality, context awareness, and instruction following, with analyses by thematic subsets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ A Prospective, Real-World Trial of Generalist vs Specialist Clinical AI in Patient-Facing Workflows: Randomized evaluation of safety, diagnostic impact, and clinician trust when integrating AI into clinical care.<br>‚Ä¢ Multi-Turn Clinical Dialogue Benchmarks for Assessing Context Retention and Systems-Based Safety Reasoning: Create and validate longitudinal, triage-rich benchmarks to test reasoning across encounters and safety-critical scenarios.<br>‚Ä¢ Calibration and Uncertainty-Aware Response Generation for Medical LLMs: Develop methods to quantify and communicate uncertainty and evaluate how this affects clinician decision-making, completeness, and context-aware guidance.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-12">

    <div class="paper">
        <h2 class="paper-title">Learning Eigenstructures of Unstructured Data Manifolds</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01103" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01103" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce a novel framework that directly learns a spectral basis for shape and manifold analysis from unstructured data, eliminating the need for traditional operator selection, discretization, and eigensolvers. Grounded in optimal-approximation theory, we train a network to decompose an implicit approximation operator by minimizing the reconstruction error in the learned basis over a chosen distribution of probe functions. For suitable distributions, they can be seen as an approximation of the Laplacian operator and its eigendecomposition, which are fundamental in geometry processing. Furthermore, our method recovers in a unified manner not only the spectral basis, but also the implicit metric's sampling density and the eigenvalues of the underlying operator. Notably, our unsupervised method makes no assumption on the data manifold, such as meshing or manifold dimensionality, allowing it to scale to arbitrary datasets of any dimension. On point clouds lying on surfaces in 3D and high-dimensional image manifolds, our approach yields meaningful spectral bases, that can resemble those of the Laplacian, without explicit construction of an operator. By replacing the traditional operator selection, construction, and eigendecomposition with a learning-based approach, our framework offers a principled, data-driven alternative to conventional pipelines. This opens new possibilities in geometry processing for unstructured data, particularly in high-dimensional spaces.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Traditional spectral pipelines require explicit operator selection, discretization (mass/stiffness matrices), and numerical eigensolvers, which are mesh-dependent, fragile, and do not scale to high-dimensional manifolds.<br>‚Ä¢ Unstructured data (point clouds) and high-dimensional image manifolds lack reliable Laplacian constructions; graph Laplacians are highly sensitive to connectivity and local sampling density, deviating from smooth manifold geometry.<br>‚Ä¢ Existing neural eigenfunction methods assume parametric Euclidean domains, require per-domain training, and cannot handle curved point-cloud manifolds or generalize across geometries; others learn operators/eigenvalues without eigenvectors or still rely on explicit matrices and eigensolvers.<br>‚Ä¢ There is a need for a unified, unsupervised, data-driven approach that directly learns spectral bases, recovers the implicit metric (sampling density), and estimates eigenvalues without explicit operator construction, scalable to arbitrary dimensional datasets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>An unsupervised network predicts an orthonormal spectral basis by minimizing progressive reconstruction error of smoothed probe functions on the kNN graph; QR orthonormalization yields normalized eigenvectors (with the first encoding the mass matrix), and eigenvalues are estimated via worst-case reconstruction errors grounded in optimal-approximation theory.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Probe Distributions for Operator Discovery on Unstructured Manifolds: Meta-learn probe function distributions to target specific implicit operators (e.g., anisotropic/scale-invariant Laplacians, biharmonic, Hamiltonian) and metrics without manual tuning.<br>‚Ä¢ Spectral Foundation Models for Any-Dimensional Data: Pretrain universal spectral bases on diverse surfaces, volumes, and high-dimensional embedding spaces to produce reusable, sampling-invariant eigenstructures that generalize across domains.<br>‚Ä¢ Optimal-Approximation Spectral Layers for Graph Neural Networks: Integrate learned spectral bases as adaptive filters/layers in GNNs to improve robustness to sampling, enhance downstream tasks, and enable end-to-end spectral learning.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Seeing the Wind from a Falling Leaf</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.00762" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.00762" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>A longstanding goal in computer vision is to model motions from videos, while the representations behind motions, i.e. the invisible physical interactions that cause objects to deform and move, remain largely unexplored. In this paper, we study how to recover the invisible forces from visual observations, e.g., estimating the wind field by observing a leaf falling to the ground. Our key innovation is an end-to-end differentiable inverse graphics framework, which jointly models object geometry, physical properties, and interactions directly from videos. Through backpropagation, our approach enables the recovery of force representations from object motions. We validate our method on both synthetic and real-world scenarios, and the results demonstrate its ability to infer plausible force fields from videos. Furthermore, we show the potential applications of our approach, including physics-based video generation and editing. We hope our approach sheds light on understanding and modeling the physical process behind pixels, bridging the gap between vision and physics. Please check more video results in our https://chaoren2357.github.io/seeingthewind/{project page}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Recover dense, time-varying 3D force fields (e.g., wind) directly from videos by observing object motion and deformation.<br>‚Ä¢ Bridge vision and physics to explain pixel dynamics with causal physical factors, enabling physics-driven video generation and editing.<br>‚Ä¢ Overcome optimization instability in differentiable physics where time integration causes exploding/vanishing gradients and photometric losses provide weak motion signals.<br>‚Ä¢ Address limitations of prior work that focuses on low-dimensional parameter ID (mass, friction, elasticity) rather than spatial force fields.<br>‚Ä¢ Go beyond force estimation methods that require tactile sensors/strong priors in controlled robotic settings, making force recovery feasible in unconstrained, natural videos.<br>‚Ä¢ Improve over video generation/editing approaches that rely on manual force specification or velocity fields without explicit forces, which hinders parameter-aware, physically grounded edits.<br>‚Ä¢ Mitigate noise and inconsistency in off-the-shelf depth/optical flow pipelines for dense 3D motion supervision.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>An end-to-end differentiable inverse-graphics pipeline represents objects with 3D Gaussians (appearance + geometry) augmented with VLM-inferred physical properties, models distributed forces via a causal tri-plane (Eulerian grid with a recurrent time encoder), and simulates motion through a differentiable MPM. Forces are recovered by optimizing a 4D sparse-tracking objective that lifts reliable 2D keypoint tracks to 3D with ARAP, maps them to Gaussian motions via barycentric interpolation, and regularizes space-time smoothness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Joint Bayesian Estimation of Material Properties and Force Fields from Video: Combine uncertainty-aware VLM priors with differentiable physics to jointly infer materials and forces with calibrated confidence.<br>‚Ä¢ Hybrid Eulerian‚ÄìLagrangian Causal Force Models for Contact and Fluid‚ÄìStructure Interaction: Extend the causal tri-plane to handle multi-object contacts and fluid coupling in complex scenes.<br>‚Ä¢ Self-Supervised 4D Keypoint Discovery for Robust Force Recovery under Camera Motion and Occlusion: Learn keypoints, depth, and camera poses end-to-end to remove reliance on external depth/pose estimators while maintaining stable force optimization.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.00387" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.00387" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent image editing models boast next-level intelligent capabilities, facilitating cognition- and creativity-informed image editing. Yet, existing benchmarks provide too narrow a scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, a knowledge-intensive benchmark for comprehensive evaluation of cognition- and creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded steps, i.e., Awareness, Interpretation, and Imagination, each corresponding to a task that poses a challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model will be made publicly available soon. Project Page: https://qnancy.github.io/wiseedit_project_page/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing image editing benchmarks are overly simplistic (single-image, explicit instructions), failing to holistically assess cognition- and creativity-informed editing.<br>‚Ä¢ Current evaluations lack full task depth across the three key steps‚ÄîAwareness (target localization), Interpretation (knowledge-based parsing), and Imagination (creative rendering)‚Äîand rarely include complex cases where multiple steps are simultaneously hard.<br>‚Ä¢ Benchmarks focus narrowly on declarative knowledge and neglect procedural and metacognitive knowledge, limiting assessment of real-world problem solving.<br>‚Ä¢ Input formats are restricted (often single-image, English-only), not reflecting practical free-form, multi-image, cross-lingual editing scenarios.<br>‚Ä¢ Conventional metrics miss whether correct knowledge is applied and how creative the composition is, overlooking Knowledge Fidelity and Creative Fusion.<br>‚Ä¢ As advanced systems (e.g., MLLM + diffusion hybrids, closed-source editors) claim cognition/creativity, the community lacks a rigorous, knowledge-intensive benchmark to reveal actual limitations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>WiseEdit is a knowledge-intensive benchmark that decomposes editing into Awareness, Interpretation, and Imagination, and instantiates four task types (including Complex) spanning declarative, procedural, and metacognitive knowledge across diverse domains with free-form, multi-image, bilingual inputs. It comprises 1,220 cases and an evaluation protocol introducing Knowledge Fidelity and Creative Fusion metrics, used to assess 21 leading models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Cognition-to-Creation: Curriculum Training across Awareness‚ÄìInterpretation‚ÄìImagination for Robust Image Editing: Leverage WiseEdit‚Äôs stepwise taxonomy to design staged training that improves end-to-end reasoning and rendering.<br>‚Ä¢ Meta-Cognitive Image Editors: Self-Evaluation and Self-Correction for Knowledge-Faithful and Creative Edits: Integrate uncertainty estimation and self-critique loops to raise Knowledge Fidelity and Creative Fusion.<br>‚Ä¢ Tool-Augmented Procedural Editing: Incorporating External Tools and Knowledge Graphs for Multi-Image, Cross-Lingual Instructions: Use tool-use and structured procedural knowledge to handle complex, causality- and logic-heavy edits.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-13">

    <div class="paper">
        <h2 class="paper-title">OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01830" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01830" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recently, two-stage fine-tuning strategies, e.g., acquiring essential driving knowledge through supervised fine-tuning (SFT) and further enhancing decision-making and planning via reinforcement fine-tuning (RFT), have shown strong potential in advancing the knowledge-driven autonomous driving (AD) paradigm. However, the learning nature of SFT still limits the generalization of reasoning, thereby constraining the full potential of driving performance. Meanwhile, current RFT approaches are primarily applied to downstream tasks, since scene understanding is an open-ended problem where corresponding rewards are difficult to quantify. To address these limitations, we propose OpenREAD, an OPEN-ended REasoning reinforced vision-language model (VLM)-based autonomous driving (AD) framework that enables end-to-end RFT across the full spectrum from high-level reasoning to low-level trajectory planning. Specifically, we begin by constructing large-scale Chain-of-Thought (CoT) annotations on open-source driving-related knowledge datasets, and employ the powerful Qwen3 large language model (LLM) as the critic in RFT to quantify reasoning quality for open-ended questions during reward modeling. Extensive experiments confirm that joint end-to-end RFT yields substantial improvements in both upstream and downstream tasks, enabling OpenREAD to achieve state-of-the-art performance on reasoning and planning benchmarks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ SFT-centric VLM/LLM approaches rely on fixed answer patterns, limiting generalization and linguistic flexibility for open-ended scene understanding.<br>‚Ä¢ RFT is mostly used for verifiable downstream tasks (e.g., planning), while open-ended reasoning lacks quantifiable rewards, blocking end-to-end RL across the stack.<br>‚Ä¢ Purely data-driven E2E models struggle with long-tail and cross-domain scenarios and lack interpretability and causal reasoning.<br>‚Ä¢ Need to bridge high-level driving knowledge (perception, action analysis, counterfactuals) with low-level trajectory planning to improve safety and performance.<br>‚Ä¢ Existing pipelines do not jointly optimize reasoning and planning, leading to suboptimal downstream motion quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>OpenREAD performs a two-stage training of Qwen3-VL-8B: CoT-based SFT cold start on LingoQA and OmniDrive to instill reasoning, followed by GRPO-based RFT using an LLM-as-critic (Qwen3-8B) with semantic similarity and trajectory ADE rewards to jointly optimize open-ended reasoning and trajectory planning under <think>/<answer> formatting constraints.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Closed-Loop LLM-as-Critic Reinforcement Learning for Autonomous Driving: Train and evaluate OpenREAD in simulation with online safety critics to assess reasoning and actions under closed-loop control.<br>‚Ä¢ Learning Continuous Reward Models for Open-Ended Driving Reasoning: Replace thresholded LLM judgments with calibrated, preference-trained dense reward models that capture contradiction, concision, and factuality.<br>‚Ä¢ Multi-Critic GRPO with Uncertainty-Aware Planning: Integrate ensembles of LLM critics and motion metrics to deliver robust, uncertainty-aware rewards that improve safety and long-tail generalization.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">CauSight: Learning to Supersense for Visual Causal Discovery</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01827" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01827" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Causal thinking enables humans to understand not just what is seen, but why it happens. To replicate this capability in modern AI systems, we introduce the task of visual causal discovery. It requires models to infer cause-and-effect relations among visual entities across diverse scenarios instead of merely perceiving their presence. To this end, we first construct the Visual Causal Graph dataset (VCG-32K), a large-scale collection of over 32,000 images annotated with entity-level causal graphs, and further develop CauSight, a novel vision-language model to perform visual causal discovery through causally aware reasoning. Our training recipe integrates three components: (1) training data curation from VCG-32K, (2) Tree-of-Causal-Thought (ToCT) for synthesizing reasoning trajectories, and (3) reinforcement learning with a designed causal reward to refine the reasoning policy. Experiments show that CauSight outperforms GPT-4.1 on visual causal discovery, achieving over a threefold performance boost (21% absolute gain). Our code, model, and dataset are fully open-sourced at project page: https://github.com/OpenCausaLab/CauSight.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Bridge the gap from perception to reasoning by discovering cause‚Äìeffect relations among visual entities rather than mere detection or spatial association.<br>‚Ä¢ Enable safe and robust downstream decision-making (e.g., robotics, autonomous driving) that requires counterfactual, intervention-aware reasoning.<br>‚Ä¢ Overcome limitations of scene graphs that encode associations/adjacency but not causal mechanisms, hindering planning and intervention.<br>‚Ä¢ Address the lack of large-scale, entity-level causal graph annotations for images to rigorously benchmark and train models.<br>‚Ä¢ Improve upon current VLMs that exhibit very low recall on visual causal discovery, indicating inadequate causal reasoning and confounder handling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>They introduce VCG-32K, a large dataset of images with entity-level causal graphs, and train CauSight by synthesizing Tree-of-Causal-Thought trajectories with MCTS for process supervision, followed by supervised fine-tuning and GRPO-based reinforcement learning with a graph-structured causal reward to optimize causal graph generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ VideoCauSight: Temporal Visual Causal Discovery with Interventions in Video: Extend CauSight to model time-evolving causal relations and validate edges via interventional signals across frames.<br>‚Ä¢ Weakly-Supervised Visual Causal Graph Learning via Counterfactual Augmentation: Reduce annotation cost by generating synthetic counterfactuals and using self-training to refine causal edges under weak labels.<br>‚Ä¢ 3D-Aware Multimodal Causal Graphs for Embodied Planning: Integrate multi-view/3D geometry and language grounding to disambiguate confounders and drive safer, causal-graph-informed robot policies.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01686" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01686" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current story visualization methods tend to position subjects solely by text and face challenges in maintaining artistic consistency. To address these limitations, we introduce DreamingComics, a layout-aware story visualization framework. We build upon a pretrained video diffusion-transformer (DiT) model, leveraging its spatiotemporal priors to enhance identity and style consistency. For layout-based position control, we propose RegionalRoPE, a region-aware positional encoding scheme that re-indexes embeddings based on the target layout. Additionally, we introduce a masked condition loss to further constrain each subject's visual features to their designated region. To infer layouts from natural language scripts, we integrate an LLM-based layout generator trained to produce comic-style layouts, enabling flexible and controllable layout conditioning. We present a comprehensive evaluation of our approach, showing a 29.2% increase in character consistency and a 36.2% increase in style similarity compared to previous methods, while displaying high spatial accuracy. Our project page is available at https://yj7082126.github.io/dreamingcomics/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Text-only story visualization lacks pixel-level spatial control, making precise multi-character placement and interaction difficult<br>‚Ä¢ Existing customization methods struggle to jointly control who appears (identity) and where they appear (layout), causing overlaps and incorrect appearances<br>‚Ä¢ Artistic style consistency (e.g., cartoon, flat-shaded) is hard to maintain due to generative models‚Äô bias toward photorealism<br>‚Ä¢ DiT-based approaches have limited explicit multi-subject layout control, and some pipelines generate subjects separately, losing inter-subject interactions<br>‚Ä¢ Scarcity of paired data containing both identity cues and spatial layout hinders development of layout-aware customization, especially for comics with diverse panel structures</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DreamingComics couples an LLM-based layout generator that converts scripts into comic-style panels and character boxes with Dream-Illustrator, a video DiT‚Äìbased image customization model. It leverages spatiotemporal priors, RegionalRoPE for region-aware positional encoding, and a masked condition loss to align subjects to designated regions while preserving identity and style.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Joint Layout-to-Image Story Synthesis with Interaction-Aware Attention: End-to-end training that unifies LLM layout planning and image generation, explicitly modeling inter-character interactions (occlusion, gaze, contact) within attention<br>‚Ä¢ Temporal Panel Coherence for Comics: Modeling cross-panel temporal consistency to maintain identity, pose, and style across sequences, enabling multi-page storytelling with stable transitions<br>‚Ä¢ RLHF-Guided Layout Planning for Comics: Learning layout generators from human edits via reinforcement learning from human feedback to improve layout quality, readability, and designer controllability</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-14">

    <div class="paper">
        <h2 class="paper-title">IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.00333" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.00333" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While large language models excel on high-resource multilingual tasks, low- and extremely low-resource Indic languages remain severely under-evaluated. We present IndicParam, a human-curated benchmark of over 13,000 multiple-choice questions covering 11 such languages (Nepali, Gujarati, Marathi, Odia as low-resource; Dogri, Maithili, Rajasthani, Sanskrit, Bodo, Santali, Konkani as extremely low-resource) plus Sanskrit-English code-mixed set. We evaluated 19 LLMs, both proprietary and open-weights, which reveals that even the top-performing GPT-5 reaches only 45.0% average accuracy, followed by DeepSeek-3.2 (43.1) and Claude-4.5 (42.7). We additionally label each question as knowledge-oriented or purely linguistic to discriminate factual recall from grammatical proficiency. Further, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. IndicParam provides insights into limitations of cross-lingual transfer and establishes a challenging benchmark for Indic languages. The dataset is available at https://huggingface.co/datasets/bharatgenai/IndicParam. Scripts to run benchmark are present at https://github.com/ayushbits/IndicParam.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Low- and extremely low-resource Indic languages are severely under-evaluated; existing resources largely ignore long-tail languages despite tens of millions of speakers.<br>‚Ä¢ Current benchmarks mostly target high/medium-resource languages, rarely cover code-mixing, and use limited question formats; they also fail to disentangle linguistic competence from factual knowledge.<br>‚Ä¢ Scarcity of web-scale corpora makes cross-lingual transfer performance unclear; there is no systematic way to quantify LLM limitations on these languages.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper constructs IndicParam, a human-curated, graduate-level QA benchmark with 13K+ multiple-choice questions across 11 low/extremely low-resource Indic languages plus Sanskrit-English code-mixing, annotated by question type (e.g., list matching, assertion‚Äìreason, ordering) and category (linguistic vs knowledge). It evaluates 19 open- and closed-weight LLMs on this benchmark to quantify accuracy and analyze cross-lingual transfer limits and format-specific performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ IndicParam-XL: Expanding Long-Tail Indic Coverage with Synthetic and Crowd-Sourced QA: Scale the benchmark to more languages and domains via multilingual data augmentation and human validation to reduce data scarcity.<br>‚Ä¢ Learning to Transfer Across Indic Scripts: Adapter and Tokenization Studies on IndicParam: Systematically test tokenization, adapters, and lightweight fine-tuning to improve cross-lingual transfer among script-related Indic languages.<br>‚Ä¢ Evaluating and Training for Code-Mixing in Indic LLMs: Beyond Sanskrit-English: Develop metrics, datasets, and training recipes for robust handling of diverse Indic code-mixing patterns and dialectal variation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Generative Video Motion Editing with 3D Point Tracks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.02015" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.02015" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Precise joint editing of camera and object motion in real videos is hard, especially under complex object dynamics, occlusions, and depth ordering.<br>‚Ä¢ Existing methods either use I2V models that lose full-scene context (conditioning only on the first frame) or V2V approaches that change camera viewpoint but cannot control fine-grained object motion; 2D tracks lack explicit depth cues for occlusion handling.<br>‚Ä¢ There is a scarcity of paired training data with controlled motion manipulations and 3D track annotations, and existing motion conditioning mechanisms are brittle to noisy tracks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Edit-by-Track is a track-conditioned video-to-video diffusion framework that conditions on the entire source video and paired 3D point tracks (source/target). A novel 3D track conditioner uses cross-attention to adaptively sample context from source tokens and splat it into source/target frame spaces with depth-aware positional encodings, and the model is fine-tuned in two stages (synthetic then real) for robust, 3D-aware motion control.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Physically-Grounded Generative Video Motion Editing: Integrate learned physics priors or differentiable simulators to model causal secondary effects (e.g., splashes, shadows) arising from edited motions.<br>‚Ä¢ Hierarchical Point-Track Conditioning for Small, Dense Objects: Develop multi-scale or uncertainty-aware track encoders that better handle densely clustered tracks and improve context extraction for small objects.<br>‚Ä¢ End-to-End Joint 3D Tracking and Editing with Visibility Reasoning: Train a unified diffusion model that simultaneously estimates 3D tracks and performs motion editing, explicitly learning visibility/occlusion to reduce reliance on external trackers.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MEGConformer: Conformer-Based MEG Decoder for Robust Speech and Phoneme Classification</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.01443" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.01443" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Conformer-based decoders for the LibriBrain 2025 PNPL competition, targeting two foundational MEG tasks: Speech Detection and Phoneme Classification. Our approach adapts a compact Conformer to raw 306-channel MEG signals, with a lightweight convolutional projection layer and task-specific heads. For Speech Detection, a MEG-oriented SpecAugment provided a first exploration of MEG-specific augmentation. For Phoneme Classification, we used inverse-square-root class weighting and a dynamic grouping loader to handle 100-sample averaged examples. In addition, a simple instance-level normalization proved critical to mitigate distribution shifts on the holdout split. Using the official Standard track splits and F1-macro for model selection, our best systems achieved 88.9% (Speech) and 65.8% (Phoneme) on the leaderboard, surpassing the competition baselines and ranking within the top-10 in both tasks. For further implementation details, the technical documentation, source code, and checkpoints are available at https://github.com/neural2speech/libribrain-experiments.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Build robust, high-accuracy decoders for MEG-based Speech Detection and Phoneme Classification, leveraging large within-subject data (LibriBrain) while adhering to competition constraints.<br>‚Ä¢ Adapt state-of-the-art ASR architectures (Conformer) to raw 306-channel MEG signals to better capture spatiotemporal structure than prior CNN/transformer baselines not tailored for MEG.<br>‚Ä¢ Address phoneme-class imbalance and distribution shift across splits (especially holdout), where conventional batch/layer normalization and naive training on non-averaged data degrade generalization.<br>‚Ä¢ Introduce MEG-specific augmentation and training strategies, as existing speech augmentations and stride/window choices are not validated for MEG.<br>‚Ä¢ Ensure reproducibility and fair comparison by following official splits/metrics; prior work often re-partitions or lacks standardized evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A compact Conformer encoder is adapted to raw MEG with a 1D conv projection (306‚Üí144) and task-specific heads: binary speech detection with MEG-oriented SpecAugment (time masking, bandstop notches) and inference smoothing, and 39-way phoneme classification with instance normalization, inverse square-root class weighting, dynamic 100-sample grouping to mimic averaged holdout, and seed ensembling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ End-to-End MEG-to-Text with CTC/Seq2Seq Heads on Conformer Backbones: Extend MEGConformer to sequential decoding for continuous speech recognition and potential speech synthesis.<br>‚Ä¢ Cross-Subject MEGConformer: Domain Adaptation and Normalization Strategies for Generalizable Speech Decoding: Develop normalization/adaptation (e.g., instance/domain adversarial) to transfer across individuals and sessions.<br>‚Ä¢ Articulatory Feature Multitask Learning for MEG Phoneme Classification: Jointly learn phonetic feature subspaces and phonemes to mitigate class imbalance and improve interpretability.<br>‚Ä¢ Scaling Laws and Self-Supervised Pretraining for MEG Decoders: Explore data scaling and SSL (contrastive/pretext tasks) to boost phoneme performance beyond single-subject datasets.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-15">

    <div class="paper">
        <h2 class="paper-title">Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.00639" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.00639" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The increasing prevalence of thyroid cancer globally has led to the development of various computer-aided detection methods. Accurate segmentation of thyroid nodules is a critical first step in the development of AI-assisted clinical decision support systems. This study focuses on instance segmentation of thyroid nodules using YOLOv5 algorithms on ultrasound images. We evaluated multiple YOLOv5 variants (Nano, Small, Medium, Large, and XLarge) across two dataset versions, with and without doppler images. The YOLOv5-Large algorithm achieved the highest performance with a dice score of 91\% and mAP of 0.87 on the dataset including doppler images. Notably, our results demonstrate that doppler images, typically excluded by physicians, can significantly improve segmentation performance. The YOLOv5-Small model achieved 79\% dice score when doppler images were excluded, while including them improved performance across all model variants. These findings suggest that instance segmentation with YOLOv5 provides an effective real-time approach for thyroid nodule detection, with potential clinical applications in automated diagnostic systems.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Accurate, automated segmentation of thyroid nodules is needed as a foundational step for AI-assisted diagnosis, but remains challenging in noisy ultrasound imagery.<br>‚Ä¢ Manual TIRADS-based assessment suffers from high inter- and intra-observer variability and elevated false-positive biopsy rates, limiting reliability and scalability.<br>‚Ä¢ Prior deep learning efforts emphasize detection or semantic segmentation (e.g., U-Net, Mask R-CNN) with limited focus on efficient instance segmentation tailored to real-time clinical use.<br>‚Ä¢ The role of Doppler ultrasound in automated segmentation is underexplored, despite being routinely excluded by clinicians and potentially informative.<br>‚Ä¢ There is a need to systematically compare model capacity vs. performance for deployable, real-time systems across resource-constrained environments.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Benchmark YOLOv5 instance segmentation variants (Nano‚ÄìXLarge) on two thyroid ultrasound datasets with and without Doppler, using patient-level splits, focal loss, cosine LR scheduling, and standardized training to measure Dice, mAP@0.5, precision, and recall. Show that including Doppler consistently boosts performance (best: YOLOv5-Large with 0.91 Dice, 0.87 mAP), highlighting an efficient, real-time segmentation solution.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Multimodal Fusion of B-Mode and Doppler for Thyroid Nodule Instance Segmentation: Design and ablate fusion strategies (early/late/cross-attention) to exploit vascular cues for boundary refinement.<br>‚Ä¢ Cross-Institutional Generalization of YOLOv5-Based Thyroid Nodule Segmentation: Domain adaptation and robust augmentation to mitigate scanner/protocol shifts and improve external validity.<br>‚Ä¢ Joint Segmentation and Risk Stratification of Thyroid Nodules via Multi-Task Learning: Integrate instance masks with TIRADS/malignancy prediction to reduce false-positive biopsies while preserving real-time performance.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.00234" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.00234" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>There has been significant progress in open-source text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing automatic speech recognition first followed by translation. This introduces additional latency, which is particularly critical in simultaneous ST (SimulST), and prevents the model from exploiting multimodal context, such as images, which can aid disambiguation. Pretrained multimodal foundation models (MMFMs) already possess strong perception and reasoning capabilities across multiple modalities, but generally lack the multilingual coverage and specialized translation performance of dedicated translation LLMs. To build an effective multimodal translation system, we propose an end-to-end approach that fuses MMFMs with translation LLMs. We introduce a novel fusion strategy that connects hidden states from multiple layers of a pretrained MMFM to a translation LLM, enabling joint end-to-end training. The resulting model, OmniFusion, built on Omni 2.5-7B as the MMFM and SeedX PPO-7B as the translation LLM, can perform speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. Experiments demonstrate that OmniFusion effectively leverages both audio and visual inputs, achieves a 1-second latency reduction in SimulST compared to cascaded pipelines and also improves the overall translation qualityCode is available at https://github.com/saikoneru/OmniFusion.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Cascaded ASR‚ÜíMT pipelines add generation steps that increase latency, which is critical for simultaneous speech translation, and prevent exploiting original audio/visual context.<br>‚Ä¢ Multimodal foundation models have strong perception/reasoning but weaker multilingual coverage and specialized translation quality than dedicated translation LLMs, yielding inferior translation if used directly.<br>‚Ä¢ Existing multimodal extensions to LLMs often train from scratch or attach only modality encoders, failing to transfer higher-level MMFM representations and forcing the LLM to learn modality mappings from scratch.<br>‚Ä¢ Naive fusion of MMFM hidden states into a translation LLM creates sequence-length/memory inflation, dimension mismatch, and requires task-dependent weighting of layers.<br>‚Ä¢ There is a need for an end-to-end multimodal, multilingual translation model that aligns audio/vision semantics to the translation space (e.g., via ASR/OCR bridges) while reducing latency and leveraging contextual slides/images.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>OmniFusion fuses a pretrained MMFM with a multilingual translation LLM via gated, layer-wise integration of the MMFM‚Äôs first/middle/last hidden states, projecting them to the LLM‚Äôs embedding space and concatenating with token embeddings for end-to-end multimodal translation. Training uses self-cascading with ASR/OCR bridge tasks across ST, speech+image, and text+image translation, freezing the MMFM and LoRA-tuning the translation LLM.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Layer-wise Fusion for Multimodal Translation LLMs: Learn task- and modality-conditioned gating over MMFM layers, with sparsity/attention mechanisms to optimize relevance and efficiency.<br>‚Ä¢ Latency-Aware Multimodal Simultaneous Translation: Jointly optimize quality‚Äìlatency trade-offs via reinforcement learning and adaptive wait-k policies that exploit audio and visual cues during decoding.<br>‚Ä¢ Scaling OmniFusion to Low-Resource Languages with Parameter-Efficient Multimodal Adapters: Extend multilingual coverage using lightweight adapters and cross-lingual alignment objectives, combining ASR/OCR bridges with self-supervised consistency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Structured Extraction from Business Process Diagrams Using Vision-Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.22448" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.22448" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Business Process Model and Notation (BPMN) is a widely adopted standard for representing complex business workflows. While BPMN diagrams are often exchanged as visual images, existing methods primarily rely on XML representations for computational analysis. In this work, we present a pipeline that leverages Vision-Language Models (VLMs) to extract structured JSON representations of BPMN diagrams directly from images, without requiring source model files or textual annotations. We also incorporate optical character recognition (OCR) for textual enrichment and evaluate the generated element lists against ground truth data derived from the source XML files. Our approach enables robust component extraction in scenarios where original source files are unavailable. We benchmark multiple VLMs and observe performance improvements in several models when OCR is used for text enrichment. In addition, we conducted extensive statistical analyses of OCR-based enrichment methods and prompt ablation studies, providing a clearer understanding of their impact on model performance.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ BPMN diagrams are often available only as images without the source XML, blocking downstream automation (simulation, migration, conformance checking, workflow integration).<br>‚Ä¢ Existing methods largely assume XML access or use handcrafted CNN+OCR pipelines, which struggle to recover complete entity‚Äìrelation structures and generalize across diverse diagram styles.<br>‚Ä¢ Current VLMs degrade on complex layouts (multi-pools/lanes), unlabeled/low-contrast text, and fine-grained visual cues lost to downsampling, leading to misclassification of flows and gateways.<br>‚Ä¢ There is a lack of standardized benchmarks pairing BPMN images with ground-truth XML to rigorously evaluate image-only structured extraction and quantify the impact of OCR and prompt design.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A zero-shot, schema-constrained prompting pipeline uses vision-language models to extract structured JSON of BPMN entities and relations directly from images, with optional OCR enrichment to fill missing labels. The approach validates/normalizes outputs and benchmarks VLM-only versus VLM+OCR across name, type, and relation tasks, including prompt ablations and statistical analyses.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Fine-Tuned Multimodal BPMN Parsers for Relation-Accurate Reconstruction: Train VLMs on expanded BPMN image‚ÄìXML corpora with graph-level supervision to improve flow directionality and gateway typing, producing executable XML.<br>‚Ä¢ High-Resolution Tiled Perception and OCR Fusion for Diagram Micro-Cues: Combine tiling/zoom strategies with learnable OCR fusion to preserve thin line styles and event markers, reducing downsampling-induced errors.<br>‚Ä¢ Schema-Constrained Decoding with Graph Validation for Image-to-BPMN: Integrate constrained decoding and post-hoc graph consistency checks to ensure type coherence and connectivity in end-to-end image-to-BPMN conversion.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 15</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button><button class="page-btn" onclick="goToPage(11)">11</button><button class="page-btn" onclick="goToPage(12)">12</button><button class="page-btn" onclick="goToPage(13)">13</button><button class="page-btn" onclick="goToPage(14)">14</button><button class="page-btn" onclick="goToPage(15)">15</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-12-02 23:13:32 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 15;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>