<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - December 17, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">December 17, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">MMGR: Multi-Modal Generative Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14691" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14691" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Perceptual metrics (FVD, IS, CLIP) overlook causal correctness, physical law adherence, and global consistency, leaving reasoning failures in generative videos/images undetected.<br>‚Ä¢ Lack of a unified, principled benchmark to assess generative reasoning across logical, spatial (2D/3D), physical, and temporal dimensions.<br>‚Ä¢ Modern video/image generators show pronounced asymmetry: moderate physical commonsense but catastrophic failures on abstract reasoning (e.g., <10% on ARC-AGI) and long-horizon spatial planning.<br>‚Ä¢ Current training paradigms are imbalanced (rich perceptual data, scarce symbolic reasoning), architectures struggle to maintain global state, and objectives reward appearance over causal correctness.<br>‚Ä¢ Need to explicitly disentangle and evaluate 2D versus 3D spatial reasoning to diagnose capabilities and failure modes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MMGR is a multi-modal benchmark and evaluation framework that maps five core reasoning abilities (Physical, Logical, 3D Spatial, 2D Spatial, Temporal) to three domains (Abstract Reasoning, Embodied Navigation, Physical Commonsense) with difficulty-controlled tasks and fine-grained, holistic metrics (e.g., Valid Solution, Overall Success, Physical Plausibility). A VLM-based evaluator (plus targeted human review) scores generations from leading video and image models to diagnose reasoning deficits beyond perceptual fidelity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Neuro-Symbolic Objectives for Reasoning-Aware Video Generation: Integrate rule-adherence and causal-consistency losses with reinforcement learning from structured feedback to move optimization beyond visual plausibility.<br>‚Ä¢ World-State Memory and Structured Latents in Generative Models: Incorporate external memory, scene graphs, or explicit 3D world-state representations to maintain global spatial-temporal consistency over long horizons.<br>‚Ä¢ Scaling Balanced Datasets for Generative Reasoning: Curate large multi-modal corpora that balance physical interactions with symbolic tasks (ARC-AGI, Sudoku, math) and empirically quantify their impact on MMGR performance.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13281" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13281" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: (i) Immersive ASMR video-audio sources. Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. (ii) Peer-Review evaluation. An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\% accuracy (random 50\%), far below that of human experts (81.25\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Detecting AI-generated videos that pair visuals with synchronized audio is increasingly difficult and socially critical, yet under-explored compared to image-only or silent-video settings.<br>‚Ä¢ Existing video generation/understanding benchmarks emphasize capabilities (e.g., physics, temporal coherence) rather than the core real-vs-fake question: can models reliably detect fakes that fool humans?<br>‚Ä¢ Prior AIGC detection benchmarks largely ignore audio and audio‚Äìvisual alignment, which are central to ASMR where small perceptual artifacts break immersion.<br>‚Ä¢ There is no unified, adversarial evaluation that jointly benchmarks creators (video generators) and reviewers (video-understanding/VLM detectors) on the same real‚Äìfake pairs.<br>‚Ä¢ Current VLM detectors often rely on superficial shortcuts (e.g., watermarks) and show bias toward predicting ‚Äúreal,‚Äù masking true limits in perceptual fidelity and audio‚Äìvisual consistency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce Video Reality Test, an ASMR-sourced benchmark with tightly coupled audio‚Äìvisual real clips and matched generator-produced fakes, evaluated under a peer-review protocol where video generators act as creators trying to fool reviewers (VLMs/humans) and reviewers aim to detect fakes. Models are scored by fooling rate (creators) and real/fake accuracy (reviewers), with analyses across audio on/off, watermark ablations, and easy/hard splits.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond Watermarks: Debiased Audio‚ÄìVisual Detectors for AI-Generated Video: Design training and evaluation pipelines that remove shortcut cues and explicitly model fine-grained audio‚Äìvisual synchronization and micro-physical consistency.<br>‚Ä¢ Adversarial Co-Training of Generators and Reviewers on Video Reality Test: Close the loop by jointly optimizing VGMs to increase fooling rates and VLMs to improve robustness, with explicit objectives for audio alignment and temporal realism.<br>‚Ä¢ Human-Centered Metrics for ASMR Realism: Develop psychophysics-grounded metrics and human-in-the-loop protocols that calibrate VLM judgements to human perception for audio‚Äìvisual immersion and authenticity.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14614" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14614" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Achieving simultaneous real-time generation (e.g., 24 FPS at 720p) and long-term geometric consistency in interactive world models<br>‚Ä¢ Preventing scene drift and ensuring coherence when revisiting locations under user keyboard/mouse control<br>‚Ä¢ Overcoming the ambiguity of discrete action signals for precise memory retrieval and the training instability of continuous camera poses across diverse scene scales<br>‚Ä¢ Mitigating long-range memory attenuation in Transformer-based video diffusion caused by positional decay over long sequences<br>‚Ä¢ Resolving teacher‚Äìstudent distribution mismatch in distillation for memory-aware autoregressive models to avoid exposure bias and error accumulation<br>‚Ä¢ Building a general-domain model that generalizes across real/stylized, first- and third-person worlds, while supporting tasks like 3D reconstruction and promptable events</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>WorldPlay is a streaming autoregressive video diffusion model that combines a Dual Action Representation (discrete keys plus continuous camera pose via PRoPE) for robust, precise control. It maintains long-term consistency with Reconstituted Context Memory (dynamic retrieval and temporal reframing) and achieves real-time few-step inference through Context Forcing, which aligns teacher‚Äìstudent memory contexts during distillation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Hybrid 3D-Implicit WorldPlay: Unified Explicit Reconstruction and Reconstituted Memory for Real-Time Consistency: Combine explicit 3D scene representations with reconstituted context memory to improve loop closure and geometric stability.<br>‚Ä¢ Learning Temporal Reframing: Adaptive Positional Embedding Scheduling for Ultra-Long-Horizon Video Generation: Train a policy to dynamically reframe memory positions, alleviating long-range decay beyond hand-crafted rules.<br>‚Ä¢ Multi-Agent WorldPlay: Real-Time Interactive World Modeling with Coordinated Actions and Shared Memory: Extend action and memory modules to multiple agents with shared context and conflict-aware control.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.12675" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.12675" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing subject-driven generators emphasize multi-subject composition but often fail to distinguish the correct target within complex reference images, causing omissions, redundancies, and subject errors.<br>‚Ä¢ Generation experts are semantically deficient with respect to instructions; their representations align poorly compared to understanding experts, leading to misinterpretation and identity drift.<br>‚Ä¢ Understanding experts, though better aligned to instructions, exhibit biases and are misaligned with generation, so naive reliance introduces irrelevant subjects and mistaken identities.<br>‚Ä¢ Real-world references contain interference and intricate details; current methods lack mechanisms to filter instruction-irrelevant regions and preserve subject identity under noise.<br>‚Ä¢ There is no comprehensive benchmark jointly evaluating composition and distinction across difficulty levels, hindering rigorous assessment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Scone uses a unified understanding-generation architecture with an understanding bridge strategy: a two-stage training first learns composition on single-candidate data, then performs visual‚Äìtext alignment and early-layer attention-based semantic masking to guide the generator, preserving subject identity and suppressing interference. The understanding expert thus conveys high-level semantics to the generation expert for end-to-end collaboration without adding parameters.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Semantic Debiasing for Unified Understanding-Generation Models: Learn dynamic debiasing of the understanding expert via causal/adversarial objectives and mask refinement to reduce instruction-irrelevant biases during guidance.<br>‚Ä¢ Layout-Aware Scone: Joint Subject Distinction and Spatial Composition: Integrate explicit scene/layout planning with the semantic bridge to control spatial relations and interactions among multiple subjects while maintaining identity fidelity.<br>‚Ä¢ SconeVid: Extending the Semantic Bridge to Video-Based Subject-Driven Generation: Generalize early-layer semantic masking and alignment to temporal modeling, ensuring consistent subject distinction and identity preservation across frames.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13660" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13660" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Robots must translate complex, spatially constrained natural-language instructions into executable 3D positional sequences (spatial traces), requiring multi-step, metric-grounded reasoning in cluttered real-world scenes.<br>‚Ä¢ Existing VLMs largely output 2D traces (pixels) and overlook multi-step reasoning with intermediate object cues, lacking 3D grounding and absolute metric understanding, which leads to suboptimal manipulation and motion planning.<br>‚Ä¢ Even methods that use 3D modalities struggle with precise 3D spatial referring and measuring in absolute-scale scenes, due to limited scale awareness and insufficient metric supervision.<br>‚Ä¢ There is a shortage of large, well-annotated datasets and benchmarks tailored to spatial tracing with step-wise compositional reasoning and metric-sensitive evaluation, hindering progress and fair comparison.<br>‚Ä¢ Practical robotics needs collision-aware, long-horizon execution across diverse robot platforms, which current models fail to robustly support due to weak metric perception and reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RoboTracer is a 3D-aware vision-language model that uses a universal spatial encoder to ingest arbitrary geometric inputs (e.g., camera intrinsics, depth) and a regression-supervised scale decoder during supervised fine-tuning to enhance metric/scale awareness. It is further trained via reinforcement fine-tuning with metric-sensitive process rewards (referring, measuring, scale) to supervise intermediate cues and generate accurate multi-step 3D spatial traces; training and evaluation are supported by the TraceSpatial dataset and TraceSpatial-Bench.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ RoboTracer-ClosedLoop: Online Spatial Trace Generation with Visual Feedback for Dynamic Scenes: Extend RoboTracer to streaming perception and closed-loop updates, enabling real-time trace adaptation under object motion and occlusions.<br>‚Ä¢ Self-Supervised Scale-Aware VLMs: Learning Metric Grounding Without Absolute Depth: Replace explicit scale regression with self-supervised objectives (multi-view consistency, SLAM cues, size priors) to reduce reliance on absolute-scale annotations.<br>‚Ä¢ Policy-Coupled Spatial Reasoning: End-to-End Training of Spatial Trace Generation and Robot Control: Jointly optimize the VLM‚Äôs spatial tracing with downstream motion planners/controllers to improve safety (collision avoidance) and task success across robot embodiments.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14051" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14051" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Post-training data opacity: current datasets have unclear composition and provenance, hindering reproducibility and obscuring the link between data characteristics and model behaviors.<br>‚Ä¢ Lack of standardized evaluation: existing practice relies on ad-hoc curation and single-score benchmarks, making fair, apples-to-apples comparisons across models and domains impossible.<br>‚Ä¢ Inefficient data curation: without systematic metrics and tooling, the community cannot reliably identify what constitutes a ‚Äúgood‚Äù dataset or allocate resources effectively.<br>‚Ä¢ Hidden redundancy and leakage: overlapping sources across datasets and benchmarks remain untracked, potentially inflating results and masking true data value.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>OpenDataArena introduces a unified training‚Äìevaluation pipeline for fair cross-model benchmarking of post-training datasets, paired with a multi-dimensional scoring framework that profiles quality across numerous axes (e.g., instruction complexity, response quality, diversity). It adds an interactive lineage explorer and fully open-source toolkit to trace provenance and reproduce training/evaluation, enabling transparent, holistic assessment of dataset value.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Data Mixing Laws for Post-Training: Derive and validate quantitative laws mapping dataset composition axes to downstream performance across models and tasks.<br>‚Ä¢ Lineage-Aware Benchmark Design: Construct benchmarks that minimize redundancy and data leakage by leveraging lineage graphs and overlap analysis.<br>‚Ä¢ Automated Dataset Composition via Multi-Objective Optimization: Optimize SFT data mixtures using ODA scores to balance performance, diversity, and efficiency.<br>‚Ä¢ Causal Attribution of Data Attributes to Model Behaviors: Use controlled interventional training within ODA to estimate causal effects of specific dataset characteristics.<br>‚Ä¢ Extending OpenDataArena to Multimodal Post-Training: Adapt scoring, lineage, and evaluation pipelines to text‚Äìimage/audio instruction datasets.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.12980" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.12980" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice. We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing VSS evaluations optimize synthetic recall‚Äìlatency against distance-based ground truth, ignoring task-level utility and leading to misleading method choices.<br>‚Ä¢ Metric misuse: chosen similarity metrics (e.g., IP vs Euclidean) often misalign with embedding/task semantics, yielding high synthetic recall but poor label recall.<br>‚Ä¢ Information Loss Funnel is unaddressed: embedding loss and data distribution sensitivity are major, unmeasured sources of end-to-end degradation; current benchmarks mostly probe only distribution effects.<br>‚Ä¢ Benchmark gaps: lack of large, modern, labeled datasets spanning real domains (image classification, face recognition, text retrieval, recommendation) and limited coverage of MIPS despite its prevalence.<br>‚Ä¢ No explainable selection guidance: practitioners lack interpretable tools linking dataset traits to appropriate VSS methods and configurations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Iceberg introduces the Information Loss Funnel and a task-centric benchmark suite (eight datasets, 1M‚Äì100M vectors) with application-level labels/metrics to evaluate 13 ANNS/MIPS methods beyond synthetic recall. It extracts lightweight meta-features and provides an interpretable decision tree to select and tune VSS methods for specific workloads, revealing re-rankings versus traditional recall‚Äìlatency leaderboards.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Task-Aware Vector Similarity Search: Jointly Optimizing Indexing with Downstream Objectives: Integrate application loss (e.g., label recall/hit-rate) into index construction and search to directly optimize task utility.<br>‚Ä¢ Metric-Aware Similarity Selection and Fusion for Learned Embeddings: Automatically select or combine similarity measures (and normalization) to align with embedding geometry trained under non-metric objectives.<br>‚Ä¢ Distribution-Aware Adaptive Indexing for Heterogeneous Vector Data: Develop indices that detect and adapt to skewed densities, modality shifts, and user segments to sustain accuracy‚Äìthroughput under dynamic distributions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14336" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14336" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ SVG files are structured for rendering efficiency rather than semantic clarity, fragmenting coherent parts and hindering animation.<br>‚Ä¢ VLMs/LLMs mis-handle SVGs because primitives are split and tags are ambiguous, making it hard to attach motions to meaningful parts.<br>‚Ä¢ Diffusion/SDS-based animation acts on raster appearance, resisting part rearrangements and producing jittery, repetitive motions without temporal regularization.<br>‚Ä¢ Instruction-tuned models that directly emit vector animation commands require massive datasets and still lack geometric/structural understanding.<br>‚Ä¢ A robust way to recover element-level semantics from noisy VLM predictions is needed to enable coherent, instruction-faithful SVG animation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Vector Prism renders each SVG primitive through multiple focused views (bounding box, isolation, highlight, zoom-in, outline), aggregates the resulting weak VLM labels via a statistical agreement matrix, and applies a Bayes decision rule to infer stable semantic groups before restructuring the SVG hierarchy. The structured SVG then allows a VLM-generated animation plan to be reliably converted into executable code by an LLM.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Semantics for SVGs Without VLM Queries: Self-Supervised Contrastive Stratification for Vector Graphics: Replace VLM-based weak labels with self-supervised multi-view learning to infer semantic groups, reducing cost and latency.<br>‚Ä¢ Interactive Vector Prism: Human-in-the-Loop Semantic Grouping and Uncertainty-Aware Animation Editing: Integrate uncertainty estimates and user feedback to correct labels on difficult primitives and refine animations.<br>‚Ä¢ End-to-End VLM‚ÄìLLM Co-Training over Structured SVGs for Robust Animation: Jointly train planning and code-generation modules on Vector Prism‚Äìstructured SVGs to improve instruction faithfulness, coherence, and generalization.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14699" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14699" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Maintaining long-term visual and semantic consistency in streaming, interactive text-to-video generation with frequent prompt switches (new subjects and scene transitions)<br>‚Ä¢ Fixed or rigid memory schemes (first-chunk sink, fixed compression, implicit trainable memory) fail to adaptively provide the right historical cues for different incoming prompts<br>‚Ä¢ Long-range memory inflates attention cost and GPU usage, hindering real-time generation; efficiency‚Äìquality trade-off is unresolved<br>‚Ä¢ Autoregressive rollouts accumulate errors over long horizons, causing drift, duplicated subjects, and loss of prompt adherence</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MemFlow augments an AR-diffusion T2V model with a dynamic KV memory: Narrative Adaptive Memory retrieves prompt-aligned historical frames via text‚Äìvisual attention and updates memory using a compact prototype of the preceding chunk, while Sparse Memory Activation limits attention to the top‚Äëk most relevant memory tokens per query to retain speed and quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning-to-Retrieve: Reinforcement-Guided Adaptive Memory for Streaming Video Generation: Learn a retrieval/activation policy that optimizes long-horizon coherence and efficiency beyond heuristic top‚Äëk.<br>‚Ä¢ Hierarchical Narrative Memory: Multi-Granular Context for Consistent Multi-Scene Video Synthesis: Combine keyframe-, chunk-, and episode-level memories to balance local/global context and stabilize performance as memory grows.<br>‚Ä¢ World- and Identity-Aware Memory for Interactive Long-Form Video: Integrate 3D scene priors and identity embeddings to improve subject/background consistency under viewpoint and scene changes.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">RecGPT-V2 Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14503" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14503" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) have demonstrated remarkable potential in transforming recommender systems from implicit behavioral pattern matching to explicit intent reasoning. While RecGPT-V1 successfully pioneered this paradigm by integrating LLM-based reasoning into user interest mining and item tag prediction, it suffers from four fundamental limitations: (1) computational inefficiency and cognitive redundancy across multiple reasoning routes; (2) insufficient explanation diversity in fixed-template generation; (3) limited generalization under supervised learning paradigms; and (4) simplistic outcome-focused evaluation that fails to match human standards. To address these challenges, we present RecGPT-V2 with four key innovations. First, a Hierarchical Multi-Agent System restructures intent reasoning through coordinated collaboration, eliminating cognitive duplication while enabling diverse intent coverage. Combined with Hybrid Representation Inference that compresses user-behavior contexts, our framework reduces GPU consumption by 60% and improves exclusive recall from 9.39% to 10.99%. Second, a Meta-Prompting framework dynamically generates contextually adaptive prompts, improving explanation diversity by +7.3%. Third, constrained reinforcement learning mitigates multi-reward conflicts, achieving +24.1% improvement in tag prediction and +13.0% in explanation acceptance. Fourth, an Agent-as-a-Judge framework decomposes assessment into multi-step reasoning, improving human preference alignment. Online A/B tests on Taobao demonstrate significant improvements: +2.98% CTR, +3.71% IPV, +2.19% TV, and +11.46% NER. RecGPT-V2 establishes both the technical feasibility and commercial viability of deploying LLM-powered intent reasoning at scale, bridging the gap between cognitive exploration and industrial utility.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLM-based recommenders incur computational inefficiency and cognitive redundancy across multiple reasoning routes, making large-scale deployment costly and slow.<br>‚Ä¢ Explanations generated from fixed templates lack diversity and personalization, weakening user trust and engagement.<br>‚Ä¢ Supervised learning limits generalization to evolving intents and new contexts, and struggles with conflicting objectives common in recommendation (e.g., tags vs. explanations).<br>‚Ä¢ Evaluation is overly outcome-focused and simplistic, misaligned with nuanced human judgment and preference standards.<br>‚Ä¢ Rich user-behavior histories are expensive to process; compressing context without losing intent signals is crucial for timeliness and cost efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RecGPT-V2 combines a hierarchical multi-agent reasoning system with hybrid representation inference to compress user-behavior context, meta-prompting for adaptive prompt generation, constrained reinforcement learning to resolve multi-reward conflicts, and an agent-as-a-judge evaluator for multi-step, human-aligned assessment‚Äîyielding ~60% GPU savings and significant gains in recall, explanation diversity, tag prediction, and online metrics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Continual Intent Reasoning with Lifelong Meta-Prompting in Recommender Systems: Enable agents and prompts to adapt online to seasonal and trend shifts without full retraining.<br>‚Ä¢ Unified Constrained Policy Optimization for Multi-Objective Recommender Alignment: A principled RL framework balancing CTR, diversity, tag accuracy, explanation acceptance, and fairness.<br>‚Ä¢ Agent-as-a-Judge Benchmark Suite for Human-Aligned Recommendation Evaluation: Standardized, multi-step agent-based metrics calibrated to human preferences across domains.<br>‚Ä¢ Privacy-Preserving Hybrid Representation Compression for On-Device and Federated Recommenders: Compress intent representations while safeguarding user data and maintaining accuracy.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13303" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13303" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ General image generation and unified MLLMs struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping from tables, leading to errors in proportions, labels, and layout.<br>‚Ä¢ Creative table visualization uniquely demands both aesthetic infographic design and strict high-fidelity representation of quantitative and relational data from dense tables, which current models often misinterpret (e.g., rendering tables verbatim).<br>‚Ä¢ Lack of task-specific data and evaluation benchmarks hinders progress; refinement modules are often a bottleneck without targeted training or feedback, and multi-dimensional evaluation of fidelity and aesthetics is missing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ShowTable is a progressive self-correcting pipeline that uses an MLLM as the central orchestrator (for prompt rewriting and post-hoc reflection) and a diffusion model as executor (for initial generation and iterative refinement), trained with 30K SFT rewriting data and a refinement regimen using a reward model (30K preference pairs) and RL on 5K samples; performance is evaluated on the new TableVisBench benchmark.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Verifiable Infographic Synthesis with Formal Constraints: Integrating differentiable or symbolic validators to enforce data-faithful proportions, labels, and relationships during generation and refinement.<br>‚Ä¢ Human-in-the-Loop Reflective Editing for Table Visualization: Combining expert feedback with MLLM reflection to iteratively correct complex design and data-mapping errors in high-stakes reports.<br>‚Ä¢ Program-Guided Diffusion for Chart Rendering from Tables: Coupling chart-spec program synthesis (e.g., Vega-Lite) with diffusion-based styling to separate correctness (spec) from aesthetics (image), enabling precise, editable visualizations.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Feedforward 3D Editing via Text-Steerable Image-to-3D</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13678" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13678" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ AI-generated 3D assets lack an efficient, feedforward way to be edited with natural language, hindering practical use in design, AR/VR, and robotics.<br>‚Ä¢ Existing 2D-to-3D editing pipelines suffer from multiview inconsistency and are slow (minutes per edit), degrading 3D quality and usability.<br>‚Ä¢ Test-time optimization methods (score distillation, inversion) are slow, require instance-specific tuning, and lack robustness.<br>‚Ä¢ Training a standalone 3D editing model from scratch is impractical due to scarcity of large-scale (image, instruction, 3D) paired data.<br>‚Ä¢ Current evaluation metrics and benchmarks for 3D editing are inadequate (text-only, 2D render-based), lacking measures of both correctness and consistency in 3D.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Steer3D augments a pretrained image-to-3D generator (TRELLIS) with a transformer-based ControlNet that cross-attends to text, enabling feedforward language-steerable 3D edits while keeping the base model frozen. It trains via supervised flow matching and DPO on 96k synthetic (image, instruction, 3D) pairs produced by an automated data engine, achieving faster, more faithful edits with better consistency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Unified Text-Steerable Image-to-3D: Joint Geometry‚ÄìTexture Editing in a Single Feedforward Model: Merge the two-stage (geometry/texture) control into one network with disentangled controls to improve efficiency and coherence.<br>‚Ä¢ Multimodal Steer3D: Language, Point, and Sketch Guidance for Precise 3D Edits: Extend control branches to handle mixed inputs (text, point selections, sketches) for fine-grained localization and user-friendly editing.<br>‚Ä¢ Preference-Driven 3D Editing at Scale: Online Data Engines and Human Feedback Optimization Beyond DPO: Integrate an online synthetic data engine with human-in-the-loop feedback and advanced preference optimization (e.g., RLHF) to strengthen instruction following and reduce no-edit failure modes.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14531" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14531" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting the representational ceiling of the base model. In this work, we propose VersatileFFN, a novel feed-forward network (FFN) that enables flexible reuse of parameters in both width and depth dimensions within a fixed parameter budget. Inspired by the dual-process theory of cognition, VersatileFFN comprises two adaptive pathways: a width-versatile path that generates a mixture of sub-experts from a single shared FFN, mimicking sparse expert routing without increasing parameters, and a depth-versatile path that recursively applies the same FFN to emulate deeper processing for complex tokens. A difficulty-aware gating dynamically balances the two pathways, steering "easy" tokens through the efficient width-wise route and allocating deeper iterative refinement to "hard" tokens. Crucially, both pathways reuse the same parameters, so all additional capacity comes from computation rather than memory. Experiments across diverse benchmarks and model scales demonstrate the effectiveness of the method. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Scaling LLMs has led to prohibitive memory footprints that hinder practical deployment on limited hardware.<br>‚Ä¢ Existing parameter-efficient techniques (pruning, quantization, LoRA) compress weights but do not increase architectural capacity, hitting the base model‚Äôs representational ceiling.<br>‚Ä¢ Standard MoE increases capacity but substantially inflates parameters and storage; even parameter-efficient MoE variants still instantiate separate expert weights.<br>‚Ä¢ Depth-only adaptive/recurrent approaches allocate more compute but ignore width-wise specialization and treat recurrence as a monolithic block.<br>‚Ä¢ There is a need to decouple capacity from parameter count and adapt computation per token (easy vs. hard) within a fixed parameter budget, reducing latency and avoiding naive k-loop FLOP blow-up.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VersatileFFN replaces the standard FFN with a dual-path module that reuses a single shared MLP both in width (virtual MoE via strided slicing into non-overlapping subspaces) and in depth (recursive FFN applications with a Gumbel-Softmax controller deciding per-token iterations). A difficulty-aware gate uses the expected loop count to fuse the two paths, adding capacity through computation while keeping parameters nearly unchanged.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ VersatileAttention: Extending Adaptive Wide-and-Deep Reuse to Transformer Self-Attention: Apply virtual expert slicing to attention heads/projections and recursive attention depth with difficulty-aware fusion for parameter-efficient reasoning.<br>‚Ä¢ Learned Virtual Expert Allocation: End-to-End Subspace Discovery for Parameter-Sharing MoE: Replace fixed strided slicing with learned masks/orthogonality constraints to discover and specialize virtual expert subspaces dynamically.<br>‚Ä¢ Compute-Aware Training Schedules for VersatileFFN: Joint Optimization of Accuracy, FLOPs, and Latency: Design controllers, losses, and early-exit policies that explicitly target compute budgets and hardware-aware parallel execution.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Olmo 3</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13961" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13961" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ The lack of fully-open, state-of-the-art language models with complete, auditable lifecycles (data, code, checkpoints, logs) limits reproducibility, safety auditing, and scientific progress.<br>‚Ä¢ Open models lag behind proprietary systems in long-context reasoning, function calling/tool use, coding, instruction following, general chat, and knowledge recall.<br>‚Ä¢ Existing evaluation practices often suffer from data contamination and inconsistent tooling, hindering fair comparison and rigorous assessment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Olmo 3 releases fully-open 7B and 32B LLMs optimized for long-context reasoning and tool use via a transparent multi-stage pipeline: Dolma 3 pretraining with mid/long-context phases, followed by Dolci SFT/DPO/RL (including RL-Zero) to produce Base, Think, Instruct, and domain-specialized variants, with all data recipes, training/eval code, checkpoints, and logs publicly available.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Scaling Fully-Open Long-Context Models to 1M Tokens: Investigate curricula, architectures, and data recipes to extend context windows while preserving reasoning, retrieval, and stability.<br>‚Ä¢ Open Distillation of Thinking Models: Compressing Olmo 3.1 Think 32B to 7B: Develop distillation strategies that retain chain-of-thought quality, tool-use reliability, and instruction following in smaller open models without proprietary data.<br>‚Ä¢ Auditable LLM Evaluation: Decontamination-Aware Benchmarks and Reproducible Pipelines: Formalize open evaluation suites with decontamination checks, public logs, and standardized protocols to minimize test leakage and enable fair comparisons.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13607" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13607" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Cross-domain heterogeneity (wide variation in response lengths and verification latency) makes RL infrastructure complex, slows training, and complicates curriculum design and hyperparameter tuning.<br>‚Ä¢ Mixing heterogeneous prompts in a single RL run leads to unstable optimization, poor scaling, and risks of catastrophic forgetting across domains.<br>‚Ä¢ Existing pipelines often treat RLHF as mere preference tuning and underutilize its potential to enhance reasoning ability and alignment for general-purpose models.<br>‚Ä¢ Current approaches struggle to maintain or improve performance across diverse benchmarks (math, code, SWE, instruction following) and to support both instruct and deep-thinking modes without regression.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Nemotron-Cascade uses cascaded domain-wise RL: it sequentially applies RL stages (RLHF alignment, instruction-following RL, math RL, code RL, SWE RL) with domain-specific reward modeling and verifiable rewards, instead of mixing domains. This orchestration reduces engineering complexity, stabilizes cross-domain performance, and resists catastrophic forgetting while supporting both instruct and deep-thinking modes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Towards Guarantees of Forgetting Resistance in Cascaded RL for LLMs: A theoretical and empirical study of why sequential, domain-wise RL preserves and sometimes enhances prior capabilities.<br>‚Ä¢ AutoCascade: Latency-Aware Curriculum and Hyperparameter Scheduling for Domain-Wise RL: An automated scheduler that orders domains and tunes rollout lengths/batch sizes based on response and verification latency.<br>‚Ä¢ Multimodal Cascade RL with Verifiable Tool Use: Extending Nemotron-Cascade to vision-language and tool-calling tasks via unified verifiers and domain-specific reward models.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">Differentiable Evolutionary Reinforcement Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13399" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13399" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the "meta-gradient" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Designing effective reward functions for complex, long-horizon reasoning tasks is difficult, brittle, and vulnerable to reward hacking<br>‚Ä¢ Outcome-based rewards are sparse and insufficient to drive efficient learning; RLHF requires costly human annotations and embeds non-scalable human priors<br>‚Ä¢ Existing automated reward optimization via derivative-free evolution is non-differentiable, treats rewards as black boxes, and is sample-inefficient, failing to capture the causal/meta-gradient between reward structure and performance<br>‚Ä¢ There is a need for a scalable, computation-driven approach that learns dense, actionable reward signals and generalizes across domains and OOD settings</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DERL is a bi-level framework where a trainable Meta-Optimizer (LLM) generates parameterized Meta-Rewards by composing executable atomic primitives, and an inner-loop policy is trained with GRPO under these rewards. Validation performance serves as the outer-loop reward to update the Meta-Optimizer via policy gradients, approximating the meta-gradient from reward structure to task success.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Atomic Primitives for Meta-Reward Construction: Automatically induce and verify new primitives from trajectories to expand DERL‚Äôs compositional search space<br>‚Ä¢ Theoretical Analysis of Meta-Gradient Convergence in DERL: Establish sample complexity and stability guarantees for bi-level policy-gradient updates<br>‚Ä¢ DERL for Safe and Robust Alignment: Evolve reward structures that penalize reward hacking and enforce safety constraints under OOD shifts<br>‚Ä¢ Scaling DERL to Multimodal and Embodied Agents: Apply meta-reward evolution to vision-language-action tasks and real-robot feedback loops<br>‚Ä¢ Hybrid DERL with Preference Learning: Combine evolved compositional rewards with human/AI preference models for richer and more calibrated guidance<br>‚Ä¢ Interpretable Meta-Rewards via Causal Decomposition: Extract human-understandable, auditable structures from evolved rewards to aid debugging and governance<br>‚Ä¢ Compute-Efficient DERL with Partial Inner-Loop Training: Use low-fidelity proxies, early stopping, or weight sharing to reduce outer-loop cost while preserving signal</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SS4D: Native 4D Generative Model via Structured Spacetime Latents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14284" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14284" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Efficiently generating high-quality, spatio-temporally consistent 4D objects from monocular videos<br>‚Ä¢ Overcoming limitations of SDS-based methods that require hours of per-instance optimization and often yield over-saturated artifacts<br>‚Ä¢ Addressing feed-forward approaches that rely on RGB photogrammetry losses, producing coarse/noisy geometry and weak temporal coherence under fast motion<br>‚Ä¢ Coping with the scarcity of 4D training data while preserving strong spatial consistency from 3D priors<br>‚Ä¢ Enabling long-sequence generation without quadratic attention costs or quality degradation, and improving robustness to occlusions and motion blur in real-world videos</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SS4D extends structured 3D latents (TRELLIS) into structured spacetime latents by adding temporal self-attention with shifted windows and hybrid 4D positional encoding (3D absolute + 1D RoPE), and aligning both the generator and VAE temporally to reduce flicker. It compresses latents along time via a factorized 4D convolutional CompNet with temporal downsampling, and uses progressive training with random masking to efficiently decode per-frame 3D Gaussians that remain coherent over long sequences.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ End-to-End SS4D: Unifying Structure and Latent Generation for Faster Native 4D Models: Integrate the structure VAE, flow transformers, and decoder into a single differentiable pipeline with joint learning and rendering supervision to improve efficiency and quality.<br>‚Ä¢ Real4D: Domain-Adaptive Training of Structured Spacetime Latents on In-the-Wild Videos: Incorporate real video datasets, weak multi-view constraints, and adversarial/perceptual losses to achieve photorealistic textures and stronger generalization beyond synthetic data.<br>‚Ä¢ Transp-HiFi4D: Modeling Transparency and High-Frequency Appearance in 4D Gaussians: Augment latents with material/BRDF channels and layered transparency, using specular-aware rendering to better handle glass, hair, fine textures, and rapid motion.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14442" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14442" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a Dreamer that employs generative models to visualize how an interaction would look; (2) a Thinker that utilizes large vision-language models to decide what object part to interact with; and (3) a Spotter that orchestrates vision foundation models to precisely locate where the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Affordance prediction simultaneously requires high-level instruction reasoning and precise low-level visual grounding, which monolithic end-to-end models entangle and struggle to balance.<br>‚Ä¢ Vision foundation models excel at fine-grained localization but lack semantic understanding, while VLMs reason well but produce coarse or inaccurate spatial predictions.<br>‚Ä¢ Supervised end-to-end approaches trained on limited annotated datasets generalize poorly to novel objects and unseen environments.<br>‚Ä¢ Joint optimization introduces a trade-off between reasoning and grounding, where improving one capability can degrade the other.<br>‚Ä¢ Monolithic designs reduce flexibility, make upgrades costly, and often cannot leverage the strongest closed-source models, capping reasoning performance.<br>‚Ä¢ There is a need for a training-free, interpretable, modular pipeline that can coordinate specialized foundation models at test time for robust zero-shot affordance reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A4-Agent decouples affordance prediction into a training-free, three-stage agentic pipeline: Dreamer imagines plausible interactions via image generation, Thinker uses VLMs to decide the actionable object part, and Spotter grounds the location with a coarse-to-fine procedure (detection/keypoints ‚Üí segmentation) to produce pixel-accurate masks. It orchestrates complementary pre-trained foundation models at inference time for zero-shot affordance reasoning and grounding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ A4-Agent-Video: Temporal Agentic Framework for Dynamic Affordance Reasoning: Extend Dreamer to video generation and Thinker/Spotter to temporal reasoning and tracking for instruction-conditioned affordances in videos.<br>‚Ä¢ 3D-A4: Zero-Shot 3D Affordance Grounding with Multimodal Agents: Integrate 3D vision (point clouds, NeRF/GSplat) so Dreamer imagines 3D interactions and Spotter outputs actionable 3D regions and contact points for robotics.<br>‚Ä¢ Auto-Orchestrator: Learning to Compose Foundation Models for Affordance Tasks: Train a meta-controller (e.g., RL with uncertainty-aware self-refinement) to adaptively select and sequence generators, VLMs, and grounding modules per task for reliability and efficiency.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14008" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14008" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to a 2x speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Masked Diffusion Models (MDMs) must repeatedly process all tokens‚Äîincluding redundant masked tokens‚Äîat every sampling step, causing slow inference.<br>‚Ä¢ MDMs use full attention, which prevents KV-cache acceleration and limits practical efficiency despite parallel decoding.<br>‚Ä¢ Existing accelerations (e.g., Block Diffusion, training-free KV caching) impose left-to-right or semi-AR constraints, sacrificing bidirectional context and harming tasks like image generation, inpainting, and editing; heuristic methods often degrade quality unpredictably.<br>‚Ä¢ There is a need for a unified multimodal model that accelerates sampling while preserving MDM advantages (bidirectional context, arbitrary-order decoding) and maintaining generation quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Sparse-LaViDa re-parameterizes MDMs to operate on sparse inputs by truncating unnecessary masked tokens, adding learnable register tokens as compact surrogates, and using a step-causal attention mask that enables KV caching while preserving bidirectional context. It supports arbitrary decoding orders and achieves up to ~2‚Äì3√ó speedups across text-to-image, image editing, and visual reasoning without quality loss.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Register Token Allocation for Sparse MDMs: Learn the number, placement, and roles of register tokens per sample/step to optimize the speed‚Äìquality trade-off.<br>‚Ä¢ Sparse-LaViDa for Video and Audio: Extending Step-Causal Attention to Spatiotemporal Tokens: Generalize the sparse parameterization and step-causal masking to video/audio with hierarchical spatiotemporal blocks and cross-modal consistency.<br>‚Ä¢ End-to-End Pretraining of Sparse MDMs with Learned Unmasking Schedules: Train sparse MDMs from scratch and jointly learn task-aware unmasking orders to maximize efficiency and fidelity across modalities.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14696" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14696" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\% to 6.9\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Monocular video-to-simulation pipelines struggle to produce physically valid human‚Äìscene interactions, leading to foot sliding, interpenetrations, and unstable tracking.<br>‚Ä¢ Existing joint human‚Äìscene reconstructions are noisy (duplicate/missing geometry from parallax and occlusion) and lack physics in the loop, causing RL policies to fail.<br>‚Ä¢ Reconstructions often miss occluded interaction surfaces (e.g., chair seats, stair treads), undermining contact reasoning and control.<br>‚Ä¢ Dense, non-convex scene meshes are inefficient for collision detection, limiting RL throughput and scalability.<br>‚Ä¢ Prior methods focus on simplistic environments with minimal scene interaction, hindering real-to-sim for in-the-wild videos.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>CRISP reconstructs camera and a global point cloud, then fits a compact set of convex planar primitives via clustering over depth, normals, and flow, augmented by contact-guided hallucination of occluded surfaces, and validates the human‚Äìscene reconstruction by training a physics-based humanoid controller with RL to track motion. This yields clean, simulatable geometry and contact-aware motion that improve stability and efficiency in simulation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond Planes: Hierarchical Primitive Abstractions for Simulatable Scene Reconstruction from Monocular Video: Extend planar fitting to a library of convex primitives (e.g., cylinders, swept surfaces) with hierarchical decomposition to capture curved/complex geometry while preserving simulation efficiency.<br>‚Ä¢ CRISP-Online: Real-Time Vid2Sim with Physics-in-the-Loop for Dynamic Human‚ÄìScene Interactions: Develop incremental SLAM, streaming contact prediction, and on-the-fly primitive updates to handle dynamic scenes and enable real-time control.<br>‚Ä¢ Deformable CRISP: Contact-Guided Reconstruction and Control with Compliant and Articulated Objects: Incorporate deformable and articulated scene modeling and RL controllers that reason about compliance and articulation during human‚Äìobject interactions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14666" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14666" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\% on long-horizon tasks, +22.0\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\% success on unseen tasks without task-specific demonstrations training (vs. 0\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Supervised fine-tuning (SFT) for VLA models requires hundreds of demonstrations per task, creating prohibitive labor and scaling costs.<br>‚Ä¢ Behavior cloning produces brittle policies that memorize trajectories, struggle with off-distribution states, and lack error recovery.<br>‚Ä¢ Prior RL post-training methods depend on oracle (ground-truth) rewards unavailable at deployment; practical test-time adaptation needs autonomous, dense feedback.<br>‚Ä¢ Progress-based reward estimators are inherently noisy, making long-horizon credit assignment and rollout termination unstable without signal smoothing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>EVOLVE-VLA performs test-time training by optimizing VLA policies with GRPO using a learned progress estimator (VLAC) as dense reward, stabilized via accumulative progress estimation with interval-based milestones and a diminishing-returns update, and a progressive horizon extension curriculum for long-horizon tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Semantic-Rule Aligned Reward Models for Embodied Agents: Calibrate and fuse vision-language progress estimators with environment success oracles to eliminate reward‚Äìcriterion mismatches.<br>‚Ä¢ Zero-Shot Cross-Task Adaptation via Foundation Progress Critics: Train broader, task-agnostic progress models that enable adaptation to unseen tasks without task-specific demonstrations or in-context examples.<br>‚Ä¢ Safe and Sample-Efficient Test-Time RL on Physical Robots: Develop safety-constrained exploration, distributed/parallel data collection, and sim-to-real transfer for reward models to make on-robot TTT practical.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14550" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14550" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Medical image restoration (MedIR) aims to recover high-quality medical images from their low-quality counterparts. Recent advancements in MedIR have focused on All-in-One models capable of simultaneously addressing multiple different MedIR tasks. However, due to significant differences in both modality and degradation types, using a shared model for these diverse tasks requires careful consideration of two critical inter-task relationships: task interference, which occurs when conflicting gradient update directions arise across tasks on the same parameter, and task imbalance, which refers to uneven optimization caused by varying learning difficulties inherent to each task. To address these challenges, we propose a task-adaptive Transformer (TAT), a novel framework that dynamically adapts to different tasks through two key innovations. First, a task-adaptive weight generation strategy is introduced to mitigate task interference by generating task-specific weight parameters for each task, thereby eliminating potential gradient conflicts on shared weight parameters. Second, a task-adaptive loss balancing strategy is introduced to dynamically adjust loss weights based on task-specific learning difficulties, preventing task domination or undertraining. Extensive experiments demonstrate that our proposed TAT achieves state-of-the-art performance in three MedIR tasks--PET synthesis, CT denoising, and MRI super-resolution--both in task-specific and All-in-One settings. Code is available at https://github.com/Yaziwel/TAT.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ All-in-One MedIR suffers from task interference: conflicting gradient updates on shared parameters across heterogeneous tasks (PET synthesis, CT denoising, MRI SR) degrade performance.<br>‚Ä¢ Task imbalance: differing learning difficulties across tasks lead to uneven optimization when using uniform loss weights, causing some tasks to dominate and others to undertrain.<br>‚Ä¢ Limitations of existing methods: task-specific models have poor cross-task generalization, redundant pipelines and resource overhead, and vulnerability to data scarcity; prior All-in-One approaches rely on fixed shared parameters (limited adaptability) and largely overlook sample-level loss balancing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TAT introduces a U-shaped Transformer with a Task Representation Extraction Network (TREN) that derives task embeddings to generate task-specific depth-wise convolution weights in Weight-Adaptive Transformer Blocks, mitigating gradient conflicts. It further employs a sample-level task-adaptive loss balancing strategy that estimates per-sample scaling via an MLP on L1-based diagnostics, preventing task domination and undertraining.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Task-Adaptive Transformers for 3D Volumetric and Multi-Slice Medical Image Restoration: Extend TAT to 3D/temporal volumes with memory-efficient weight generation and spatiotemporal attention.<br>‚Ä¢ Uncertainty-Guided Sample-Level Loss Balancing in All-in-One MedIR: Incorporate aleatoric/epistemic uncertainty into sigma estimation to enhance robustness and clinical safety across tasks and domains.<br>‚Ä¢ Cross-Modal Knowledge Distillation for Task Representation Learning in Unified MedIR: Distill knowledge from task-specific expert models to refine TREN embeddings and improve task-adaptive weight generation across modalities.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14273" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14273" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\% on NExT-GQA and 4.6\% on ReXTime, while also enhancing average answer accuracy by 2.4\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\% on long-video benchmarks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LVLMs lack precise temporal awareness, leading to mislocalized evidence and hallucinated answers in grounded video QA (GVQA).<br>‚Ä¢ GRPO-based methods mainly optimize format and IoU (and sometimes answer accuracy) but cannot verify that localized segments truly contain the visual evidence needed for correct reasoning.<br>‚Ä¢ Standard GRPO applies uniform token-level credit and na√Øvely sums multi-faceted rewards, blurring the distinction between localization and answer generation signals.<br>‚Ä¢ Long-form videos under tight context budgets force coarse visual tokens that miss fine-grained details, degrading both grounding precision and answer accuracy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Zoom-Zero is a coarse-to-fine RL framework that first predicts query-conditioned temporal spans and preliminary answers, then zooms into the localized segments with higher per-frame token resolution for fine-grained visual verification. It introduces a zoom-in accuracy reward to enforce evidence fidelity within the span and a token-selective credit assignment (TokenAdv) to decouple and target rewards for localization versus answer tokens under GRPO.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Multi-Scale Zoom Policies for Long-Video LVLMs: Learn a policy that dynamically schedules when and how much to zoom based on uncertainty and evidence density across time.<br>‚Ä¢ Causal Token-Level Credit Assignment for GVQA: Use causal attribution (e.g., interventions or Shapley-style credit) to refine TokenAdv and more accurately assign rewards to grounding vs answer-generation tokens.<br>‚Ä¢ Evidence-Consistent RL Objectives for Spatiotemporal Grounding: Extend the zoom-in accuracy reward to include spatial grounding and cross-modal checks, ensuring localized segments contain verifiable visual-text evidence.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14067" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14067" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ AR language models are bottlenecked by sequential left-to-right decoding, yielding low throughput in memory-bounded settings.<br>‚Ä¢ Diffusion language models promise parallel generation but lag in speed due to poor KV cache compatibility and limited decoding parallelism, and are costly to train from scratch because they must learn all permutations.<br>‚Ä¢ Existing AR-to-dLM conversions use fully bidirectional attention, causing excessive context corruption, hindering KV caching, and drifting weights away from AR causality, which hurts accuracy.<br>‚Ä¢ There is a training‚Äìtest mismatch: uniform masking during training vs left-to-right, confidence-based sampling at test time, degrading downstream performance.<br>‚Ä¢ Practical guidance is lacking on block sizes and attention patterns that preserve pretrained AR capabilities while enabling fast parallel decoding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Convert pretrained AR models into dLMs via continuous pretraining with block-wise attention that remains causal across blocks and conditions each corrupted block on clean context, enabling KV caching and preserving AR weight distributions; complement this with position-dependent token masking that increases masking probabilities for later tokens to mimic test-time left-to-right behavior.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Block-wise Attention for Domain-Specific dLMs: Learn task- and domain-aware block sizes and conditioning strategies to optimize accuracy‚Äìthroughput across varied workloads.<br>‚Ä¢ Learning-to-Mask: Meta-Optimized Position-Dependent Masking for Diffusion LMs: Use reinforcement/meta-learning to tune masking schedules that best match downstream decoding confidence profiles.<br>‚Ä¢ Scaling Laws for AR-to-dLM Conversion: Establish data/model/step scaling laws that predict accuracy‚Äìthroughput trade-offs and optimal training budgets for efficient AR-to-dLM pretraining.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">Janus: Disaggregating Attention and Experts for Scalable MoE Inference</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13525" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13525" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Mixture-of-Experts (MoE) model inference is challenging due to high resource demands and dynamic workloads. Existing solutions often deploy the entire model as a single monolithic unit, which applies a unified resource configuration to both attention and expert modules despite their different requirements, leading to limited scalability and resource inefficiency. In this paper, we propose Janus, a scalable MoE inference system that disaggregates attention and experts on separate GPU sub-clusters, enabling each module to be managed and scaled independently. Janus incorporates three key designs for efficient, disaggregated MoE inference. First, it proposes an adaptive two-phase communication scheme that exploits intra- and inter-node bandwidth hierarchies for low-latency data exchange. Second, motivated by the memory-bound nature of MoE modules, Janus introduces a lightweight scheduler and implements it as a GPU kernel to balance the number of activated experts across GPUs at minimal overhead, thereby reducing inference latency. Third, Janus performs fine-grained resource management to dynamically adjust expert placement and independently scale attention and MoE resources to improve overall efficiency. Evaluation shows Janus achieves up to 3.9 higher perGPU throughput than state-of-the-art systems while meeting per-token latency requirements.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Large MoE models have expert-dominated memory footprints, making monolithic deployments resource-inefficient and hard to elastically scale while meeting token-level SLOs.<br>‚Ä¢ Attention and MoE layers have fundamentally different performance characteristics (MoE is memory-bound; latency scales with activated experts), so uniform resource configurations waste capacity and limit throughput.<br>‚Ä¢ Disaggregation creates frequent many-to-many small-message communication between attention and MoE instances; standard collectives assume symmetric groups and fixed sizes, causing high latency and poor fit.<br>‚Ä¢ Expert activation must be scheduled across many GPUs with microsecond-level overhead; prior systems pin experts or use random scheduling, leading to activation imbalance and latency spikes.<br>‚Ä¢ Existing systems scale coarsely at the model-instance level (coupled DP/EP), lacking fine-grained, independent scaling and activation-aware expert replication/placement to maximize per-GPU throughput under SLOs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Janus disaggregates attention and MoE layers onto separate GPU sub-clusters and introduces adaptive two-phase communication with MoE-side gating to cut cross-cluster transfers; it uses a GPU-resident, synchronization-free Activated-Expert-Balanced Scheduling (AEBS) plus activation-aware expert replication and placement, enabling independent scaling of attention/MoE to meet TPOT SLOs with higher per-GPU throughput.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning-Augmented Janus: Predictive Expert Replication and Scheduling for MoE Inference: Integrate online learning to anticipate hot experts and co-activation patterns, improving AEBS and replica placement under bursty workloads.<br>‚Ä¢ Janus-T: Co-Designing Tensor Parallelism with Disaggregated Attention‚ÄìMoE for Very Large Models: Extend Janus to tensor-parallel models, jointly optimizing communication, scheduling, and scaling across DP/EP/TP.<br>‚Ä¢ QoS-Aware Multi-Tenant Janus: Isolation and Fairness in Disaggregated MoE Serving: Add admission control and SLO-aware scaling/scheduling to support multiple tenants with predictable latency and efficient GPU sharing.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Spherical Leech Quantization for Visual Tokenization and Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14697" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14697" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization (Œõ_{24}-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Visual tokenizers lag far behind language models in vocabulary size, limiting compression and generation capacity despite the higher information density of visual data.<br>‚Ä¢ Existing non-parametric quantization (LFQ, BSQ, FSQ) is heuristic and often requires ad hoc entropy regularization or hyperparameter tricks, leading to instability and codebook collapse.<br>‚Ä¢ There is no principled geometric design for spherical quantization; current codebooks (e.g., BSQ) are not densest packings and underperform in high dimensions.<br>‚Ä¢ Training autoregressive models with very large codebooks is memory-intensive and typically relies on complex mechanisms (index subgrouping, bit flipping), hindering scalability and simplicity.<br>‚Ä¢ A unified theoretical framework is needed to relate NPQ methods to lattice coding, clarifying when and why entropy penalties are necessary.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper unifies NPQ via lattice coding and introduces Spherical Leech Quantization (Œõ24-SQ), which normalizes latents to the unit sphere and quantizes to the 196,560 first-shell points of the Leech lattice, yielding high-dispersion codes that remove the need for entropy regularization and enable efficient large-codebook autoregression with memory-optimized classification and factorized d-itwise prediction.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Spherical Lattice Quantization via Learnable Rotations and Multi-Shell Selection: Jointly learn encoder rotations to align latent distributions with Leech lattice Voronoi regions and dynamically select lattice shells to control bitrate and reconstruction quality.<br>‚Ä¢ Hierarchical Leech Lattice Codebooks for Multi-Scale and Residual Tokenization: Build nested subsets of Œõ24 to support multi-scale/residual quantization, enabling variable capacity, efficient nearest-neighbor search, and improved autoregressive training.<br>‚Ä¢ Generalized d-Itwise Autoregression for Non-Binary Lattice Codes: Develop architectures and objectives for factorized per-dimension prediction with theoretical guarantees on calibration and memory efficiency across spherical lattice codebooks.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">RePo: Language Models with Context Re-Positioning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14391" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14391" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, f_œÜ, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Fixed positional schemes (linear or constant indices) impose a rigid contextual structure that does not reflect actual token dependencies.<br>‚Ä¢ Uninformative position assignments increase extraneous cognitive load (per CLT), wasting limited working memory that could be used for reasoning and attention allocation.<br>‚Ä¢ Existing LLMs are brittle under noisy contexts, structured inputs, and long sequences because they cannot reorganize or chunk context adaptively.<br>‚Ä¢ Pre-defined, non-learned indices prevent dynamic re-positioning, hindering attention to distant but relevant information.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>REPO introduces a differentiable position assignment module fœï that learns continuous, non-linear token positions conditioned on context, replacing fixed integer or constant indices. Trained via continual pretraining (on OLMo-2 1B), it re-positions tokens to reflect dependencies, reducing extraneous load and improving attention to relevant distant information.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Task-Adaptive Context Re-Positioning for Retrieval-Augmented Generation: Jointly learn re-positioning with retriever signals to suppress distractors and surface relevant passages in long, noisy contexts.<br>‚Ä¢ Multimodal Re-Positioning Transformers: Extend REPO to align and reorder text, code, tables, and vision/audio tokens in a shared continuous position space for structure-aware grounding.<br>‚Ä¢ Scaling Laws and Theory of Learned Positional Maps: Characterize how REPO capacity, data scale, and context length affect extraneous load reduction and performance, offering principled design guidelines.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14620" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14620" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of Japanese benchmarks that require integrated visual‚Äìtextual understanding where question text is embedded within images; most existing Japanese LMM evaluations provide text and images separately.<br>‚Ä¢ MMMU-Pro‚Äôs image-embedded evaluation exists only in English, leaving Japanese multimodal perception unassessed.<br>‚Ä¢ Manual creation of image-based VQA benchmarks is costly and unscalable; a scalable, low-cost construction pipeline is needed.<br>‚Ä¢ Open-source LMMs struggle on integrated Japanese visual-text tasks (near random on JMMMU-Pro), revealing gaps in OCR and multimodal reasoning.<br>‚Ä¢ Real-world usage (e.g., screenshots with Japanese text and graphics) demands evaluation that mirrors interleaved text‚Äìimage layouts and diverse backgrounds.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce JMMMU-Pro by embedding each JMMMU question‚Äôs text and image into a single composite image, built via Vibe Benchmark Construction‚Äîan image-generation-driven pipeline (using Nano Banana Pro) that composes visuals and clean Japanese text with diverse layouts, followed by human verification and prompt refinement for quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Layout-Aware Multimodal Reasoning for Japanese OCR Benchmarks: Develop architectures that jointly attend to text regions and visual context to improve integrated perception and reasoning on JMMMU-Pro.<br>‚Ä¢ VibeBench-Auto: Fully Automated Quality Control for Image-based Benchmark Construction: Replace human verification with multimodal QC agents to automatically detect text legibility, alignment, and semantic fidelity in the Vibe pipeline.<br>‚Ä¢ Cross-Script OCR-Reasoning Pretraining for Japanese LMMs: Pretrain unified OCR + reasoning modules across kanji/kana/romaji with synthetic layout diversity to close the performance gap on JMMMU-Pro.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MobileWorldBench: Towards Semantic World Modeling For Mobile Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14014" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14014" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Pixel-space next-frame prediction is ill-suited for mobile GUI agents due to high complexity in rendering layouts, dynamic content, and exact visuals, leading to low-fidelity and mismatched future states.<br>‚Ä¢ Existing world models focus on videos and raw pixels, lacking semantic understanding of GUI actions and UI structure needed for effective planning in mobile apps.<br>‚Ä¢ The domain lacks a dedicated benchmark to evaluate VLMs as semantic world models for GUI agents, hindering progress and comparability.<br>‚Ä¢ There is no large-scale, tailored dataset to train VLMs on language-based state transitions in mobile GUIs, limiting their predictive and planning capabilities.<br>‚Ä¢ Current agent frameworks do not integrate semantic world models to improve task success, leaving a gap between prediction and actionable planning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Formulate GUI world modeling as natural language state-transition prediction using vision-language models, introduce MobileWorldBench to evaluate this capability, and release the 1.4M-sample MobileWorld dataset to finetune VLMs; integrate the trained VLM world models into mobile agent planning to improve task success.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Structured GUI State Machines from Language-Conditioned World Models: Derive formal, app-specific state graphs from semantic transitions to enable robust planning and verification.<br>‚Ä¢ Cross-App Generalization in Semantic World Models for Mobile and Web Interfaces: Benchmark and train models to transfer semantic world modeling across unseen apps and interface modalities.<br>‚Ä¢ Uncertainty-Aware Semantic World Modeling for Mobile Agents: Incorporate probabilistic predictions and calibrated confidence into planning to handle variable content and partial observability.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13655" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13655" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ No systematic, cross-architecture comparison of abliteration tools exists, forcing researchers to rely on anecdotes and trial-and-error for tool selection.<br>‚Ä¢ Safety-capability trade-offs are poorly characterized: researchers need methods that suppress refusals while minimizing distribution shift (KL divergence) and preserving benchmarks (especially math reasoning on GSM8K).<br>‚Ä¢ Tool compatibility and resource constraints vary widely (e.g., TransformerLens dependence, quantization, runtime), and alignment method differences (DPO vs RLHF) may drive model-dependent outcomes without empirical guidance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A reproducible evaluation framework compares four abliteration tools across 16 instruction-tuned LLMs using standardized metrics (KL divergence, refusal rate/ASR, and MMLU/GSM8K/HellaSwag), separating compatibility from quantitative subsets and contrasting single-pass versus Bayesian-optimized approaches.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Multi-Dimensional Concept Cone Abliteration: Evaluating subspace-based methods beyond single-direction removal to improve refusal suppression with minimal capability loss.<br>‚Ä¢ AutoAblate: Capability-Constrained Bayesian Optimization for Resource-Aware Abliteration Across Architectures: Jointly optimize refusal reduction under explicit capability and KL constraints with adaptive layer selection.<br>‚Ä¢ Defending Against Abliteration: Extended-Refusal and Representation Rerouting as Robust Safety Training: Compare defense strategies and quantify robustness using the paper‚Äôs standardized metrics across diverse models.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-11">

    <div class="paper">
        <h2 class="paper-title">TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13106" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13106" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ RLVR for LLM reasoning is bottlenecked by expensive ground-truth annotations, limiting scalability to specialized domains (e.g., medicine, finance)<br>‚Ä¢ Purely unsupervised RLVR (entropy/majority-vote/self-certainty) tends to reinforce incorrect consensus, causing late-stage collapse without external grounding<br>‚Ä¢ Naively mixing supervised and unsupervised RLVR yields marginal gains because it ignores how labeled and unlabeled data should interact during learning<br>‚Ä¢ Classic semi-supervised assumptions (shared label space/feature similarity) fail in RLVR where each instance has an instance-specific solution space, requiring a new bridge between labeled and unlabeled data</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>TRAPO is a semi-supervised RLVR algorithm that anchors training with a small labeled set and dynamically selects reliable unlabeled samples by matching their pass-rate trajectories to those of labeled data, using trajectory cosine similarity with top-p/threshold gating. It applies a hybrid reward (ground-truth for labeled, consistency-based pseudo-rewards for selected unlabeled) within group-based policy optimization to stabilize learning and prevent collapse while improving data efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Active Trajectory-Guided Labeling for RLVR: Select a minimal set of unlabeled samples to annotate that maximally improves trajectory alignment coverage and stabilizes training<br>‚Ä¢ Multi-Signal Trajectory Alignment for Semi-Supervised Reasoning RL: Fuse pass-rate trends with additional signals (entropy, calibration, disagreement, hidden-state drift) for more robust reliability estimation<br>‚Ä¢ Extending TRAPO to Non-Verifiable and Partial-Feedback Tasks: Adapt trajectory guidance to tasks with weak verifiers, critique-based feedback, or preference signals instead of exact answers<br>‚Ä¢ Theoretical Guarantees for Trajectory-Aligned Semi-Supervised RL: Provide generalization bounds and convergence/stability analyses under noisy pseudo-rewards and selection thresholds<br>‚Ä¢ Curriculum and Domain Adaptation via Trajectory-Based Selection: Use trajectory dynamics to schedule task difficulty and transfer across domains/OOD tasks with adaptive similarity metrics</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.12941" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.12941" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ The inherent gap between shallow (local) and deep (global) feature pyramids in CNN/Transformer hybrids leads to poor alignment of semantics, harming boundary precision and instance integrity‚Äîcritical for accurate urban mapping and GIS applications.<br>‚Ä¢ Existing approaches insufficiently integrate complementary global and local cues; weak fusion yields ambiguous masks on complex, multi-scale building structures, degrading downstream planning and analytics.<br>‚Ä¢ Ambiguity in uncertain regions is not explicitly addressed, and transformer windowing is typically hand-crafted and content-agnostic, causing attention to irrelevant tokens and unreliable predictions in challenging scenes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>UAGLNet introduces a cooperative CNN‚ÄìTransformer encoder with a hybrid interaction block (CIB: MKFM + MHSA) to align local/global semantics, a Global-Local Fusion (GLF) module to complementarily merge hierarchies, and an Uncertainty-Aggregated Decoder (UAD) that estimates pixel-wise uncertainty via reparameterized Gaussian sampling to attenuate low-confidence regions during fusion.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Content-Adaptive Token Partitioning for Transformers in Building Extraction: Replace fixed windowing with learned, input-aware token grouping to improve global attention relevance and reduce irrelevant context aggregation.<br>‚Ä¢ Uncertainty-Guided Domain Adaptation Across Cities for Robust Building Extraction: Exploit UL/UG maps to select reliable pseudo-labels and weight adaptation losses, enabling transfer to new geographies with limited annotation.<br>‚Ä¢ Multimodal Global-Local Fusion with LiDAR/SAR for Resilient Building Segmentation: Extend GLF to fuse multispectral, LiDAR, and SAR cues, using uncertainty to reconcile modality conflicts and improve robustness under occlusion and varying illumination.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.14440" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.14440" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Synthetic training videos (e.g., copy-paste shifts from ImageNet) fail to model real-world dynamics such as perspective changes, articulated/multi-object motion, and camera motion, limiting temporal learning for video instance segmentation.<br>‚Ä¢ Unsupervised single-frame masks are temporally noisy and quality-varying, lacking identity linkage across frames; there is a need to build temporally coherent pseudo-labels without human annotations.<br>‚Ä¢ Existing unsupervised video methods often rely on optical flow/point tracks as direct supervision or focus on single foreground (VOS), and supervised propagation needs labels‚Äînone directly address fully unsupervised, instance-level VIS on real videos.<br>‚Ä¢ Training with sparse pseudo-annotations causes false-negative penalties on unannotated frames; a learning framework is needed that enables implicit mask propagation without penalizing missing labels.<br>‚Ä¢ Achieving strong zero-shot generalization with real videos (instead of massive synthetic datasets) remains underexplored and important for practicality and domain fidelity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>S2D first discovers high-quality, temporally coherent keymasks by clustering mask visibility patterns from point tracks and refining associations via a point-to-mask Jaccard matching. It then trains a VideoMask2Former with a Temporal DropLoss in a teacher‚Äìstudent Sparse-To-Dense distillation to propagate from sparse anchors to dense labels, yielding a real-video‚Äìtrained, state-of-the-art unsupervised VIS model.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Temporal-Feature-Guided Keymask Discovery without Explicit Tracking: Replace point-track priors with learned spatiotemporal features or diffusion-based correspondences to improve robustness and reduce dependency on trackers.<br>‚Ä¢ Video Copy-Paste and Scale-Aware Augmentations for Small-Object VIS: Develop video-consistent copy-paste pipelines that preserve motion cues to boost recall of small/thin structures in unsupervised settings.<br>‚Ä¢ Domain-Mixed Unsupervised VIS for Driving Scenarios: Curate and filter domain-specific real videos (e.g., driving) for S2D to improve zero-shot transfer to road scenes via domain-aware distillation.<br>‚Ä¢ Long-Range Consistency Distillation for Multi-Frame Training: Extend S2D from 2-frame snippets to long clips with cycle-consistency and temporal smoothness constraints for improved identity stability.<br>‚Ä¢ Unified Unsupervised Video Panoptic Segmentation via Sparse-to-Dense Distillation: Generalize S2D to jointly learn things and stuff with panoptic masks, leveraging temporal grouping and distillation across categories.<br>‚Ä¢ Active Pseudo-Label Selection for Keymask Discovery: Introduce uncertainty- and diversity-driven sampling to select and refine keymasks that maximize downstream propagation quality.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-12">

    <div class="paper">
        <h2 class="paper-title">Hierarchical Dataset Selection for High-Quality Data Sharing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.10952" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.10952" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Efficiently selecting entire datasets from heterogeneous sources to improve a target model under resource, bandwidth, and labeling constraints is critical, yet underexplored.<br>‚Ä¢ Existing methods focus on instance-level selection, assume uniform relevance across sources, ignore dataset- and group-level structure, and can waste exploration or pick misleading samples.<br>‚Ä¢ Practical data sharing happens in dataset-level units from repositories or institutions; algorithms need to exploit hierarchical structure to generalize from limited observations and avoid noisy or redundant sources.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DaSH (Dataset Selection via Hierarchies) models utility at both group (e.g., repository/institution) and dataset levels, leveraging hierarchical grouping to generalize utility estimates from few observations and reduce exploration. It sequentially selects datasets predicted to boost downstream performance, avoiding noisy sources and achieving higher accuracy with fewer evaluations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Federated DaSH: Privacy-Preserving Hierarchical Dataset Selection Across Institutions: Extend DaSH to federated settings with differential privacy and limited sharing, enabling secure cross-institution dataset utility estimation.<br>‚Ä¢ Meta-DaSH: Metadata-Augmented Hierarchical Utility Modeling for Cross-Domain Dataset Selection: Integrate rich dataset metadata and provenance into the hierarchy to improve relevance estimation and transfer across domains.<br>‚Ä¢ Theoretical Guarantees for Hierarchical Dataset Selection: Derive sample complexity and regret bounds for hierarchical utility modeling to quantify exploration-efficiency and robustness under sparse relevant sources.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.10945" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.10945" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing referring video datasets and methods overemphasize static attributes (color, shape) and salient single objects, allowing identification from a single frame and underutilizing motion cues.<br>‚Ä¢ Lack of multi-modal, motion-centric benchmarks (text + audio) with generalized expressions (including multi-object and no-target cases) and tracking annotations limits progress in RVOS, AVOS, RMOT, and RMEG.<br>‚Ä¢ Current approaches struggle with capturing both fleeting and long-term actions and reasoning about temporal order; frame-feature inputs constrain LLMs to short sequences and hinder object-level temporal understanding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce MeViSv2, a large-scale multi-modal dataset of motion-centric referring expressions (text/audio) with segmentation and tracking annotations, supporting RVOS, AVOS, RMOT, and RMEG. Propose LMPM++: language-conditional queries produce object embeddings fed as tokens to an LLM for long-sequence temporal reasoning, augmented by a temporal-order contrastive loss.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Object-Token LLMs for Long-Horizon Motion Grounding: Scale object-token representations and sequence modeling to minute-long videos to improve motion reasoning for RVOS/AVOS/RMOT.<br>‚Ä¢ Audio-Text Unified Motion Expression Alignment for Referring Video Segmentation: Jointly pretrain acoustic and textual motion cues with video object trajectories for robust multi-modal grounding and disambiguation.<br>‚Ä¢ Reasoning over Implicit and No-Target Expressions in Video: Counterfactual Motion Segmentation: Model implicit queries and no-target cases via counterfactual reasoning and uncertainty estimation to decide when and what to segment.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.10342" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.10342" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large-scale Vision-Language Models (VLMs) exhibit impressive complex reasoning capabilities but remain largely unexplored in visual sequential planning, i.e., executing multi-step actions towards a goal. Additionally, practical sequential planning often involves non-optimal (erroneous) steps, challenging VLMs to detect and correct such steps. We propose Corrective Sequential Planning Benchmark (CoSPlan) to evaluate VLMs in error-prone, vision-based sequential planning tasks across 4 domains: maze navigation, block rearrangement, image reconstruction,and object reorganization. CoSPlan assesses two key abilities: Error Detection (identifying non-optimal action) and Step Completion (correcting and completing action sequences to reach the goal). Despite using state-of-the-art reasoning techniques such as Chain-of-Thought and Scene Graphs, VLMs (e.g. Intern-VLM and Qwen2) struggle on CoSPlan, failing to leverage contextual cues to reach goals. Addressing this, we propose a novel training-free method, Scene Graph Incremental updates (SGI), which introduces intermediate reasoning steps between the initial and goal states. SGI helps VLMs reason about sequences, yielding an average performance gain of 5.2%. In addition to enhancing reliability in corrective sequential planning, SGI generalizes to traditional planning tasks such as Plan-Bench and VQA.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ VLMs are underexplored in visual sequential planning, especially when actions must be executed over time to reach a goal.<br>‚Ä¢ Real-world planning often includes non-optimal (erroneous) steps; current models struggle to detect and correct these errors.<br>‚Ä¢ Existing benchmarks are largely text-only, assume ideal instructions, or lack temporal/spatial grounding, limiting real-world applicability.<br>‚Ä¢ Popular reasoning strategies (e.g., Chain-of-Thought) do not model spatial relations, and static scene graphs fail to capture intermediate states and evolving context.<br>‚Ä¢ State-of-the-art VLMs perform near random on corrective sequential planning, indicating poor utilization of contextual cues and weak sequence tracking.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A training-free Scene Graph Incremental updates (SGI) framework that refines scene graphs step-by-step to synthesize intermediate states from initial to goal, enabling explicit tracking of scene evolution. SGI supports error detection and step completion, improving VLM robustness on CoSPlan tasks (‚âà5.2% average gain) and generalizing to PlanBench and VQA.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Scene-Graph Dynamics for Corrective Planning: Train neural modules to predict and verify scene-graph transitions, replacing hand-crafted SGI updates with learned dynamics.<br>‚Ä¢ Embodied CoSPlan in 3D and Robotics: Extend corrective sequential planning to 3D embodied environments and real robots with physics and continuous control.<br>‚Ä¢ Uncertainty-Aware Corrective Planning with Noisy Perception: Model observation and instruction noise, with probabilistic SGI and active error verification.<br>‚Ä¢ From Multiple-Choice to Open-Ended Corrective Planning: Evaluate free-form action generation and natural-language rationales beyond discrete options.<br>‚Ä¢ Long-Horizon Video Corrective Planning via Memory-Augmented SGI: Integrate temporal memory to handle extended action sequences and delayed error correction.<br>‚Ä¢ Human-in-the-Loop Corrective Guidance for VLM Planners: Incorporate minimal human feedback to resolve ambiguity and accelerate recovery from compounding errors.<br>‚Ä¢ Multi-Agent Corrective Planning with Shared Scene Graphs: Study coordination and conflict resolution when multiple agents update and act on a shared scene graph.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-13">

    <div class="paper">
        <h2 class="paper-title">ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07328" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07328" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose ContextAnyone, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: https://github.com/ziyang1106/ContextAnyone{https://github.com/ziyang1106/ContextAnyone}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Maintaining character consistency in text-to-video requires preserving broader contextual appearance (hairstyle, outfit, body shape) across diverse motions and scenes, which current models struggle with.<br>‚Ä¢ Existing personalization methods primarily inject facial features, capturing only partial identity cues and neglecting full-context appearance, leading to identity drift.<br>‚Ä¢ Pixel- or channel-level fusion approaches do not ensure full utilization of reference information; standard cross-attention provides limited, unstable control over how reference cues influence temporal tokens during denoising.<br>‚Ä¢ Jointly handling a reconstructed reference frame and generated video frames with standard continuous RoPE can cause temporal collapse, confusing reference and video tokens.<br>‚Ä¢ Scarcity of challenging reference-to-video datasets; naive reference extraction (copying a video frame) enables trivial pixel copying and poor generalization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ContextAnyone is a DiT-based context-aware diffusion framework that jointly reconstructs the reference image and generates video frames by concatenating reference and video latents and optimizing a dual-guidance loss (diffusion + reference reconstruction). It injects identity via dual encoders (CLIP for semantics, Video VAE for fine details), reinforces reference cues with an Emphasize-Attention module and unidirectional reference-to-video flow, and stabilizes temporal modeling with Gap-RoPE that separates positional embeddings of reference and video tokens.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Multi-Reference, Multi-Character Context-Aware T2V: Extend ContextAnyone to multiple references and interacting characters with disentangled identity slots and cross-character attention for coherent interactions.<br>‚Ä¢ Long-Horizon Story Consistency in Video Diffusion: Introduce hierarchical memory/timeline mechanisms to maintain outfit and prop continuity across minute-long sequences.<br>‚Ä¢ Pose- and Layout-Guided ContextAnyone: Integrate controllable signals (pose, depth, scene graphs) into Emphasize-Attention for precise motion and staging while preserving identity.<br>‚Ä¢ Reference-Video to Video: Learning from Dynamic Exemplars: Use a short reference video instead of a single image to learn motion/style priors and improve robustness under pose and lighting changes.<br>‚Ä¢ 3D-Aware Context Preservation for Viewpoint Changes: Incorporate 3D priors (NeRF/Gaussians) to maintain identity and attire consistency under large camera motions and novel views.<br>‚Ä¢ Self-Supervised R2V Dataset Generation at Scale: Build automated pipelines with edit-quality scoring and identity verification to produce diverse, challenging, and reliable training data without manual curation.<br>‚Ä¢ Efficient Distillation of Reference-Aware Attention: Distill Emphasize-Attention into lightweight adapters for real-time or mobile deployment without sacrificing identity fidelity.<br>‚Ä¢ Reference-Aware In-Place Video Editing: Adapt the framework to edit existing videos while preserving target identity and contextual appearance under real-world footage.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 13</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button><button class="page-btn" onclick="goToPage(11)">11</button><button class="page-btn" onclick="goToPage(12)">12</button><button class="page-btn" onclick="goToPage(13)">13</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-12-17 23:08:39 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 13;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>