<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - October 24, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            font-weight: 300;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #2c3e50;
            margin-bottom: 15px;
            line-height: 1.3;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.3em;
            display: flex;
            align-items: center;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 20px;
            border-radius: 6px;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 20px;
            border-radius: 6px;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 20px;
            border-radius: 6px;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 20px;
            border-radius: 6px;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">October 24, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.19600" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.19600" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In the quest for scientific progress, communicating research is as vital as the discovery itself. Yet, researchers are often sidetracked by the manual, repetitive chore of building project webpages to make their dense papers accessible. While automation has tackled static slides and posters, the dynamic, interactive nature of webpages has remained an unaddressed challenge. To bridge this gap, we reframe the problem, arguing that the solution lies not in a single command, but in a collaborative, hierarchical process. We introduce AutoPage, a novel multi-agent system that embodies this philosophy. AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline from narrative planning to multimodal content generation and interactive rendering. To combat AI hallucination, dedicated "Checker" agents verify each step against the source paper, while optional human checkpoints ensure the final product aligns perfectly with the author's vision, transforming the system from a mere tool into a powerful collaborative assistant. To rigorously validate our approach, we also construct PageBench, the first benchmark for this new task. Experiments show AutoPage not only generates high-quality, visually appealing pages but does so with remarkable efficiency in under 15 minutes for less than \0.1. Code and dataset will be released at https://mqleet.github.io/AutoPage_ProjectPage/{Webpage}$.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>论文聚焦“论文到项目网页”的自动化生成任务，旨在减少研究者为项目页手工搭建所耗费的大量时间，同时提升页面质量与一致性。项目网页作为研究传播的关键载体，需要滚动、交互与动态可视化等能力，超出静态海报/幻灯片的表达范式。现有端到端LLM或海报/幻灯片/视频生成方法多为单步、固定画布，常出现版式不合理、事实偏差与缺乏人类反馈控制的问题，且此前缺乏专门用于网页生成的评测基准。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>作者提出AutoPage：一个协作式多智能体、由粗到细的流水线，包含叙事规划、跨模态内容生成、交互式页面渲染三阶段，并在每阶段引入“Checker”校验与可选人类介入。叙事规划阶段用MinerU与Docling将PDF解析为Markdown/资产库（段落摘要、图表与标题映射），由Page Content Planner产出适配网页叙事的纲要。内容生成阶段坚持“先文本后视觉”，先写清晰段落再精选匹配图表，之后由Content Checker核对文本–视觉一致性，并支持作者用自然语言微调。渲染阶段基于带标签的模板库匹配样式，生成HTML/CSS/JS并用HTML Checker做版式与视觉质量检查，作者可继续指令式调整导航、配色与组件。作者同时构建PageBench基准（1500+项目页语料，测试集100、模板库87）与评测协议（可读性PPL、语义一致性、压缩感知信息准确性，以及VLM评审的视觉要素准确性/布局与凝练/美学分）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">📊</span>实验结果</h3>
            <div class="results">
                <p>在PageBench上，AutoPage对多种骨干模型均显著提升内容与视觉质量：如与GPT‑4o‑mini结合将美学分2.71→2.95、布局与凝练2.08→2.38。与Gemini‑2.5‑Flash结合时，语义一致性0.684→0.742、视觉要素准确性2.82→3.13，并在压缩感知信息准确性上显著提升（AutoPage‑GPT‑4o‑mini达到1.941，为最高）。对较弱模型的提升更为明显，例如Qwen的视觉要素准确性由2.52→3.01，缩小与强模型的差距；用户研究中AutoPage获最高偏好分7.16/10。系统高效低成本：单页<15分钟、成本<0.1美元；消融显示移除内容/HTML校验显著降分（如美学2.69→1.90），验证校验与多轮修正机制的必要性。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>强化事实对齐与可追溯性：结合检索/引用链与段落级证据高亮，降低幻觉、提升可审计性。扩展交互与可定制性：引入在线Demo、代码沙盒与动画组件，支持多模板自动风格迁移与领域自适应。学习型模板与布局：基于大规模网页对学习布局选择和组件编排策略，替代规则式匹配以提升视觉一致性与多样性。拓展场景与多语言：面向技术报告、毕业论文、白皮书与行业博客，支持多语生成与评测。优化效能与隐私：进一步降低时延与成本，探索本地/开源VLM推理与隐私友好型解析与渲染。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.19779" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.19779" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, a goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target model's knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, a novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes a reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of a draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15\%). The code is publicly available at https://github.com/yuezhouhu/adaspec.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>论文关注于加速推理的Speculative Decoding（SD）中“草稿模型-目标模型”对齐不足的问题：传统知识蒸馏（KD）在所有token上最小化KL与SD的真实目标（最大化接受率α）不一致，且小模型容量有限，常把学习能力浪费在难以拟合的token上，导致α偏低与收敛不稳。该问题重要在于SD的速度提升高度依赖α与块效率，若对齐不足，端到端加速收益受限。现有方法（如DistillSpec）多以全量token、全局发散度优化为主，未考虑“可学性差异”，难在大规模差距下高效迁移教师知识。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>AdaSPEC提出“选择性知识蒸馏”：先训练一个参考模型Mref（由草稿模型初始化，用目标模型指导，前向KL蒸馏），再用Mref对token进行筛选，仅在“对草稿模型更可学”的token子集上进行蒸馏。具体做法是计算逐token损失Lref和Ldraft，与ΔL = Ldraft − Lref，据此选取ΔL较大的前k% token集合S，并最小化S上的Ldraft（见算法2，第13页；流程见图1，第4页）。关键贡献包括：将“可学性”显式量化并作为过滤依据；将蒸馏目标聚焦于易学token以提升α；提供简单易复现实现（~100行代码，见Listing 2，第15页）与稳健的超参（如k≈0.4，图4显示较小k更优，第9页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">📊</span>实验结果</h3>
            <div class="results">
                <p>在Pythia-31M→1.4B与CodeGen-350M→Phi-2两组配置、GSM8K/Alpaca/MBPP/CNN-DM/XSUM五项任务上，AdaSPEC的α在所有设置均高于DistillSpec（表1，第6页），如MBPP最优轮次从49.88%升至65.12%（+15.24%），GSM8K三轮从57.58%升至62.63%。分布级分析显示：接受率直方图整体右移、top-2 logit margin更大且负值更少、逐token KL分布整体左移（图2，第7页），说明对齐与置信度同步提升。实际端到端速度在vLLM上提升10–20%（表5，第9页），与EAGLE集成后进一步带来+7.45% tokens/s与+1.0%训练准确率（表6，第9页），在更大模型（Qwen2.5 0.5B→32B）与混合数据上同样有效（表7、表8，第9页）。案例分析还表明其错误几乎为基线错误的子集（图3，第8页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>可进一步研究更智能的筛选策略：结合不确定性与上下文难度的自适应k、样本级/层级级动态过滤、从易到难的课程式蒸馏，或将“难token”以辅助损失温和纳入。直接以接受率/块效率/墙钟时间为目标引入可微近似或强化学习，缩小“指标-目标”偏差。与树式/多步验证（EAGLE、SpecInfer等）和在线SD更紧密结合，探索多草稿/集成、跨族/跨分词器对齐与在线自适应迁移。理论上可刻画容量-接受率的标度律与泛化边界，并扩展到多模态/代码/长文本场景与更鲁棒的发散度或正则化设计。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20579" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20579" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Most video reasoning models only generate textual reasoning traces without indicating when and where key evidence appears. Recent models such as OpenAI-o3 have sparked wide interest in evidence-centered reasoning for images, yet extending this ability to videos is more challenging, as it requires joint temporal tracking and spatial localization across dynamic scenes. We introduce Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal evidence into video reasoning, and carefully collect training data and design training strategies to address the aforementioned challenges. The model highlights key timestamps, objects, and bounding boxes alongside its answers, allowing reasoning to be grounded in concrete visual observations. To enable this functionality, we first curate and build two high-quality datasets, STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed temporal and spatial annotations, since most existing datasets offer either temporal spans for videos or spatial boxes on images, lacking unified spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start reinforcement learning strategy with multiple specially designed rewards that jointly encourage answer accuracy, temporal alignment, and spatial precision. On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance, raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent improvements are also observed on a broad range of video understanding benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond accuracy, the reasoning traces produced by Open-o3 Video also provide valuable signals for test-time scaling, enabling confidence-aware verification and improving answer reliability.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>论文聚焦于视频推理中缺乏“可核验”的证据链：现有方法多仅给出文本化思路，难以指出关键证据何时出现、位于何处（见第1-2页与图1第2页）。在动态场景中同时做到时间与空间的精确定位更难，根源在于缺乏统一的时空标注数据，以及训练中时空对齐的优化困难（第2页）。尤其在强化学习阶段，空间奖励往往依赖正确时间点，时间预测不准会导致奖励稀疏与训练停滞（第3页）。因此，需要一种能把答案与“时间戳+边界框”显式绑定的时空证据中心化推理框架。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>作者提出Open-o3 Video：一个非agent、单模型框架，直接在推理链中输出带时间戳与框的证据片段，实现“with frames”的可视化思考（第3页）。为此构建两套数据集：用于SFT的STGR-CoT-30k与用于RL的STGR-RL-36k，并额外新注5.9k高质量时空样本；数据由Gemini 2.5 Pro初标、Qwen2.5-VL核验框与一致性检查组成（图2第4页，附录A.3-A.6）。训练采用“两阶段+GSPO”策略：先冷启动SFT学会结构化输出，再用GSPO强化学习稳定优化序列级奖励（图3第5页，4.2节）。奖励设计包含答案正确度、思考奖励与格式奖励；思考奖励内引入自适应时间接近度（前期松后期严）与“时间门控”的空间IoU计算，解决时间-空间耦合下的稀疏与误奖问题（第6-7页，公式(3)(4)）。此外加入绝对时间戳提示、关键帧插入与证据感知的测试时尺度化投票以提升鲁棒性（第7页与附录A.1、A.5第16页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">📊</span>实验结果</h3>
            <div class="results">
                <p>在V-STAR上取得SOTA：What准确率61.0，mAM 33.7、mLGM 46.6，相比Qwen2.5-VL-7B分别提升+27.5、+14.4、+24.2，并超过GPT-4o与Gemini-2-Flash（表1第8页）。在VideoMME、WorldSense、VideoMMMU与TVGBench上也有一致增益，如长视频+4.1%、感知相关+3.1%/ +3.3%，TVGBench mIoU +4.5（表2第9页）。消融显示RL>纯SFT，SFT+RL最佳；GSPO优于GRPO（+0.9 mAM/+1.3 mLGM），证实序列级优化对长链稳定性与定位有效（表3第9页）。去除自适应时间接近或时间门控均显著退化（表4第10页）；高质量时空数据对性能贡献明显（表5第10页）；证据感知投票优于多数投票（WorldSense +1.0、VideoMMMU +1.0，表7第16页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>面向更长、更复杂视频与小目标场景，需扩充更细粒度与多样化的时空监督，并改进长上下文与记忆机制（附录A.7第17页）。多模态融合方面，可将语音/音频纳入统一的“文本-时间-空间-音频”证据链对齐与奖励设计（结论第9页）。训练上可探索自监督/半监督的时空对齐目标、跨数据源一致性正则、以及更强的时序一致性与多目标跟踪约束。推理上可研究端到端的证据规划与裁剪策略、分层或代理式工具使用，以及更稳健的证据置信评估与动态集成，以进一步提升可核验性与鲁棒性。</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 1</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>← 上一页</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button></span>
            <button id="next-btn" onclick="changePage(1)">下一页 →</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-10-24 13:11:45 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 1;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
</body>
</html>