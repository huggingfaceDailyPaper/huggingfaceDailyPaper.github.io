<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - October 24, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            font-weight: 300;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #2c3e50;
            margin-bottom: 15px;
            line-height: 1.3;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.3em;
            display: flex;
            align-items: center;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 20px;
            border-radius: 6px;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 20px;
            border-radius: 6px;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 20px;
            border-radius: 6px;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 20px;
            border-radius: 6px;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">October 24, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.19600" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.19600" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In the quest for scientific progress, communicating research is as vital as the discovery itself. Yet, researchers are often sidetracked by the manual, repetitive chore of building project webpages to make their dense papers accessible. While automation has tackled static slides and posters, the dynamic, interactive nature of webpages has remained an unaddressed challenge. To bridge this gap, we reframe the problem, arguing that the solution lies not in a single command, but in a collaborative, hierarchical process. We introduce AutoPage, a novel multi-agent system that embodies this philosophy. AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline from narrative planning to multimodal content generation and interactive rendering. To combat AI hallucination, dedicated "Checker" agents verify each step against the source paper, while optional human checkpoints ensure the final product aligns perfectly with the author's vision, transforming the system from a mere tool into a powerful collaborative assistant. To rigorously validate our approach, we also construct PageBench, the first benchmark for this new task. Experiments show AutoPage not only generates high-quality, visually appealing pages but does so with remarkable efficiency in under 15 minutes for less than \0.1. Code and dataset will be released at https://mqleet.github.io/AutoPage_ProjectPage/{Webpage}$.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>论文聚焦将学术论文自动生成交互式项目网页（paper-to-page），以减少研究者在模板改造上的重复劳动并提升传播效率。现有自动化大多面向静态海报/幻灯片/视频，难以处理网页的可滚动结构、交互元素与风格偏好（见第1页Fig.1）。端到端LLM方案常出现不合理布局、缺乏人类反馈且易幻觉，难以保证事实一致性与视觉品质。为此，作者将任务重构为“协作式、分层、粗到细”的流程，并引入多轮验证与可选人类校对以确保可控性与可靠性。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>作者提出AutoPage多智能体系统，采用三阶段粗到细管线（第4页Fig.2）：(1) 叙事规划与结构化：用MinerU与Docling将PDF解析为Markdown与资产库，Page Content Planner生成网页大纲，并由检查器校验。 (2) 多模态内容生成：坚持“先文后图”，先生成段落，再从资产库挑选并插入最相关图表；随后由Content Checker核验文本–视觉一致性，并提供可选人类微调。 (3) 交互式渲染：基于带标签的模板库进行匹配，整合内容并生成HTML/CSS/JS，最后由HTML Checker做版式与样式体检，并可接受人类指令微调样式。同时构建PageBench基准（第5–6页）：汇集1500+项目页，含100篇测试集与87个风格模板，并提出内容（PPL、语义保真、压缩感知信息准确度）与视觉（VLM裁判的视觉准确性/版式一致性/美学分）两类指标。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">📊</span>实验结果</h3>
            <div class="results">
                <p>在PageBench上，AutoPage显著提升不同骨干模型的页面质量（表1，第6页）：相对GPT‑4o‑mini，审美分由2.71升至2.95、版式与凝练由2.08升至2.38；相对Gemini‑2.5‑Flash，语义保真由0.684升至0.742、视觉元素准确性由2.82升至3.13，压缩感知信息准确度由1.276升至1.591（表3，第12页）。对较弱骨干（如Qwen）提升更大：视觉准确性由2.52跃升至3.01，显著缩小与强模型的差距（表1）。用户研究（20人，强制打分）显示AutoPage平均得分7.16，优于Grok4‑fast(6.93)与Gemini(6.79)（第7页Fig.3）。验证器消融表明其关键性：移除全部检查器后，视觉准确性由3.13降至2.75、美学分由2.69降至1.90、版式由2.15降至1.60（表2，第11页）；系统生成一页约4–20分钟、成本$0.06–$0.20，常见配置< $0.1（第8页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>可将模板匹配从静态标签扩展为学习式风格检索与自适应主题生成，增强个性化与跨领域泛化。针对内容一致性，可训练更强的多模态“检查员”（VLM-as-Judge/奖励模型），结合检索与可验证推理进一步抑制幻觉。拓展交互能力，如嵌入可执行演示、在线可视化与代码复现实验流水线，并支持多语言与无障碍设计。评测层面，可丰富客观视觉指标与人机联合评测协议，构建跨领域、更大规模的对齐标注；同时探索基于RL/自反思的端到端优化，使压缩–准确度在任务级目标上最优。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.19779" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.19779" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, a goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target model's knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, a novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes a reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of a draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15\%). The code is publicly available at https://github.com/yuezhouhu/adaspec.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>论文关注加速大模型推理的Speculative Decoding（SD）中，草稿模型与目标模型对齐不足导致的低接受率问题。现有做法多用知识蒸馏（如最小化全token的前向KL或TVD）来对齐分布，但这与SD真正目标（最大化接受率）不一致，且会把有限容量浪费在难学、反正也难被接受的token上，导致收敛困难与次优性能。作者观察到token可学习性差异很大，小模型难以同时拟合所有token，需面向SD目标的选择性训练策略。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>提出AdaSPEC：一种选择性知识蒸馏框架，分两步进行。首先训练参考模型（由草稿模型初始化）用目标模型做教师，以前向KL在下游数据上蒸馏，得到对教师更贴近的“可学习性探针”。随后对每个token计算相对目标模型的token级损失：Ldraft与Lref，并以ΔL=Ldraft−Lref选取Top-k（通常k≈0.4）作为“最有效学习”的token子集，过滤掉难学或已学会的token，仅在选中子集上对草稿模型进行KL蒸馏。关键贡献包括：基于参考模型的token可学习性判别、与SD接受率目标对齐的选择性蒸馏损失、以及简洁易复用的训练流程（算法与最小实现见文中算法与代码片段，页13-15）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">📊</span>实验结果</h3>
            <div class="results">
                <p>在Pythia-31M→1.4B与CodeGen-350M→Phi-2两种配置、GSM8K/Alpaca/MBPP/CNN-DM/XSUM五项任务上，AdaSPEC的接受率在3-epoch与“最优epoch”两种设置下均优于DistillSpec（表1，页6），最高提升约15%（如MBPP从49.88%至65.12%）。分布层面，AdaSPEC显著提高正logit margin比例、降低负margin与token级KL（图2，页7），表明对齐更紧密。端到端加速上，基于vLLM的单卡A100推理在多任务上获得约10–20%速度提升（表5，页9），与先进SD算法EAGLE集成也带来精度与速度双提升（表6，页9）；扩展到更大组合（Qwen0.5B→32B）同样提升接受率（84.43%→86.21%，表7，页9），混合任务训练中亦更少遗忘（表8，页9）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>可进一步探索自适应/动态过滤（随训练进度或上下文难度调整k与阈值），以及在线SD场景的即时可学习性估计。目标函数层面，可直接优化接受率/块效率或将其作为强化学习/最优化约束，或进行span/句子级的选择性蒸馏以捕捉长程依赖。体系结构上，可与树式/多步验证（如EAGLE-2）或多头/多token草稿生成组合，并与量化、剪枝等高效化手段协同。跨任务与多数据配比的课程学习、跨模型族的token映射与校准、以及更鲁棒的温度/校准策略也值得研究。</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20579" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20579" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Most video reasoning models only generate textual reasoning traces without indicating when and where key evidence appears. Recent models such as OpenAI-o3 have sparked wide interest in evidence-centered reasoning for images, yet extending this ability to videos is more challenging, as it requires joint temporal tracking and spatial localization across dynamic scenes. We introduce Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal evidence into video reasoning, and carefully collect training data and design training strategies to address the aforementioned challenges. The model highlights key timestamps, objects, and bounding boxes alongside its answers, allowing reasoning to be grounded in concrete visual observations. To enable this functionality, we first curate and build two high-quality datasets, STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed temporal and spatial annotations, since most existing datasets offer either temporal spans for videos or spatial boxes on images, lacking unified spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start reinforcement learning strategy with multiple specially designed rewards that jointly encourage answer accuracy, temporal alignment, and spatial precision. On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance, raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent improvements are also observed on a broad range of video understanding benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond accuracy, the reasoning traces produced by Open-o3 Video also provide valuable signals for test-time scaling, enabling confidence-aware verification and improving answer reliability.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">🎯</span>研究动机</h3>
            <div class="motivation">
                <p>现有视频推理模型多只输出文本式“理由”，缺少明确的时间点与空间位置证据，难以验证答案是否与画面一致；而视频相比图像还需在动态场景中同时进行时序跟踪与空间定位。论文指出两大瓶颈：一是缺乏同时含有时间与空间标注、且带推理链的高质量数据；二是训练上空间奖励依赖准确时间戳，早期时间预测不准会导致空间奖励稀疏乃至“空间崩塌”。该问题的重要性在于提高可解释性与可验证性，尤其在长视频与遮挡、运动频繁的复杂场景中（见第1–2页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">🔧</span>研究方法</h3>
            <div class="method">
                <p>作者提出Open-o3 Video：一个非Agent的一体化框架，在<think>/<answer>中显式生成带时间戳与边框的证据（“think with frames”），实现答案与何时何地的可验证对齐。为此构建两套数据：用于SFT的STGR-CoT-30k与用于RL的STGR-RL-36k，并新增5.9k高质量时空样本；配套的标注流水线包含Gemini 2.5 Pro初标、边框过滤与一致性检查（见图2，第4页）。训练采用“两阶段”：先冷启动SFT学会结构化、落地式输出，再用GSPO进行序列级RL，联合三类奖励（答案/思考/格式）；其中思考奖励含“自适应时间接近度”（σ退火以缓解稀疏并逐步收紧对齐）与“时间门控”（仅在时间接近时计算空间IoU），并以16帧均匀采样与绝对时间戳增强时序感（见图3，第5–7页；附录A.1）。关键技术贡献包括：统一的时空落地推理格式、成体系的STGR数据、GSPO稳定长链优化、以及自适应时间接近与时间门控两项奖励设计。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">📊</span>实验结果</h3>
            <div class="results">
                <p>在V-STAR基准上，Open-o3 Video取得SOTA：What准确率61.0、mAM 33.7、mLGM 46.6，相比Qwen2.5-VL-7B提升+14.4%（mAM）与+24.2%（mLGM），并超过GPT-4o（见表1，第8页）。在VideoMME、WorldSense、VideoMMMU、TVGBench上亦有一致增益，如长视频+4.1%、WorldSense识别+3.1%、VideoMMMU感知+3.3%、TVGBench mIoU +4.5（见表2，第9页）。消融显示：RL> SFT，SFT+RL最佳；GSPO优于GRPO；去除自适应时间接近或时间门控均显著降分；高质量时空标注带来最大提升（见表3–5，第9–10页）。另外，利用显式证据的“置信感知投票”在推理时比多数投票更稳健（+1.0，见表7与图6，第16与19页）。</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">💡</span>研究思路</h3>
            <div class="idea">
                <p>数据层面可进一步扩展更长时长、更小目标与多目标跟踪的时空标注，探索半监督/合成数据增强与更强一致性校验。算法层面可引入更丰富的奖励（如跨帧身份与轨迹一致性、因果约束）、更优的帧/片段选择策略与记忆/层级式时序建模，或结合可微裁剪/外部工具以精细化证据获取。模态层面将文本–时间–空间与音频/语音对齐，支持多步推理与更复杂事件理解（论文也将其作为后续方向，见附录A.7）。推理层面可把显式证据用于不确定性校准、自动验证与交互式“证据-答案”循环，同时优化效率以便长视频低延迟部署。</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 1</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>← 上一页</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button></span>
            <button id="next-btn" onclick="changePage(1)">下一页 →</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-10-24 13:16:40 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 1;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
</body>
</html>