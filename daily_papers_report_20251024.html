<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - October 24, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            font-weight: 300;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #2c3e50;
            margin-bottom: 15px;
            line-height: 1.3;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.3em;
            display: flex;
            align-items: center;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 20px;
            border-radius: 6px;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 20px;
            border-radius: 6px;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 20px;
            border-radius: 6px;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 20px;
            border-radius: 6px;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">October 24, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.19600" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.19600" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>In the quest for scientific progress, communicating research is as vital as the discovery itself. Yet, researchers are often sidetracked by the manual, repetitive chore of building project webpages to make their dense papers accessible. While automation has tackled static slides and posters, the dynamic, interactive nature of webpages has remained an unaddressed challenge. To bridge this gap, we reframe the problem, arguing that the solution lies not in a single command, but in a collaborative, hierarchical process. We introduce AutoPage, a novel multi-agent system that embodies this philosophy. AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline from narrative planning to multimodal content generation and interactive rendering. To combat AI hallucination, dedicated "Checker" agents verify each step against the source paper, while optional human checkpoints ensure the final product aligns perfectly with the author's vision, transforming the system from a mere tool into a powerful collaborative assistant. To rigorously validate our approach, we also construct PageBench, the first benchmark for this new task. Experiments show AutoPage not only generates high-quality, visually appealing pages but does so with remarkable efficiency in under 15 minutes for less than \0.1. Code and dataset will be released at https://mqleet.github.io/AutoPage_ProjectPage/{Webpage}$.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>è®ºæ–‡èšç„¦â€œè®ºæ–‡åˆ°é¡¹ç›®ç½‘é¡µâ€çš„è‡ªåŠ¨åŒ–ç”Ÿæˆä»»åŠ¡ï¼Œæ—¨åœ¨å‡å°‘ç ”ç©¶è€…ä¸ºé¡¹ç›®é¡µæ‰‹å·¥æ­å»ºæ‰€è€—è´¹çš„å¤§é‡æ—¶é—´ï¼ŒåŒæ—¶æå‡é¡µé¢è´¨é‡ä¸ä¸€è‡´æ€§ã€‚é¡¹ç›®ç½‘é¡µä½œä¸ºç ”ç©¶ä¼ æ’­çš„å…³é”®è½½ä½“ï¼Œéœ€è¦æ»šåŠ¨ã€äº¤äº’ä¸åŠ¨æ€å¯è§†åŒ–ç­‰èƒ½åŠ›ï¼Œè¶…å‡ºé™æ€æµ·æŠ¥/å¹»ç¯ç‰‡çš„è¡¨è¾¾èŒƒå¼ã€‚ç°æœ‰ç«¯åˆ°ç«¯LLMæˆ–æµ·æŠ¥/å¹»ç¯ç‰‡/è§†é¢‘ç”Ÿæˆæ–¹æ³•å¤šä¸ºå•æ­¥ã€å›ºå®šç”»å¸ƒï¼Œå¸¸å‡ºç°ç‰ˆå¼ä¸åˆç†ã€äº‹å®åå·®ä¸ç¼ºä¹äººç±»åé¦ˆæ§åˆ¶çš„é—®é¢˜ï¼Œä¸”æ­¤å‰ç¼ºä¹ä¸“é—¨ç”¨äºç½‘é¡µç”Ÿæˆçš„è¯„æµ‹åŸºå‡†ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>ä½œè€…æå‡ºAutoPageï¼šä¸€ä¸ªåä½œå¼å¤šæ™ºèƒ½ä½“ã€ç”±ç²—åˆ°ç»†çš„æµæ°´çº¿ï¼ŒåŒ…å«å™äº‹è§„åˆ’ã€è·¨æ¨¡æ€å†…å®¹ç”Ÿæˆã€äº¤äº’å¼é¡µé¢æ¸²æŸ“ä¸‰é˜¶æ®µï¼Œå¹¶åœ¨æ¯é˜¶æ®µå¼•å…¥â€œCheckerâ€æ ¡éªŒä¸å¯é€‰äººç±»ä»‹å…¥ã€‚å™äº‹è§„åˆ’é˜¶æ®µç”¨MinerUä¸Doclingå°†PDFè§£æä¸ºMarkdown/èµ„äº§åº“ï¼ˆæ®µè½æ‘˜è¦ã€å›¾è¡¨ä¸æ ‡é¢˜æ˜ å°„ï¼‰ï¼Œç”±Page Content Planneräº§å‡ºé€‚é…ç½‘é¡µå™äº‹çš„çº²è¦ã€‚å†…å®¹ç”Ÿæˆé˜¶æ®µåšæŒâ€œå…ˆæ–‡æœ¬åè§†è§‰â€ï¼Œå…ˆå†™æ¸…æ™°æ®µè½å†ç²¾é€‰åŒ¹é…å›¾è¡¨ï¼Œä¹‹åç”±Content Checkeræ ¸å¯¹æ–‡æœ¬â€“è§†è§‰ä¸€è‡´æ€§ï¼Œå¹¶æ”¯æŒä½œè€…ç”¨è‡ªç„¶è¯­è¨€å¾®è°ƒã€‚æ¸²æŸ“é˜¶æ®µåŸºäºå¸¦æ ‡ç­¾çš„æ¨¡æ¿åº“åŒ¹é…æ ·å¼ï¼Œç”ŸæˆHTML/CSS/JSå¹¶ç”¨HTML Checkeråšç‰ˆå¼ä¸è§†è§‰è´¨é‡æ£€æŸ¥ï¼Œä½œè€…å¯ç»§ç»­æŒ‡ä»¤å¼è°ƒæ•´å¯¼èˆªã€é…è‰²ä¸ç»„ä»¶ã€‚ä½œè€…åŒæ—¶æ„å»ºPageBenchåŸºå‡†ï¼ˆ1500+é¡¹ç›®é¡µè¯­æ–™ï¼Œæµ‹è¯•é›†100ã€æ¨¡æ¿åº“87ï¼‰ä¸è¯„æµ‹åè®®ï¼ˆå¯è¯»æ€§PPLã€è¯­ä¹‰ä¸€è‡´æ€§ã€å‹ç¼©æ„ŸçŸ¥ä¿¡æ¯å‡†ç¡®æ€§ï¼Œä»¥åŠVLMè¯„å®¡çš„è§†è§‰è¦ç´ å‡†ç¡®æ€§/å¸ƒå±€ä¸å‡ç»ƒ/ç¾å­¦åˆ†ï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ“Š</span>å®éªŒç»“æœ</h3>
            <div class="results">
                <p>åœ¨PageBenchä¸Šï¼ŒAutoPageå¯¹å¤šç§éª¨å¹²æ¨¡å‹å‡æ˜¾è‘—æå‡å†…å®¹ä¸è§†è§‰è´¨é‡ï¼šå¦‚ä¸GPTâ€‘4oâ€‘miniç»“åˆå°†ç¾å­¦åˆ†2.71â†’2.95ã€å¸ƒå±€ä¸å‡ç»ƒ2.08â†’2.38ã€‚ä¸Geminiâ€‘2.5â€‘Flashç»“åˆæ—¶ï¼Œè¯­ä¹‰ä¸€è‡´æ€§0.684â†’0.742ã€è§†è§‰è¦ç´ å‡†ç¡®æ€§2.82â†’3.13ï¼Œå¹¶åœ¨å‹ç¼©æ„ŸçŸ¥ä¿¡æ¯å‡†ç¡®æ€§ä¸Šæ˜¾è‘—æå‡ï¼ˆAutoPageâ€‘GPTâ€‘4oâ€‘miniè¾¾åˆ°1.941ï¼Œä¸ºæœ€é«˜ï¼‰ã€‚å¯¹è¾ƒå¼±æ¨¡å‹çš„æå‡æ›´ä¸ºæ˜æ˜¾ï¼Œä¾‹å¦‚Qwençš„è§†è§‰è¦ç´ å‡†ç¡®æ€§ç”±2.52â†’3.01ï¼Œç¼©å°ä¸å¼ºæ¨¡å‹çš„å·®è·ï¼›ç”¨æˆ·ç ”ç©¶ä¸­AutoPageè·æœ€é«˜åå¥½åˆ†7.16/10ã€‚ç³»ç»Ÿé«˜æ•ˆä½æˆæœ¬ï¼šå•é¡µ<15åˆ†é’Ÿã€æˆæœ¬<0.1ç¾å…ƒï¼›æ¶ˆèæ˜¾ç¤ºç§»é™¤å†…å®¹/HTMLæ ¡éªŒæ˜¾è‘—é™åˆ†ï¼ˆå¦‚ç¾å­¦2.69â†’1.90ï¼‰ï¼ŒéªŒè¯æ ¡éªŒä¸å¤šè½®ä¿®æ­£æœºåˆ¶çš„å¿…è¦æ€§ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>å¼ºåŒ–äº‹å®å¯¹é½ä¸å¯è¿½æº¯æ€§ï¼šç»“åˆæ£€ç´¢/å¼•ç”¨é“¾ä¸æ®µè½çº§è¯æ®é«˜äº®ï¼Œé™ä½å¹»è§‰ã€æå‡å¯å®¡è®¡æ€§ã€‚æ‰©å±•äº¤äº’ä¸å¯å®šåˆ¶æ€§ï¼šå¼•å…¥åœ¨çº¿Demoã€ä»£ç æ²™ç›’ä¸åŠ¨ç”»ç»„ä»¶ï¼Œæ”¯æŒå¤šæ¨¡æ¿è‡ªåŠ¨é£æ ¼è¿ç§»ä¸é¢†åŸŸè‡ªé€‚åº”ã€‚å­¦ä¹ å‹æ¨¡æ¿ä¸å¸ƒå±€ï¼šåŸºäºå¤§è§„æ¨¡ç½‘é¡µå¯¹å­¦ä¹ å¸ƒå±€é€‰æ‹©å’Œç»„ä»¶ç¼–æ’ç­–ç•¥ï¼Œæ›¿ä»£è§„åˆ™å¼åŒ¹é…ä»¥æå‡è§†è§‰ä¸€è‡´æ€§ä¸å¤šæ ·æ€§ã€‚æ‹“å±•åœºæ™¯ä¸å¤šè¯­è¨€ï¼šé¢å‘æŠ€æœ¯æŠ¥å‘Šã€æ¯•ä¸šè®ºæ–‡ã€ç™½çš®ä¹¦ä¸è¡Œä¸šåšå®¢ï¼Œæ”¯æŒå¤šè¯­ç”Ÿæˆä¸è¯„æµ‹ã€‚ä¼˜åŒ–æ•ˆèƒ½ä¸éšç§ï¼šè¿›ä¸€æ­¥é™ä½æ—¶å»¶ä¸æˆæœ¬ï¼Œæ¢ç´¢æœ¬åœ°/å¼€æºVLMæ¨ç†ä¸éšç§å‹å¥½å‹è§£æä¸æ¸²æŸ“ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.19779" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.19779" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, a goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target model's knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, a novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes a reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of a draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15\%). The code is publicly available at https://github.com/yuezhouhu/adaspec.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>è®ºæ–‡å…³æ³¨äºåŠ é€Ÿæ¨ç†çš„Speculative Decodingï¼ˆSDï¼‰ä¸­â€œè‰ç¨¿æ¨¡å‹-ç›®æ ‡æ¨¡å‹â€å¯¹é½ä¸è¶³çš„é—®é¢˜ï¼šä¼ ç»ŸçŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰åœ¨æ‰€æœ‰tokenä¸Šæœ€å°åŒ–KLä¸SDçš„çœŸå®ç›®æ ‡ï¼ˆæœ€å¤§åŒ–æ¥å—ç‡Î±ï¼‰ä¸ä¸€è‡´ï¼Œä¸”å°æ¨¡å‹å®¹é‡æœ‰é™ï¼Œå¸¸æŠŠå­¦ä¹ èƒ½åŠ›æµªè´¹åœ¨éš¾ä»¥æ‹Ÿåˆçš„tokenä¸Šï¼Œå¯¼è‡´Î±åä½ä¸æ”¶æ•›ä¸ç¨³ã€‚è¯¥é—®é¢˜é‡è¦åœ¨äºSDçš„é€Ÿåº¦æå‡é«˜åº¦ä¾èµ–Î±ä¸å—æ•ˆç‡ï¼Œè‹¥å¯¹é½ä¸è¶³ï¼Œç«¯åˆ°ç«¯åŠ é€Ÿæ”¶ç›Šå—é™ã€‚ç°æœ‰æ–¹æ³•ï¼ˆå¦‚DistillSpecï¼‰å¤šä»¥å…¨é‡tokenã€å…¨å±€å‘æ•£åº¦ä¼˜åŒ–ä¸ºä¸»ï¼Œæœªè€ƒè™‘â€œå¯å­¦æ€§å·®å¼‚â€ï¼Œéš¾åœ¨å¤§è§„æ¨¡å·®è·ä¸‹é«˜æ•ˆè¿ç§»æ•™å¸ˆçŸ¥è¯†ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>AdaSPECæå‡ºâ€œé€‰æ‹©æ€§çŸ¥è¯†è’¸é¦â€ï¼šå…ˆè®­ç»ƒä¸€ä¸ªå‚è€ƒæ¨¡å‹Mrefï¼ˆç”±è‰ç¨¿æ¨¡å‹åˆå§‹åŒ–ï¼Œç”¨ç›®æ ‡æ¨¡å‹æŒ‡å¯¼ï¼Œå‰å‘KLè’¸é¦ï¼‰ï¼Œå†ç”¨Mrefå¯¹tokenè¿›è¡Œç­›é€‰ï¼Œä»…åœ¨â€œå¯¹è‰ç¨¿æ¨¡å‹æ›´å¯å­¦â€çš„tokenå­é›†ä¸Šè¿›è¡Œè’¸é¦ã€‚å…·ä½“åšæ³•æ˜¯è®¡ç®—é€tokenæŸå¤±Lrefå’ŒLdraftï¼Œä¸Î”L = Ldraft âˆ’ Lrefï¼Œæ®æ­¤é€‰å–Î”Lè¾ƒå¤§çš„å‰k% tokené›†åˆSï¼Œå¹¶æœ€å°åŒ–Sä¸Šçš„Ldraftï¼ˆè§ç®—æ³•2ï¼Œç¬¬13é¡µï¼›æµç¨‹è§å›¾1ï¼Œç¬¬4é¡µï¼‰ã€‚å…³é”®è´¡çŒ®åŒ…æ‹¬ï¼šå°†â€œå¯å­¦æ€§â€æ˜¾å¼é‡åŒ–å¹¶ä½œä¸ºè¿‡æ»¤ä¾æ®ï¼›å°†è’¸é¦ç›®æ ‡èšç„¦äºæ˜“å­¦tokenä»¥æå‡Î±ï¼›æä¾›ç®€å•æ˜“å¤ç°å®ç°ï¼ˆ~100è¡Œä»£ç ï¼Œè§Listing 2ï¼Œç¬¬15é¡µï¼‰ä¸ç¨³å¥çš„è¶…å‚ï¼ˆå¦‚kâ‰ˆ0.4ï¼Œå›¾4æ˜¾ç¤ºè¾ƒå°kæ›´ä¼˜ï¼Œç¬¬9é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ“Š</span>å®éªŒç»“æœ</h3>
            <div class="results">
                <p>åœ¨Pythia-31Mâ†’1.4Bä¸CodeGen-350Mâ†’Phi-2ä¸¤ç»„é…ç½®ã€GSM8K/Alpaca/MBPP/CNN-DM/XSUMäº”é¡¹ä»»åŠ¡ä¸Šï¼ŒAdaSPECçš„Î±åœ¨æ‰€æœ‰è®¾ç½®å‡é«˜äºDistillSpecï¼ˆè¡¨1ï¼Œç¬¬6é¡µï¼‰ï¼Œå¦‚MBPPæœ€ä¼˜è½®æ¬¡ä»49.88%å‡è‡³65.12%ï¼ˆ+15.24%ï¼‰ï¼ŒGSM8Kä¸‰è½®ä»57.58%å‡è‡³62.63%ã€‚åˆ†å¸ƒçº§åˆ†ææ˜¾ç¤ºï¼šæ¥å—ç‡ç›´æ–¹å›¾æ•´ä½“å³ç§»ã€top-2 logit marginæ›´å¤§ä¸”è´Ÿå€¼æ›´å°‘ã€é€token KLåˆ†å¸ƒæ•´ä½“å·¦ç§»ï¼ˆå›¾2ï¼Œç¬¬7é¡µï¼‰ï¼Œè¯´æ˜å¯¹é½ä¸ç½®ä¿¡åº¦åŒæ­¥æå‡ã€‚å®é™…ç«¯åˆ°ç«¯é€Ÿåº¦åœ¨vLLMä¸Šæå‡10â€“20%ï¼ˆè¡¨5ï¼Œç¬¬9é¡µï¼‰ï¼Œä¸EAGLEé›†æˆåè¿›ä¸€æ­¥å¸¦æ¥+7.45% tokens/sä¸+1.0%è®­ç»ƒå‡†ç¡®ç‡ï¼ˆè¡¨6ï¼Œç¬¬9é¡µï¼‰ï¼Œåœ¨æ›´å¤§æ¨¡å‹ï¼ˆQwen2.5 0.5Bâ†’32Bï¼‰ä¸æ··åˆæ•°æ®ä¸ŠåŒæ ·æœ‰æ•ˆï¼ˆè¡¨7ã€è¡¨8ï¼Œç¬¬9é¡µï¼‰ã€‚æ¡ˆä¾‹åˆ†æè¿˜è¡¨æ˜å…¶é”™è¯¯å‡ ä¹ä¸ºåŸºçº¿é”™è¯¯çš„å­é›†ï¼ˆå›¾3ï¼Œç¬¬8é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>å¯è¿›ä¸€æ­¥ç ”ç©¶æ›´æ™ºèƒ½çš„ç­›é€‰ç­–ç•¥ï¼šç»“åˆä¸ç¡®å®šæ€§ä¸ä¸Šä¸‹æ–‡éš¾åº¦çš„è‡ªé€‚åº”kã€æ ·æœ¬çº§/å±‚çº§çº§åŠ¨æ€è¿‡æ»¤ã€ä»æ˜“åˆ°éš¾çš„è¯¾ç¨‹å¼è’¸é¦ï¼Œæˆ–å°†â€œéš¾tokenâ€ä»¥è¾…åŠ©æŸå¤±æ¸©å’Œçº³å…¥ã€‚ç›´æ¥ä»¥æ¥å—ç‡/å—æ•ˆç‡/å¢™é’Ÿæ—¶é—´ä¸ºç›®æ ‡å¼•å…¥å¯å¾®è¿‘ä¼¼æˆ–å¼ºåŒ–å­¦ä¹ ï¼Œç¼©å°â€œæŒ‡æ ‡-ç›®æ ‡â€åå·®ã€‚ä¸æ ‘å¼/å¤šæ­¥éªŒè¯ï¼ˆEAGLEã€SpecInferç­‰ï¼‰å’Œåœ¨çº¿SDæ›´ç´§å¯†ç»“åˆï¼Œæ¢ç´¢å¤šè‰ç¨¿/é›†æˆã€è·¨æ—/è·¨åˆ†è¯å™¨å¯¹é½ä¸åœ¨çº¿è‡ªé€‚åº”è¿ç§»ã€‚ç†è®ºä¸Šå¯åˆ»ç”»å®¹é‡-æ¥å—ç‡çš„æ ‡åº¦å¾‹ä¸æ³›åŒ–è¾¹ç•Œï¼Œå¹¶æ‰©å±•åˆ°å¤šæ¨¡æ€/ä»£ç /é•¿æ–‡æœ¬åœºæ™¯ä¸æ›´é²æ£’çš„å‘æ•£åº¦æˆ–æ­£åˆ™åŒ–è®¾è®¡ã€‚</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2510.20579" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2510.20579" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Most video reasoning models only generate textual reasoning traces without indicating when and where key evidence appears. Recent models such as OpenAI-o3 have sparked wide interest in evidence-centered reasoning for images, yet extending this ability to videos is more challenging, as it requires joint temporal tracking and spatial localization across dynamic scenes. We introduce Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal evidence into video reasoning, and carefully collect training data and design training strategies to address the aforementioned challenges. The model highlights key timestamps, objects, and bounding boxes alongside its answers, allowing reasoning to be grounded in concrete visual observations. To enable this functionality, we first curate and build two high-quality datasets, STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed temporal and spatial annotations, since most existing datasets offer either temporal spans for videos or spatial boxes on images, lacking unified spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start reinforcement learning strategy with multiple specially designed rewards that jointly encourage answer accuracy, temporal alignment, and spatial precision. On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance, raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent improvements are also observed on a broad range of video understanding benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond accuracy, the reasoning traces produced by Open-o3 Video also provide valuable signals for test-time scaling, enabling confidence-aware verification and improving answer reliability.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ¯</span>ç ”ç©¶åŠ¨æœº</h3>
            <div class="motivation">
                <p>è®ºæ–‡èšç„¦äºè§†é¢‘æ¨ç†ä¸­ç¼ºä¹â€œå¯æ ¸éªŒâ€çš„è¯æ®é“¾ï¼šç°æœ‰æ–¹æ³•å¤šä»…ç»™å‡ºæ–‡æœ¬åŒ–æ€è·¯ï¼Œéš¾ä»¥æŒ‡å‡ºå…³é”®è¯æ®ä½•æ—¶å‡ºç°ã€ä½äºä½•å¤„ï¼ˆè§ç¬¬1-2é¡µä¸å›¾1ç¬¬2é¡µï¼‰ã€‚åœ¨åŠ¨æ€åœºæ™¯ä¸­åŒæ—¶åšåˆ°æ—¶é—´ä¸ç©ºé—´çš„ç²¾ç¡®å®šä½æ›´éš¾ï¼Œæ ¹æºåœ¨äºç¼ºä¹ç»Ÿä¸€çš„æ—¶ç©ºæ ‡æ³¨æ•°æ®ï¼Œä»¥åŠè®­ç»ƒä¸­æ—¶ç©ºå¯¹é½çš„ä¼˜åŒ–å›°éš¾ï¼ˆç¬¬2é¡µï¼‰ã€‚å°¤å…¶åœ¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œç©ºé—´å¥–åŠ±å¾€å¾€ä¾èµ–æ­£ç¡®æ—¶é—´ç‚¹ï¼Œæ—¶é—´é¢„æµ‹ä¸å‡†ä¼šå¯¼è‡´å¥–åŠ±ç¨€ç–ä¸è®­ç»ƒåœæ»ï¼ˆç¬¬3é¡µï¼‰ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½æŠŠç­”æ¡ˆä¸â€œæ—¶é—´æˆ³+è¾¹ç•Œæ¡†â€æ˜¾å¼ç»‘å®šçš„æ—¶ç©ºè¯æ®ä¸­å¿ƒåŒ–æ¨ç†æ¡†æ¶ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ”§</span>ç ”ç©¶æ–¹æ³•</h3>
            <div class="method">
                <p>ä½œè€…æå‡ºOpen-o3 Videoï¼šä¸€ä¸ªéagentã€å•æ¨¡å‹æ¡†æ¶ï¼Œç›´æ¥åœ¨æ¨ç†é“¾ä¸­è¾“å‡ºå¸¦æ—¶é—´æˆ³ä¸æ¡†çš„è¯æ®ç‰‡æ®µï¼Œå®ç°â€œwith framesâ€çš„å¯è§†åŒ–æ€è€ƒï¼ˆç¬¬3é¡µï¼‰ã€‚ä¸ºæ­¤æ„å»ºä¸¤å¥—æ•°æ®é›†ï¼šç”¨äºSFTçš„STGR-CoT-30kä¸ç”¨äºRLçš„STGR-RL-36kï¼Œå¹¶é¢å¤–æ–°æ³¨5.9ké«˜è´¨é‡æ—¶ç©ºæ ·æœ¬ï¼›æ•°æ®ç”±Gemini 2.5 Proåˆæ ‡ã€Qwen2.5-VLæ ¸éªŒæ¡†ä¸ä¸€è‡´æ€§æ£€æŸ¥ç»„æˆï¼ˆå›¾2ç¬¬4é¡µï¼Œé™„å½•A.3-A.6ï¼‰ã€‚è®­ç»ƒé‡‡ç”¨â€œä¸¤é˜¶æ®µ+GSPOâ€ç­–ç•¥ï¼šå…ˆå†·å¯åŠ¨SFTå­¦ä¼šç»“æ„åŒ–è¾“å‡ºï¼Œå†ç”¨GSPOå¼ºåŒ–å­¦ä¹ ç¨³å®šä¼˜åŒ–åºåˆ—çº§å¥–åŠ±ï¼ˆå›¾3ç¬¬5é¡µï¼Œ4.2èŠ‚ï¼‰ã€‚å¥–åŠ±è®¾è®¡åŒ…å«ç­”æ¡ˆæ­£ç¡®åº¦ã€æ€è€ƒå¥–åŠ±ä¸æ ¼å¼å¥–åŠ±ï¼›æ€è€ƒå¥–åŠ±å†…å¼•å…¥è‡ªé€‚åº”æ—¶é—´æ¥è¿‘åº¦ï¼ˆå‰æœŸæ¾åæœŸä¸¥ï¼‰ä¸â€œæ—¶é—´é—¨æ§â€çš„ç©ºé—´IoUè®¡ç®—ï¼Œè§£å†³æ—¶é—´-ç©ºé—´è€¦åˆä¸‹çš„ç¨€ç–ä¸è¯¯å¥–é—®é¢˜ï¼ˆç¬¬6-7é¡µï¼Œå…¬å¼(3)(4)ï¼‰ã€‚æ­¤å¤–åŠ å…¥ç»å¯¹æ—¶é—´æˆ³æç¤ºã€å…³é”®å¸§æ’å…¥ä¸è¯æ®æ„ŸçŸ¥çš„æµ‹è¯•æ—¶å°ºåº¦åŒ–æŠ•ç¥¨ä»¥æå‡é²æ£’æ€§ï¼ˆç¬¬7é¡µä¸é™„å½•A.1ã€A.5ç¬¬16é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ“Š</span>å®éªŒç»“æœ</h3>
            <div class="results">
                <p>åœ¨V-STARä¸Šå–å¾—SOTAï¼šWhatå‡†ç¡®ç‡61.0ï¼ŒmAM 33.7ã€mLGM 46.6ï¼Œç›¸æ¯”Qwen2.5-VL-7Båˆ†åˆ«æå‡+27.5ã€+14.4ã€+24.2ï¼Œå¹¶è¶…è¿‡GPT-4oä¸Gemini-2-Flashï¼ˆè¡¨1ç¬¬8é¡µï¼‰ã€‚åœ¨VideoMMEã€WorldSenseã€VideoMMMUä¸TVGBenchä¸Šä¹Ÿæœ‰ä¸€è‡´å¢ç›Šï¼Œå¦‚é•¿è§†é¢‘+4.1%ã€æ„ŸçŸ¥ç›¸å…³+3.1%/ +3.3%ï¼ŒTVGBench mIoU +4.5ï¼ˆè¡¨2ç¬¬9é¡µï¼‰ã€‚æ¶ˆèæ˜¾ç¤ºRL>çº¯SFTï¼ŒSFT+RLæœ€ä½³ï¼›GSPOä¼˜äºGRPOï¼ˆ+0.9 mAM/+1.3 mLGMï¼‰ï¼Œè¯å®åºåˆ—çº§ä¼˜åŒ–å¯¹é•¿é“¾ç¨³å®šæ€§ä¸å®šä½æœ‰æ•ˆï¼ˆè¡¨3ç¬¬9é¡µï¼‰ã€‚å»é™¤è‡ªé€‚åº”æ—¶é—´æ¥è¿‘æˆ–æ—¶é—´é—¨æ§å‡æ˜¾è‘—é€€åŒ–ï¼ˆè¡¨4ç¬¬10é¡µï¼‰ï¼›é«˜è´¨é‡æ—¶ç©ºæ•°æ®å¯¹æ€§èƒ½è´¡çŒ®æ˜æ˜¾ï¼ˆè¡¨5ç¬¬10é¡µï¼‰ï¼›è¯æ®æ„ŸçŸ¥æŠ•ç¥¨ä¼˜äºå¤šæ•°æŠ•ç¥¨ï¼ˆWorldSense +1.0ã€VideoMMMU +1.0ï¼Œè¡¨7ç¬¬16é¡µï¼‰ã€‚</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ğŸ’¡</span>ç ”ç©¶æ€è·¯</h3>
            <div class="idea">
                <p>é¢å‘æ›´é•¿ã€æ›´å¤æ‚è§†é¢‘ä¸å°ç›®æ ‡åœºæ™¯ï¼Œéœ€æ‰©å……æ›´ç»†ç²’åº¦ä¸å¤šæ ·åŒ–çš„æ—¶ç©ºç›‘ç£ï¼Œå¹¶æ”¹è¿›é•¿ä¸Šä¸‹æ–‡ä¸è®°å¿†æœºåˆ¶ï¼ˆé™„å½•A.7ç¬¬17é¡µï¼‰ã€‚å¤šæ¨¡æ€èåˆæ–¹é¢ï¼Œå¯å°†è¯­éŸ³/éŸ³é¢‘çº³å…¥ç»Ÿä¸€çš„â€œæ–‡æœ¬-æ—¶é—´-ç©ºé—´-éŸ³é¢‘â€è¯æ®é“¾å¯¹é½ä¸å¥–åŠ±è®¾è®¡ï¼ˆç»“è®ºç¬¬9é¡µï¼‰ã€‚è®­ç»ƒä¸Šå¯æ¢ç´¢è‡ªç›‘ç£/åŠç›‘ç£çš„æ—¶ç©ºå¯¹é½ç›®æ ‡ã€è·¨æ•°æ®æºä¸€è‡´æ€§æ­£åˆ™ã€ä»¥åŠæ›´å¼ºçš„æ—¶åºä¸€è‡´æ€§ä¸å¤šç›®æ ‡è·Ÿè¸ªçº¦æŸã€‚æ¨ç†ä¸Šå¯ç ”ç©¶ç«¯åˆ°ç«¯çš„è¯æ®è§„åˆ’ä¸è£å‰ªç­–ç•¥ã€åˆ†å±‚æˆ–ä»£ç†å¼å·¥å…·ä½¿ç”¨ï¼Œä»¥åŠæ›´ç¨³å¥çš„è¯æ®ç½®ä¿¡è¯„ä¼°ä¸åŠ¨æ€é›†æˆï¼Œä»¥è¿›ä¸€æ­¥æå‡å¯æ ¸éªŒæ€§ä¸é²æ£’æ€§ã€‚</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 1</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-10-24 13:11:45 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 1;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
</body>
</html>