<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - December 10, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">December 10, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08765" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08765" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing motion-control methods provide coarse, global steering and fail to deliver precise, fine-grained local motion control.<br>â€¢ Dense guidance signals are impractical as used: optical flow requires extra inference models and accumulates errors; pixel-space point trajectories lack contextual features, making texture and motion alignment difficult.<br>â€¢ Most pipelines depend on auxiliary motion encoders/fusion (e.g., ControlNet), complicating architectures and hindering scalable fine-tuning; evaluation is limited by small, short, and sparsely annotated benchmarks that miss long-range and multi-object dynamics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Wan-Move projects user-specified point trajectories into the VAE latent space and propagates the first-frame latent features along these trajectories to form a motion-aware condition feature that directly guides an off-the-shelf image-to-video diffusion model without architectural changes. This latent feature replication eliminates auxiliary motion encoders, enabling precise local/global motion control and scalable fine-tuning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Latent FlowNet: Learning Continuous Flow Fields for Context-Rich Motion Control: Replace discrete point tracks with learned latent flow fields to capture neighborhood context and enable smoother, complex deformations.<br>â€¢ Text-to-Trajectory: Generating Latent Motion Paths from Natural Language for Controllable I2V: Map textual motion descriptions to latent trajectories to remove manual trajectory specification and broaden usability.<br>â€¢ Motion Decomposition in Latent Space: Separating Camera and Object Dynamics for Robust Control: Factor latent guidance into global camera motion and per-object paths to improve stability and user controllability in complex scenes.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08478" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08478" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, "click-to-run" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Deployment friction and fragmentation: Desktop viewers (SIBR, engine plug-ins) are heavy, driver/toolchain dependent, hard to share, and ill-suited for rapid experimentation.<br>â€¢ WebGL limitations: Existing web viewers rely on legacy pipelines with CPU-side sorting, limited compute, and thus poor scalability and weak support for dynamic scenes, avatars, and generative post-processing.<br>â€¢ Lack of a unified, extensible runtime: No standard way to plug heterogeneous 3DGS-family methods (MLP-based 3DGS, 4DGS, avatars) into a single browser pipeline with per-frame inference and correct global compositing.<br>â€¢ Reproducibility and accessibility gap: Researchers need a zero-install, click-to-run, browser-native platform to reproduce, compare, and deploy dynamic Gaussian methods and world-model components.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Visionary unifies ONNX-based per-frame Gaussian generation (via a standardized Gaussian Generator contract) with a high-throughput WebGPU renderer that performs GPU-side preprocessing and global radix sorting for correct, real-time compositing. A three.js plug-in and optional ONNX post-processing enable plug-and-play integration of 3DGS variants (MLP-based, 4DGS, avatars) and generative effects entirely in the browser.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Physics-Aware Gaussian World Models in the Browser: Couple 3DGS with differentiable physics (e.g., MPM) and collision handling to simulate realistic interactions in a web-native runtime.<br>â€¢ A Unified Generationâ€“Reconstruction Loop for Interactive World Models: Fuse IGV-style video generators with explicit Gaussian reconstruction to enable closed-loop, geometry-consistent world modeling and editing.<br>â€¢ Scalable Web-Native 3DGS: Progressive Streaming, Compression, and Distributed Inference: Develop streaming/LOD, pruning/compression, and partitioned ONNX/WebGPU execution to support city-scale scenes under browser memory constraints.<br>â€¢ Web-Native 3D Agents on Visionary: Integrating Multimodal LLMs with 3DGS Environments: Build spatially grounded agents that perceive, reason, and act within Visionary scenes for embodied tasks and interactive evaluation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07951" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07951" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Achieve high-fidelity and temporally consistent face swapping over long, complex video sequences demanded by cinematic production.<br>â€¢ Preserve target identity while coherently integrating the source videoâ€™s expressions, lighting, motion, and fine-grained attributes.<br>â€¢ Handle challenging filming conditions (long takes, complex illumination, exaggerated expressions, heavy facial makeup, semi-transparent occlusions) where existing methods degrade.<br>â€¢ Reduce manual effort in production workflows through flexible, controllable editing.<br>â€¢ Overcome scarcity of paired data for reliable reference-guided training.<br>â€¢ Address limitations of GAN-based frame-by-frame methods that suffer from temporal inconsistency and identity drift.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>LIVINGSWAP is a video reference-guided face swapping model that conditions on keyframes to inject the target identity and performs temporal stitching by combining keyframe conditioning with video reference guidance, ensuring stable identity and high-fidelity reconstruction across long sequences; training is supported by a paired Face2Face dataset with reversed pairs for reliable supervision.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Self-Supervised Video Reference-Guided Face Swapping Without Paired Data: Remove reliance on paired datasets using cycle consistency, temporal pseudo-labels, and contrastive objectives.<br>â€¢ 3D-Aware LIVINGSWAP for Multi-View Cinematic Face Swapping: Integrate 3D geometry or neural radiance fields to handle extreme poses, multi-camera setups, and view-consistent identity preservation.<br>â€¢ Real-Time LIVINGSWAP for On-Set Production: Optimize the model and guidance pipeline for low-latency, resource-efficient deployment to support live cinematic editing workflows.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07802" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07802" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing multi-shot video generation (MSV) fails to model long-range cross-shot context, causing memory loss and inconsistency as scenes progress.<br>â€¢ Fixed-window attention discards earlier shots when the window slides, breaking narrative coherence beyond the window.<br>â€¢ Keyframe conditioning limits cross-shot context to a single image per shot, weakening propagation of complex narrative cues and storyline adherence.<br>â€¢ MSV requires persistent identities and environments across discontinuous scenes, and reasoning about what should stay invariant vs. change (time, location, viewpoint, actions).<br>â€¢ Lack of high-quality, referential multi-shot datasets and training schemes tuned for next-shot generation hinders scalable, coherent narrative modeling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>OneStory reformulates MSV as autoregressive next-shot generation, leveraging a pretrained image-to-video backbone and two modules: Frame Selection builds a global memory by selecting semantically relevant frames from prior shots, and Adaptive Conditioner compresses them via importance-guided patchification for direct conditioning; trained on a curated 60K multi-shot dataset with unified three-shot and progressive coupling strategies.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Hierarchical Memory for Hour-Long Multi-Shot Video Generation: Learn multi-scale memory (shot-, scene-, episode-level) to extend OneStory from minutes to hours while preserving global coherence.<br>â€¢ Multimodal Referential Narratives for Coherent Multi-Shot Video: Integrate audio, dialogue, and scene graphs to enrich cross-shot context and improve character/action consistency.<br>â€¢ Learning Dynamic Memory Policies for Next-Shot Video Generation: Train a policy network to adaptively select and weight past frames/shots based on narrative importance and uncertainty for robust autoregression.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07843" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07843" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Autoregressive decoding is inherently sequential, causing high inference latency on complex tasks where long chains of thought are needed and cannot be sped up by simply adding more compute.<br>â€¢ Ensemble-style parallelism (best-of-N, self-consistency) increases accuracy but not time-to-answer, duplicating computation and leaving critical-path latency unchanged.<br>â€¢ Search-based methods (e.g., tree-of-thought) rely on hand-crafted heuristics that limit scalability and generality across problem types.<br>â€¢ Prior adaptive parallel reasoning approaches either require customized inference engines (modifying position embeddings/KV caches/attention) that hinder deployment or suffer notable accuracy drops versus strong sequential baselines.<br>â€¢ High-quality parallel reasoning trajectories for real-world tasks are scarceâ€”human annotation is costly and SOTA LLMs struggle to generate reliable parallel tracesâ€”making supervised training difficult.<br>â€¢ Reinforcement learning for parallel reasoning is underexplored, with challenges in thread-wise credit assignment and ensuring train-test consistency of parallel execution.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>ThreadWeaver implements fork-join parallel reasoning via lightweight control tokens and a state-machine orchestrator that runs on standard autoregressive engines (no changes to position embeddings or KV caches), trains with a two-stage parallel trajectory generator and trie-based sequence merging for SFT, and applies parallelization-aware GRPO to jointly optimize accuracy and token-latency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Nested ThreadWeaver: Extending Adaptive Fork-Join to Deeply Nested and Dynamic Branching Structures: Generalize beyond single-level parallel blocks to support nested branches and dynamic spawning/joining with stability and data quality safeguards.<br>â€¢ Learned Join Operators for LLM Parallel Reasoning: End-to-End Training of Reducers and Cross-Thread Summarization: Replace rule-based concatenation with differentiable, trained reducers that compress and integrate thread outputs while minimizing context bloat.<br>â€¢ Hardware-Aware Scheduling for Parallel LLM Reasoning: Co-Optimizing Orchestrators with Inference Engines: Design schedulers that allocate threads based on cluster topology, KV/prefix caching efficiency, and compute budgets to maximize wall-clock speedups under real serving constraints.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06864" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06864" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 AP_{50} on YouTubeVIS-2019 val set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ High annotation cost in VIS due to the need for pixel-accurate masks and temporally consistent instance identities across frames.<br>â€¢ Prior unsupervised methods often depend on optical-flow models trained with human labels and commonly handle only single objects or fixed instance counts, limiting true unsupervised and multi-instance capability.<br>â€¢ Synthetic training videos (e.g., VideoCutLER) suffer from synthetic-to-real domain gaps and lack realistic motion, constraining generalization to real-world videos.<br>â€¢ Self-training without robust quality control admits noisy pseudo-labels; classifier confidence correlates poorly with mask IoU, leading to error accumulation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>AutoQ-VIS pretrains a VideoMask2Former VIS model and a Mask Scoring R-CNNâ€“style quality predictor on synthetic videos, then performs multi-round self-training on unlabeled real videos by selecting pseudo-masks using a quality score (predicted IoU Ã— detection confidence). The pipeline freezes the quality predictor, applies DropLoss to suppress near-zero-IoU masks, uses adaptive fusion to merge new and old track-level annotations, and balances sampling between synthetic and pseudo-labeled data.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Curriculum-Guided Quality Thresholding for Unsupervised VIS: Learn an adaptive, uncertainty-aware schedule for pseudo-label thresholds and sampling to reduce noise while maximizing data utilization across rounds.<br>â€¢ Temporal Consistency-Aware Quality Prediction for VIS: Extend the quality predictor from frame-wise to sequence-level by incorporating temporal features and track stability cues for better mask selection.<br>â€¢ Generative Motion Priors for Synthetic Video Augmentation in Unsupervised VIS: Use learned motion fields or video diffusion to synthesize realistic dynamics, narrowing the synthetic-to-real gap before self-training.<br>â€¢ EMA Teacherâ€“Student Calibration for Quality-Guided Self-Training in VIS: Introduce an exponential moving average teacher and post-hoc calibration to stabilize pseudo-label quality and mitigate confirmation bias.<br>â€¢ Minimal-Verification Loops for Approaching the Practical Limit in Unsupervised VIS: Add sparse human verification or automatic uncertainty querying to correct hardest pseudo-labels and close the gap to oracle performance.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06628" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06628" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Scarcity of diverse, longâ€‘horizon robotic manipulation data limits scalable imitation learning and worldâ€‘model training.<br>â€¢ Maintaining longâ€‘horizon causal and logical coherence is hard; small subâ€‘task errors cascade into task failure.<br>â€¢ Bridging semanticâ€‘toâ€‘pixel generation: accurately translating abstract language commands into precise spatiotemporal interactions.<br>â€¢ Enforcing physical plausibility (object permanence, collision dynamics, interaction forces) in generated videos.<br>â€¢ Limitations: trajectoryâ€‘free video models show logical discontinuities/detail drift on long horizons; trajectoryâ€‘guided methods require manual masks/paths and lack autonomy/scalability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MIND-V is a hierarchical video generation system that uses a VLM-driven Semantic Reasoning Hub plus an affordance localizer to decompose tasks and plan trajectories, encodes them into a domain-invariant Behavioral Semantic Bridge (masks, phaseâ€‘decomposed trajectories, transition points), and conditions a DiT-based Motor Video Generator via spatiotemporal guidance. It adds Staged Visual Future Rollouts for proposeâ€“verifyâ€“refine at test time and GRPO post-training with a Physical Foresight Coherence reward from a world model (plus aesthetic scoring) to align outputs with physics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ End-to-End Learning of Behavioral Semantic Bridges for Robot Control: Learn BSB representations directly from languageâ€“videoâ€“action corpora to remove hand-engineered affordance modules and enable tighter coupling to control.<br>â€¢ Physics-Aware Reward Models via Differentiable Simulation for Video Generation: Replace/augment V-JEPA-based PFC with simulator-backed, gradient-informed rewards to more strictly enforce contact and dynamics constraints.<br>â€¢ From Synthetic to Policy: Imitation Learning at Scale from MIND-V Generated Manipulation Videos: Systematically train control policies on MIND-V outputs and evaluate sim-to-real transfer, data efficiency, and long-horizon success.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Arbitrage: Efficient Reasoning via Advantage-Aware Speculation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05033" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05033" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to sim2times at matched accuracy.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Long chain-of-thought inference is memory-bound and slow, creating high latency and high compute cost in LLM reasoning.<br>â€¢ Token-level speculative decoding suffers low acceptance in reasoning tasks due to semantically equivalent token mismatches, leading to repeated, unnecessary rejections.<br>â€¢ Existing step-level methods (e.g., RSD) use absolute PRM thresholds that are advantage-blind, frequently triggering costly target regenerations that yield little or no quality gain, wasting compute.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>ARBITRAGE trains a lightweight router to predict the probability that the target model will outperform the draft on each reasoning step and routes accordingly: accept the draft if predicted advantage â‰¤ threshold, otherwise escalate to the target. This advantage-aware, step-level speculative decoding approximates an oracle that chooses the higher-quality step, improving efficiencyâ€“accuracy trade-offs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Beyond Greedy Arbitrage: Non-Myopic Advantage-Aware Speculative Decoding: Optimize routing decisions over multiple future steps with lookahead or RL to maximize global qualityâ€“latency objectives.<br>â€¢ Cascaded Arbitrage: Multi-Stage Routers for Draftâ€“Target Model Hierarchies: Extend advantage-aware routing to multi-level model cascades, selecting the smallest model expected to deliver a meaningful improvement.<br>â€¢ Calibrated Arbitrage: Uncertainty-Aware Advantage Prediction for Robust Reasoning: Incorporate calibration and uncertainty estimation in the router to adapt thresholds per instance and step, reducing erroneous escalations and rejections.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DeepCode: Open Agentic Coding</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07921" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07921" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing LLM coding agents struggle with high-fidelity document-to-repository synthesis, achieving ~42% replication vs. ~72% for human experts on PaperBench<br>â€¢ Core obstacle: information overload versus finite LLM context windows, collapsing effective signal-to-noise ratio when naively concatenating specs and code history<br>â€¢ Four key failure modes: poor specification preservation, broken global cross-file consistency, gaps from underspecified designs, and lack of executable faithfulness<br>â€¢ Current tools remain assistive (code completion) and lack principled information-flow management; multi-agent pipelines and larger models/context alone yield modest gains<br>â€¢ Solving doc-to-repo synthesis is important for autonomous scientific reproduction, accelerating evaluation and discovery, and enabling agentic software engineering from high-level specifications</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DeepCode reframes doc-to-repo synthesis as an information-flow/channel optimization problem and orchestrates four operationsâ€”blueprint distillation (source compression), stateful code memory (structured indexing), retrieval-augmented generation (conditional knowledge injection), and automated verification (closed-loop error correction). Implemented in three phases (Blueprint Generation, Iterative Code Generation with CodeMem and CodeRAG, and Sandbox/Static Verification), it maximizes task-relevant signals under finite context budgets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Signal Routing for Agentic Coding: Design algorithms that dynamically budget context, compress sources, and route memory based on uncertainty and task phase to maximize effective SNR.<br>â€¢ Formal Verification-Augmented DeepCode: Integrate type systems, contracts, property-based testing, and lightweight formal methods into the verification loop to guarantee executable faithfulness.<br>â€¢ Multimodal Doc-to-Repo Synthesis Across Domains: Extend hierarchical parsing and blueprint distillation to equations, figures, and UI/design docs for robust reproduction of ML papers and full-stack applications.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.02231" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.02231" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ MLLMs lack rigorous evaluation of fine-grained, speaker-centric audiovisual reasoningâ€”aligning who speaks, what is said, and when it occurs.<br>â€¢ This capability is critical for real-world applications (e.g., video dialog agents, meeting transcription), yet current models often over-rely on a single modality and struggle with temporal grounding.<br>â€¢ Existing benchmarks are either visually solvable or focus on coarse audio events, and legacy speech datasets use closed-set/frame-level labels, making them incompatible with open-ended, language-based evaluation of speaker-level reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce AV-SpeakerBench, a curated speaker-centric benchmark of 3,212 multiple-choice questions with fusion-grounded design and expert, temporally precise annotations that require aligning speech content with visible speakers over time. The paper evaluates diverse MLLMs under audio-on conditions and analyzes fusion gains and error patterns to diagnose audiovisual integration weaknesses.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning to Listen and Look: Training MLLMs for Speaker-Centric Audiovisual Fusion: Develop objectives and datasets that explicitly align spoken phrases with visible identities, boosting cross-modal fusion and temporal grounding.<br>â€¢ Temporal Grounding Networks for Multimodal Turn-Taking in Real-World Conversations: Design architectures that model speech timing, overlaps, and speaker transitions to improve who-when-what reasoning.<br>â€¢ Multilingual AV-SpeakerBench: Benchmarking Cross-Lingual Speaker-Centric Audiovisual Reasoning: Extend the benchmark to multilingual and culturally diverse settings to assess cross-lingual audiovisual fusion and robustness.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08153" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08153" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce TreeGRPO, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) High sample efficiency, achieving better performance under same training samples (2) Fine-grained credit assignment via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) Amortized computation where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves 2.4times faster training while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ RL post-training for diffusion/flow generative models is computationally prohibitive because each policy update requires sampling full, costly denoising trajectories.<br>â€¢ GRPO-style methods suffer poor sample efficiency and coarse credit assignment (uniform terminal reward per trajectory), obscuring step-wise contributions and limiting optimization.<br>â€¢ Existing efficiency tricks (e.g., hybrid sampling/sliding windows) often trade away final performance, leaving a suboptimal efficiencyâ€“reward Pareto frontier and impeding scalable alignment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>TreeGRPO recasts the denoising process as a search tree, branching from shared noise/prefixes to explore multiple candidate continuations while reusing computation. Final-image rewards are backpropagated along the tree to compute step-wise advantages, enabling GRPO-style updates with higher sample efficiency and amortized compute per forward pass.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive TreeGRPO: Learning Branching Policies for Budget-Aware RL Post-Training â€” dynamically optimize branching depth/width per prompt based on reward uncertainty and compute budget to further improve the efficiencyâ€“reward trade-off.<br>â€¢ Multi-Objective TreeGRPO: Vector-Valued Rewards and Pareto-Optimal Policy Optimization â€” extend tree-advantage backpropagation to jointly optimize across multiple reward models with explicit Pareto-aware updates.<br>â€¢ Human-in-the-Loop TreeGRPO: Online Preference Elicitation and Reward Model Co-Training â€” integrate real-time human feedback with tree-based exploration to refine reward models and policies during post-training.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Efficiently Reconstructing Dynamic Scenes One D4RT at a Time</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08924" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08924" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing pipelines for dynamic 4D reconstruction are fragmented and slow, relying on multiple task-specific decoders or test-time optimization, which hinders unified depth, correspondence, and camera estimation.<br>â€¢ Prior feedforward methods struggle to establish correspondences in dynamic regions and often fix the camera reference or assume constant intrinsics, limiting true 4D scene understanding.<br>â€¢ Dense dynamic tracking across all pixels is computationally prohibitive with naive O(T^2HW) querying, and current approaches either lack dynamic correspondences or incur heavy per-query costs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>D4RT encodes a video into a Global Scene Representation with a transformer and decodes independent spatio-temporal point queries via a lightweight cross-attention decoder to directly predict a pointâ€™s 3D position at any target time and camera coordinate. By varying the query (u, v, tsrc, ttgt, tcam), the same interface unifies point tracks, point clouds, depth, and camera parameters (extrinsics via Umeyama alignment; intrinsics via pinhole constraints), and an occupancy-grid strategy enables efficient dense tracking.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Query Planning for Dense 4D Reconstruction: Learn uncertainty- and visibility-aware query schedules that minimize compute while preserving coverage, further improving the occupancy-grid approach.<br>â€¢ Joint Estimation of Generalized Camera Models in D4RT: Extend the decoder to directly infer lens distortion and non-pinhole parameters end-to-end, replacing post-hoc non-linear refinement.<br>â€¢ Streaming D4RT: Long-Horizon 4D Reconstruction with Hierarchical Global Representations: Develop memory- and compute-efficient encoders for high-resolution, long videos via hierarchical/streaming scene representations.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">Modular Neural Image Signal Processing</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08564" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08564" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ End-to-end, monolithic neural ISPs generalize poorly to unseen cameras because their mappings are tightly coupled to the training camera.<br>â€¢ Lack of control and visibility over intermediate ISP stages limits debuggability, calibration, and the ability to match diverse user-preference styles.<br>â€¢ High memory and computational demands of prior neural ISPs hinder practical deployment (e.g., on-device or real-time processing).<br>â€¢ Traditional ISPs require careful hand-engineering per stage; a learning-based yet modular approach can streamline development while retaining control.<br>â€¢ Users need post-editable re-rendering and flexible picture styles from raw captures, which monolithic pipelines do not readily support.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A modular neural ISP that decomposes the pipeline into lightweight learned stages (e.g., denoising, white balance, color conversion, tone/quality enhancement) with explicit intermediate controls, designed to be camera-agnostic and to support post-editable re-rendering via a user-interactive tool. The framework maintains moderate model sizes and achieves competitive rendering accuracy while improving scalability, debuggability, and generalization to unseen cameras without retraining.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Style Transfer in Modular ISP for Personalized Rendering: Learn per-stage style controls and priors to dynamically match user preferences across diverse picture styles.<br>â€¢ Cross-Camera Domain Adaptation for Modular Neural ISPs Without Retraining: Develop unsupervised or metadata-guided adaptation mechanisms that fine-tune module behaviors for new sensors on-the-fly.<br>â€¢ Real-Time On-Device Modular ISP with Energy-Aware Scheduling: Co-design modules and runtime scheduling to meet latency and power budgets while preserving rendering quality.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08186" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08186" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, "grounds slowly" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, "moves fast" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ End-to-end VLN/VLA models map vision-language inputs to short-horizon discrete actions, producing fragmented, unnatural motion and high latency due to frequent large VLM calls.<br>â€¢ Reasoning, global planning, and local control are entangled, yielding poor hierarchical coordination and limiting agile, high-frequency decisions needed for dynamic obstacle avoidance.<br>â€¢ Existing synchronized pipelines struggle to deliver smooth, continuous trajectories and real-time adaptability in physically realistic, dynamic environments.<br>â€¢ Monolithic training tends to erode the VLMâ€™s generalization when adapting to low-level control; a decoupled approach is needed to preserve reasoning while enabling efficient local policy learning.<br>â€¢ Pixel-grounding or text-action methods often operate at low frequency and underuse rich VLM latent knowledge, leading to shallow coupling between reasoning and execution.<br>â€¢ Current benchmarks underrepresent social dynamics; there is no standard evaluation for human-aware navigation, safety (e.g., collision rate), and task recovery in crowds.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>DualVLN is an asynchronous dual-system model where a 7B VLM (System 2) slowly grounds mid-term pixel waypoints and produces latent goal features, while a lightweight diffusion transformer policy (System 1) rapidly converts these goals plus high-frequency RGB into smooth continuous trajectories via flow-matching; systems are trained decoupled, with learnable latent queries bridging VLM reasoning to local control.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Human-Aware Dual-System Navigation via Predictive Social Motion Models: Incorporate human trajectory forecasting and social-compliance objectives to lower collision rates and improve detours/recovery.<br>â€¢ Latent-Only Planning: Self-Supervised Discovery of Implicit Waypoints for VLN: Replace explicit pixel goals with learned latent anchors using contrastive grounding to reduce supervision and enhance robustness.<br>â€¢ Memory-Augmented DualVLN for Kilometer-Scale Instruction Following: Add topological maps and episodic memory to System 2 for long-horizon cross-building navigation with adaptive slow-fast scheduling.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06776" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06776" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior "adaptation" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Autoregressive (AR) decoding is strictly sequential, creating an inference throughput bottleneck for LLMs.<br>â€¢ Training large diffusion language models (DLMs) from scratch is compute-prohibitive and wastes knowledge in strong AR checkpoints.<br>â€¢ Existing AR-to-diffusion adaptations (logit shifts, random attention-mask growth, naive weight transplant) do not address the core mismatch between AR causality and intra-block bidirectionality, leading to poor scalability and knowledge loss.<br>â€¢ Full-sequence diffusion is inefficient on long contexts and misaligned with left-to-right inductive biases, lacking KV-cache reuse and trainâ€“inference consistency.<br>â€¢ Naive block-diffusion training is data-inefficient (only the active block supervises), risking AR knowledge forgetting and slow convergence.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Treat AR as block-diffusion with blocksize=1 and adapt via a context-causal attention mask (causal in committed context, bidirectional only within the active block), parallel training with concatenated noised/clean views under a structured mask, an auxiliary AR loss on the clean branch, and a gradual block-size growth curriculum, keeping trainâ€“inference alignment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Block-Size Curriculum for Diffusion LLMs: Dynamically grow block size and anneal AR-loss weight using uncertainty/entropy signals to optimize speedâ€“quality trade-offs during adaptation.<br>â€¢ Context-Causal Multimodal Block-Diffusion: Extend the context-causal adaptation path to visionâ€“language and other modalities, preserving causal conditioning while enabling intra-block bidirectional refinement.<br>â€¢ Scaling Laws and Theory for AR-to-Block-Diffusion Adaptation: Formalize optimization dynamics and inductive-bias transfer under context-causal masking, predicting performance, compute, and curriculum schedules across model/data scales.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08868" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08868" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing agent benchmarks skew toward academic/synthetic tasks and fail to reflect the dynamic, complex, and high-stakes nature of real e-commerce scenarios.<br>â€¢ Current evaluations underemphasize agentsâ€™ capabilities in tool-using, deep information retrieval, multi-step reasoning, and cross-source knowledge integration.<br>â€¢ Lack of authentic, verifiable, and timely datasets grounded in genuine user demands; many rely on LLM-synthesized questions with weak difficulty calibration.<br>â€¢ Need fine-grained, category- and difficulty-aware assessments to reveal domain-specific strengths/weaknesses and practical utility.<br>â€¢ Benchmarks must be dynamic to track evolving policies, markets, and trends while mitigating data contamination.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>EcomBench is a human-in-the-loop, expert-curated benchmark built from genuine e-commerce user demands, spanning seven task categories and three difficulty levels with verifiable answers. High-difficulty items are selected via a tool-hierarchy process using LLMs equipped with specialized e-commerce tools to retain tasks requiring long-horizon reasoning; an LLM judge provides binary scoring, and quarterly updates keep the benchmark current.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ EcomBench-Interact: Evaluating Foundation Agents in Interactive E-commerce Environments: Extend beyond QA to assess agentsâ€™ end-to-end workflows with real tools/APIs (ads, storefronts, logistics), measuring reliability and safety.<br>â€¢ EcomBench-Forecast: Benchmarking Predictive and Decision-Oriented Tasks in E-commerce: Add forecasting (demand, trends), price elasticity, and inventory optimization with time-stamped ground truths.<br>â€¢ AutoEcomBench: Tool-Augmented, Verifiable Synthesis to Scale Benchmark Curation: Develop semi-automated, tool-hierarchyâ€“driven task generation and formal verification to reduce human costs while preserving realism.<br>â€¢ EcomBench-Multimodal: Assessing Reasoning over Listings, Images, Tables, and Screenshots: Introduce multimodal tasks combining product images, invoices, and UI screenshots to evaluate integrated decision-making.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08358" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08358" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing monocular 3D tracking methods largely assume a static camera and operate in camera-centric coordinates, failing to disentangle camera motion from dynamic object motion needed for analysis and novel view synthesis.<br>â€¢ Current approaches track only sparse points (often from the first frame) and cannot densely track almost all pixels across all frames, missing newly emerging objects.<br>â€¢ Inaccurate dynamic masks and the computational burden of dense tracking lead to suboptimal bundle adjustment and camera pose estimation in dynamic scenes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>TrackingWorld upsamples arbitrary sparse 2D tracks into dense per-frame trajectories, prunes redundant tracks in overlapped regions, and uses an optimization-based framework with an as-static-as-possible constraint to robustly estimate world-centric camera poses; the estimated poses and monocular depth priors are then used to back-project dense 2D tracks into 3D.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ End-to-End World-Centric Monocular 3D Tracking Without Foundation Model Priors: Jointly learn depth, dynamic masks, camera poses, and dense tracks in a single network to replace external modules and minimize optimization overhead.<br>â€¢ Real-Time TrackingWorld: Efficient Dense World-Centric Tracking on Edge Devices: Develop incremental solvers and model compression/pruning to achieve near-real-time dense 3D tracking with world-centric pose estimation.<br>â€¢ Self-Supervised Dynamic Mask Refinement for Robust Pose Estimation in Monocular Videos: Online refine dynamic/background segmentation using motion-consistency and 3D reprojection losses to reduce contamination in camera pose estimation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.06531" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.06531" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Manual identification of brain tumors in MRI is time-consuming and error-prone given rising case volumes, necessitating reliable automation.<br>â€¢ Existing deep learning models often lack generalization, showing strong training/augmented-data results but weaker validation/test performance on real, unseen data.<br>â€¢ Prior approaches frequently suffer from overfitting, low test accuracy, and limited robustness across datasets and institutions.<br>â€¢ Accurate tumor segmentation is crucial for treatment planning, yet many methods struggle to delineate precise boundaries and multi-scale tumor features.<br>â€¢ There is a practical need for CAD systems that achieve high accuracy while remaining deployable (e.g., mobile/web) in resource-constrained clinical settings.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes two novel deep learning architectures: SAETCN for multiclass brain tumor classification and SAS-Net for tumor segmentation, both built around a Self-Attention Enhancement Block that fuses residual skip connections with inception-style multi-scale convolutions. SAETCN stacks 16 such blocks across four modules, while SAS-Net uses a U-Net-like encoder-decoder with Segmental Feature Decoding blocks (upsampling + residual-inception fusion) to produce precise segmentations, achieving state-of-the-art results on multiple MRI datasets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ SAETCN-3D: Volumetric Self-Attention Enhanced Networks for 3D Multimodal MRI Classification and Segmentation: Extend SAETCN/SAS-Net to 3D convolutions and integrate T1/T1c/T2/FLAIR to capture volumetric context and modality synergy, with comprehensive BraTS-style evaluation.<br>â€¢ Domain-Generalized Brain Tumor CAD via Self-Supervised Pretraining and Test-Time Adaptation: Combine self-supervised MRI pretraining and test-time adaptation to improve robustness across institutions, scanners, and protocols, reducing performance drops on out-of-distribution data.<br>â€¢ Uncertainty-Aware and Calibrated SAETCN for Clinical Decision Support: Integrate Bayesian or ensemble uncertainty estimation and probability calibration to provide confidence-aware predictions and safer triage, validated in clinician-in-the-loop workflows.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.05325" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.05325" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large reasoning models achieve strong performance on complex tasks by generating extended chains of thought, but they often "overthink": continuing to reason long after they have enough information to answer correctly. This wastes inference-time compute and can hurt accuracy. Existing attempts to stop early either manipulate decoding with extra sampling and heuristics, rely on auxiliary verifier models, or operate only as post-hoc analysis pipelines without formal guarantees. We introduce LYNX, an online early-exit mechanism that turns a model's own hidden-state awareness into confidence-controlled stopping decisions. LYNX attaches exit decisions to naturally occurring reasoning cues (e.g., "hmm", "wait") during generation, trains a lightweight probe on hidden states at those cue tokens using supervision from forced exits, and wraps the resulting scores in split conformal prediction to obtain distribution-free control over premature exits. Crucially, we train and calibrate this probe once on a generic mathematical corpus and reuse it unchanged across benchmarks, decoding temperatures, and even non-mathematical tasks. Across three model families spanning 1.5B to 32B parameters, a single mathematically trained probe per base model yields strong accuracy--efficiency tradeoffs. On GSM8K, LYNX matches or improves baseline accuracy while reducing tokens by 40--65\%; on MATH-500 it improves accuracy by up to 12 points with roughly 35--60\% fewer tokens; on AIME 2024 it recovers baseline accuracy with more than 50\% token savings; and on CommonsenseQA, a non-math benchmark, it transfers zero-shot with modest accuracy gains and up to 70\% fewer tokens. Compared to state-of-the-art early-exit methods, LYNX offers competitive or superior Pareto frontiers while remaining fully online, requiring no proxy models at inference, and providing explicit, user-tunable confidence guarantees.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Reasoning LLMs overthink: they often continue chains of thought after having enough evidence, wasting tokens and sometimes degrading final accuracy by talking themselves out of correct intermediate solutions.<br>â€¢ Redundant or looping reasoning inflates inference cost and can exhaust context without producing an answer, making compute allocation a reliability, not just efficiency, issue.<br>â€¢ Existing early-exit methods often manipulate decoding (extra sampling/prompts), rely on external verifier/proxy models, or operate post-hoc, providing no online stopping with formal guarantees.<br>â€¢ Thresholds in prior work are brittle across datasets, models, and temperatures, require per-task retuning, may hide true costs via speculative sampling, and lack distribution-free control over premature exits.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>LYNX attaches exit decisions to naturally occurring reasoning cues (e.g., â€œhmmâ€, â€œwaitâ€), trains a lightweight probe on cue-token hidden states using self-supervision from forced exits, and conformalizes probe scores with split conformal prediction to deliver user-tunable, distribution-free control over incorrect early exits. A single probe per base model is trained once on a generic math corpus and reused unchanged across benchmarks, temperatures, and even non-math tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Conformal Early Exits Beyond Exchangeability: Robust Guarantees under Prompt and Domain Shift: Extend LYNXâ€™s statistical control to weaker assumptions, online recalibration, and covariate-shiftâ€“aware conformal methods.<br>â€¢ Exit-Aware Pretraining: Shaping Hidden States for Reliable Online Stopping: Co-train base models with exit objectives so hidden states at cues better encode sufficiency signals for safer, earlier exits.<br>â€¢ Multimodal LYNX: Confidence-Controlled Early Exits for Visionâ€“Language and Code Reasoning: Generalize cue-triggered conformal exits to VLMs and tool-using/code models with modality- or action-specific cues.<br>â€¢ Unsupervised Cue Discovery for Dynamic Exits Across Languages and Domains: Learn domain- and language-agnostic cue sets and probe locations via representation clustering or attention-pattern mining.<br>â€¢ Cost-Optimal Reasoning: Jointly Optimizing Accuracy, Latency, and Risk with LYNX: Integrate compute budgets and user risk preferences to derive policies that balance token savings, accuracy, and miscoverage guarantees.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08406" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08406" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on in-the-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using a promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while a padding-based parallel strategy enables efficient multi-human inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging in-the-wild videos, without any retraining. Our code and demo are available at: https://github.com/gaomingqi/sam-body4d.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Per-frame image-based HMR applied to videos lacks temporal continuity, causing jittery meshes, identity drift, and missing reconstructions when detections fail.<br>â€¢ In-the-wild scenes with camera motion, clutter, and frequent occlusions break tracking; occlusions provide incomplete visual evidence and lead to hallucinated pose/shape.<br>â€¢ Video-based HMR methods rely on optimization with large annotated video datasets and handcrafted losses, limiting scalability and generalization.<br>â€¢ Existing pipelines are inefficient for multi-person, multi-frame inference and struggle to maintain identity-consistent 4D trajectories without retraining.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A training-free pipeline that generates identity-consistent temporal masklets with SAM 3, refines them via a diffusion-based occlusion-aware completion, and uses the refined masks as encoder prompts to SAM 3D Body to recover temporally coherent 4D human meshes. Efficiency and stability are enhanced by padding-based parallel batching, test-time temporal smoothing, and reusing shape/scale across frames.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Generative Occlusion Completion with 3D Body Priors for Training-Free Video HMR: Combine diffusion-based amodal segmentation with kinematic/physical priors to yield geometrically consistent completions and reduce hallucinations under severe occlusions.<br>â€¢ Multi-Modal Prompting for Identity-Consistent Masklets in Crowded Scenes: Incorporate text, audio, and interaction cues to disambiguate identities and maintain long-term tracking during disappearance/reappearance and weak visual signals.<br>â€¢ Real-Time Edge Deployment of Parallel Training-Free 4D HMR: Develop resource-aware batching, compression, and streaming refinement to run SAM-Body4D on mobile/AR devices while preserving temporal stability.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.07197" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.07197" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ 3D Gaussian Splatting (3DGS) demands massive memory and computation to store and render millions of Gaussians, which becomes even more severe in 4D dynamic scenes.<br>â€¢ Existing efficient GS methods are fragmented; there is no unified framework that categorizes approaches or synthesizes core ideas, making it hard to navigate redundancy reduction while preserving quality.<br>â€¢ The community lacks consolidated datasets, evaluation metrics, and representative benchmarks tailored to efficient static and dynamic GS, hindering fair comparison and progress.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>A survey that systematically categorizes efficient Gaussian Splatting techniques for both 3D and 4D into two directionsâ€”Parameter Compression and Restructuring Compressionâ€”while summarizing trends, compiling datasets/metrics/benchmarks, and articulating limitations and future research avenues.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Unified Compression Framework for Static and Dynamic Gaussian Splatting: A joint optimization approach that bridges parameter compression and restructuring compression to maximize quality under memory and compute budgets.<br>â€¢ Motion-Aware 4D Gaussian Splatting Compression: Exploiting temporal redundancy via trajectory-linked Gaussians and learned temporal codecs to enable real-time rendering of dynamic scenes with compact representations.<br>â€¢ Hardware-Co-Design for Real-Time Efficient Gaussian Splatting: Compression schemes and data layouts co-optimized with GPU/edge hardware and streaming pipelines to scale compact GS to large scenes and mobile devices.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">MemLoRA: Distilling Expert Adapters for On-Device Memory Systems</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04763" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04763" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context. Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private. However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment. Even though Small Language Models (SLMs) are more suitable for on-device inference than LLMs, they cannot achieve sufficient performance. Additionally, these LLM-based systems lack native visual capabilities, limiting their applicability in multimodal contexts. In this paper, we introduce (i) MemLoRA, a novel memory system that enables local deployment by equipping SLMs with specialized memory adapters, and (ii) its vision extension MemLoRA-V, which integrates small Vision-Language Models (SVLMs) to memory systems, enabling native visual understanding. Following knowledge distillation principles, each adapter is trained separately for specific memory operationsx2013knowledge extraction, memory update, and memory-augmented generation. Equipped with memory adapters, small models enable accurate on-device memory operations without cloud dependency. On text-only operations, MemLoRA outperforms 10times larger baseline models (e.g., Gemma2-27B) and achieves performance comparable to 60times larger models (e.g., GPT-OSS-120B) on the LoCoMo benchmark. To evaluate visual understanding operations instead, we extend LoCoMo with challenging Visual Question Answering tasks that require direct visual reasoning. On this, our VLM-integrated MemLoRA-V shows massive improvements over caption-based approaches (81.3 vs. 23.7 accuracy) while keeping strong performance in text-based tasks, demonstrating the efficacy of our method in multimodal contexts.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Current memory-augmented systems depend on large, cloud-accessed LLMs for extraction, update, and retrieval, causing latency, cost, and privacy issues that block on-device deployment.<br>â€¢ Small Language Models (SLMs) are efficient for local inference but underperform on core memory operations, lacking a way to reach LLM-level quality.<br>â€¢ Existing pipelines treat images via captions, losing fine-grained visual, spatial, and numeric details; memory systems lack native visual reasoning.<br>â€¢ LLMs have limited context windows and cannot retain long-term, user-specific knowledge across sessions, undermining personalization.<br>â€¢ Benchmarks like LoCoMo evaluate primarily text-based capabilities and miss native multimodal (image) reasoning, leaving an evaluation gap.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MemLoRA equips small (vision) language models with specialized LoRA adaptersâ€”each distilled (output-only) for a specific memory operation: knowledge extraction, memory update, and memory-augmented generationâ€”then dynamically switches adapters for local, efficient execution. MemLoRA-V extends this to SVLMs with a vision expert adapter for native VQA, eliminating caption intermediaries and enabling multimodal memory understanding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Adapter Routing for On-Device Memory Systems: Learn a lightweight controller to select and compose expert adapters per turn/token, improving accuracy-efficiency trade-offs across memory operations.<br>â€¢ Online Continual Distillation for Personalized Memory Adapters: Incrementally distill from teacher outputs and user interactions to update adapters over time while mitigating catastrophic forgetting and preserving privacy.<br>â€¢ Federated Distillation of Multimodal Memory Adapters: Train and refine text and vision adapters collaboratively across devices without sharing raw data, enabling privacy-preserving, robust on-device memory systems.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.04434" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.04434" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Fast, geometry-generalizing surrogates for unsteady flow remain challenging. We present a time-dependent, geometry-aware Deep Operator Network that predicts velocity fields for moderate-Re flows around parametric and non-parametric shapes. The model encodes geometry via a signed distance field (SDF) trunk and flow history via a CNN branch, trained on 841 high-fidelity simulations. On held-out shapes, it attains sim 5% relative L2 single-step error and up to 1000X speedups over CFD. We provide physics-centric rollout diagnostics, including phase error at probes and divergence norms, to quantify long-horizon fidelity. These reveal accurate near-term transients but error accumulation in fine-scale wakes, most pronounced for sharp-cornered geometries. We analyze failure modes and outline practical mitigations. Code, splits, and scripts are openly released at: https://github.com/baskargroup/TimeDependent-DeepONet to support reproducibility and benchmarking.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ High computational cost and meshing effort of high-fidelity CFD for unsteady, incompressible flows over complex shapes, hindering large design sweeps and real-time inference<br>â€¢ Need for geometry-generalizing surrogates that work across parametric and non-parametric shapes and resolutions<br>â€¢ Autoregressive error accumulation causing unstable long-horizon rollouts in time-dependent predictions<br>â€¢ Limitations of existing methods: geometry-aware operators (point clouds/meshes/graphs) often lack explicit temporal history exploitation; temporal operators assume fixed grids or weak geometry conditioning, reducing robustness across diverse shapes</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Extend Geometric DeepONet to unsteady 2D flows by encoding implicit geometry via a signed distance field in the trunk and recent velocity history via a lightweight CNN in the branch, fused to predict next-step velocities. Assess single-step and rollout fidelity using physics-centric diagnostics (probe phase error and divergence norms) across varied shape families.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Divergence-Constrained Operator Networks for Stable Long-Horizon Unsteady Flow Prediction: Enforce incompressibility via hard/soft divergence penalties and rollout-aware training to reduce drift and preserve physical consistency.<br>â€¢ Corner-Resilient Geometry Encodings for Neural Operators Using Augmented SDFs and Boundary Physics: Design corner-sensitive SDFs and boundary-aware losses/conditioning to better capture sharp edges and mitigate wake errors around non-smooth geometries.<br>â€¢ History-Adaptive Operator Models: Learning Optimal Temporal Context for Robust Wake Dynamics: Develop attention-based or adaptive-history encoders that select and weight past frames to improve phase fidelity and reduce error accumulation in fine-scale wakes.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08923" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08923" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ MLLMs often produce different answers for semantically identical content depending on whether it is provided as text, image, or mixed input, undermining reliability in multimodal applications.<br>â€¢ Existing evaluations frequently conflate OCR failures with reasoning inconsistency and often test single models, lacking a systematic benchmark that controls readability and pretraining exposure.<br>â€¢ Practical trends (e.g., rendering text as images to reduce token costs) lack evidence that reasoning on rendered text matches native text, risking modality-driven errors.<br>â€¢ The modality gapâ€”text and image embeddings occupying distinct regions in joint spaceâ€”remains insufficiently linked to behavioral inconsistency and needs mechanistic assessment.<br>â€¢ The effect of visual characteristics (resolution, color, font) and vision token budgets on reasoning is underexplored, even when OCR is correct.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce REST and REST+, benchmarks that present identical semantic content in text, image, and mixed formats and evaluate 15 MLLMs under OCR-controlled conditions using SOEBENCH and established tasks. They compute a cross-modal consistency score, systematically vary visual properties (font, resolution, color, vision tokens), and correlate consistency with cross-modal cosine similarity of internal representations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Consistency-Regularized Training for Multimodal LLMs: Add objectives that penalize cross-modal output divergence and align embeddings across modalities to reduce the modality gap.<br>â€¢ Adaptive Rendering and Vision Tokenization for Text-Heavy Inputs: Learn policies to choose resolution, color, and vision token budgets that minimize cost while preserving cross-modal reasoning consistency.<br>â€¢ REST-X: Scaling Cross-Modal Consistency Benchmarks to Multilingual, Document, and Real-World Tasks: Extend REST/REST+ to diverse domains and languages to stress-test consistency under broader conditions.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.08309" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.08309" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Procedural noise (e.g., Perlin/Simplex) provides infinite, seed-consistent, random-access generation but lacks realism, hierarchical organization, and large-scale geographic coherence.<br>â€¢ Learned terrain models (GANs/diffusion) typically operate on bounded canvases or tiles, losing infinite extensibility, seed-consistency, constant-time random access, and real-time performance needed for interactive worlds.<br>â€¢ Existing large-scale/infinite approaches (e.g., standard MultiDiffusion, tile blending, order-dependent growth) either require finite canvases, sacrifice order invariance and seed consistency, or anchor global structure in procedural kernels rather than learned context, preventing coherent planet-scale synthesis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Terrain Diffusion introduces InfiniteDiffusionâ€”an order-invariant, seed-consistent extension of MultiDiffusion to unbounded domainsâ€”paired with a hierarchical planetary-to-local diffusion stack and a compact Laplacian elevation encoding. An infinite-tensor streaming framework and few-step consistency distillation enable real-time, constant-time random-access generation of coherent, infinite terrain on consumer GPUs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Physically-Consistent Hydrology and Erosion in InfiniteDiffusion Worlds: Integrate differentiable river networks, drainage basins, and erosion processes to produce time-evolving landforms while preserving seed-consistency and random access.<br>â€¢ Multimodal Control of Planet-Scale Terrain via Geologic and Climatic Priors: Condition the hierarchical diffusion stack on plate tectonics, lithology, and climate normals to steer continental layouts, mountain belts, and biome distributions with user-controllable priors.<br>â€¢ 3D Volumetric Terrain Diffusion for Materials and Vegetation Layers: Extend from 2D elevation fields to layered 3D voxels (rock/soil/water/vegetation) to support physically based rendering, gameplay mechanics, and environmental simulation at planetary scale.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 9</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-12-10 23:08:09 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 9;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>