<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 13, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 13, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.08892" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.08892" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Generalist agents cannot yet complete hours-long missions in real time within open-ended 3D worlds; prior RL agents excel only in closed, single-goal environments with shaped rewards.<br>â€¢ Existing LLM/VLM agents struggle with precise, high-frequency keyboardâ€“mouse control; common GUI abstractions ignore relative mouse dynamics and fine-grained key states, limiting applicability to games.<br>â€¢ Real-time operation is hindered by high latency; stepwise ReAct-style reasoning at every step is computationally costly and brittle for continuous control.<br>â€¢ Multimodal perception and long-horizon memory are underdeveloped; most VLA models are reactive and lack historical context, reducing temporal coherence and planning ability.<br>â€¢ Commercial games provide no APIs, have asynchronous dynamics, and hide internal states/rewards, making standard RL pipelines impractical; a unified, scalable, human-interface-based recipe is needed.<br>â€¢ Cross-game generalization is weak; agents often overfit to a single environment and fail to transfer across distinct worlds and interaction dynamics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Lumine is a 7B VLM-based visionâ€“languageâ€“action agent (built on Qwen2-VL-7B) that processes raw pixels at 5 Hz, uses hybrid thinking to optionally produce inner-monologue reasoning, and outputs concise textual keyboardâ€“mouse actions at 30 Hz via action chunking. A three-stage curriculum (1731 h gameplay pretraining, 200 h instruction following, 15 h reasoning), context-as-memory (short-term recent steps and preserved reasoning), and end-to-end latency optimizations (25.3Ã— reduction) enable real-time, long-horizon control and strong zero-shot cross-game transfer.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning to Trigger Thought: Adaptive Reasoning Policies for Real-Time VLA Agents: Optimize when and how the agent enters thinking mode to balance latency and decision quality, via supervision or reinforcement with performanceâ€“latency trade-offs.<br>â€¢ Scaling Context-as-Memory: Retrieval-Augmented Long-Horizon Control in Open Worlds: Extend Lumineâ€™s context window with retrieval/summarization mechanisms to sustain multi-hour coherence and improve planning across missions.<br>â€¢ Unified Human-Interface Action Spaces for Cross-Environment Transfer: Systematically study training on one game and zero-shot transfer to many, leveraging standardized keyboardâ€“mouse action modeling and curricula for robust generalization.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MADD: Multi-Agent Drug Discovery Orchestra</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.08217" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.08217" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Hit identification is a central challenge in early drug discovery, traditionally requiring substantial experimental resources. Recent advances in artificial intelligence, particularly large language models (LLMs), have enabled virtual screening methods that reduce costs and improve efficiency. However, the growing complexity of these tools has limited their accessibility to wet-lab researchers. Multi-agent systems offer a promising solution by combining the interpretability of LLMs with the precision of specialized models and tools. In this work, we present MADD, a multi-agent system that builds and executes customized hit identification pipelines from natural language queries. MADD employs four coordinated agents to handle key subtasks in de novo compound generation and screening. We evaluate MADD across seven drug discovery cases and demonstrate its superior performance compared to existing LLM-based solutions. Using MADD, we pioneer the application of AI-first drug design to five biological targets and release the identified hit molecules. Finally, we introduce a new benchmark of query-molecule pairs and docking scores for over three million compounds to contribute to the agentic future of drug design.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Hit identification in early drug discovery is expensive and resourceâ€‘intensive; scalable virtual screening is needed to cut cost and time.<br>â€¢ Existing AI/LLM tools have grown complex, limiting accessibility for wetâ€‘lab researchers to test hypotheses endâ€‘toâ€‘end.<br>â€¢ Pure LLMs are not competitive for targetâ€‘specific molecule generation and property prediction required in early discovery.<br>â€¢ Integrating many specialized tools and automatically assembling effective, targetâ€‘aware pipelines remains challenging.<br>â€¢ Current agentic systems do not robustly automate the full hit discovery workflow from naturalâ€‘language queries to prioritized hits.<br>â€¢ There is a lack of accessible, comprehensive benchmarks/datasets covering the full pipeline for training and fair evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MADD is a fourâ€‘agent, LLMâ€‘orchestrated system that turns naturalâ€‘language queries into targetâ€‘adaptive hitâ€‘identification pipelines, coordinating de novo molecule generation, dockingâ€‘based affinity scoring, physicochemical and synthesizability filtering, and prioritization via specialized tools/models. It is validated across seven cases and accompanied by an open benchmark of queryâ€“molecule pairs and >3M docking scores.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ AutoPlanner: Learning Adaptive Multiâ€‘Agent Orchestration for Endâ€‘toâ€‘End Hit Discovery: Train planning/policy modules (e.g., RL/metaâ€‘learning) to select tools and pipeline branches per target, constraints, and feedback to maximize hit quality.<br>â€¢ WetLabâ€‘RL: Closing the Loop Between Agentic Design and Experimental Feedback: Integrate active learning from assay data to continuously update generators/scorers and improve calibration of docking and property models.<br>â€¢ MADDâ€‘Bench++: A Comprehensive Multiâ€‘Objective Benchmark for Agentic Drug Design: Expand the dataset with standardized protocols, ADMET/profiling labels, synthesis/cost/robustness metrics, and strong open baselines for fair comparison.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.08633" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.08633" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit's use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: https://time-to-move.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Diffusion-based video generators lack precise, user-controllable motion when conditioned only on text or a single image, limiting practical editing and directing capabilities.<br>â€¢ Existing motion-conditioned methods typically require model-specific fine-tuning, which is computationally expensive, slows iteration, and restricts portability across backbones.<br>â€¢ Creators need intuitive and low-effort ways (e.g., cut-and-drag, depth-based reprojection) to specify motion while preserving source appearance.<br>â€¢ Text-only prompting cannot guarantee pixel-level appearance fidelity, motivating methods that jointly control motion and appearance without sacrificing natural dynamics.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Time-to-Move (TTM) is a training-free, plug-and-play sampling approach for image-to-video diffusion that treats crude reference animations as coarse motion cues and uses image conditioning to preserve appearance. A dual-clock denoising schedule enforces strong alignment within motion-specified regions while allowing flexible synthesis elsewhere, achieving precise motion control with no extra training or runtime overhead.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Dual-Clock Priors for Motion-Controlled Video Diffusion: Train a lightweight policy to adaptively decide region-wise alignment strength and timing, improving robustness over fixed schedules.<br>â€¢ 3D-Aware Time-to-Move: Geometry-Guided Camera and Object Motion Control Without Training: Incorporate monocular depth/scene flow and pose cues to better handle complex camera trajectories, parallax, and occlusions.<br>â€¢ Interactive Multimodal Motion Prompting for Video Diffusion: Combine cut-and-drag, sketches, and text with TTM for real-time, user-in-the-loop motion and appearance editing.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">TiDAR: Think in Diffusion, Talk in Autoregression</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.08923" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.08923" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Autoregressive (AR) decoding is memory-bound and generates one token per step, underutilizing GPU compute density and limiting throughput, especially at small batch sizes.<br>â€¢ Diffusion LLMs enable parallel token generation but suffer quality degradation when decoding multiple tokens per step due to intra-step independence assumptions and weaker conditioning; many also lack exact KV cache support.<br>â€¢ Existing speculative decoding frameworks either rely on weaker, separate drafters (low acceptance, limited speedup) or impose AR-like sequential logic on diffusion (losing parallelism and still degrading quality).<br>â€¢ There is a need for a single, serving-friendly model that simultaneously achieves AR-level quality and diffusion-level parallelism by exploiting "free token slots" without adding significant overhead.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>TiDAR uses a hybrid attention architecture to compute diffusion-based parallel drafts and AR-based verification/sampling within a single forward pass, partitioning the sequence into prefix, last-step drafts, and next-step pre-drafts and reusing exact KV cache. Structured masks enable simultaneous diffusion drafting and AR rejection sampling, leveraging free token slots to increase throughput while preserving AR-level quality.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive TiDAR: Learning to Schedule Draft and Verify Lengths for Optimal Throughputâ€“Quality Trade-offs: Develop a controller that dynamically adjusts the number of pre-drafted and verified tokens per step based on context entropy, cache state, and hardware profiling.<br>â€¢ Multimodal TiDAR: Extending Dual-Mode Draft-and-Talk to Visionâ€“Language Generation: Integrate the TiDAR masking and dual-loss training into multimodal transformers to parallelize text and visual token drafting while maintaining AR-level coherence.<br>â€¢ Theoretical Foundations of Dual-Mode Generation: Consistency Bounds Between Diffusion Marginals and AR Chains in TiDAR: Provide formal analyses of acceptance rates, bias, and sequence-level coherence when combining diffusion marginals with AR rejection sampling, guiding mask and loss design.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.09148" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.09148" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Static, non-interactive data pipelines decouple data synthesis from training, wasting capacity on easy cases and neglecting hard, underrepresented tool-use scenarios.<br>â€¢ Synthetic tool-call datasets often contain noisy or incorrect annotations (e.g., wrong arguments, incomplete calls), and current pipelines lack mechanisms to detect and correct these labels.<br>â€¢ Heavy reliance on costly closed-source APIs for data generation and judging limits scalability; open-source substitutions frequently reduce data quality without robust verification.<br>â€¢ Absence of a model-aware, iterative curriculum prevents targeted remediation of failure modes and systematic expansion of challenging, diverse tool-use cases.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>LoopTool is a closed-loop, model-aware framework that interleaves GRPO training with three modulesâ€”Greedy Capability Probing to diagnose mastered vs. failed cases via greedy decoding, Judge-Guided Label Verification using an open-source judge (Qwen3-32B) to correct noisy labels, and Error-Driven Data Expansion to synthesize diverse hard examples from failuresâ€”starting from an automated tool-augmented seed corpus. This fully open-source pipeline iteratively purifies data and targets weaknesses, enabling an 8B model to surpass its 32B data generator on tool-use benchmarks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Multi-Judge Consensus for Label Verification in Tool-Use Training: Combine multiple open-source judges with confidence calibration to further reduce label noise and bias beyond a single-judge setup.<br>â€¢ Curriculum-Guided Error-Driven Data Expansion for Multi-Step Tool Planning: Integrate difficulty estimation and planning-depth control into EDDE to generate curricula for complex, multi-hop tool-call sequences.<br>â€¢ Self-Play Teacher Amplification via Execution-Grounded Feedback for Tool Use: Replace or augment external judging with self-consistency and runtime execution checks to provide scalable, environment-grounded supervision.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">WMPO: World Model-based Policy Optimization for Vision-Language-Action Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.09515" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.09515" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the "imagined" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Imitation learningâ€“trained VLA policies are brittle, cannot learn from failures, and suffer compounding errors when off-distribution<br>â€¢ Real-world on-policy RL is impractical for VLA due to extreme sample inefficiency, safety concerns, and engineering overhead; accurate simulators are hard to build<br>â€¢ Latent-space world models mismatch VLAâ€™s pixel-level, web-scale visual pretraining, hindering effective integration for policy optimization<br>â€¢ Short-horizon predictions and hand-shaped rewards invite reward hacking and misalignment; reliable outcome-based rewards need full, long-horizon rollouts<br>â€¢ Learned world models trained on expert demonstrations struggle to reproduce policy rollouts and failures; long-horizon generation suffers from visual drift and actionâ€“frame misalignment, making on-policy RL difficult</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>WMPO trains a VLA policy via on-policy GRPO entirely on imagined trajectories from an action-conditioned, pixel-space video diffusion world model pretrained on large robot datasets and fine-tuned with policy rollouts (policy behavior alignment). It adds noisy-frame conditioning, frame-level action control, and a lightweight clip-based success classifier to enable stable long-horizon generation and sparse outcome rewards without real-world interaction.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Bridging WMPO to POMDPs with Multimodal State Estimation: Extend WMPO to handle partial observability by integrating proprioception, depth, and tactile inputs with belief-state world modeling<br>â€¢ Uncertainty-Aware WMPO: Risk-Bounded Policy Optimization under Model Bias: Incorporate model uncertainty (ensembles or diffusion variance) into GRPO to avoid exploitation of world-model errors and ensure robust policies<br>â€¢ Lifelong Co-Training at Scale: Automated Curriculum for World Models and VLA Policies: Develop an active, alternating update framework that continuously collects behavior data, refines the world model, and scales WMPO across diverse tasks and environments</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.06805" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.06805" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language answering tasks. Despite their strengths, these models often encounter challenges in achieving complex reasoning tasks such as mathematical problem-solving. Previous works have focused on fine-tuning on specialized mathematical datasets. However, these datasets are typically distilled directly from teacher models, which capture only static reasoning patterns and leaving substantial gaps compared to student models. This reliance on fixed teacher-derived datasets not only restricts the model's ability to adapt to novel or more intricate questions that extend beyond the confines of the training data, but also lacks the iterative depth needed for robust generalization. To overcome these limitations, we propose \method, a Mathematical Self-Evolving framework for MLLMs. In contrast to traditional one-shot fine-tuning paradigms, \method iteratively refines the model through cycles of inference, reflection, and reward-based feedback. Specifically, we leverage iterative fine-tuning by incorporating correct reasoning paths derived from previous-stage inference and integrating reflections from a specialized Outcome Reward Model (ORM). To verify the effectiveness of \method, we evaluate it on a suite of challenging benchmarks, demonstrating significant performance gains over backbone models. Notably, our experimental results on MathVL-test surpass the leading open-source multimodal mathematical reasoning model QVQ. Our code and models are available at https://zheny2751\allowbreak-dotcom.github.io/\allowbreak MathSE.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Multimodal LLMs excel at vision-language tasks but struggle with complex, step-wise mathematical reasoning.<br>â€¢ Existing fine-tuning relies on static, teacher-distilled CoT datasets that lack iterative depth and adaptability to novel or harder problems.<br>â€¢ There is a gap between teacher-derived reasoning patterns and the student modelâ€™s evolving inference distribution, limiting generalization.<br>â€¢ Current approaches often evaluate only final answers, lacking mechanisms to detect and correct errors within full reasoning trajectories.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MathSE is a self-evolving framework that iteratively fine-tunes MLLMs via cycles of inference, step-level evaluation with an Outcome Reward Model (ORM), and GPT-4o-based reflection to repair flawed reasoning paths; correct and refined trajectories are then used for reward-guided fine-tuning. This loop progressively aligns training data with the modelâ€™s own reasoning behavior, improving multimodal mathematical problem-solving.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Beyond Teacher Distillation: Fully Self-Evolving Multimodal Reasoning without External LLMs: Remove GPT-4o dependence by training internal ORM and reflection modules, using RL with ORM-derived rewards.<br>â€¢ Generalized Outcome Reward Models for STEM Diagrams and Tables: Extend ORM to multi-task STEM domains, enabling step-wise validation across diverse visual-text formats (diagrams, tables, plots).<br>â€¢ Curriculum-Guided Iterative Reflection for Multimodal MathSE: Develop adaptive difficulty scheduling that selects problems by uncertainty/gain to optimize each self-evolving stage.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.06251" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.06251" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>User interface (UI) development requires translating design mockups into functional code, a process that remains repetitive and labor-intensive. While recent Vision-Language Models (VLMs) automate UI-to-Code generation, they generate only static HTML/CSS/JavaScript layouts lacking interactivity. To address this, we propose WebVIA, the first agentic framework for interactive UI-to-Code generation and validation. The framework comprises three components: 1) an exploration agent to capture multi-state UI screenshots; 2) a UI2Code model that generates executable interactive code; 3) a validation module that verifies the interactivity. Experiments demonstrate that WebVIA-Agent achieves more stable and accurate UI exploration than general-purpose agents (e.g., Gemini-2.5-Pro). In addition, our fine-tuned WebVIA-UI2Code models exhibit substantial improvements in generating executable and interactive HTML/CSS/JavaScript code, outperforming their base counterparts across both interactive and static UI2Code benchmarks. Our code and models are available at https://zheny2751-dotcom.github.io/webvia.github.io/{https://webvia.github.io}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing VLM-powered UI-to-Code systems mostly generate static HTML/CSS/JavaScript, lacking real GUI interactions (click, input, selection, state transitions), making them unusable in practical workflows.<br>â€¢ There is no systematic agentic exploration to capture multi-state UI behavior; general-purpose web agents are unstable and inaccurate for discovering and grounding interactive elements.<br>â€¢ Generated code is rarely behavior-verified; current evaluations emphasize visual fidelity or instruction following instead of behavior-grounded interactivity and executable correctness.<br>â€¢ Interactive benchmarks often require manual annotations and pre-specified commands, limiting scalability and realism; evaluation paradigms diverge from actual user interaction flows.<br>â€¢ Real webpages are noisy and unstable (ads, async loading), hindering reproducible training/evaluation, highlighting the need for controlled synthetic environments that still cover diverse interaction patterns.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>WebVIA is an agentic pipeline that explores webpages to capture multi-state screenshots/DOM and construct an interaction graph, trains VLM-based UI2Code models to emit executable interactive HTML/CSS/JavaScript, and then validates interactivity by executing the code and checking the learned transitions in a controlled WebEnv. The framework also introduces a scalable, flow-based benchmark (UIFlow2Code) for behavior-grounded assessment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ RL-Driven Verifiable UI2Code with Verifier-in-the-Loop: Jointly train the exploration agent and the UI2Code generator using validation feedback as reward to optimize for executable, behavior-correct code.<br>â€¢ Cross-Framework Interactive UI Synthesis: Extend WebVIA from vanilla HTML/JS to component frameworks (React/Vue/Svelte) with state management, event handlers, and reusable componentization.<br>â€¢ Self-Supervised Interaction Discovery at Web Scale: Develop an active-learning web agent that autonomously mines, denoises, and labels multi-step interaction flows from real sites, augmenting synthetic environments for robust training.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07499" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07499" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion models have demonstrated strong generative performance when using guidance methods such as classifier-free guidance (CFG), which enhance output quality by modifying the sampling trajectory. These methods typically improve a target output by intentionally degrading another, often the unconditional output, using heuristic perturbation functions such as identity mixing or blurred conditions. However, these approaches lack a principled foundation and rely on manually designed distortions. In this work, we propose Adversarial Sinkhorn Attention Guidance (ASAG), a novel method that reinterprets attention scores in diffusion models through the lens of optimal transport and intentionally disrupt the transport cost via Sinkhorn algorithm. Instead of naively corrupting the attention mechanism, ASAG injects an adversarial cost within self-attention layers to reduce pixel-wise similarity between queries and keys. This deliberate degradation weakens misleading attention alignments and leads to improved conditional and unconditional sample quality. ASAG shows consistent improvements in text-to-image diffusion, and enhances controllability and fidelity in downstream applications such as IP-Adapter and ControlNet. The method is lightweight, plug-and-play, and improves reliability without requiring any model retraining.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Guidance sampling in diffusion models lacks a principled foundation, relying on heuristics (e.g., identity masks, Gaussian blurs) or poorly trained auxiliary models to create "undesirable" paths, with no clear notion of optimal perturbation.<br>â€¢ Classifier-Free Guidance (CFG) and related methods are sensitive to guidance scales, can degrade output quality, and often require additional training; attention misalignments during sampling further reduce reliability and controllability.<br>â€¢ There is a need for a lightweight, plug-and-play, theoretically grounded approach that improves fidelity, text-image alignment, and robustness across unconditional/conditional generation and downstream frameworks (ControlNet, IP-Adapter) with minimal overhead.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>ASAG casts self-attention as an entropy-regularized optimal transport problem and applies adversarial Sinkhorn updates with cost Mâ†“=QKâŠ¤ to minimize queryâ€“key similarity, producing high-entropy, doubly-stochastic attention that weakens misleading alignments. The resulting weaker score enters a contrastive guidance rule Ïµâ€²=Ïµ+s(Ïµâˆ’ËœÏµ), yielding reliable improvements without retraining.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Layer-wise Adversarial Sinkhorn Guidance for Diffusion Sampling: Automatically select attention layers, Î», and iteration counts per timestep/prompt to optimize fidelityâ€“controllability trade-offs under compute budgets.<br>â€¢ Provable Reliability of OT-based Adversarial Guidance in Diffusion Models: Derive convergence and stability bounds for ASAG, quantifying effects of Î» and guidance scale s on FID, alignment metrics, and interaction with CFG/ControlNet.<br>â€¢ Multimodal ASAG for Cross-Attention in Textâ€“Imageâ€“Video Generation: Extend adversarial OT guidance to cross-attention across modalities and time, improving temporal consistency and controllability in video and 3D generative tasks.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Adapting Web Agents with Synthetic Supervision</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.06101" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.06101" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Web agents struggle to adapt to new websites due to the scarcity of environment specific tasks and demonstrations. Recent works have explored synthetic data generation to address this challenge, however, they suffer from data quality issues where synthesized tasks contain hallucinations that cannot be executed, and collected trajectories are noisy with redundant or misaligned actions. In this paper, we propose SynthAgent, a fully synthetic supervision framework that aims at improving synthetic data quality via dual refinement of both tasks and trajectories. Our approach begins by synthesizing diverse tasks through categorized exploration of web elements, ensuring efficient coverage of the target environment. During trajectory collection, we refine tasks when conflicts with actual observations are detected, mitigating hallucinations while maintaining task consistency. After collection, we conduct trajectory refinement with a global context to mitigate potential noise or misalignments. Finally, we fine-tune open-source web agents on the refined synthetic data to adapt them to the target environment. Experimental results demonstrate that SynthAgent outperforms existing synthetic data methods, validating the importance of high-quality synthetic supervision. The code will be publicly available at https://github.com/aiming-lab/SynthAgent.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Web agents struggle to adapt to unseen websites due to scarce environment-specific tasks and demonstrations, while human collection is costly and non-scalable.<br>â€¢ Existing synthetic data methods suffer from low quality: hallucinated or underspecified tasks that cannot be executed, and noisy, redundant, or misaligned trajectories.<br>â€¢ Random/surface-level exploration leads to poor coverage and diversity; offline tutorials can be outdated and world-model simulations introduce additional hallucinations.<br>â€¢ Prior practices risk test task leakage in training, and RL-based approaches are expensive and impractical for complex, realistic websites.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SynthAgent is a fully synthetic supervision framework with dual refinement: it generates diverse, environment-aware tasks via categorized exploration, refines tasks online during trajectory collection using conflict predicates (ExistsUI, MissingArgs, Stall) and evidence-driven principles, applies post-hoc trajectory edits (remove, reorder, drop, keep) for global consistency, and fine-tunes open-source web agents on the refined dataset.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Conflict-Aware Task Refinement Policies for Web Agents: Train lightweight detectors and policies to automatically identify and resolve taskâ€“environment conflicts (e.g., missing UI, missing arguments, stalls) without LLM prompting.<br>â€¢ Structured Exploration via Functional Element Graphs for Synthetic Web Task Generation: Construct and leverage functional graphs of categorized elements and transitions to optimize coverage and diversity in synthetic task synthesis across deep websites.<br>â€¢ Quality Metrics and Automated Editors for Synthetic Trajectories in Web Agents: Define reliability metrics and develop trainable edit models that programmatically clean trajectories (remove, reorder, drop) at scale to reduce noise and misalignment.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Motif 2 12.7B technical report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07464" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07464" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Achieve competitive LLM capabilities under constrained compute and moderate parameter counts, pushing the efficiency frontier without relying on massive models.<br>â€¢ Improve attention representational efficiency by disentangling signal amplification from noise suppression, addressing limitations of standard attention that mixes these roles.<br>â€¢ Remove training system bottlenecks in large-batch, long-sequence regimes (e.g., redundant full-matrix computations in Distributed Muon, memory-bound activations) to increase throughput and reduce memory.<br>â€¢ Design data-efficient pretraining and post-training that balance general fluency with structured reasoning and code without heavy RL or extreme token budgets.<br>â€¢ Provide an open-weight, transparently trained baseline for scalable attention and efficient reasoning, addressing scarcity of such models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Scale Motif-2.6B to 12.7B via width-preserving hypercloning and depth expansion, integrate Grouped Differential Attention to separate signal and noise-control head groups, and train with a curriculum on 5.5T tokens using MuonClip plus system-level optimizations (fused PolyNorm kernels and a Parallel Muon optimizer); finalize with a three-stage supervised fine-tuning pipeline to enhance instruction following and reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Grouped Differential Attention: Learnable head grouping and dynamic signal/noise ratios that adapt to task and context to maximize representational efficiency.<br>â€¢ Token-Efficient Curriculum Schedulers for Mid-Scale LLMs: Systematic ablations of mixture schedules, reasoning annealing, and sequence-length growth to minimize tokens while preserving performance.<br>â€¢ Auto-Tuned Parallel Muon for Hybrid 3D/4D Parallelism: Extend pipelined all-to-all Muon with automated chunk-size and load-balancing policies across TP+HSDP/FSDP to optimize throughput and memory at scale.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Agentic Refactoring: An Empirical Study of AI Coding Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.04824" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.04824" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Agentic coding tools, such as OpenAI Codex, Claude Code, and Cursor, are transforming the software engineering landscape. These AI-powered systems function as autonomous teammates capable of planning and executing complex development tasks. Agents have become active participants in refactoring, a cornerstone of sustainable software development aimed at improving internal code quality without altering observable behavior. Despite their increasing adoption, there is a critical lack of empirical understanding regarding how agentic refactoring is utilized in practice, how it compares to human-driven refactoring, and what impact it has on code quality. To address this empirical gap, we present a large-scale study of AI agent-generated refactorings in real-world open-source Java projects, analyzing 15,451 refactoring instances across 12,256 pull requests and 14,988 commits derived from the AIDev dataset. Our empirical analysis shows that refactoring is a common and intentional activity in this development paradigm, with agents explicitly targeting refactoring in 26.1% of commits. Analysis of refactoring types reveals that agentic efforts are dominated by low-level, consistency-oriented edits, such as Change Variable Type (11.8%), Rename Parameter (10.4%), and Rename Variable (8.5%), reflecting a preference for localized improvements over the high-level design changes common in human refactoring. Additionally, the motivations behind agentic refactoring focus overwhelmingly on internal quality concerns, with maintainability (52.5%) and readability (28.1%). Furthermore, quantitative evaluation of code quality metrics shows that agentic refactoring yields small but statistically significant improvements in structural metrics, particularly for medium-level changes, reducing class size and complexity (e.g., Class LOC median Î” = -15.25).</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Absence of a large-scale empirical baseline on agentic refactoring (frequency, types, intent, and code-quality impact) in real-world projects<br>â€¢ Need to compare agentic vs. human refactoring to assess current agent capabilities and maturity<br>â€¢ Practitioners lack evidence on whether agentic refactoring delivers structural maintainability gains or mostly cosmetic cleanups<br>â€¢ Existing LLM studies focus on prompt-based generation or bug fixing; they do not capture autonomous (agentic) refactoring workflows<br>â€¢ Traditional refactoring tools are underused and there are no benchmarks for high-level (architectural) refactorings; smell reduction effects remain unclear<br>â€¢ Implicit (tangled) refactoring within feature/bug-fix commits may increase review burden and erode trust, but this cost is unquantified</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Large-scale empirical mining of AIDev agentic PRs/commits in Java; detect 103 refactoring types with RefactoringMiner, label agentic refactoring via self-affirmed intent keywords, classify refactoring abstraction levels, infer refactoring purposes using GPT-4.1-mini with human validation, and assess beforeâ€“after code-quality changes (metrics and smells) via DesigniteJava with non-parametric statistical tests and FDR correction.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Benchmarking High-Level Agentic Refactoring: A Gold-Standard Dataset and Evaluation Protocol: Curate architectural refactoring cases with behavior-preserving tests and target post-conditions to systematically evaluate agent performance.<br>â€¢ Untangling Agentic Commits: Quantifying the Review Cost of Implicit Refactoring: Measure review effort, merge latency, revert rates, and trust signals for agentic commits that mix feature changes with refactoring.<br>â€¢ Smell-Aware Planning for Coding Agents: Integrating Static Analysis into Autonomous Refactoring Loops: Integrate tools like DesigniteJava via MCP to let agents detect smells, set structural targets (e.g., reduce WMC/fan-out), and iteratively plan/validate refactorings.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">Stemming Hallucination in Language Models Using a Licensing Oracle</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.06073" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.06073" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Language models exhibit remarkable natural language generation capabilities but remain prone to hallucinations, generating factually incorrect information despite producing syntactically coherent responses. This study introduces the Licensing Oracle, an architectural solution designed to stem hallucinations in LMs by enforcing truth constraints through formal validation against structured knowledge graphs. Unlike statistical approaches that rely on data scaling or fine-tuning, the Licensing Oracle embeds a deterministic validation step into the model's generative process, ensuring that only factually accurate claims are made. We evaluated the effectiveness of the Licensing Oracle through experiments comparing it with several state-of-the-art methods, including baseline language model generation, fine-tuning for factual recall, fine-tuning for abstention behavior, and retrieval-augmented generation (RAG). Our results demonstrate that although RAG and fine-tuning improve performance, they fail to eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring that only valid claims were generated with 89.1% accuracy in factual responses. This work shows that architectural innovations, such as the Licensing Oracle, offer a necessary and sufficient solution for hallucinations in domains with structured knowledge representations, offering guarantees that statistical methods cannot match. Although the Licensing Oracle is specifically designed to address hallucinations in fact-based domains, its framework lays the groundwork for truth-constrained generation in future AI systems, providing a new path toward reliable, epistemically grounded models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLMs hallucinate because transformers optimize for linguistic coherence, not truth, lacking mechanisms to enforce epistemic constraints in fact-based tasks.<br>â€¢ Existing statistical methods (fine-tuning for recall/abstention, RAG) improve averages but cannot provide deterministic guarantees; they suffer from catastrophic forgetting, ripple effects, the fine-tuning paradox, and an epistemic mismatch (no principled abstention).<br>â€¢ Formal validation tools (knowledge graphs, SHACL) exist but are not integrated into the generative loop; there is a need for an architectural solution that couples generation to real-time, verifiable constraints with guaranteed rejection of invalid claims.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce a Licensing Oracle: a middleware that extracts model-generated factual claims into RDF triples, validates them against a knowledge graph with SHACL constraints, and only licenses validated claims for emissionâ€”otherwise deterministically abstains with provenance logging.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ From Noisy Claims to Verified Triples: Robust Extraction for Licensing Oracles: Quantify and improve claim extraction (NER/semantic parsing) fidelity, analyze its effect on abstention precision and false validations, and develop joint extractionâ€“validation pipelines.<br>â€¢ Beyond Knowledge Graphs: Truth-Constrained Generation for Unstructured Domains: Extend the Oracle to unstructured evidence by auto-formalizing retrieved text into symbolic facts and integrating neuro-symbolic validation for domains without prebuilt KGs (e.g., clinical, legal).<br>â€¢ Semantic-Constrained Decoding: Integrating SHACL at the Token Level: Embed SHACL-aware constraints directly into decoding (beam/search) to preclude unlicensed continuations, reducing latency and tightening guarantees beyond post-hoc middleware validation.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 5</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-13 23:06:16 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 5;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>