<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 25, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 25, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">General Agentic Memory Via Deep Research</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.18423" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.18423" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of "just-in time (JIT) compilation" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Static AOT (ahead-of-time) memory compresses history, causing severe information loss and failing to meet fine-grained information needs at request time.<br>‚Ä¢ Fixed, precomputed memory structures cannot flexibly adapt to ad-hoc or unforeseen queries requiring nuanced interpretation and integration.<br>‚Ä¢ Reliance on domain expertise and handcrafted heuristics limits generalization across domains and tasks.<br>‚Ä¢ AI agents must manage complex, rapidly expanding contexts from internal reasoning and external feedback; existing systems struggle to preserve and search complete histories efficiently.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>GAM adopts a JIT (just-in-time) dual-agent architecture: a Memorizer creates lightweight summaries while storing full session pages in a universal page-store, and a Researcher performs iterative deep research (planning, search, reflection) guided by the memory to retrieve and integrate task-specific information at runtime. This design leverages LLM agentic capabilities and test-time scalability, and enables end-to-end optimization via reinforcement learning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning to Memorize: Reinforcement Learning for End-to-End Optimization of Agentic Memory: Jointly optimize the Memorizer and Researcher with task-driven rewards to balance fidelity, efficiency, and downstream performance.<br>‚Ä¢ Multimodal General Agentic Memory: Extending JIT Deep Research to Text, Code, and Vision: Build multimodal page-stores and cross-modal retrieval/integration to support complex agent workflows beyond text.<br>‚Ä¢ Scalable Page-Store Indexing and Neural Retrieval for GAM: Develop hybrid symbolic‚Äìneural indexing, schemas, and approximate nearest neighbor retrieval to accelerate deep research over very large histories.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19304" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19304" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Cross-environment agent learning is largely unmeasured due to the absence of a standardized, extensible set of heterogeneous environments with controllable rule distributions (rewards, observations, transitions).<br>‚Ä¢ There is no unified representation of agent learning procedures, making prompt/code/model updates hard to compare, reuse, and scale across settings.<br>‚Ä¢ Existing agentic learning primarily reports gains within single domains, implicitly assuming fixed environment distributions and failing to generalize across diverse worlds.<br>‚Ä¢ Automated environment construction efforts mostly scale data within fixed applications or rely on simulators, not on creating new environments with diverse rule systems; human-built environments are costly and narrow.<br>‚Ä¢ Without this infrastructure, we cannot systematically assess how different learning strategies perform or scale as environment diversity increases.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>AutoEnv treats environments as factorizable distributions over transitions, observations, and rewards, and uses layered abstractions with coding agents to automatically synthesize and validate diverse worlds at low cost, yielding AutoEnv-36 (36 environments, 358 levels) where current LMs achieve 12‚Äì49% normalized reward. It also formalizes agentic learning via a component-centric Selection‚ÄìOptimization‚ÄìEvaluation framework, instantiates eight learning methods, and empirically studies fixed vs. adaptive method performance across heterogeneous environments.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Meta-Selection for Cross-Environment Agent Learning: Learn a policy that maps environment descriptors and early trajectory signals to the optimal learning method within the Selection‚ÄìOptimization‚ÄìEvaluation space.<br>‚Ä¢ AutoEnv-Curriculum: Difficulty-Aware Environment Generation for Robust Generalization: Use active, failure-driven generation to adapt the environment distribution and build curricula that target agent weaknesses across rule families.<br>‚Ä¢ Theory of Component-Centric Learning Across Heterogeneous Worlds: Provide formal analyses and bounds on when Selection, Optimization signals, and component updates transfer across environments and how gains diminish with increasing diversity.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Computer-Use Agents as Judges for Generative User Interface</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.15567" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.15567" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Current GUIs are human-centric, forcing Computer-Use Agents (CUAs) to mimic human behaviors and prioritize aesthetics over agent efficiency; coder-generated UIs remain optimized for humans rather than agents.<br>‚Ä¢ There is no scalable benchmark or reliable, automated functional verification for agent-evaluated, generated UIs across diverse applications; VLM-as-judge is biased and manual checks are impractical.<br>‚Ä¢ Raw CUA navigation trajectories are long and hard to interpret, limiting actionable feedback for UI redesign; navigation remains a key bottleneck even when functionality exists.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce AUI-Gym and a Coder‚ÄìCUA collaboration where Coders generate and iteratively revise UIs while CUAs act as judges via task-solvability and navigation feedback distilled through a single-image CUA Dashboard; a GPT-5 verifier synthesizes per-task rule-based checkers, enabling measurement of Function Completeness and CUA Success Rate for feedback-driven redesign.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Co-Evolving Designers and Judges: Joint Training of Coders and Computer-Use Agents for Agent-Native UIs: Formalize and optimize the Markov Design Process to co-train coders and CUAs, aligning UI generation and navigation policies for maximal agent success.<br>‚Ä¢ Beyond the Web: Agent-Native Interface Generation for Desktop, Mobile, and Multi-Window Systems: Extend AUI-Gym to OS-level environments with multi-window workflows, richer modalities, and accessibility layers to assess agent-centric design at scale.<br>‚Ä¢ Learning Robust Verifiers and Metrics for Agent Usability: Develop learned or hybrid verifiers resilient to DOM changes and dynamics, and introduce standardized agent-usability metrics beyond Function Completeness and Success Rate.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19365" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19365" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Pixel diffusion models force a single DiT to jointly learn high-frequency details and low-frequency semantics, leading to slow training/inference and degraded semantic modeling, hindering performance relative to latent diffusion.<br>‚Ä¢ Transformers are adept at low-frequency semantics but struggle with high-frequency noise; existing multi-scale/cascaded pixel diffusion methods still entangle frequencies per timestep and rely on complex denoising schedules, reducing efficiency and quality.<br>‚Ä¢ Standard flow-matching loss treats all frequencies equally, failing to prioritize perceptually salient components and suppress insignificant high-frequency noise, which complicates optimization and harms visual fidelity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>DeCo decouples frequency modeling by using a DiT on downsampled inputs to produce low-frequency semantic conditioning, while a lightweight attention-free pixel decoder (modulated via AdaLN-Zero) reconstructs high-frequency details at full resolution. A frequency-aware flow-matching loss applies block-wise DCT (YCbCr) and JPEG-derived perceptual weights to emphasize salient frequencies, alongside standard FM and REPA alignment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Perceptual Frequency Weights for Flow Matching in Pixel Diffusion: Replace fixed JPEG priors with learned, data-adaptive frequency weighting to further improve fidelity and convergence.<br>‚Ä¢ DeCo-V: Frequency-Decoupled Pixel Diffusion for Video Generation with Spatio-Temporal Frequency Conditioning: Extend DeCo to video by decoupling spatial and temporal frequencies using 3D DCT/motion-aware conditioning.<br>‚Ä¢ Dynamic Frequency Routing in Hybrid Latent‚ÄìPixel Diffusion: A router that assigns frequency bands between a latent generator and pixel decoder to balance efficiency and detail, enabling end-to-end hybrid diffusion.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19399" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19399" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Long-form deep research is hard to evaluate; static rubrics underspecify quality dimensions and miss nuanced, instance-specific requirements.<br>‚Ä¢ Most open deep research systems are trained via RLVR on short-form, easily verifiable QA, which does not teach multi-step synthesis, attribution, and citation-heavy reasoning.<br>‚Ä¢ Closed-book or general rubrics rely on parametric LM knowledge, lack grounding in newly searched external evidence, fail to adapt to the policy‚Äôs behaviors, and are prone to reward hacking.<br>‚Ä¢ Prior agents often hard-code a single search tool and lack scalable, asynchronous tool integration; open models need cost-efficient training that matches proprietary systems while supporting accurate citations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RLER co-evolves instance-specific, externally grounded rubrics with the policy: the model samples on-policy rollouts, a rubric LM generates discriminative rubrics from those traces, a judge LM scores final answers, and GRPO optimizes the policy. DR Tulu-8B uses an MCP-based agent with think/tool/answer/cite actions to learn planning, diverse tool use, and fine-grained citation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Active Rubric Co-Evolution for Robust Long-Form Evaluation: Design active rollout selection and rubric diversity objectives to maximize discriminative power and reduce reward hacking, with stability and generalization analysis.<br>‚Ä¢ Citation-Centric Rewards: Aligning Claims to Fine-Grained Evidence in Deep Research Agents: Integrate snippet-level citation verification into RLER to reward localized, accurate attribution and improve factuality across domains.<br>‚Ä¢ Meta-Tool Selection Policies for Domain-Adaptive Deep Research: Learn meta-policies that adaptively choose and parameterize heterogeneous tools (web, paper, medical databases) conditioned on task type to boost efficiency and accuracy.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.18050" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.18050" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Native 4K multi‚Äìaspect-ratio (AR) text-to-image generation triggers coupled failures: 2D RoPE positional drift/aliasing across resolutions/ARs, VAE compression erasing high-frequency detail, and skewed gradients across timesteps/frequency bands causing unstable optimization.<br>‚Ä¢ Training-free scaling and tiled/global‚Äìlocal decoding preserve the original positional scheme and introduce coherence gaps or AR extrapolation issues; native-4K backbones prioritize token efficiency without jointly fixing position‚Äìcompression‚Äìloss coupling.<br>‚Ä¢ Public 4K datasets are small, landscape-biased, and lack rich metadata (quality/aesthetics/IQA, subject tags), limiting AR coverage, human-centric content, and regime-targeted sampling.<br>‚Ä¢ Existing wavelet/perceptual objectives and latent SR/self-cascade adapters are post-hoc and do not resolve the fundamental VAE fidelity‚Äìthroughput trade-off or timestep imbalance at true 4K.<br>‚Ä¢ 1D RoPE extrapolation tricks (e.g., NTK/YaRN) offer little guidance for 2D token grids at 4K, leading to ghosting/striping under extreme ARs.<br>‚Ä¢ The field lacks a unified 4K framework that co-designs data, positional encoding, VAE reconstruction, frequency/SNR-aware objectives, and timestep‚Äìaesthetic curricula.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>UltraFlux co-designs data and model: it introduces MultiAspect-4K-1M (1M 4K images with balanced multi-AR coverage, bilingual captions, VLM/IQA metadata) and a Flux-based DiT enhanced with Resonance 2D RoPE + YaRN for 4K/AR-aware positional encoding, a post-trained F16 VAE decoder, an SNR-Aware Huber Wavelet objective, and Stage-wise Aesthetic Curriculum Learning that focuses high-aesthetic supervision on high-noise steps. This combination yields stable, detail-preserving native 4K generation across diverse ARs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Resonance Fields: Learning Continuous 2D Positional Spectra for Resolution- and AR-Universal Diffusion Transformers: Generalize Resonance 2D RoPE into a learnable, data-driven spectral field that adapts frequencies across resolutions/ARs end-to-end for robust multi-AR 4K extrapolation.<br>‚Ä¢ Rate‚ÄìDistortion Co-Training of VAEs and DiTs for Native 4K Generation: Jointly optimize compression and generation with content-adaptive, spatially varying downsampling and RD-aware losses to maximize high-frequency fidelity without sacrificing throughput.<br>‚Ä¢ 4K Preference Optimization with Noise-Conditioned Reinforcement Learning: Extend SACL by injecting human/VLM preference signals at high-noise timesteps and across ARs, performing multi-objective RL to improve aesthetics, semantic alignment, and photorealism at native 4K.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">In-Video Instructions: Visual Signals as Generative Control</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19401" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19401" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Text-only prompting is global and coarse, lacking explicit spatial grounding; it struggles to bind instructions to the correct objects in multi-object or cluttered scenes.<br>‚Ä¢ Existing controllable video approaches depend on special conditioning channels (e.g., depth, boxes, trajectories) and training or complex pipelines, limiting zero-shot usability and simple workflows.<br>‚Ä¢ There is a need to harness video models‚Äô emerging ability to read and reason over in-frame visual signals (text, arrows, trajectories) as actionable controls for image-to-video generation without retraining.<br>‚Ä¢ Complex tasks (multi-object, multi-step) require an interface that provides precise localization and temporal ordering, which conventional prompts cannot express unambiguously.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>In-Video Instructions overlay short textual commands and arrows directly onto the initial frame, optionally paired with a fixed global prompt (‚ÄúFollow the instructions step by step‚Äù), letting pretrained video generators interpret these annotations as part of the scene. This training-free protocol enables spatially grounded, fine-grained control of object and camera motions and sequential actions across models like Veo 3.1, Kling 2.5, and Wan 2.2.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Invisible In-Video Instructions: Stealth Visual Tags for Training-Free Video Control: Encode instructions as imperceptible or machine-readable markers to avoid visible overlays and improve robustness to occlusion.<br>‚Ä¢ BenchIVI: A Benchmark for Spatially-Grounded Controllable Video Generation: Establish quantitative metrics and datasets for localization accuracy, instruction compliance, multi-object/multi-step success, and camera control.<br>‚Ä¢ Learning to Follow Natural Visual Signals: From Traffic Signs to UI Cues: Investigate models‚Äô ability to interpret naturally occurring visual signals (e.g., traffic lights, signboards) and design training/evaluation protocols for real-world instruction following.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Budget-Aware Tool-Use Enables Effective Agent Scaling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.17006" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.17006" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Scaling test-time computation improves performance across different tasks on large language models (LLMs), which has also been extended to tool-augmented agents. For these agents, scaling involves not only "thinking" in tokens but also "acting" via tool calls. The number of tool calls directly bounds the agent's interaction with the external environment. However, we find that simply granting agents a larger tool-call budget fails to improve performance, as they lack "budget awareness" and quickly hit a performance ceiling. To address this, we study how to scale such agents effectively under explicit tool-call budgets, focusing on web search agents. We first introduce the Budget Tracker, a lightweight plug-in that provides the agent with continuous budget awareness, enabling simple yet effective scaling. We further develop BATS (Budget Aware Test-time Scaling), an advanced framework that leverages this awareness to dynamically adapt its planning and verification strategy, deciding whether to "dig deeper" on a promising lead or "pivot" to new paths based on remaining resources. To analyze cost-performance scaling in a controlled manner, we formalize a unified cost metric that jointly accounts for token and tool consumption. We provide the first systematic study on budget-constrained agents, showing that budget-aware methods produce more favorable scaling curves and push the cost-performance Pareto frontier. Our work offers empirical insights toward a more transparent and principled understanding of scaling in tool-augmented agents.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Standard tool-augmented agents lack budget awareness; simply increasing tool-call budgets does not improve performance, causing shallow search behavior and a rapid performance ceiling.<br>‚Ä¢ Existing test-time scaling largely optimizes token usage and lacks a unified, transparent cost metric that accounts for both token and tool-call economic costs, hindering fair cost‚Äìperformance comparisons.<br>‚Ä¢ Agents lack dynamic strategies to adapt planning and verification to remaining resources (when to dig deeper vs. pivot), leading to inefficient spending and unfavorable cost‚Äìperformance scaling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper formalizes a unified economic cost metric (tokens + tool calls) and introduces Budget Tracker (a plug-in providing continuous budget signals) and BATS (an adaptive framework that adjusts planning and verification to dig deeper or pivot based on remaining budget) to enable effective budget-aware tool use in search agents.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Multi-Tool Budget Allocation for Generalist Agents: Extend budget-aware planning to heterogeneous toolsets with dynamic, cross-tool budget allocation policies that optimize global cost‚Äìperformance.<br>‚Ä¢ Learning Budget-Aware Policies via Reinforcement and Preference Optimization: Train agents end-to-end to internalize budget signals (e.g., via RL or preference learning) and learn optimal act/think trade-offs under explicit economic constraints.<br>‚Ä¢ Cost-Aware Benchmarks and Pareto Evaluation for Tool-Augmented Agents: Create standardized datasets and metrics with unified cost models to benchmark and compare agents on Pareto frontiers across tasks and budgets.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19418" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19418" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ VLMs struggle with perception-intensive tasks (e.g., spatial reasoning, counting, geometric awareness) because dense visual information is poorly captured in discrete text tokens.<br>‚Ä¢ Projecting continuous visual cues (boundaries, layout, depth, geometry) into language space causes information loss, limiting fine-grained visual reasoning.<br>‚Ä¢ Text-only Chain-of-Thought can misdirect and even degrade visual reasoning on spatial benchmarks, revealing a mismatch between continuous vision and symbolic language tokens.<br>‚Ä¢ Reliance on external vision tools restores some perception but adds latency/GPU cost and constrains performance to tool capabilities, reducing end-to-end efficiency and autonomy.<br>‚Ä¢ There is a need for grounded, interpretable reasoning that links semantic language with continuous perceptual signals within the VLM itself.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Chain-of-Visual-Thought (CoVT) equips VLMs with compact continuous visual tokens‚Äîdistilled from lightweight experts (segmentation, depth, edges, self-supervised features)‚Äîthat are autoregressively predicted within the reasoning chain and trained via reconstruction/distillation losses through lightweight decoders. At inference, the model reasons directly in this visual token space (‚âà20 tokens) for efficient, fine-grained, and optionally interpretable visual reasoning, improving performance across diverse perception benchmarks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Adaptive Visual Tokenizers for Chain-of-Visual-Thought: Develop task- and instance-adaptive token budgets and content-aware tokenization to optimize efficiency‚Äìaccuracy trade-offs.<br>‚Ä¢ Chain-of-Visual-Thought for Video: Continuous Spatio-Temporal Tokens for VLMs: Extend CoVT to temporal reasoning with motion, tracking, and 3D dynamics for video understanding and robotics.<br>‚Ä¢ Self-Supervised Visual Thought Distillation without External Experts: Replace expert-dependent supervision with self-supervised objectives to learn visual thought tokens end-to-end, reducing reliance on external models.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Pillar-0: A New Frontier for Radiology Foundation Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.17803" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.17803" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Current radiology foundation models reduce 3D CT/MRI to 2D slices and downsample 12‚Äì16-bit voxels to 8-bit, losing volumetric context and clinically critical grayscale contrast.<br>‚Ä¢ Standard transformer architectures are computationally prohibitive for full-resolution 3D volumes, limiting practical training and inference at native fidelity.<br>‚Ä¢ Existing benchmarks are not clinically grounded (e.g., 2D JPEGs, simplified VQA tasks), lacking scalable, high-quality labels for diverse real-world findings, while imaging demand outpaces workforce capacity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Pillar-0 preserves native contrast via modality-specific multi-window tokenization and efficiently encodes long-context 3D volumes with an Atlas hierarchical multi-scale attention backbone, pretrained through asymmetric CLIP-style alignment to a frozen LLM text encoder. RATE complements this by using LLMs to extract structured labels for 366 radiologic findings from reports, enabling scalable, clinically grounded evaluation and finetuning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Federated Pillar-0: Privacy-Preserving Multi-Institution Pretraining for Volumetric Imaging: Evaluate federated learning across hospitals to improve generalization without sharing raw data.<br>‚Ä¢ Adaptive WindowNet: End-to-End Learnable Multi-Window Tokenizers for Cross-Modality Medical Imaging: Replace fixed radiology presets with learned, patient-specific windowing to better capture tissue contrast across CT/MRI.<br>‚Ä¢ Pillar-0-Omni: Multimodal Fusion of 3D Imaging and EHR for Prognosis and Decision Support: Integrate Pillar-0‚Äôs 3D embeddings with clinical text and tabular data to enhance risk prediction and triage.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Plan-X: Instruct Video Generation via Semantic Planning</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.17986" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.17986" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured "semantic sketches" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Diffusion Transformers struggle with high-level semantic reasoning, compositional instructions, multi-stage actions, and long-horizon planning, leading to prompt misalignment and visual hallucinations.<br>‚Ä¢ Prompt enhancement via LLMs offers coarse global text control but lacks frame-level, spatio-temporal structure, causing semantic drift and poor instruction fidelity.<br>‚Ä¢ Implicit LLM embeddings or fixed queries provide weak spatial/temporal grounding and do not flexibly handle variable video lengths/resolutions, overburdening diffusion models.<br>‚Ä¢ There is a need for an interpretable multimodal bridge that translates text and visual context into structured, time-aligned guidance for video synthesis across T2V, I2V, and continuation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Plan-X decouples planning from synthesis: a Qwen-based multimodal LLM (Semantic Planner) autoregressively generates text-aligned spatio-temporal tokens (TA-Tok from SigLIP2) as sparse keyframe ‚Äúsemantic sketches,‚Äù which guide a DiT video generator via a dedicated semantic conditioning branch with 3D spatio-temporal RoPE alongside text. A two-stage training (semantic-branch pretraining, then end-to-end finetuning with planner outputs) enables instruction-aligned, long-horizon, hallucination-resistant video generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Closed-Loop Plan-X: Feedback-Driven Refinement of Semantic Tokens During Video Synthesis: Incorporate generator/discriminator or perceptual feedback to iteratively adjust the planner‚Äôs token sequence and reduce semantic drift online.<br>‚Ä¢ Hierarchical Plan-X: Multi-Rate and Multi-Scale Semantic Planning for Long-Horizon Video Generation: Design coarse-to-fine semantic sketches across time and space to extend planning to minute-scale videos with complex, multi-stage actions.<br>‚Ä¢ Editable Semantics: Token-Level Instruction Editing for Precise Human‚ÄìObject Interaction Control: Enable interactive editing and constraints over TA-Tok sequences to achieve fine-grained, localized control and semantic cross-transfer across inputs.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">HunyuanVideo 1.5 Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.18870" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.18870" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of accessible open-source video generators that match closed-source SOTA models in visual quality and motion coherence<br>‚Ä¢ Computational inefficiency of large MoE architectures (e.g., Wan2.2 with 27B parameters, 14B activated), making inference resource-heavy<br>‚Ä¢ Quality limitations of lightweight models using high-compression 3D VAEs, including unstable motion and inadequate aesthetic fidelity for professional use<br>‚Ä¢ Need for a unified framework that supports both text-to-video and image-to-video across varying durations/resolutions while running efficiently on consumer-grade GPUs</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>An 8.3B unified Diffusion Transformer (DiT) with selective and sliding tile attention (SSTA), glyph-aware text encoding, progressive pre-/post-training, and curated data generates 480p‚Äì720p videos (5‚Äì10s), followed by a dedicated video super-resolution network to upscale to 1080p. This two-stage pipeline delivers state-of-the-art visual quality and motion coherence with efficient inference on consumer GPUs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Selective and Sliding Tile Attention for Minute-Long Open-Source Video Generation: Extend SSTA with dynamic temporal memory and windowing to stably generate longer videos while maintaining consumer-grade efficiency<br>‚Ä¢ Glyph-Aware Multilingual Prompt Encoding for Cross-Lingual Video Generation Quality: Generalize glyph-aware text encoding beyond bilingual settings to support diverse scripts and languages, improving prompt adherence and visual fidelity<br>‚Ä¢ Joint Diffusion‚ÄìSuper-Resolution Training for Native 4K Text-to-Video: Integrate the diffusion backbone and SR stage into end-to-end training to produce temporally consistent 4K outputs without separate upscaling</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.17729" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.17729" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resources across steps. We introduce a similarity-driven alignment that serializes each tool call, embeds signatures with a sentence encoder, and performs similarity-bucketed Hungarian matching to obtain auditable one-to-one correspondences. On top of this alignment, we report interpretable metrics that decouple semantic fidelity from workflow consistency. The benchmark spans 28 servers with 231 tools, and provides standardized trajectories curated through an Executor & Judge pipeline with human verification; an auxiliary four large language models (LLMs) judge ensemble reports end-task Task Completion and information grounding. Evaluations of representative state-of-the-art Multimodal LLMs (MLLMs) reveal persistent gaps in multimodal MCP tool use, particularly in argument fidelity and structure consistency, underscoring the need for methods that jointly reason over images, text, and tool graphs. Our Benchmark's anonymous repository is at https://github.com/EtaYang10th/Open-M3-Bench</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Absence of a principled benchmark for multimodal tool use under MCP that reflects realistic multi-hop, multi-threaded workflows requiring visual grounding, textual reasoning, cross-tool dependencies, and persistence of intermediate resources.<br>‚Ä¢ Lack of auditable alignment between predicted and reference tool calls, making it hard to disentangle semantic fidelity (arguments, grounding) from workflow consistency (structure, ordering) in complex tool graphs.<br>‚Ä¢ Existing MCP benchmarks are mostly text-only and linear (API planning or database queries), failing to evaluate multimodal function calling and heterogeneous server/tool interactions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>M3-Bench introduces a similarity-driven alignment that serializes tool calls, embeds tool-call signatures with a sentence encoder, and performs similarity-bucketed Hungarian matching to produce one-to-one correspondences; on this alignment, it reports interpretable metrics that separate semantic fidelity from workflow consistency, supported by standardized trajectories (28 servers, 231 tools) curated via an Executor & Judge pipeline with human verification and a four-LLM judge ensemble for task completion and grounding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Graph-Grounded Multimodal Reasoning for MCP Agents: Develop models that jointly reason over images, text, and tool graphs to improve multi-hop, multi-threaded planning and cross-tool dependency handling.<br>‚Ä¢ Learning to Align: Supervising MLLM Tool Use with Similarity-Bucketed Hungarian Signals: Use alignment correspondences as training signals to enhance argument fidelity and structure consistency in tool-call sequences.<br>‚Ä¢ Persistent Memory for Multithreaded MCP Workflows: Design agent memory and resource management mechanisms to maintain and reuse intermediate artifacts across steps/threads, boosting workflow coherence and task completion.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13288" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13288" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Shared-LLM multi-agent systems limit role specialization; planner and tool executors face heterogeneous state‚Äìaction distributions, hurting accuracy in specialized domains.<br>‚Ä¢ Vertical multi-agent training with distinct LLMs is hard: agents act at different frequencies with variable sub-agent invocations, causing asynchronous updates and misaligned credit assignment.<br>‚Ä¢ Separate-server deployment breaks end-to-end gradient flow, making conventional joint backpropagation infeasible at scale.<br>‚Ä¢ Variable numbers of sub-trajectories per rollout prevent fixed-shape batching, destabilizing group baselines and hindering efficient policy-gradient training.<br>‚Ä¢ Existing MAS-RL methods either rely on costly value functions (PPO), freeze sub-agents, or assume synchronous dual-agent loops; none align heterogeneous trajectories while preserving hierarchical credit assignment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>M-GRPO extends GRPO to vertical multi-agent systems by computing hierarchical group-relative advantages for planner and sub-agents and aligning variable sub-agent trajectories to fixed-size batches via duplication/drop. A decoupled training pipeline co-trains distinct LLMs on separate servers using minimal shared statistics, with composite rewards (format, correctness, expert feedback) for stable, scalable tool-augmented reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive M-GRPO: Online Control of Sub-Invocation Target d for Bias-Reduced Trajectory Alignment‚ÄîDesign an adaptive scheduler that adjusts duplication/drop based on rollout statistics to improve stability and sample efficiency.<br>‚Ä¢ Hierarchical M-GRPO for 1+n Vertical Architectures: Role-Specific Baselines and Credit Propagation‚ÄîExtend to multiple specialized sub-agents with per-role advantage normalization and cross-agent credit assignment.<br>‚Ä¢ Causal Credit Assignment in Multi-Agent GRPO: Counterfactual Estimators for Tool-Use Impact‚ÄîIncorporate counterfactual and Shapley-style baselines to more precisely attribute reward to sub-agent contributions.<br>‚Ä¢ Process-Level Rewards for Tool Executors: Integrating PRMs with Group-Relative Objectives‚ÄîCombine process reward models for sub-agent action quality with M-GRPO to better guide tool usage and intermediate reasoning.<br>‚Ä¢ Distributed On-Policy Synchronization for Decoupled MAS Training‚ÄîDevelop protocols and buffers that maintain on-policy data alignment across clusters without cross-server backpropagation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MIST: Mutual Information Via Supervised Training</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.18945" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.18945" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Accurate mutual information (MI) estimation across diverse joint distributions, sample sizes, and dimensions is challenging; classical estimators suffer from bias‚Äìvariance trade-offs and degrade in high dimensions or with limited samples.<br>‚Ä¢ Existing neural MI estimators often optimize variational bounds rather than MI directly, can be slow and unstable, and typically lack calibrated uncertainty quantification.<br>‚Ä¢ Confidence intervals from bootstrap are computationally expensive and can be miscalibrated; few estimators natively handle permutation invariance or variable sample sizes/dimensions.<br>‚Ä¢ There is a need for fast, differentiable, trainable MI estimators that generalize to unseen distributions and can be embedded into larger learning pipelines and adapted to different data modalities.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MIST parameterizes an MI estimator as a neural network trained end-to-end on a large synthetic meta-dataset with known ground-truth MI, using a two-dimensional attention mechanism to ensure permutation invariance and handle variable sample sizes/dimensions. It optimizes a quantile regression loss to output MI quantiles (approximating the sampling distribution) and leverages normalizing flows to adapt meta-datasets to target data modalities, yielding fast, uncertainty-aware, differentiable inference.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ MIST-CMI: Supervised Estimation of Conditional Mutual Information with Quantile Uncertainty: Extend MIST to conditional and multivariate information measures (e.g., I(X;Y|Z)) with calibrated quantile outputs.<br>‚Ä¢ Flow-Adapted MIST for Domain-Specific Modalities: Use normalizing flows to tailor meta-training to images, text, graphs, and tabular data, evaluating cross-modality generalization and calibration.<br>‚Ä¢ Generalization Guarantees for Data-Driven MI Estimators: Develop theoretical risk and calibration bounds for supervised MI estimators under distribution shift, sample-size variation, and dimensional scaling.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">Controllable Layer Decomposition for Reversible Multi-Layer Image Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16249" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16249" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This work presents Controllable Layer Decomposition (CLD), a method for achieving fine-grained and controllable multi-layer separation of raster images. In practical workflows, designers typically generate and edit each RGBA layer independently before compositing them into a final raster image. However, this process is irreversible: once composited, layer-level editing is no longer possible. Existing methods commonly rely on image matting and inpainting, but remain limited in controllability and segmentation precision. To address these challenges, we propose two key modules: LayerDecompose-DiT (LD-DiT), which decouples image elements into distinct layers and enables fine-grained control; and Multi-Layer Conditional Adapter (MLCA), which injects target image information into multi-layer tokens to achieve precise conditional generation. To enable a comprehensive evaluation, we build a new benchmark and introduce tailored evaluation metrics. Experimental results show that CLD consistently outperforms existing methods in both decomposition quality and controllability. Furthermore, the separated layers produced by CLD can be directly manipulated in commonly used design tools such as PowerPoint, highlighting its practical value and applicability in real-world creative workflows.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Raster compositing of multiple RGBA layers is irreversible, making layer-level edits impossible once merged into a single image.<br>‚Ä¢ Existing matting/inpainting or multi-stage pipelines (detection‚Üísegmentation‚Üímatting‚Üíinpainting) lack controllability, suffer error propagation, and produce artifacts (messy edges, background leakage).<br>‚Ä¢ Foundation models (e.g., SAM, zero-shot matting) are instance-level, ignore partial transparency, and perform poorly on graphic-design images; they also cannot handle sequential/nested decomposition.<br>‚Ä¢ No standardized benchmark or metrics exist for user-controllable layer decomposition, limiting fair evaluation and progress.<br>‚Ä¢ Designers need fine-grained, controllable separation that preserves crisp boundaries, alpha, depth order, and a coherent background, guided by simple inputs like bounding boxes.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>CLD uses a DiT-based backbone (LD-DiT) to jointly generate background and per-box foreground RGBA layers, guided by user-provided bounding boxes, with MLCA injecting layer-aligned conditional image features and LA-RoPE encoding inter-layer spatial positions. Training employs flow matching, a transparency-aware multi-layer autoencoder, a composite-image reconstruction objective, and dual-condition classifier-free guidance to improve coherence and controllability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Interactive Hierarchical Layer Editing with Recursive CLD: Enable real-time, stepwise user interactions for nested elements, with uncertainty cues and editable constraints to refine sequential decomposition.<br>‚Ä¢ Temporally Consistent Video Layer Decomposition with CLD: Extend CLD to video by adding temporal positional encodings and cross-frame MLCA, ensuring stable alpha and layer identities across time.<br>‚Ä¢ Domain-Adaptive Transparency Modeling for Graphic Designs: Improve alpha precision via domain adaptation, synthetic RGBA generation, and disentangled losses tailored to solid colors, crisp edges, and stylized textures.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.18373" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.18373" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ VLMs struggle to reason about physics-driven video dynamics, failing to integrate 3D spatial layouts, motion patterns, and temporal cues for coherent understanding.<br>‚Ä¢ Physical laws are implicit and manifest diversely, and the scarcity of dense, entity-level spatiotemporal/motion annotations causes poor generalization and surface-level correlations.<br>‚Ä¢ Training on real videos induces strong plausibility priors, leading VLMs to overlook or hallucinate physics abnormalities‚Äîespecially prevalent in AIGC content.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MASS injects explicit spatial‚Äìtemporal signals into VLMs by encoding depth-based 3D trajectories, visual grounding, and motion tracking, aligning them with language via a spatiotemporal awareness module; the VLM is then post-trained with supervised fine-tuning and GRPO reinforcement on MASS-Bench, a physics-centric QA dataset with dense entity-level annotations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ PhysAware-Gen: Physics-Guided Video Generation with Motion-Aware Grounding to enforce physical plausibility in AIGC via MASS-style spatial‚Äìtemporal signals.<br>‚Ä¢ Differentiable PhysVLM: Integrating differentiable physics constraints into VLMs for causal and counterfactual video reasoning beyond visual priors.<br>‚Ä¢ MASS-Bench-Long: A long-horizon, multi-entity dynamics benchmark extending MASS-Bench to complex interactions, prediction tasks, and abnormality detection across extended sequences.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PRInTS: Reward Modeling for Long-Horizon Information Seeking</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19314" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19314" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Long-horizon information seeking requires multi-step reasoning with external tools (search, browsing, code), which current LLM agents struggle to execute and synthesize across extended trajectories.<br>‚Ä¢ Existing process reward models (PRMs) provide coarse, binary judgments for short reasoning, failing to evaluate full steps that combine tool interactions with reasoning across multiple quality dimensions (e.g., interpreting tool outputs, tool call informativeness, planning next actions).<br>‚Ä¢ Rapid context accumulation in long-horizon tasks overwhelms PRMs that lack mechanisms to compress and preserve essential information for step evaluation.<br>‚Ä¢ Finetuning agent LLMs for information seeking is computationally costly and model-family specific; a model-agnostic test-time method is needed to boost diverse agents.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>PRINTS is a generative process reward model that produces dense, multi-dimensional scores for each step by reasoning about tool interactions and their outputs, and simultaneously performs trajectory summarization to compress growing context while retaining essentials for evaluation. It is used at test time for best-of-n sampling to rank and select higher-quality actions, improving long-horizon information-seeking across agents and tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ PRInTS-RL: End-to-End Agent Training with PRM-Guided Reinforcement Learning: Integrate PRINTS scoring and summarization into agent training loops to jointly optimize decision-making and tool use.<br>‚Ä¢ Adaptive Multi-Dimensional PRMs for Heterogeneous Tool Ecosystems: Extend PRINTS to dynamically tailor scoring dimensions to diverse tools (e.g., code, web, APIs) with tool-specific evaluators and calibration.<br>‚Ä¢ Uncertainty-Calibrated PRInTS: Confidence-Aware Reward Modeling for Long-Horizon Decisions: Add probabilistic uncertainty estimates to dense scoring to better rank candidate steps and mitigate error propagation over long trajectories.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16301" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16301" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Upsample Anything, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only approx0.419 s per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling. Project page: https://seominseok0429.github.io/Upsample-Anything/{https://seominseok0429.github.io/Upsample-Anything/}</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Vision Foundation Models output features downsampled by 14‚Äì16√ó, forcing heavy decoders for pixel-level tasks that are costly, memory-intensive, and brittle to architecture/resolution changes.<br>‚Ä¢ Existing feature upsamplers rely on dataset-level training (paired data or pseudo-labels), require retraining per backbone/dataset, and are constrained to low resolutions due to memory.<br>‚Ä¢ Prior test-time optimization approaches (e.g., implicit FeatUp) are computationally expensive (~49 s per 224√ó224 image), preventing practical, scalable deployment.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Upsample Anything performs lightweight per-image test-time optimization to learn pixelwise anisotropic Gaussian parameters (œÉx, œÉy, Œ∏, œÉr) that define a continuous spatial‚Äìrange splatting kernel guided by HR RGB reconstruction. The learned edge-aware kernel is then applied to low-resolution foundation features to produce high-resolution outputs, effectively bridging 2D Gaussian Splatting and Joint Bilateral Upsampling without any dataset-level training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Temporal Upsample Anything: Test-Time Splatting for Video with Motion-Aware Consistency: Extend the framework to videos by incorporating optical flow and temporal regularization to maintain edge-aware, temporally consistent upsampling.<br>‚Ä¢ Multi-Modal Guidance Selection for Universal TTO Upsampling: Develop adaptive mechanisms that select or fuse guidance cues (RGB, depth, semantics) at test time to further improve cross-modality edge preservation and reconstruction quality.<br>‚Ä¢ 3D Upsample Anything: Gaussian‚ÄìBilateral Splatting for Volumetric Feature Upsampling: Generalize the anisotropic spatial‚Äìrange kernel learning to 3D voxel/point-cloud features to upsample volumetric representations for autonomous driving and robotics.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">EvoVLA: Self-Evolving Vision-Language-Action Model</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16166" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16166" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Stage hallucination in long-horizon manipulation: agents exploit coarse evaluation signals and visual shortcuts, reporting progress without true task completion.<br>‚Ä¢ Sample inefficiency and brittle exploration: sparse rewards and pixel-based curiosity collapse in cluttered, high-dimensional scenes, hindering efficient VLA fine-tuning.<br>‚Ä¢ Fragile long-horizon memory and missing benchmarks: naive history compression causes catastrophic forgetting; lack of stage-grounded datasets impedes training and evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>EvoVLA is a self-supervised VLA fine-tuning framework that couples Stage-Aligned Reward (triplet contrastive learning with Gemini-generated hard negatives) and Pose-Based Object Exploration (curiosity grounded in relative object‚Äìgripper pose) with a selective Long-Horizon Memory module (context selection and gated fusion) to suppress stage hallucination and improve sample efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ LLM-Augmented Stage Semantics for Robust Reward Alignment: Integrate online LLM verification to generate and validate hard negatives and stage boundaries, strengthening SAR across diverse tasks.<br>‚Ä¢ Pose-Driven Curiosity with Contact-Aware Dynamics for Manipulation: Extend POE by modeling contact events and physics-informed priors to guide exploration in complex, cluttered environments.<br>‚Ä¢ Self-Evolving Memory for VLAs via Differentiable Write Policies: Learn adaptive context selection and gated fusion policies end-to-end to scale memory reliability to hundreds of stages and tasks.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Flow Map Distillation Without Data</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19428" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19428" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Data-based flow map distillation suffers from Teacher‚ÄìData Mismatch: the dataset-induced noising paths (Àúp_t) differ from the teacher‚Äôs true generative paths (ÀÜp_t), so students learn the wrong dynamics.<br>‚Ä¢ Mismatch arises in practice when teachers generalize beyond training data, use CFG or post-hoc fine-tuning (distribution shift), or when proprietary training data is unavailable‚Äîleading to degraded distilled performance.<br>‚Ä¢ Existing fast flows trained from scratch or data-based distillation still rely on iterative solvers or external data, and accumulate numerical/approximation errors without principled self-correction.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>FreeFlow: a data-free flow map distillation framework that samples only from the prior and trains a predictor‚Äìcorrector. It aligns the student‚Äôs generating velocity with the teacher via a prior-anchored objective, then corrects compounding errors by aligning the student‚Äôs noising (marginal) velocity to the teacher using a VSD-style IKL objective estimated with an online velocity model.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Trajectory-Wide Correction for Data-Free Flow Map Distillation: Extend correction beyond endpoints to full predicted trajectories with second-order or multi-point constraints to further suppress error accumulation.<br>‚Ä¢ Data-Free Distillation for Text-Conditional and Multimodal Generative Models: Generalize prior-only distillation to text/image/audio conditions with adaptive guidance schedules and condition-robust velocity alignment.<br>‚Ä¢ Convergence Guarantees for Predictor‚ÄìCorrector Data-Free Flow Distillation: Establish theoretical error bounds and stability conditions for generating/noising velocity alignment under finite-step, finite-capacity students and imperfect teachers.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-8">

    <div class="paper">
        <h2 class="paper-title">AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.16397" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.16397" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model. Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\% ROUGE-N F1 compared to Trafilatura's 63.6\%, with exceptional structured element preservation (90.9\% for code blocks, 94.0\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ HTML-to-text extraction in web corpora is treated as a fixed step, causing loss of main content and corruption of structured elements (formulas, code, tables), which undermines LLM pretraining quality.<br>‚Ä¢ Heuristic extractors (e.g., Trafilatura, Resiliparse) rely on text-density and hand-crafted DOM rules, lack semantic understanding, struggle with non-standard layouts, and have limited improvement pathways and scalability.<br>‚Ä¢ Demonstrating that extraction quality materially impacts downstream model performance is essential; current curation focuses on filtering/deduplication while overlooking structure preservation and narrative coherence.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MinerU-HTML reformulates HTML content extraction as sequence labeling with a 0.6B LM using dual HTML representations (Simplified for compact input, Mapping for faithful reconstruction), constrained decoding, and post-processing to assemble clean Main-HTML before converting to Markdown. A template-aware clustering/distillation strategy scales the pipeline to Common Crawl, enabling construction of the 7.3T-token AICC corpus and yielding superior structure preservation and downstream performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning to Parse the Web: Scaling Model-Based HTML Extraction with Larger LMs and Continual Template Distillation: Explore larger base models and continual distillation to improve extraction accuracy and web-scale efficiency.<br>‚Ä¢ Multimodal HTML Parsing: Joint DOM‚ÄìCSS‚ÄìRender Integration for Dynamic, Visual-Aware Structure Preservation: Incorporate page rendering (screenshots) and JS execution with DOM semantics to better handle dynamic content and complex layouts.<br>‚Ä¢ Joint Web Data Curation: End-to-End Optimization of Extraction, Filtering, and Deduplication for LLM Pretraining: Co-design and jointly learn extraction and quality filtering pipelines, measuring holistic impact on downstream tasks and structure-sensitive benchmarks.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Representational Stability of Truth in Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19166" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19166" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) are widely used for factual tasks such as "What treats asthma?" or "What is the capital of Latvia?". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to 40% flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes (leq 8.2%). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLMs lack a well-understood and measurable stability in their internal representations of True, False, and Neither, making it unclear how robustly they encode veracity under semantic perturbations.<br>‚Ä¢ Instability in veracity representations contributes to hallucinations and sensitivity to small prompt/context changes, undermining reliability and safety in factual tasks.<br>‚Ä¢ Existing approaches either probe separability of true/false in activation space or analyze outputs under context changes, but they do not provide a unified diagnostic for which statement types disrupt latent veracity geometry.<br>‚Ä¢ Prior probes often ignore Neither cases or familiarity effects, and models are optimized for output accuracy rather than coherent, stable truth assignments.<br>‚Ä¢ The field lacks tools to disentangle epistemic familiarity from linguistic form in shaping LLMs‚Äô veracity structure.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Define representational stability by training a linear probe (sAwMIL) on LLM activations to separate True from Not True, then perturb labels (e.g., treating Familiar Fictional or Unfamiliar Synthetic statements as True) and quantify decision boundary rotations and flip rates. Apply this diagnostic across 16 open-source models and three factual domains to assess how epistemic familiarity versus linguistic form affects veracity geometry.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Training LLMs with Stability-Regularized Truth Representations: Integrate boundary-rotation penalties and flip-rate constraints into training to enforce coherent truth assignments under label perturbations.<br>‚Ä¢ Causal Probing of Epistemic Familiarity in Language Models: Use controlled interventions and synthetic datasets to disentangle familiarity effects from linguistic form in shaping veracity representations.<br>‚Ä¢ Cross-Domain and Multimodal Veracity Stability Benchmark: Extend representational stability evaluation to additional domains and multimodal models (vision-language, code) and track stability across model versions and continual learning.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.18922" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.18922" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Foundation video models operate purely in RGB space, lacking explicit geometry, which limits spatial reasoning and downstream world-simulation tasks.<br>‚Ä¢ Existing 3D/4D methods are siloed (generation vs. reconstruction) or restricted to static scenes, lacking a single model that unifies single-image generation, sparse-frame completion, and full-video reconstruction.<br>‚Ä¢ Naive RGB‚Äìgeometry coupling in diffusion (channel-wise or spatial-wise concatenation) causes cross-modal interference and rapid degradation of appearance or geometry, especially under low-resource finetuning.<br>‚Ä¢ Prior diffusion-based geometry methods assume clean RGB conditioning and struggle in joint generation where both RGB and geometry are noisy.<br>‚Ä¢ There is a need for a unified conditioning interface that robustly handles varying input sparsities while preserving strong pretrained video priors.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>One4D extends a flow-matching video diffusion backbone with Decoupled LoRA Control‚Äîtwo modality-specific LoRA branches on shared frozen weights plus zero-initialized control links that gradually learn pixel-wise RGB‚ÄìXYZ consistency‚Äîand Unified Masked Conditioning to pack single/sparse/full inputs into a masked conditioning video. It jointly generates synchronized RGB frames and pointmaps and recovers cameras/depth via lightweight global optimization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Cross-Modal Control Links for Joint RGB‚ÄìGeometry Diffusion: Design gated/attention-based control links and dynamic linking schedules to further enhance consistency while protecting RGB fidelity under varying compute budgets.<br>‚Ä¢ Camera-Constrained 4D Generation: Text-to-Video with User-Guided Trajectories: Incorporate explicit camera parameter conditioning to enable controllable camera paths and scene layout during unified 4D generation and reconstruction.<br>‚Ä¢ Self-Training One4D at Scale: Geometry Learning from Unlabeled Videos: Develop cycle-consistency and multi-view constraints to self-train geometry and refine pseudo labels, reducing reliance on synthetic datasets and improving robustness in-the-wild.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-9">

    <div class="paper">
        <h2 class="paper-title">Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.17792" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.17792" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ World models lack quantitative, task-oriented evaluation for mapless path planning toward text-specified semantic targets in real-world environments.<br>‚Ä¢ Existing benchmarks focus on visual fidelity or physics consistency, not on goal-reaching ability, trajectory accuracy, and directional consistency required for robot planning.<br>‚Ä¢ No standardized pipeline to convert WM-generated videos into executable robot paths (camera motion), including handling monocular scale ambiguity.<br>‚Ä¢ Datasets for semantic navigation rarely include explicit navigation targets, implicit semantic prompts, and SLAM-based ground-truth trajectories for rigorous evaluation.<br>‚Ä¢ Unclear whether small amounts of real robot data can adapt open-source WMs to outperform proprietary models on planning tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Target-Bench provides a robot-collected dataset with explicit/implicit semantic targets and a two-stage benchmark that decodes camera motion from WM-generated videos (using VGGT, SpaTracker, ViPE with scale recovery) and evaluates paths via ADE, FDE, MR, Soft Endpoint, and Approach Consistency into a weighted overall score. It also demonstrates data-efficient fine-tuning of a 5B open-source WM on 325 scenarios to significantly improve planning performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Closed-Loop World Model Navigation with Online Replanning: Integrate the world decoder into the robot control loop to enable iterative replanning and evaluate robustness in unstructured environments.<br>‚Ä¢ Memory-Augmented World Models for Receding-Horizon Semantic Navigation: Exploit latent memory to maintain scene consistency and achieve long-horizon goal pursuit under implicit semantic targets.<br>‚Ä¢ Metric-Consistent Video-to-Path Reconstruction for WM Planning: Develop learned scale recovery and multi-sensor/multi-view reconstruction to reduce monocular ambiguities and raise the benchmark ceiling.<br>‚Ä¢ Action-Conditioned World Models for Executable Mapless Plans: Couple video generation with action diffusion or policy heads to output controllable trajectories directly from semantic prompts.<br>‚Ä¢ Data-Efficient Domain Adaptation of World Models for Robotics: Systematically study small-scale, high-quality real-world fine-tuning strategies across diverse environments and target types.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.19319" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.19319" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Single-view HOI video generation cannot capture complete 3D geometry, leading to geometric distortions and unrealistic motion, especially under heavy occlusions in hand-object interactions.<br>‚Ä¢ 3D HOI approaches depend on high-quality motion capture and controlled lab setups, limiting scalability, diversity, and generalization to real-world scenarios.<br>‚Ä¢ Existing multi-view methods either translate views one-by-one (breaking consistency) or only support simple background-free assets/fixed viewpoints; prior joint video‚Äìmotion diffusion uses motion signals (optical flow, sparse keypoints, pixel depth) that lack 3D awareness and temporal smoothness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SyncMV4D introduces a Multi-view Joint Diffusion (MJD) based on a Diffusion Transformer with inter-view geometry attention and multimodality modulation to co-generate synchronized multi-view videos and motion pseudo-videos represented as enhanced 4D point tracks (2D anchors + per-frame metric depth). A Diffusion Points Aligner (DPA) then refines coarse motions into globally aligned 4D point tracks, forming a closed-loop where refined 4D tracks guide subsequent denoising for mutual enhancement.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Physics-Aware SyncMV4D: Incorporating contact, force, and differentiable physics priors into the joint diffusion loop to improve physically faithful hand-object interactions.<br>‚Ä¢ Camera-Free In-the-Wild Multi-View HOI Synthesis: Learning implicit camera calibration and depth scale from uncalibrated, casually captured videos to enable robust multi-view generation without reference cameras.<br>‚Ä¢ Beyond Hands: Full-Body and Multi-Object SyncMV4D: Extending the 4D point-track representation and inter-view attention to whole-body HOI and multiple interacting objects with complex occlusions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Fidelity-Aware Recommendation Explanations via Stochastic Path Integration</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.18047" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.18047" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Explanation fidelity, which measures how accurately an explanation reflects a model's true reasoning, remains critically underexplored in recommender systems. We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), a model-agnostic approach that adapts path-integration techniques to the sparse and implicit nature of recommendation data. To overcome the limitations of prior methods, SPINRec employs stochastic baseline sampling: instead of integrating from a fixed or unrealistic baseline, it samples multiple plausible user profiles from the empirical data distribution and selects the most faithful attribution path. This design captures the influence of both observed and unobserved interactions, yielding more stable and personalized explanations. We conduct the most comprehensive fidelity evaluation to date across three models (MF, VAE, NCF), three datasets (ML1M, Yahoo! Music, Pinterest), and a suite of counterfactual metrics, including AUC-based perturbation curves and fixed-length diagnostics. SPINRec consistently outperforms all baselines, establishing a new benchmark for faithful explainability in recommendation. Code and evaluation tools are publicly available at https://github.com/DeltaLabTLV/SPINRec.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Explanation fidelity in recommender systems is underexplored; most work optimizes persuasiveness or satisfaction without verifying faithfulness to model reasoning.<br>‚Ä¢ Standard path-integration and gradient-based methods fail on sparse, binary implicit-feedback data, especially when using unrealistic all-zero baselines that yield weak or misleading attributions.<br>‚Ä¢ Existing model-agnostic explainers (LIME, SHAP, influence-based) lack rigorous fidelity benchmarking and counterfactual protocols; prior metrics often conflate supportive/contradictory features and use coarse masking.<br>‚Ä¢ Recommenders exploit both observed and unobserved interactions; explanations must capture presence and absence effects, which a single baseline cannot model across diverse user behaviors.<br>‚Ä¢ There is a need for scalable, model-agnostic, high-fidelity explanations and standardized counterfactual evaluation across models and datasets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SPINRec stochastically samples plausible user-history baselines from the empirical distribution and performs path integration from each baseline to the user vector to compute attribution maps, then selects the map that maximizes a counterfactual fidelity metric. This design captures both observed and unobserved interaction effects and yields stable, personalized, high-fidelity explanations with linear scalability.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Adaptive Baseline Samplers for Fidelity-Aware Recommender Explanations: Train a conditional generative sampler that tailors baseline distributions to users and models, optimizing fidelity or multi-objective criteria.<br>‚Ä¢ Guided Stochastic Path Integration for Graph and Sequential Recommenders: Extend SPINRec with non-linear/guided paths and apply it to graph and sequence models (e.g., LightGCN, SASRec) to exploit structural/contextual dependencies.<br>‚Ä¢ Multi-Objective Explainability in Recommenders: Balancing Fidelity, Stability, and Persuasiveness: Integrate user-centric metrics with fidelity to produce explanations that are faithful, stable, and effective for end-users, with controllable trade-offs.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-10">

    <div class="paper">
        <h2 class="paper-title">Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.18024" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.18024" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present a method for extracting monosemantic neurons, defined as latent dimensions that align with coherent and interpretable concepts, from user and item embeddings in recommender systems. Our approach employs a Sparse Autoencoder (SAE) to reveal semantic structure within pretrained representations. In contrast to work on language models, monosemanticity in recommendation must preserve the interactions between separate user and item embeddings. To achieve this, we introduce a prediction aware training objective that backpropagates through a frozen recommender and aligns the learned latent structure with the model's user-item affinity predictions. The resulting neurons capture properties such as genre, popularity, and temporal trends, and support post hoc control operations including targeted filtering and content promotion without modifying the base model. Our method generalizes across different recommendation models and datasets, providing a practical tool for interpretable and controllable personalization. Code and evaluation resources are available at https://github.com/DeltaLabTLV/Monosemanticity4Rec.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Latent user/item embeddings in two-tower recommenders are opaque, limiting interpretability, trust, debugging, and accountability.<br>‚Ä¢ Existing SAE methods for LLMs focus on single shared spaces and geometric reconstruction, failing to preserve user‚Äìitem interaction semantics and ranking fidelity in recommenders.<br>‚Ä¢ Current explanation techniques illuminate outputs but do not reveal or control the underlying embedding semantics.<br>‚Ä¢ Common Top-K sparsity objectives in SAEs can cause unstable dynamics and dead neurons, making them ill-suited for recommender embeddings.<br>‚Ä¢ Practical need for post-hoc, fine-grained control (e.g., filtering, promotion) without retraining the base model.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Post-hoc train a prediction-aware Sparse Autoencoder over user and item embeddings, combining an embedding-level reconstruction with a novel prediction-level loss that backpropagates through a frozen recommender scoring function to preserve interaction semantics. Sparsity is enforced via L1+KL penalties (optionally in a Matryoshka hierarchy), yielding monosemantic neurons that enable interpretable, neuron-level interventions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Causal Monosemantic Steering in Recommender Systems: From Neuron Edits to Real-World Outcomes: Develop causal evaluation frameworks (IPS/DR, counterfactuals, A/B tests) to rigorously measure the impact of neuron-level interventions on exposure, engagement, and fairness.<br>‚Ä¢ Online, Continual Prediction-Aware SAEs for Industrial-Scale Two-Tower Recommenders: Design efficient, streaming training and drift-robust SAEs that align with evolving user‚Äìitem interactions, handle cold-starts, and operate at web scale.<br>‚Ä¢ Fairness-Constrained Monosemantic Control for Exposure, Diversity, and Safety: Integrate fairness, safety, and diversity constraints into neuron steering to promote equitable exposure while maintaining recommendation fidelity.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.12810" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.12810" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at https://github.com/linaagh98/MSRNet{https://github.com/linaagh98/MSRNet}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Camouflaged objects have high similarity with backgrounds (color, texture, size), making segmentation intrinsically difficult.<br>‚Ä¢ Existing COD methods struggle in complex scenes with multiple and small/tiny objects, as well as under low-light and occlusion.<br>‚Ä¢ Current multi-scale fusion often merges features indiscriminately, causing conflicts across non-adjacent resolutions and degrading fine/local details.<br>‚Ä¢ Decoder strategies inadequately preserve global context from low-resolution maps, limiting robust detection of multiple objects.<br>‚Ä¢ There is a need for selective, attention-driven scale integration and recursive refinement to balance global context and local detail.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MSRNet uses a PVTv2-based encoder over an image pyramid to extract multi-scale features, integrates them per resolution via Attention-Based Scale Integration Units with multi-head spatial attention, then applies a novel recursive-feedback decoder with Multi-Granularity Fusion Units to progressively refine features while propagating global context from lower to higher resolutions (optimized with BCE + Uncertainty Awareness Loss).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Scale Selection for Camouflaged Object Detection: Learn to select and weight image-pyramid scales dynamically (e.g., via reinforcement learning) to maximize accuracy while reducing computation.<br>‚Ä¢ Temporal Recursive-Feedback Networks for Video COD: Extend the recursive-feedback decoding with temporal attention/memory to capture motion cues and improve robustness under occlusion in video sequences.<br>‚Ä¢ MSRNet-Lite: Efficient Multi-Scale Recursive Network for Edge Devices: Design parameter-efficient ABSIU/MGFU variants with pruning and quantization to achieve real-time COD with minimal accuracy loss.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 10</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button><button class="page-btn" onclick="goToPage(8)">8</button><button class="page-btn" onclick="goToPage(9)">9</button><button class="page-btn" onclick="goToPage(10)">10</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-25 23:09:41 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 10;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>