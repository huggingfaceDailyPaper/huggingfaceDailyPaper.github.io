<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 14, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 14, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.10629" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.10629" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Diffusion models struggle to generate beyond training resolutions; direct high-resolution sampling is slow, expensive, and prone to repetition, distortions, and texture breakdown.<br>â€¢ Post-hoc pixel-space super-resolution adds quadratic latency with output size and often causes oversmoothing and semantic drift because it operates after decoding.<br>â€¢ NaÃ¯ve latent resizing (e.g., bicubic) departs from the manifold of valid latents, yielding unnatural textures upon decoding.<br>â€¢ Reference-based latent SR pipelines (e.g., DemoFusion, LSRNA) require additional diffusion stages, auxiliary guidance, and tight VAE coupling, increasing latency and limiting generality.<br>â€¢ The upsampling step is the principal bottleneck; a single-pass latent-space method that preserves high-frequency structure without extra diffusion is needed for scalable, high-fidelity synthesis.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes the Latent Upscaler Adapter (LUA), a lightweight feed-forward module placed between the generator and a frozen VAE decoder that upsamples latents by Ã—2/Ã—4 using a shared Swin-style backbone with scale-specific pixel-shuffle heads, then decodes once to produce the high-resolution image. LUA is trained via a three-stage latentâ€“pixel curriculum (latent L1/FFT, joint downsampled/high-frequency image losses, and edge-aware refinement) and generalizes across VAEs by changing only the first convolution with minimal fine-tuning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Latent Refinement-and-Upscaling: Consistency-Gated Adapters for Artifact Suppression: Combine latent upscaling with lightweight refinement modules and uncertainty-aware gating to suppress artifacts while preserving semantics before a single decode.<br>â€¢ Temporal-LUA: Single-Decode Latent Upscaling for High-Resolution Video with Temporal Consistency: Extend LUA to video by adding temporal priors or recurrent attention, with losses enforcing inter-frame consistency and sharp high-frequency detail.<br>â€¢ Structure-Preserving Latent Upscaling for Conditional Image-to-Image Tasks: Adapt LUA to depth-to-image and semantic map-to-image pipelines using structural priors and geometry-aware losses to maintain layout and boundaries during latent upscaling.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">PAN: A World Model for General, Interactable, and Long-Horizon World Simulation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.09057" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.09057" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Existing video generators lack causal control, interactivity, and long-horizon consistency, operating in prompt-to-full-video mode without state/action modeling.<br>â€¢ Current world models are domain-specific with restrictive action spaces, limiting generalization across diverse environments and interaction formats.<br>â€¢ Encoder-only latent objectives (e.g., JEPA) risk collapse/indefinability and produce ungrounded latent transitions not tied to realizable observations.<br>â€¢ Long-horizon video rollouts suffer error accumulation and discontinuities at chunk boundaries, degrading temporal coherence over time.<br>â€¢ Public video datasets are short and not action-conditioned; training requires large-scale videoâ€“action pairs and dense, temporally grounded captions.<br>â€¢ Coherent, interactive world simulation is critical for agents to imagine consequences, reason counterfactually, and plan over extended horizons.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>PAN implements a Generative Latent Prediction (GLP) world model that predicts action-conditioned latent states via an LLM-based autoregressive backbone grounded by a VLM encoder, and reconstructs observations with a diffusion video decoder trained using flow-matching to tie latent dynamics to sensory data. A Causal Shift-Window Denoising Process Model (Causal Swin-DPM) enables chunked, causally masked sliding-window denoising to smooth transitions and curb error accumulation for long-horizon simulation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Scaling GLP with Hierarchical and Mixed Latents for Long-Horizon World Modeling: Extend PAN to hierarchical, multi-scale, mixed discreteâ€“continuous latents and mixed backbones to capture higher-order temporal dynamics.<br>â€¢ Learning Unified Action Spaces Beyond Language for Interactive World Simulation: Integrate structured/environmental actions, tool APIs, and multimodal commands with learned affordances for richer interactivity.<br>â€¢ Causal Benchmarks and Metrics for Evaluating World Models: Develop standardized datasets and metrics for counterfactual correctness, intervention consistency, and closed-loop planning success.<br>â€¢ Object-Centric and Physics-Grounded GLP: Incorporate object-level abstractions and physical priors/constraints to improve controllability, sample efficiency, and generalization.<br>â€¢ Multi-Agent PAN: Modeling Social Interaction and Strategic Behavior in Open Worlds: Extend latent dynamics to multi-agent settings with communication, coordination, and game-theoretic reasoning.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.08521" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.08521" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>While specialized AI models excel at isolated video tasks like generation or understanding, real-world applications demand complex, iterative workflows that combine these capabilities. To bridge this gap, we introduce UniVA, an open-source, omni-capable multi-agent framework for next-generation video generalists that unifies video understanding, segmentation, editing, and generation into cohesive workflows. UniVA employs a Plan-and-Act dual-agent architecture that drives a highly automated and proactive workflow: a planner agent interprets user intentions and decomposes them into structured video-processing steps, while executor agents execute these through modular, MCP-based tool servers (for analysis, generation, editing, tracking, etc.). Through a hierarchical multi-level memory (global knowledge, task context, and user-specific preferences), UniVA sustains long-horizon reasoning, contextual continuity, and inter-agent communication, enabling interactive and self-reflective video creation with full traceability. This design enables iterative and any-conditioned video workflows (e.g., text/image/video-conditioned generation rightarrow multi-round editing rightarrow object segmentation rightarrow compositional synthesis) that were previously cumbersome to achieve with single-purpose models or monolithic video-language models. We also introduce UniVA-Bench, a benchmark suite of multi-step video tasks spanning understanding, editing, segmentation, and generation, to rigorously evaluate such agentic video systems. Both UniVA and UniVA-Bench are fully open-sourced, aiming to catalyze research on interactive, agentic, and general-purpose video intelligence for the next generation of multimodal AI systems. (https://univa.online/)</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Real-world video creation requires composite, iterative, agentic workflows that span understanding, segmentation, editing, and generationâ€”capabilities that isolated single-task models cannot coordinate.<br>â€¢ Monolithic video-language models are inflexible, hard to extend with new tools, and inefficient for complex multi-step workflows.<br>â€¢ Existing LLM-agent frameworks (e.g., HuggingGPT, VideoAgent) demonstrate planning but are not specialized for detailed video operations and lack unified end-to-end editing/generation support.<br>â€¢ Current pipelines are brittle, labor-intensive, non-interactive, and lack automation, proactive assistance, contextual continuity, and long-horizon reasoning.<br>â€¢ There is no standardized benchmark to rigorously evaluate multi-step, agentic video systems across understanding, editing, segmentation, and generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>UniVA proposes a Plan-and-Act dual-agent architecture: a planner parses user intent into structured steps, and executor agents invoke modular MCP-based tool servers (analysis, generation, editing, tracking) to complete them. A hierarchical multi-level memory (global knowledge, task context, user preferences) enables long-horizon reasoning, inter-agent communication, and interactive, self-reflective, traceable video creation; UniVA-Bench provides multi-step evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Adaptive Self-Reflective Planning for Long-Horizon Video Agents: Enhance plannerâ€“executor loops with uncertainty-aware monitoring, dynamic replanning, and failure recovery for ultra-long, multi-scene workflows.<br>â€¢ Learning-to-Tool: Automatic Tool Discovery and Composition in MCP-based Video Agents: Enable agents to autonomously discover, validate, and compose new video tools from API schemas, improving extensibility and performance.<br>â€¢ UniVA-Bench 2.0: Comprehensive Evaluation of Agentic Video Creation and Editing: Expand benchmarks and metrics to assess proactivity, traceability, cross-modal consistency, cinematic quality, and user satisfaction in multi-round co-creation.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">Black-Box On-Policy Distillation of Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.10643" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.10643" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Proprietary API teachers (e.g., GPT-5) do not expose logits or hidden states, making white-box distillation infeasible.<br>â€¢ Conventional black-box distillation (sequence-level supervised fine-tuning) provides shallow imitation, suffers from exposure bias, and lacks on-policy learning signals.<br>â€¢ Incompatible tokenizers between teacher and student hinder likelihood-based objectives, necessitating tokenizer-agnostic supervision.<br>â€¢ On-policy reverse-KLD methods effective in white-box settings cannot be applied in black-box scenarios due to missing probability-level feedback on student generations.<br>â€¢ Fixed reward models in RLHF are vulnerable to reward hacking; an adaptive, co-evolving reward signal is needed for stable supervision.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>GAD frames the student LLM as a generator and trains a discriminator to distinguish teacher vs. student responses; the student is optimized in a minimax game to fool the discriminator, which serves as an on-policy, adaptive reward model enabling black-box distillation without logits.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Theoretical Convergence of On-Policy Adversarial Distillation: Formalize and analyze stability, sample complexity, and convergence guarantees of GAD under realistic LLM training regimes.<br>â€¢ Preference-Augmented GAD: Combining Human Feedback with On-Policy Discriminators: Integrate human preference signals into the discriminator to jointly capture teacher mimicry and human-aligned objectives.<br>â€¢ Tokenizer-Agnostic Adversarial Distillation: Learning Across Heterogeneous Vocabularies: Develop alignment strategies and objectives that further improve cross-tokenizer distillation robustness and efficiency.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.09780" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.09780" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Group Relative Policy Optimization (GRPO) has demonstrated great utilization in post-training of Large Language Models (LLMs). In GRPO, prompts are answered by the model and, through reinforcement learning, preferred completions are learnt. Owing to the small communication volume, GRPO is inherently suitable for decentralised training as the prompts can be concurrently answered by multiple nodes and then exchanged in the forms of strings. In this work, we present the first adversarial attack in decentralised GRPO. We demonstrate that malicious parties can poison such systems by injecting arbitrary malicious tokens in benign models in both out-of-context and in-context attacks. Using empirical examples of math and coding tasks, we show that adversarial attacks can easily poison the benign nodes, polluting their local LLM post-training, achieving attack success rates up to 100% in as few as 50 iterations. We propose two ways to defend against these attacks, depending on whether all users train the same model or different models. We show that these defenses can achieve stop rates of up to 100%, making the attack impossible.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Decentralized GRPO is vulnerable to adversarial poisoning via shared completions, enabling attackers to teach arbitrary malicious behavior to benign models.<br>â€¢ Existing RL and federated learning attack/defense paradigms (e.g., reward-model poisoning, gradient aggregation) do not apply because GRPO uses verifiable rewards, shared prompts, and no gradient aggregation.<br>â€¢ There is a need for practical, low-communication defenses tailored to both homogeneous and heterogeneous model settings to ensure safe, reliable LLM post-training across tasks like math and coding.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper formalizes vertical and horizontal decentralized GRPO and introduces a completion-sharing poisoning attack that injects malicious tokens via in-context and out-of-context mechanisms. It proposes two defensesâ€”one for homogeneous models and one for heterogeneous modelsâ€”that block or filter adversarial completions, achieving up to 100% stoppage in experiments.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Certified Robustness in Decentralized GRPO: Formal Guarantees Against Completion Poisoning: Develop certifiable defenses and robustness bounds for vertical and horizontal GRPO under completion-level adversaries.<br>â€¢ Adaptive Reward Modeling for Poisoning-Resilient GRPO: Design reward models that detect and penalize adversarial token patterns while preserving benign learning, using consistency checks and adaptive scoring.<br>â€¢ Securing Decentralized LLM Post-Training: Benchmarks, Threat Models, and Protocols: Establish standardized datasets, adversary models, and deployment protocols to evaluate and harden decentralized GRPO across tasks and model heterogeneity.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Depth Anything 3: Recovering the Visual Space from Any Views</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.10647" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.10647" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Unify 3D visual geometry across arbitrary numbers of views (single, multi-view, video) with or without known poses, avoiding task-specific, brittle pipelines.<br>â€¢ Overcome limitations of existing unified models that rely on complex, bespoke architectures and redundant multi-task targets, which hinder scalability and use of large pretrained backbones.<br>â€¢ Provide a minimal, sufficient prediction target for consistent geometry and pose recovery; point maps alone are insufficient and explicit rotation prediction is fragile.<br>â€¢ Address noisy/incomplete real-world depth supervision by leveraging high-quality pseudo-labels to train generalist models at scale.<br>â€¢ Establish a comprehensive benchmark that directly evaluates pose accuracy, reconstruction quality, and feed-forward novel view synthesis performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Depth Anything 3 uses a single pretrained ViT (e.g., DINOv2) with input-adaptive cross-view self-attention and a dual-DPT head to jointly predict per-view depth and per-pixel ray maps, plus a lightweight camera head for convenience. It is trained via a teacherâ€“student paradigm where a synthetic-trained monocular teacher generates dense, scale-aligned pseudo-depth for noisy real data, enabling spatially consistent reconstruction from any views.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Depth Anything 4: Unified Dynamic Scene Geometry from Any Views: Extend the depthâ€“ray formulation and cross-view transformer to handle non-rigid motion, per-object dynamics, and time-consistent depth/pose for videos.<br>â€¢ Language-Guided Any-View Geometry Foundation Models: Integrate language and interaction cues to produce semantics-aware, editable 3D reconstructions and task-conditioned geometry (e.g., robotics, AR).<br>â€¢ Scale-Aware Self-Supervised Any-View Pretraining from Internet Video: Develop large-scale self-supervised training with cycle-consistency, photometric/geometric constraints, and ray-depth alignment to reduce reliance on labeled data while improving generalization.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Superpositional Gradient Descent: Harnessing Quantum Principles for Model Training</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.01918" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.01918" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models (LLMs) are increasingly trained with classical optimization techniques like AdamW to improve convergence and generalization. However, the mechanisms by which quantum-inspired methods enhance classical training remain underexplored. We introduce Superpositional Gradient Descent (SGD), a novel optimizer linking gradient updates with quantum superposition by injecting quantum circuit perturbations. We present a mathematical framework and implement hybrid quantum-classical circuits in PyTorch and Qiskit. On synthetic sequence classification and large-scale LLM fine-tuning, SGD converges faster and yields lower final loss than AdamW. Despite promising results, scalability and hardware constraints limit adoption. Overall, this work provides new insights into the intersection of quantum computing and deep learning, suggesting practical pathways for leveraging quantum principles to control and enhance model behavior.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Classical optimizers (SGD/Adam/AdamW) struggle to efficiently explore high-dimensional, non-convex loss landscapes in LLMs, leading to slow convergence and entrapment in local minima/saddle points.<br>â€¢ There is a need to enhance parameter-space exploration during training without sacrificing stability or requiring major architectural changes.<br>â€¢ The mechanisms by which quantum-inspired principles can concretely improve classical training are underexplored, lacking practical, testable frameworks.<br>â€¢ Pure quantum approaches face scalability and hardware constraints; a pragmatic hybrid that leverages quantum ideas on classical infrastructure is needed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Superpositional Gradient Descent augments Adam-style updates with a quantum-inspired sine-modulated perturbation (Î»Â·sin(Ï€Î¸)Â·âˆ‡L) applied to a subset of parameters to mimic superposition/interference and improve exploration, and injects amplitudes from a small parameterized quantum circuit into transformer attention via Qiskit to shape the optimization landscape.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Scaling Superpositional Gradient Descent to Billion-Parameter Transformers: Develop sparse/dynamic perturbation schemes and distributed quantum-simulation strategies to apply SGD at LLM scale and benchmark efficiency/accuracy.<br>â€¢ Adaptive Quantum Perturbations for Optimizer Control: Learn the quantum weight Î» and qubit-targeted parameter subsets online, using meta-learning or reinforcement signals to balance exploration and stability.<br>â€¢ Hardware-in-the-Loop Quantum-Assisted Training on NISQ Devices: Deploy SGDâ€™s circuit components on real quantum processors with error mitigation, studying latency, noise robustness, and end-to-end training benefits.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Solving a Million-Step LLM Task with Zero Errors</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.09030" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.09030" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLMs exhibit non-negligible per-step error rates that compound, making long-horizon tasks (hundreds to millions of dependent steps) fail inevitably.<br>â€¢ Benchmarks focus on short, independent tasks; even state-of-the-art models derail on Towers of Hanoi beyond 5â€“6 disks due to execution failures.<br>â€¢ Monolithic agents degrade with increasing context and lack fine-grained, scalable error correction and unreliable-output detection.<br>â€¢ Many real-world, safety-critical processes require zero-error execution; a 1% per-step error rate is unacceptable.<br>â€¢ Existing ensembling/voting occurs at coarse action levels and is impractical for millions of dependent steps; improving base LLMs alone is costly and insufficient.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MAKER implements a massively decomposed agentic process (MDAP): extreme single-step decomposition handled by microagents, first-to-ahead-by-k voting for per-step error correction, and red-flagging to discard risky outputs, yielding formal scaling laws and a demonstrated zero-error solution to a >1M-step Towers of Hanoi task using relatively small LLMs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Discovering Optimal Task Decompositions for MDAPs: Learn or search for minimal-step decompositions that maximize per-step correctness and minimize cost across diverse domains.<br>â€¢ Adaptive Voting and Uncertainty-Aware Error Correction in Microagent Systems: Integrate confidence estimation, semantic equivalence checks, and dynamic k selection to strengthen per-step voting.<br>â€¢ Modeling Multi-Candidate Sequential Races for MDAP Scaling Laws: Develop closed-form analyses beyond binary worst-cases to optimize sampling budgets, k, and success probabilities.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AlphaResearch: Accelerating New Algorithm Discovery with Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.08522" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.08522" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large language models have made significant progress in complex but easy-to-verify problems, yet they still struggle with discovering the unknown. In this paper, we present AlphaResearch, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct a novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote a transparent evaluation process, we construct AlphaResearchComp, a new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the ``packing circles'' problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct a comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ LLMs excel on well-defined, easy-to-verify tasks but struggle to independently discover novel algorithms beyond existing human knowledge.<br>â€¢ Existing verification is biased: LLM-as-a-judge favors familiar ideas, while execution-only agents validate feasibility yet miss scientific novelty and impact.<br>â€¢ The ideationâ€“execution gap: idea-centric systems produce infeasible concepts; execution-centric systems converge to technically correct but uninteresting solutions due to the absence of combined real-world (peer-review) and execution rewards.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>AlphaResearch is an autonomous LLM ensemble that operates in a dual research environment, combining execution-based program verification with a learned peer-review reward model trained on real papers and review scores. It iteratively proposes ideas, implements code, evaluates with both rewards, and optimizes proposals, benchmarked on the AlphaResearchComp suite of open-ended algorithmic problems.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Real-World Research Rewards: Calibrated Peer-Review Models for Open-Ended Algorithm Discovery: Improve, debias, and domain-adapt the reward model using richer review data, citation/context signals, and continual calibration to better align novelty and impact.<br>â€¢ Closing the Ideationâ€“Execution Gap: Hybrid Program Synthesis and Search in Autonomous Algorithm Discovery Agents: Integrate symbolic synthesis, formal verification, and test-time search to reliably translate innovative ideas into performant, constraint-satisfying implementations.<br>â€¢ AlphaResearchComp 2.0: A Scalable, Reproducible Benchmark for Autonomous Algorithm Discovery Across Domains: Expand the benchmark with more diverse open-ended problems, stricter reproducibility pipelines, and multi-metric evaluations to stress-test discovery agents.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.10507" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.10507" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent progress in large language models (LLMs) has led to impressive performance on a range of tasks, yet advanced instruction following (IF)-especially for complex, multi-turn, and system-prompted instructions-remains a significant challenge. Rigorous evaluation and effective training for such capabilities are hindered by the lack of high-quality, human-annotated benchmarks and reliable, interpretable reward signals. In this work, we introduce AdvancedIF (we will release this benchmark soon), a comprehensive benchmark featuring over 1,600 prompts and expert-curated rubrics that assess LLMs ability to follow complex, multi-turn, and system-level instructions. We further propose RIFL (Rubric-based Instruction-Following Learning), a novel post-training pipeline that leverages rubric generation, a finetuned rubric verifier, and reward shaping to enable effective reinforcement learning for instruction following. Extensive experiments demonstrate that RIFL substantially improves the instruction-following abilities of LLMs, achieving a 6.7% absolute gain on AdvancedIF and strong results on public benchmarks. Our ablation studies confirm the effectiveness of each component in RIFL. This work establishes rubrics as a powerful tool for both training and evaluating advanced IF in LLMs, paving the way for more capable and reliable AI systems.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of high-quality, human-annotated benchmarks and rubrics for evaluating complex, multi-turn, and system-prompted instruction following<br>â€¢ Absence of reliable, interpretable reward signals for RL on instruction following; RLVR for math/code does not transfer to IF<br>â€¢ Preference-based reward models require large pairwise datasets, yield opaque signals, and are prone to reward hacking<br>â€¢ Need scalable, reliable rubric generators and verifiers; existing LLM-generated prompts/rubrics and generic LLM judges are inconsistent<br>â€¢ Difficulty rigorously assessing carried context and system prompt steerability in multi-turn settings, hindering training and evaluation</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce AdvancedIF, a 1.6k+ expert-authored benchmark with granular rubrics for complex, multi-turn, and system-level instructions, and propose RIFLâ€”a rubric-based RL pipeline that auto-generates rubrics, finetunes an LLM judge as a rubric verifier, and computes decomposed rubric-satisfaction rewards with reward shaping to mitigate reward hacking.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Robust Rubric Generation via Self-Refinement and Uncertainty Calibration: Build rubric generators that quantify confidence, detect ambiguity, and iteratively refine criteria to improve scalability and reliability<br>â€¢ Adversarially Robust Verifiers for Instruction Following: Train rubric verifiers with adversarial negatives, invariance checks, and consistency regularization to reduce reward hacking and superficial cue exploitation<br>â€¢ From Rubrics to Programs: Compiling Natural-Language Rubrics into Executable Checkers for IF: Translate rubric criteria into symbolic or neuro-symbolic checkers to provide verifiable, compositional, and auditable rewards at scale</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.10547" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.10547" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests. Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Text-to-image models often produce homogeneous outputs, failing to capture real-world variability across instances of the same concept.<br>â€¢ Existing metrics frequently conflate diversity with fidelity (e.g., FID), leading to misleading or non-actionable diversity assessments.<br>â€¢ Underspecified evaluation (without explicit concept and factor of variation) yields ambiguous human annotations and unstable model rankings.<br>â€¢ Absence of a standardized, concept- and attribute-aware benchmark prevents fair, granular comparison and diagnosis of model weaknesses.<br>â€¢ Practitioners lack tools to identify which concepts/attributes (e.g., color, species, background) models struggle with, limiting targeted improvement.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes an attribute-conditional evaluation framework that pairs a curated prompt set (concepts with explicit factors of variation) with a structured human evaluation template and compares models using binomial tests on human annotations. It also benchmarks multiple image embeddings for diversity measurement and uses the framework to rank T2I models by diversity and surface category-specific failures.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Attribute-Conditional Automatic Diversity Metrics from Human Judgments: Train preference models on the proposed human annotations to replace or augment human evaluation with reliable automated metrics per concept-attribute.<br>â€¢ Active Stress Testing for Diversity in T2I Models: An active-learning system that adaptively discovers hardest conceptâ€“attribute pairs to expose diversity failures and guide model/data interventions.<br>â€¢ Causal Attribute Disentanglement for Controllable Diversity in Text-to-Image Generation: Incorporate causal structure over factors of variation to explicitly control and evaluate diversity along interpretable attributes.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Music Flamingo: Scaling Music Understanding in Audio Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.10289" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.10289" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Music Flamingo, a novel large audio-language model designed to advance music (including song) understanding in foundational audio models. While audio-language research has progressed rapidly, music remains challenging due to its dynamic, layered, and information-dense nature. Progress has been further limited by the difficulty of scaling open audio understanding models, primarily because of the scarcity of high-quality music data and annotations. As a result, prior models are restricted to producing short, high-level captions, answering only surface-level questions, and showing limited generalization across diverse musical cultures. To address these challenges, we curate MF-Skills, a large-scale dataset labeled through a multi-stage pipeline that yields rich captions and question-answer pairs covering harmony, structure, timbre, lyrics, and cultural context. We fine-tune an enhanced Audio Flamingo 3 backbone on MF-Skills and further strengthen multiple skills relevant to music understanding. To improve the model's reasoning abilities, we introduce a post-training recipe: we first cold-start with MF-Think, a novel chain-of-thought dataset grounded in music theory, followed by GRPO-based reinforcement learning with custom rewards. Music Flamingo achieves state-of-the-art results across 10+ benchmarks for music understanding and reasoning, establishing itself as a generalist and musically intelligent audio-language model. Beyond strong empirical results, Music Flamingo sets a new standard for advanced music understanding by demonstrating how models can move from surface-level recognition toward layered, human-like perception of songs. We believe this work provides both a benchmark and a foundation for the community to build the next generation of models that engage with music as meaningfully as humans do.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Music understanding in audioâ€“language models is hard due to musicâ€™s dynamic, layered, and information-dense nature (key, harmony, tempo, timbre, structure, vocals).<br>â€¢ Scarcity of high-quality, richly annotated music data limits scaling and training of open audio understanding models.<br>â€¢ Existing models produce short, high-level captions, answer only surface-level questions, and generalize poorly across diverse musical cultures.<br>â€¢ Standard ALM tasks and methods (captioning, transcription, retrieval) are not directly suited to musicâ€™s specialized attributes and reasoning needs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Curate MF-Skills, a large-scale music dataset with rich captions and QA covering harmony, structure, timbre, lyrics, and cultural context, and fine-tune an enhanced Audio Flamingo 3 backbone. Improve reasoning via a post-training recipe: cold-start with MF-Think chain-of-thought data grounded in music theory, then apply GRPO-based reinforcement learning with custom rewards.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Cross-Cultural Music Understanding with Multilingual Audioâ€“Language Models: Expand datasets and training to better capture diverse musical traditions, languages, and styles for robust global generalization.<br>â€¢ Symbolicâ€“Audio Joint Training for Theory-Aware Music Reasoning: Integrate aligned symbolic representations (scores/MIDI) with audio to strengthen reasoning about harmony, structure, and rhythm.<br>â€¢ Long-Context Music Perception in ALMs: Develop architectures and benchmarks for album-length and performance-scale understanding, capturing macro-structure, motifs, and narrative across extended durations.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07685" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07685" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Deep Research tasks are open-ended, require multi-step reasoning and cross-document synthesis, and produce long-form answers with many valid solutions, making evaluation difficult.<br>â€¢ Existing QA and many DR benchmarks focus on short, easily verifiable answers and fail to capture DRâ€™s conceptual breadth, logical nesting depth, and exploration/open-endedness.<br>â€¢ Prior DR evaluations often rely on LLM-generated rubrics or reference reports, introducing circularity and limited human oversight, and many are narrow in scope.<br>â€¢ There is a lack of standardized, human-authored, fine-grained rubrics that separate mandatory vs. optional criteria, include negative checks, and support partial credit.<br>â€¢ The field needs scalable, robust, expert-aligned evaluation protocols to diagnose factual grounding and reasoning quality in DR agents, amid dynamic information sources.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Introduce RESEARCHRUBRICS: a domain-diverse benchmark of realistic prompts paired with 2,593 human-authored, fine-grained rubrics (including negative and required/optional criteria), guided by a tri-axial task complexity framework (breadth, nesting depth, exploration). Provide human and LLM-as-judge evaluation protocols with binary vs. ternary grading to measure rubric adherence, and empirically assess state-of-the-art DR agents (e.g., Gemini and OpenAI) to reveal performance gaps.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Learning Deep Research Agents with Rubric-Guided Reinforcement: Use rubric adherence as structured rewards to train DR agents that optimize factual grounding, reasoning soundness, and clarity.<br>â€¢ Building Calibrated LLM Judges for Open-Ended Research Evaluation: Develop judge models and protocols that better align with expert graders, incorporating ternary grading, uncertainty calibration, and bias mitigation.<br>â€¢ Complexity-Aware Planning for Deep Research: Design agents that explicitly model and adapt to the tri-axial complexity (breadth, depth, exploration) to improve retrieval strategies, reasoning decomposition, and synthesis quality.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.10047" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.10047" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Zero-shot anomaly classification (AC) and segmentation (AS) methods aim to identify and outline defects without using any labeled samples. In this paper, we reveal a key property that is overlooked by existing methods: normal image patches across industrial products typically find many other similar patches, not only in 2D appearance but also in 3D shapes, while anomalies remain diverse and isolated. To explicitly leverage this discriminative property, we propose a Mutual Scoring framework (MuSc-V2) for zero-shot AC/AS, which flexibly supports single 2D/3D or multimodality. Specifically, our method begins by improving 3D representation through Iterative Point Grouping (IPG), which reduces false positives from discontinuous surfaces. Then we use Similarity Neighborhood Aggregation with Multi-Degrees (SNAMD) to fuse 2D/3D neighborhood cues into more discriminative multi-scale patch features for mutual scoring. The core comprises a Mutual Scoring Mechanism (MSM) that lets samples within each modality to assign score to each other, and Cross-modal Anomaly Enhancement (CAE) that fuses 2D and 3D scores to recover modality-specific missing anomalies. Finally, Re-scoring with Constrained Neighborhood (RsCon) suppresses false classification based on similarity to more representative samples. Our framework flexibly works on both the full dataset and smaller subsets with consistently robust performance, ensuring seamless adaptability across diverse product lines. In aid of the novel framework, MuSc-V2 achieves significant performance improvements: a +23.7% AP gain on the MVTec 3D-AD dataset and a +19.3% boost on the Eyecandies dataset, surpassing previous zero-shot benchmarks and even outperforming most few-shot methods. The code will be available at The code will be available at https://github.com/HUST-SLOW/MuSc-V2{https://github.com/HUST-SLOW/MuSc-V2}.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Industrial zero-shot anomaly classification/segmentation needs to work without any labeled normals or prompts, yet anomalies are diverse, rare, and multimodal (2D/3D), making robust detection hard.<br>â€¢ Existing 2D/3D zero-shot methods rely on CLIP prompts or comparisons to labeled normal banks, overlooking rich mutual consistency among unlabeled samples in production lines.<br>â€¢ Standard 3D KNN grouping mixes points from discontinuous surfaces, yielding deviant tokens and false positives.<br>â€¢ Fixed-scale or average neighborhood aggregation dilutes small anomalies and misses variable-sized defects across both 2D and 3D.<br>â€¢ Single-modality systems miss anomalies that are prominent only in the other modality (2D-visible vs 3D-visible defects).<br>â€¢ Image-level AC based on max pixel scores is sensitive to local noise and weak anomalies, causing false positives/negatives.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>MuSc-V2 is a training-free zero-shot multimodal framework that mutually scores 2D/3D patches across unlabeled samples to exploit normal consistency and anomaly isolation. It introduces IPG for surface-consistent 3D grouping, SNAMD with similarity-weighted pooling for multi-scale features, cross-modal anomaly enhancement (CAE) to recover modality-specific misses, and RsCon manifold re-scoring with constrained neighborhoods for stable sample-level classification.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ Prompt-Free Cross-Factory Mutual Scoring for Industrial Anomaly Detection: Extend MuSc-V2 to transfer across production lines with adaptive sample graphs and robustness to domain shifts.<br>â€¢ Temporal MuSc: Zero-Shot Mutual Scoring with Spatio-Temporal Consistency for Video-Based Industrial Inspection: Incorporate temporal cues and motion-aware grouping to reduce noise and capture evolving defects.<br>â€¢ Camera-Free Cross-Modal Alignment for Zero-Shot Anomaly Detection: Learn projection-free 2Dâ€“3D correspondences to replace geometry/camera-based mapping in CAE.<br>â€¢ Uncertainty-Aware Mutual Scoring with Calibration and Active Subset Selection: Model score uncertainty to calibrate thresholds and actively select minimal unlabeled subsets for efficient inference.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.10017" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.10017" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Agents need fine-grained, instruction-conditioned understanding of what to act on, where actionable parts are, and how to interact (mask, motion type, motion axis) to enable physically grounded collaboration.<br>â€¢ Existing methods are object-centric and treat affordance grounding and motion estimation disjointly, lacking coherent, instruction-driven reasoning.<br>â€¢ Prior benchmarks (e.g., SceneFun3D) assume instruction-agnostic motion prediction for all parts, limiting context-aware and task-conditioned performance.<br>â€¢ Applying 2D-native MLLMs to 3D spatial tasks faces a modality gap, hindering physically grounded inference from 3D inputs.<br>â€¢ Video-based approaches incur high computational overhead, process redundant frames, and suffer from viewpoint limitations, underutilizing concise 3D point cloud geometry.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>AffordBot bridges 3D point clouds to 2D-compatible MLLMs by rendering surround-view images and projecting 3D affordance candidates, then runs a tailored chain-of-thought with an active perception step to select the most informative viewpoint followed by stepwise affordance grounding and interaction inference to output triplets (mask, motion type, motion axis).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ 3D-Native Multimodal LLMs for Affordance Reasoning: Develop architectures that ingest 3D geometry directly to perform instruction-conditioned triplet prediction without 2D rendering.<br>â€¢ End-to-End Differentiable AffordBot with Real-Robot Feedback: Train the projection, viewpoint selection, and reasoning modules jointly using differentiable rendering and closed-loop physical interaction signals.<br>â€¢ Active Viewpoint Planning for Instruction-Conditioned 3D Reasoning: Integrate reinforcement learning to optimize viewpoint selection policies that maximize affordance grounding and motion inference accuracy under task constraints.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.09715" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.09715" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the user's ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Instruction-based image editing applies edits with fixed, discrete strengths, preventing fine-grained, continuous control over each instruction.<br>â€¢ Lack of disentangled per-instruction control in multi-instruction prompts limits interpretability and user steerability; edits often interfere with each other.<br>â€¢ Prior slider methods focus on text-to-image and typically require per-attribute training (e.g., separate LoRAs), perform poorly with multiple edits, and transfer poorly to real-image editing.<br>â€¢ Users need continuous suppression-to-amplification of edits while preserving spatial locality and global semantic consistencyâ€”current approaches resort to resampling without systematic control.<br>â€¢ A lightweight, model-agnostic solution that plugs into state-of-the-art editors (e.g., FLUX-Kontext, Qwen-Image-Edit) without per-instruction fine-tuning is missing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>SliderEdit trains a single set of low-rank adaptation matrices on instruction-relevant token embeddings in MMDiT editors using a Partial Prompt Suppression loss that matches outputs with and without a target instruction, learning how to neutralize its effect. At inference, scaling these adapter weights exposes per-instruction sliders that continuously suppress, apply, or amplify each edit while maintaining disentanglement and visual consistency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ SliderEdit-Video: Temporally Consistent Continuous Instruction-Based Video Editing: Extend instruction-level sliders to video with temporal coherence and motion-aware adapters.<br>â€¢ Universal Instruction Sliders via Model-Agnostic Adapters: Learn adapters that transfer across diverse editing backbones and tokenizers without retraining.<br>â€¢ Attention-Guided Instruction Token Localization for Stronger Disentanglement: Improve automatic identification and weighting of instruction-relevant tokens to further reduce edit entanglement and spillover.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.07790" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.07790" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Sentiments about the reproducibility of cited papers in downstream literature offer community perspectives and have shown as a promising signal of the actual reproducibility of published findings. To train effective models to effectively predict reproducibility-oriented sentiments and further systematically study their correlation with reproducibility, we introduce the CC30k dataset, comprising a total of 30,734 citation contexts in machine learning papers. Each citation context is labeled with one of three reproducibility-oriented sentiment labels: Positive, Negative, or Neutral, reflecting the cited paper's perceived reproducibility or replicability. Of these, 25,829 are labeled through crowdsourcing, supplemented with negatives generated through a controlled pipeline to counter the scarcity of negative labels. Unlike traditional sentiment analysis datasets, CC30k focuses on reproducibility-oriented sentiments, addressing a research gap in resources for computational reproducibility studies. The dataset was created through a pipeline that includes robust data cleansing, careful crowd selection, and thorough validation. The resulting dataset achieves a labeling accuracy of 94%. We then demonstrated that the performance of three large language models significantly improves on the reproducibility-oriented sentiment classification after fine-tuning using our dataset. The dataset lays the foundation for large-scale assessments of the reproducibility of machine learning papers. The CC30k dataset and the Jupyter notebooks used to produce and analyze the dataset are publicly available at https://github.com/lamps-lab/CC30k .</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">ðŸŽ¯</span>Research Motivation</h3>
            <div class="motivation">
                <p>â€¢ Lack of datasets capturing reproducibility-oriented sentiment (ROS) in citation contexts; existing sentiment and citation corpora do not encode reproducibility/replicability cues.<br>â€¢ Automated assessment of reproducibility remains difficult; community signals embedded in citation contexts are underutilized despite their potential to predict actual reproducibility.<br>â€¢ Off-the-shelf sentiment models and LLMs fail to reliably detect ROS, indicating the need for domain-specific training data.<br>â€¢ Severe class imbalance (especially scarce negative ROS) hampers effective model training and evaluation.<br>â€¢ Need scalable, high-quality annotation pipelines that do not rely exclusively on domain experts while maintaining high label accuracy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ”§</span>Research Method</h3>
            <div class="method">
                <p>Construct CC30k, a 30,734-sample dataset of ML citation contexts labeled Positive/Negative/Neutral for reproducibility-oriented sentiment via a pipeline: collect contexts from citing papers (S2GA) of ML reproducibility and original studies, filter ambiguous citation marks, crowdsource labels with carefully curated workers and majority voting, validate (~94% accuracy), and augment negatives through a controlled, human-verified supervised labeling step. Demonstrate utility by fine-tuning LLMs, which significantly improves ROS classification performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">ðŸ’¡</span>Research Ideas</h3>
            <div class="idea">
                <p>â€¢ RAG-Enhanced Reproducibility Prediction from Citation Contexts: Combine CC30k-trained ROS classifiers with retrieval over full texts to predict paper-level reproducibility scores.<br>â€¢ Cross-Domain Generalization of Reproducibility-Oriented Sentiment Models: Evaluate and adapt ROS models trained on ML to other fields (e.g., biomedical, social sciences).<br>â€¢ Causal Links Between Citation Sentiment and Actual Reproducibility Outcomes: Quantify how ROS signals correlate with independent replication attempts over time.<br>â€¢ Few-Shot Detection of Negative Reproducibility Signals: Develop methods to identify rare negative ROS with minimal supervision and robust uncertainty estimation.<br>â€¢ Multimodal Reproducibility Assessment Integrating Code and Data Artifacts: Fuse ROS signals with code availability, execution logs, and dataset metadata to improve prediction.<br>â€¢ Disambiguating Ambiguous Citation Contexts with Structured Parsing: Build models to detect, segment, and resolve multi-citation contexts for cleaner ROS labeling.<br>â€¢ Temporal Dynamics of Reproducibility Sentiment in ML: Analyze how ROS evolves across versions, venues, and time, and its relation to community adoption and benchmarks.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 6</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>â† ä¸Šä¸€é¡µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button></span>
            <button id="next-btn" onclick="changePage(1)">ä¸‹ä¸€é¡µ â†’</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-14 23:07:22 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 6;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>