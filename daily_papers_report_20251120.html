<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - November 20, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">November 20, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.14993" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.14993" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Scaling text-to-video to high resolution and >5 s duration is constrained by the quadratic cost of spatio-temporal attention, causing slow training/inference and high memory use (page 4‚Äì5; NABLA target)<br>‚Ä¢ Current diffusion/video generators often require large Numbers of Function Evaluations (e.g., ~100 steps), leading to slow sampling; efficient distillation without quality loss is limited (page 5)<br>‚Ä¢ Lack of end-to-end data curation and multi-stage post-training (SFT + RL) limits realism, prompt alignment, and editing controllability across image and video (pages 1, 4‚Äì5)<br>‚Ä¢ Few openly available, high-quality, adaptable foundation models unify T2I, I2I editing, T2V, and I2V with released code/weights (pages 1, 5)<br>‚Ä¢ Training/inference inefficiencies (VAE bottlenecks, text encoder overhead, GPU memory pressure) hinder practical deployment at scale (page 5)<br>‚Ä¢ Human-evaluable quality parity with leading systems requires architectural and inference optimizations validated by side-by-side testing (pages 1, 5)</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Introduce a CrossDiT-based family of image/video diffusion transformers (2B/6B/19B) with Neighborhood Adaptive Block-Level Attention (NABLA) to sparsify spatio-temporal attention (‚âà90% sparsity), yielding ‚âà2.7√ó speedups at maintained quality, and a multi-stage pipeline of pretraining, supervised fine-tuning, RL-based adversarial post-training, and combined CFG + trajectory-segmented consistency distillation to reduce NFE from 100 to 16. System-level optimizations include VAE acceleration, text encoder quantization, and F/HSDP with activation checkpointing to cut latency and memory.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond Ten Seconds: Scaling NABLA-Sparsified CrossDiT to Minute-Long, 4K Text-to-Video Generation: Extend sparse spatio-temporal attention and memory strategies for long-horizon, ultra-high-resolution videos with consistent dynamics.<br>‚Ä¢ NABLA++: Learned Structure-Aware Sparse Attention for Video Diffusion: Learn data-adaptive sparsity patterns and schedules to further reduce complexity while preserving motion and semantics.<br>‚Ä¢ One Model to Edit Them All: Unified Cross-Modal Editing for Image and Video with RL-Feedback: A single model for T2I/I2I/T2V/I2V editing trained with multi-objective RL for fidelity, alignment, and temporal consistency.<br>‚Ä¢ 8-Step, 8-Bit: Extreme Distillation and Quantization for Real-Time On-Device Video Generation: Combine aggressive consistency distillation and post-training quantization for mobile/edge deployment.<br>‚Ä¢ Character and Scene Persistence in Generative Video via Cross-Frame Identity Tokens: Introduce persistent identity and layout tokens to maintain subject, style, and scene continuity across long clips.<br>‚Ä¢ Data Curation at Scale: Self-Supervised Scoring and Clustering Pipelines for Multi-Modal Generative Pretraining: Automate quality, diversity, and cultural-bias-aware filtering to improve pretraining and SFT datasets.<br>‚Ä¢ RLHF for Video: Human Preference and Adversarial Feedback for Motion Quality and Prompt Following: Design preference models and adversarial critics tailored to temporal coherence, cinematography, and instruction alignment.<br>‚Ä¢ Benchmarking Beyond CLIP/FVD: A Comprehensive Human+Automated Suite for Video Prompt Following, Aesthetics, and Temporal Consistency: Establish robust evaluation protocols and datasets to standardize progress in generative video.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.15065" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.15065" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of a dedicated benchmark to evaluate ‚Äúreasoning via video,‚Äù where intermediate reasoning unfolds through frames rather than text; existing evaluations emphasize visual fidelity and coherence instead of reasoning quality.<br>‚Ä¢ Multimodal reasoning is largely expressed via text (Chain-of-Thought) even for visual tasks, missing video‚Äôs explicit spatial layouts and temporal continuity that are critical for spatial planning and multi-step reasoning.<br>‚Ä¢ Absence of fine-grained, objective, trajectory-level metrics for video reasoning; prior work often relies on manual inspection or coarse measures that do not capture path optimality or rule compliance.<br>‚Ä¢ No systematic modality comparison between video models (reasoning via video) and VLMs (reasoning via text), leaving unclear whether video generation offers unique reasoning advantages.<br>‚Ä¢ Neglect of training and inference scaling analyses for video reasoning (e.g., the roles of supervised fine-tuning and test-time diverse sampling) that are known to matter in LLM reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Propose VR-Bench, a maze-solving benchmark (7,920 procedurally generated videos across Regular, Irregular, 3D, Trapfield, Sokoban) that elicits chain-of-frame reasoning and evaluates trajectories with path-matching metrics (Exact Match, Success Rate, Precision Rate, Step Deviation) plus rule-compliance (VLM-based judge) and Maze Fidelity. Using supervised fine-tuning on open-source video models and test-time diverse sampling (Pass@K), the paper demonstrates video-based reasoning that surpasses VLMs and generalizes across difficulty, textures, and maze types.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Video Olympiad: Solving Physics and Math Problems via Video-Based Reasoning: Extend VR-Bench to Olympiad-level physics/mathematics tasks where models reason by generating temporally grounded visual solutions.<br>‚Ä¢ Embodied VR-Bench: Interactive Video Reasoning with Actionable Environments: Introduce interactive settings requiring models to plan, act, and simulate environment dynamics, evaluating closed-loop video reasoning.<br>‚Ä¢ Self-Consistency in Video Space: Test-Time Scaling Strategies for Video Reasoning: Systematize diverse sampling, selection, and aggregation methods (Pass@K, reranking) to boost reliability of video reasoning.<br>‚Ä¢ Rule-Aware RL for Video Reasoning: Learning with VLM Critics and Maze Fidelity: Train video models with reinforcement signals from rule-compliance judges and structural fidelity metrics to internalize physical and spatial constraints.<br>‚Ä¢ From Mazes to Open-World Navigation: Scaling Chain-of-Frame Planning to Real Videos: Apply chain-of-frame reasoning to real-world navigation and manipulation videos, studying robustness under complex, noisy scenes.<br>‚Ä¢ Unified Interleaved Video‚ÄìText CoT: Bridging Reasoning via Video and Text: Combine interleaved visual frames and textual rationales to exploit complementary strengths of video dynamics and symbolic abstraction.<br>‚Ä¢ Robust Video Reasoning under Texture and Domain Shifts: Benchmark and train methods that sustain reasoning accuracy under heavy style, lighting, and viewpoint shifts using explicit invariance objectives.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.15593" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.15593" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Key drivers of success/failure in AI research agents are poorly understood due to long, multi-step agent trajectories and tool-use complexity, making targeted improvement difficult.<br>‚Ä¢ There is no standard way to quantify or control ideation diversity in agents; prior work lacks actionable metrics linking ideation choices to outcomes.<br>‚Ä¢ Existing evaluations over-rely on Kaggle's medal rate, which is coarse, inconsistent across tasks, and sensitive to human score distributions, obscuring true agent capability.<br>‚Ä¢ The impact of agent design (LLM backbone, scaffold, memory/prompting) on ideation diversity and performance has not been systematically analyzed at scale.<br>‚Ä¢ There is no causal evidence that ideation diversity improves performance; past observations are largely correlational or outside the ML-agent setting.<br>‚Ä¢ Large-scale trajectory analysis is computationally prohibitive, limiting ablations and robust conclusions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Measure ideation diversity by computing Shannon entropy over the distribution of model architectures proposed in agents‚Äô initial Draft nodes, then causally test its effect by ablating diversity via prompt design (removing diversity cues and adaptive complexity) across multiple scaffolds and tasks. Validate findings with a large trajectory bank (11,000 runs) and a controlled experiment, and corroborate results using alternative metrics (valid submission rate, average normalized score, percentile, ELO) beyond medal rate.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Diversity-Aware Search Policies for AI Research Agents: Learn search policies that explicitly optimize a diversity‚Äìfeasibility objective to generate varied yet implementable plans.<br>‚Ä¢ Decoupled Ideation‚ÄìImplementation Agents for Causal Attribution: Use separate LLMs (or modules) for planning and coding to isolate and measure the causal impact of ideation diversity.<br>‚Ä¢ Beyond MLE-Bench: Cross-Benchmark Validation of Ideation Diversity Effects: Test whether diversity‚Äìperformance relations generalize to newer and non-Kaggle ML benchmarks and domains.<br>‚Ä¢ Adaptive Diversity Control via RL and Prompt Optimization: Automatically tune diversity controls (prompt cues, memory, sampling) to balance exploration‚Äìexploitation per task.<br>‚Ä¢ Feasibility Prediction to Guide Diverse Planning: Train predictors of implementability/runtime to steer diverse ideation toward plans the agent can reliably execute.<br>‚Ä¢ Robust, Human-Independent Agent Metrics: Develop and standardize evaluation suites (e.g., ELO, normalized scores) that reduce dependence on human leaderboards and threshold artifacts.<br>‚Ä¢ Population- and Multi-Agent Approaches to Ideation Diversity: Orchestrate heterogeneous agent populations to diversify initial plans while coordinating toward feasible solutions.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">VisPlay: Self-Evolving Vision-Language Models from Images</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.15661" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.15661" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ RL post-training for VLMs depends on human-annotated labels or task-specific verifiers, which are costly, brittle, and limit scalability.<br>‚Ä¢ Self-evolution is underexplored for VLMs: visual grounding makes task generation and automatic reward design hard without labels, and prior methods often rely on external tools/models.<br>‚Ä¢ Abundant unlabeled images exist online, but current pipelines lack a way to turn them into challenging, answerable training signals while avoiding mode collapse and hallucination.<br>‚Ä¢ Existing RL approaches optimize narrow, verifiable tasks and struggle to jointly balance question difficulty with answer quality and diversity in a stable, scalable loop.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>VisPlay decomposes a single base VLM into an Image-Conditioned Questioner and a Multimodal Reasoner that co-evolve via GRPO: the Questioner is rewarded by uncertainty- and diversity-aware signals to generate challenging yet answerable image-grounded questions, while the Reasoner is trained with verifiable pseudo-labels (majority-vote) on curated, moderate-confidence samples. Iterating this loop improves visual reasoning, compositional generalization, and reduces hallucination without human labels.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Scaling VisPlay to 10B+ Vision-Language Models with Memory-Efficient Self-Play: Study scaling laws, stability, and compute-efficient rollouts to extend self-evolution to larger backbones.<br>‚Ä¢ Self-Verifying Multimodal Rewards: Faithful Automatic Verification for Data-Free VLM RL: Develop learned or hybrid verifiers and uncertainty calibration to robustly validate self-generated data and prevent error accumulation.<br>‚Ä¢ VisPlay-Video: Self-Evolving Multimodal Reasoning from Unlabeled Videos: Extend the questioner‚Äìreasoner loop to temporal and spatiotemporal reasoning with video-consistency rewards and multi-frame grounding.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.15186" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.15186" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by a small number of target labels and the reliance on long, detailed expert-level text inputs, creating a barrier to practical use. To address these limitations, we introduce a new paradigm: instruction-guided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level CXR lesion grounding.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ CXR lesion segmentation models are limited by few target labels and require long, expert-level prompts, making them impractical for broad clinical use (pp.1‚Äì3).<br>‚Ä¢ No existing large-scale dataset pairs pixel-level masks with simple, user-friendly instructions across multiple lesion types; most prior resources offer coarse boxes, single-type masks, or lack text‚Äìmask linkage (Table 1, p.2).<br>‚Ä¢ There is a need to handle diverse instruction intents‚Äîspecific location, global search, and absence confirmation‚Äîplus concise textual explanations; general-domain VLMs transfer poorly, with near-zero empty-target accuracy (Table 4, p.7).<br>‚Ä¢ Constructing such a dataset is hard without costly expert annotations; the key challenge is deriving grounded lesion masks and aligned instruction‚Äìanswer pairs from raw image‚Äìreport pairs automatically (Sec. 3, p.3).<br>‚Ä¢ Reducing radiologists‚Äô workload in precisely localizing lesions is clinically important given the ubiquity of CXR and the labor-intensive nature of contouring (p.1).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>They propose a fully automated multimodal pipeline that converts MIMIC-CXR image‚Äìreport pairs into grounded lesion masks and 1.1M instruction‚Äìanswer pairs (MIMIC-ILS) using LLM-based report structuring/location mapping, diffusion editing (RadEdit) to form anomaly maps, anatomy masks (CXAS), YOLO detections, and a filtering/post-processing algorithm to verify locations and derive empty targets. A VLM (ROSALIA) is then fine-tuned by integrating LISA with SAM, using a [SEG] token to produce masks and brief text from simple instructions, achieving strong IoU and high empty-target accuracy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond MIMIC-ILS: Instruction-Guided Lesion Segmentation Across Modalities (CT, MRI): Extend the automated grounding pipeline and ILS paradigm beyond CXRs to CT/MRI and additional thoracic pathologies.<br>‚Ä¢ Trustworthy ILS: Uncertainty-Aware Absence Confirmation and Calibration in Chest X-ray Segmentation: Quantify and calibrate uncertainty for empty-target decisions, enabling safer clinical deployment.<br>‚Ä¢ Physician-in-the-Loop ILS: Interactive Instruction Refinement and Active Mask Correction: Incorporate clinician feedback to iteratively refine instructions and masks, improving data quality and model reliability.<br>‚Ä¢ Generalizable ILS: Domain Adaptation from MIMIC-ILS to Multi-Institutional Chest X-rays: Develop adaptation methods for robust performance across hospitals, devices, and patient populations.<br>‚Ä¢ Self-Improving ILS: Pseudo-Label Denoising and Diffusion-Aided Mask Refinement: Bootstrap higher-quality masks via consistency training, diffusion edits, and component-level denoising.<br>‚Ä¢ Explainable ILS: Joint Visual‚ÄìLinguistic Rationales for Lesion Grounding: Generate faithful rationales linking text instructions to segmented regions for transparent decision-making.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.14349" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.14349" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The proliferation of hour-long videos (e.g., lectures, podcasts, documentaries) has intensified demand for efficient content structuring. However, existing approaches are constrained by small-scale training with annotations that are typical short and coarse, restricting generalization to nuanced transitions in long videos. We introduce ARC-Chapter, the first large-scale video chaptering model trained on over million-level long video chapters, featuring bilingual, temporally grounded, and hierarchical chapter annotations. To achieve this goal, we curated a bilingual English-Chinese chapter dataset via a structured pipeline that unifies ASR transcripts, scene texts, visual captions into multi-level annotations, from short title to long summaries. We demonstrate clear performance improvements with data scaling, both in data volume and label intensity. Moreover, we design a new evaluation metric termed GRACE, which incorporates many-to-one segment overlaps and semantic similarity, better reflecting real-world chaptering flexibility. Extensive experiments demonstrate that ARC-Chapter establishes a new state-of-the-art by a significant margin, outperforming the previous best by 14.0% in F1 score and 11.3% in SODA score. Moreover, ARC-Chapter shows excellent transferability, improving the state-of-the-art on downstream tasks like dense video captioning on YouCook2.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Explosion of hour‚Äëlong videos (lectures, podcasts, documentaries) creates a need for automatic, navigable chaptering and hierarchical summaries for efficient retrieval and consumption.<br>‚Ä¢ Existing methods are trained on small datasets with short, coarse labels (often only brief titles), limiting generalization to nuanced topic shifts in long videos.<br>‚Ä¢ Modeling long-horizon, multimodal semantics (video+audio/ASR+on‚Äëscreen text) under strict context budgets remains challenging; many approaches rely on a single modality.<br>‚Ä¢ Lack of large-scale, bilingual datasets with temporally grounded, hierarchical annotations hampers training and evaluation.<br>‚Ä¢ Current evaluation metrics (e.g., SODA with one-to-one matching) mis-handle variable annotation granularity and under-reward valid many-to-one matches, misaligning with human judgment.<br>‚Ä¢ Prior work reports performance saturation on small corpora, leaving open whether scaling data and label richness improves long-video chaptering.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>ARC-Chapter is an instruction-tuned multimodal LLM (Qwen2.5‚ÄëVL‚Äë7B) that ingests sampled video frames and timestamped ASR to generate timestamped chapters with short titles, structured (title/abstract/intro) summaries, and dense video descriptions; it is trained on a million-scale bilingual dataset built via an automated pipeline that fuses Whisper ASR with Qwen2.5‚ÄëVL captions/OCR and LLM reasoning. The framework introduces the GRACE metric for granularity‚Äërobust many‚Äëto‚Äëone matching and applies GRPO reinforcement learning with a temporal-only reward to sharpen boundary localization.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ GRACE++: Differentiable Granularity-Robust Objectives for End-to-End Chaptering: Integrate GRACE-like many-to-one alignment into training (via differentiable surrogates or RL) to jointly optimize temporal and semantic quality.<br>‚Ä¢ Audio Beyond ASR: Efficient Long-Context Audio Representations for Chaptering: Develop compressed, aligned audio tokenization to capture non-speech cues (music, silence, SFX) alongside ASR text.<br>‚Ä¢ Streaming ARC-Chapter: Low-Latency Online Chaptering for Live and Long Videos: Design a streaming, memory-augmented model that emits chapters incrementally with latency/quality trade-offs.<br>‚Ä¢ Personalizable Chaptering: Preference- and Task-Aware Granularity Control: Learn to adapt chapter granularity and style to user intent, domain, or device constraints.<br>‚Ä¢ Multilingual Expansion: Cross-Lingual and Low-Resource Chaptering with Unified Pretraining: Extend beyond EN‚ÄìZH via multilingual pretraining and alignment to generalize to low-resource languages and code-switching.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">MHR: Momentum Human Rig</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.15586" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.15586" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present MHR, a parametric human body model that combines the decoupled skeleton/shape paradigm of ATLAS with a flexible, modern rig and pose corrective system inspired by the Momentum library. Our model enables expressive, anatomically plausible human animation, supporting non-linear pose correctives, and is designed for robust integration in AR/VR and graphics pipelines.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Prior body models derive skeletal joints from the surface, entangling shape and skeleton, causing incorrect vertex‚Äìjoint correlations and limiting direct control over bone lengths and proportions (harder keypoint fitting and retargeting)<br>‚Ä¢ LBS alone introduces artifacts; dense non-linear correctives create spurious long-range dependencies, while purely sparse linear correctives lack expressivity<br>‚Ä¢ Data-driven facial expression spaces (e.g., FLAME) are dense, non-semantic, and leak pose, conflicting with artist workflows that require sparse, semantic controls<br>‚Ä¢ Existing pipelines offer limited, non-robust integration for AR/VR and production (e.g., insufficient LOD support, optimized skinning weights that lack structure/locality for artists)<br>‚Ä¢ Licensing and dataset compliance in prior models hinder open, industry-friendly deployment<br>‚Ä¢ Even ATLAS, while decoupled, uses FLAME expressions and a skeleton not optimized for additive pose correctives, limiting practical adoption</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MHR introduces an expressive, industry-ready parametric human model that decouples an internal 127-joint skeleton (204 parameters: 136 pose, 68 skeletal transforms) from the external mesh, uses artist-defined LBS with multi-LOD, and provides FACS-based (72), semantic facial expressions. It employs sparse, locality-regularized non-linear pose correctives by combining a per-joint local MLP on 6D rotations with geodesic-initialized, ReLU-masked linear maps, and learns a partitioned identity space (body/head/hands) from large, compliant datasets.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ MHR-Eyes: Integrating Explicit Eyeball Geometry into a Decoupled Parametric Human Rig: Add anatomically accurate eyeballs and gaze controls to improve realism and capture/fitting robustness.<br>‚Ä¢ DentTongue-MHR: An Explicit Mouth, Teeth, and Tongue System for Speech-Accurate Facial Animation: Incorporate a rigged oral cavity to enable faithful speech articulation and lip‚Äìteeth‚Äìtongue interactions.<br>‚Ä¢ Shape-Conditioned Pose Correctives for MHR: Learn pose correctives conditioned on identity shape to improve deformation fidelity across diverse body types.<br>‚Ä¢ SoftBody-MHR: Coupling Decoupled Rigs with Soft-Tissue and Clothing Dynamics: Integrate data-driven soft-tissue and garment models while preserving skeleton/mesh decoupling for real-time use.<br>‚Ä¢ RT-MHR: Real-Time Optimization and Deployment of MHR in AR/VR: Develop fast, robust per-frame fitting and streaming pipelines with strict influence limits and multi-LOD rendering.<br>‚Ä¢ Stylized-MHR: Extending a Decoupled Human Rig to Stylized Characters: Generalize MHR‚Äôs rig and corrective system to non-photorealistic, stylized character topologies and proportions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13524" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13524" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ VLN agents rely on static, one-shot instructions and cannot handle dynamic goals, multi-turn dialogue, or interactive guidance.<br>‚Ä¢ High-level planning is decoupled from social intention modeling, so agents fail to interpret socially salient cues and act contextually.<br>‚Ä¢ Existing simulators lack rich, dynamic, socially grounded elements (moving pedestrians/vehicles, weather, diverse human behaviors), limiting realism and generalization.<br>‚Ä¢ Absence of closed-loop evaluation where agents can self-assess, ask for help, and adapt in real time; interaction as an information modality is underexplored.<br>‚Ä¢ Current datasets often miss continuous control, dynamic human interactions, and rich multimodal annotations needed for human-centric navigation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>FreeAskWorld introduces an LLM-driven, synchronous closed-loop simulator and dataset that integrate a people-simulation pipeline (LLM-generated profiles/schedules and navigation styles, SMPL-X motions, appearance variation) with dynamic world systems (traffic, weather) and a Direction Inquiry Task where agents proactively ask for and use guidance. It outputs rich multimodal supervision (RGB panoramas, depth, semantics, occupancy, dialogs, trajectories) and supports benchmarking and fine-tuning of VLN models in socially dynamic environments.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Interaction-as-Information: Learning to Ask, Fuse, and Weight Human Guidance for Embodied Navigation: Build uncertainty-aware policies that decide when to inquire, how much to ask, and how to fuse dialog with perception for robust planning.<br>‚Ä¢ Memory-Augmented Social Navigation in Dynamic Crowds: Develop long-horizon planners with multimodal memory to handle crowded, changing scenes while maintaining social compliance.<br>‚Ä¢ Negotiation and Multi-Agent Task Coordination in FreeAskWorld: Extend beyond direction inquiries to negotiation, collaboration, and role-based coordination among human-like agents and robots.<br>‚Ä¢ Real-to-Sim-to-Real Transfer for Human-Centric Navigation: Leverage the URDF-enabled simulator to train and deploy social navigation policies that transfer to physical robots with minimal performance loss.<br>‚Ä¢ Generative Human Societies for Robust Embodied Learning: Use generative models to synthesize high-fidelity appearances and diverse social behaviors, enabling scalable domain randomization and better generalization.<br>‚Ä¢ Measuring Socially Grounded Interaction: A Benchmark Suite and Metrics Beyond SR/SPL: Propose new metrics (politeness, collision-free yield, inquiry efficiency, social compliance) and tasks for comprehensive evaluation.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Aligning Generative Music AI with Human Preferences: Methods and Challenges</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.15038" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.15038" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in generative AI for music have achieved remarkable fidelity and stylistic diversity, yet these systems often fail to align with nuanced human preferences due to the specific loss functions they use. This paper advocates for the systematic application of preference alignment techniques to music generation, addressing the fundamental gap between computational optimization and human musical appreciation. Drawing on recent breakthroughs including MusicRL's large-scale preference learning, multi-preference alignment frameworks like diffusion-based preference optimization in DiffRhythm+, and inference-time optimization techniques like Text2midi-InferAlign, we discuss how these techniques can address music's unique challenges: temporal coherence, harmonic consistency, and subjective quality assessment. We identify key research challenges including scalability to long-form compositions, reliability amongst others in preference modelling. Looking forward, we envision preference-aligned music generation enabling transformative applications in interactive composition tools and personalized music services. This work calls for sustained interdisciplinary research combining advances in machine learning, music-theory to create music AI systems that truly serve human creative and experiential needs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Likelihood-based training in music models fails to capture nuanced human preferences (aesthetics, emotion, cultural context), creating a persistent alignment gap.<br>‚Ä¢ Music‚Äôs hierarchical, long-range temporal structure demands coherence across bars, phrases, and full compositions (temporal and harmonic consistency) that current models/metrics don‚Äôt fully ensure.<br>‚Ä¢ Existing automatic metrics (e.g., FAD, IS, CLAP) only partially correlate with human judgment, leading to unreliable evaluation and an ‚Äúevaluation paradox.‚Äù<br>‚Ä¢ Preferences are subjective, culturally contingent, and dynamic; current systems lack robust personalization and cross-cultural competency.<br>‚Ä¢ Practical limitations include the scarcity and privateness of large-scale preference datasets (hurting reproducibility), and computational overheads for training-time RLHF/DPO and inference-time optimization.<br>‚Ä¢ Multi-objective trade-offs (text adherence, audio quality, style, structure) are hard to balance reliably within current generation pipelines.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper synthesizes a two-pronged framework for preference alignment in music: training-time optimization (RLHF and Direct Preference Optimization within autoregressive and diffusion architectures) and inference-time optimization (e.g., contrastive decoding, preference-conditioned sampling, control vector steering, tree-search), exemplified by MusicRL, DiffRhythm+, and Text2midi-InferAlign. It analyzes how these approaches address music-specific challenges (temporal/harmonic coherence, subjectivity) and outlines evaluation and deployment considerations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ OpenMusicPref: A Cross-Cultural, Large-Scale Preference Dataset for Music Generation: Create an open, privacy-preserving, demographically balanced dataset of pairwise preferences spanning genres, cultures, and use-cases.<br>‚Ä¢ Hierarchical Preference Models for Long-Form Composition: Learn multi-scale rewards capturing bar/phrase/form-level coherence and harmonic tension-resolution to guide long-duration generation.<br>‚Ä¢ UniAlign: A Unified Inference-Time Multi-Objective Optimizer for Text-to-Music: Low-latency, plug-and-play inference framework that jointly optimizes text adherence, harmony, structure, and aesthetic quality.<br>‚Ä¢ PersonalizeMe: Few-Shot Preference Alignment for Individual Listeners: Efficient user embedding and meta-learning to adapt models to individual tastes with minimal data and strong privacy guarantees.<br>‚Ä¢ CrossModal-Sync: Real-Time Video‚ÄìMusic Alignment with Affective and Structural Consistency: Joint modeling of affect, rhythm, and scene dynamics for adaptive scoring of videos in real time.<br>‚Ä¢ RobustPref: Safety, Bias, and Adversarial Robustness in Preference-Aligned Music AI: Detect and mitigate cultural bias, mode collapse, and reward hacking; defend against adversarial prompts.<br>‚Ä¢ Continual-Align: Dynamic Preference Tracking and Lifelong Music Alignment: Model preference drift over time with incremental learning and automated monitoring to trigger re-alignment.<br>‚Ä¢ UniEval-Music: A Universal, Cross-Cultural Evaluation Suite for Preference-Aligned Music: Build validated metrics and protocols that correlate with human judgments across genres, cultures, and long-form pieces.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Mixture of States: Routing Token-Level Dynamics for Multimodal Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.12207" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.12207" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-k hidden states and is trained with an Œµ-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to 4times larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Bridge the mismatch between static text encodings and the intrinsically dynamic, timestep-dependent diffusion process, which is critical for precise, controllable generation and editing.<br>‚Ä¢ Overcome fixed/rigid fusion schemes (cross- and self-attention) that either use only final-layer text features or incur quadratic cost with long joint sequences, limiting efficiency and expressiveness.<br>‚Ä¢ Remove the MoT constraint of symmetric, layer-by-layer coupling that requires identical depths/widths, preventing use of heterogeneous text/vision backbones.<br>‚Ä¢ Replace one-size-fits-all conditioning with token-specific, adaptive selection across all text layers, since different tokens and timesteps need different semantic states.<br>‚Ä¢ Achieve state-of-the-art quality under tight compute budgets by making conditioning sparse and learnable, rather than dense and hand-crafted.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MoS introduces a dual-tower architecture linked by a lightweight, token-wise router that, at each denoising timestep and generation block, sparsely top-k selects and aggregates layer-level hidden states from the understanding tower conditioned on the prompt, noisy latent, and timestep. The routed features are projected and concatenated into the generation block; the router is trained with an Œµ-greedy strategy for exploration, enabling dynamic, token- and time-dependent fusion between asymmetric transformers.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Mixture of States for Video and 4D Generation: Extend MoS with spatiotemporal routing that conditions across frames and timesteps to ensure temporal coherence and fine-grained control in video/4D synthesis.<br>‚Ä¢ Bandit- and RL-Optimized Routing for Diffusion: Replace Œµ-greedy with uncertainty-aware bandits or reinforcement learning and differentiable sparse gates (e.g., Gumbel) to learn routing policies that adapt compute and improve alignment.<br>‚Ä¢ End-to-End Co-Training and Parameter-Efficient MoS: Jointly adapt the frozen understanding tower via adapters/LoRA and learn router+generator together, studying scaling laws and asymmetry for improved alignment under tight compute.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Medal S: Spatio-Textual Prompt Model for Medical Segmentation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2511.13001" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2511.13001" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Multi-modal heterogeneity and large-class coverage in 3D medical imaging demand models that preserve full 3D context and work consistently across CT, MRI, PET, ultrasound, and microscopy.<br>‚Ä¢ Existing spatial-prompt methods downsample or compress prompts, losing voxel-level detail and misaligning with text cues; they also segment classes sequentially, causing long inference times.<br>‚Ä¢ Text-only prompt approaches lack spatial awareness and native-resolution refinement, limiting self-iterative correction and practical interactive use.<br>‚Ä¢ Spatio-textual misalignment between volumetric masks and text embeddings degrades accuracy in multi-class settings.<br>‚Ä¢ Fixed patch sizes versus variable target sizes lead to target‚Äìpatch ratio imbalance (FP/FN errors), requiring adaptive resampling.<br>‚Ä¢ Clinical constraints on memory and runtime necessitate efficient, parallel multi-class segmentation with high fidelity.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Medal S is an end-to-end model that fuses native-resolution spatial prompts with text prompts via channel-wise alignment and a lightweight 3D convolutional refinement, enabling parallel multi-class segmentation and iterative self-refinement. It is complemented by dynamic resampling, a two-stage coarse-to-fine inference strategy, optimized text preprocessing, and probability-aware post-processing.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ UltraSound-Adapt: Dynamic Target-Aware Resampling for Large-Structure Ultrasound Segmentation: Enhance Medal S with modality-specific resampling and patching policies to better handle large targets and anisotropic spacings in ultrasound.<br>‚Ä¢ Uncertainty-Aware Spatio-Textual Alignment for Small-Lesion Detection: Integrate uncertainty modeling into the channel-wise alignment module to improve robustness on tiny, low-contrast lesions.<br>‚Ä¢ Robust Text-Guided Segmentation under Noisy Labels: Develop noise-robust training (e.g., bootstrapped losses and consistency regularization) for datasets with ambiguous boundaries and imperfect annotations.<br>‚Ä¢ Beyond Prompts: Unified 2D‚Äì3D Interactive Prompting for Clinical Editing: Extend Medal S to support diverse 2D/3D prompts (points, scribbles, lassos) at native resolution for rapid clinician-in-the-loop corrections.<br>‚Ä¢ Federated Medal S: Continual and Cross-Site Adaptation for Multi-Institutional Deployment: Build privacy-preserving, federated and continual learning pipelines to maintain performance across hospitals and scanners without sharing raw data.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 4</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-11-20 23:06:11 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 4;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>