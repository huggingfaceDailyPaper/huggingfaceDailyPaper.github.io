<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - December 24, 2025</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">December 24, 2025</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">SemanticGen: Video Generation in Semantic Space</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.20619" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.20619" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Slow convergence and high compute cost of diffusion in VAE latent space; state-of-the-art training often needs hundreds of thousands of GPU-hours (p2).<br>‚Ä¢ Difficulty scaling to long videos due to quadratic full-sequence attention and token explosion (e.g., a 60s, 480p, 24fps clip expands to >0.5M tokens), making bi-directional diffusion impractical (p2).<br>‚Ä¢ Existing mitigations (sparse attention, autoregressive, or hybrid diffusion‚Äìautoregressive) suffer temporal drift, quality degradation, and train‚Äìinference gaps, especially on long-form generation (pp2‚Äì3).<br>‚Ä¢ Lack of global planning when modeling low-level latents directly; videos are redundant and need compact high-level semantics to maintain scene/character consistency over time (p2, Fig. 2).<br>‚Ä¢ Direct sampling in high-dimensional semantic spaces converges slowly and is hard for diffusion to sample from; a compressed, well-conditioned semantic space is needed (pp4‚Äì5, Fig. 8).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SemanticGen is a two-stage diffusion framework: first generate compact, compressed semantic video representations from text, then condition a second diffusion model on these semantics to produce VAE latents for decoding. It uses a learnable MLP with KL regularization to compress semantic features, in-context conditioning for VAE latent denoising, full attention in semantic space, and shifted-window attention in VAE space to scale efficiently to long videos (Figs. 3 and 5).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Tokenizer Matters: A Systematic Benchmark of Video Semantic Encoders for Generative Planning: Compare V-JEPA2, VideoMAE2, 4DS, and CLIP-style encoders on convergence speed, fidelity, and long-term coherence within the SemanticGen paradigm.<br>‚Ä¢ Hi-Tempo Semantics: High-FPS, Highly Compressed Tokenizers for Preserving High-Frequency Dynamics: Design semantic encoders that keep high temporal sampling while remaining compact to capture fast motions and flicker, addressing losses observed at low-fps semantics (Fig. 14).<br>‚Ä¢ End-to-End SemanticGen: Joint Learning of Semantic Compression and Pixel Decoding for Texture-Level Consistency: Jointly train the semantic compression MLP, semantic generator, and VAE latent generator (with residual/texture heads) to improve fine-detail consistency in minute-scale videos and reduce training‚Äìinference gaps.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.19673" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.19673" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing RL for LLMs treats the model as a single monolithic policy, ignoring the residual stream‚Äôs layer/module contributions and how reasoning emerges across depth.<br>‚Ä¢ Lack of a samplable internal policy view: prior logit-lens style analyses decode tokens but don‚Äôt define trainable, layer-wise policies; they miss how uncertainty evolves and how modules write to the residual stream.<br>‚Ä¢ Missing diagnostics for reasoning dynamics: no standard metrics to track exploration‚Üíconvergence across layers/modules; architectural differences (e.g., Qwen‚Äôs progressive FFN vs. Llama‚Äôs late collapse) are underexploited.<br>‚Ä¢ Practical RL issues (entropy collapse, unstable exploration, overlong outputs) stem from optimizing the whole policy at once without establishing strong early-layer foundations.<br>‚Ä¢ Existing RLVR work largely tunes rewards/entropy globally and uses coarse interpretability; it doesn‚Äôt align internal policies where foundational reasoning features are formed.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Decompose the LLM policy into Internal Layer and Modular Policies by projecting intermediate hidden states and module outputs with the unembedding matrix, then analyze their entropy and entropy change to reveal reasoning stages. Based on this, propose Bottom-up Policy Optimization (BuPO): first optimize a selected lower-layer internal policy via an InterGRPO objective for a few steps, then switch to standard GRPO to guide the overall policy from the bottom up.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Layer-Aware RL for LLMs: Dynamic Scheduling of Internal Policy Targets: Adaptively select layers/modules and switch timing using entropy change and residual cosine signals to maximize stable exploration and timely convergence.<br>‚Ä¢ Cross-Model Internal Policy Distillation: Transfer Qwen3‚Äôs progressive FFN Exploration‚ÄìIntegration‚ÄìConvergence pattern to Llama via layer-wise policy distillation and module-specific constraints.<br>‚Ä¢ Entropy-Shaped Reinforcement Learning for Progressive Reasoning: Add objectives that match target internal-entropy profiles across depth, preserving exploration in lower layers and encouraging principled convergence in upper layers.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">LongVideoAgent: Multi-Agent Reasoning with Long Videos</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.20618" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.20618" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Long-video QA remains hard: information is sparsely distributed over hour-scale episodes across subtitles and visuals, making fine-grained and temporally extended reasoning difficult (see the failure case illustration in Figure 1, p.1).<br>‚Ä¢ Non-agentic, single-pass models compress or aggressively downsample videos into lossy summaries, weakening temporal grounding and making fine-grained evidence recovery impossible once compressed (Abstract; Introduction).<br>‚Ä¢ Existing toolsets are limited (generic captioning/retrieval), often missing precise objects, OCR, subtle temporal cues, or exact temporal grounding needed for complex scenes (p.2).<br>‚Ä¢ Current approaches underutilize multi-step decision-making; they lack reinforced planning about what to observe next, when to stop, and how to act efficiently under context/latency constraints (p.2‚Äì3).<br>‚Ä¢ There is a need for interpretable, efficient pipelines that actively localize relevant segments and augment subtitles with targeted visual details; episode-level evaluation benchmarks are scarce‚Äîhence LongTVQA/LongTVQA+ (p.3, p.6; performance gains summarized in Table 2, p.6).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A multi-agent architecture in which a master LLM plans up to K steps and coordinates a GroundingAgent to temporally localize question-relevant segments and a VisionAgent to extract targeted visual facts; the master accumulates textual evidence (subtitles, clip tags, vision outputs) and answers once sufficient evidence is gathered (Figure 2, p.4; Algorithm 1, p.5). The master is trained with GRPO using simple rewards (format validity per step and final answer correctness) to learn concise, correct, and efficient multi-step plans while keeping grounding and vision agents fixed (p.5).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ End-to-End Joint Optimization of Grounding and Vision Modules in Long-Video Agents: Train grounding and vision agents jointly with the master via multi-agent RL to improve robustness and temporal precision beyond frozen specialists (motivated by Limitations, p.9).<br>‚Ä¢ Audio-Enhanced LongVideoAgent with On-the-Fly ASR and Acoustic Cues: Integrate raw audio, ASR, and non-speech acoustic signals to complement subtitles for stronger temporal grounding and disambiguation (Conclusion, p.8; Limitations, p.9).<br>‚Ä¢ Reward Shaping for Faithful and Efficient Long-Video Reasoning: Design richer rewards (e.g., temporal-grounding accuracy, tool-use cost, uncertainty calibration) to further improve plan quality and efficiency over the current two-signal scheme (p.5; Limitations, p.9).</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">SpatialTree: How Spatial Abilities Branch Out in MLLMs</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.20617" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.20617" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive "thinking" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Spatial evaluation is fragmented and task-centric, lacking a unified capability-centric hierarchy to reveal how spatial abilities progress from perception to reasoning and agentic interaction (Figure 1, page 2).<br>‚Ä¢ Existing benchmarks under-cover higher levels and miss cross-level dependencies; they rarely include agentic L4 tasks and standardized action spaces, making it hard to study transfer across levels (page 3).<br>‚Ä¢ Prior works emphasize narrow sub-domains and omit key sub-abilities (e.g., orientation, localization, affordance), preventing comprehensive, granular diagnosis across 27 sub-abilities (abstract; pages 4‚Äì5 outline missing L1 facets).<br>‚Ä¢ Training recipes (e.g., na√Øve RL that encourages extensive chain-of-thought) can improve complex reasoning but harm intuitive perception, highlighting a lack of strategies that balance ‚Äúthinking‚Äù vs. ‚Äúperceiving‚Äù (abstract).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes SpatialTree, a cognitive-science-inspired hierarchical taxonomy (L1‚ÄìL4) and the SpatialTree-Bench: a capability-centric benchmark built by unifying prior datasets and a Spatial Engine that augments data/annotations, plus curated L4 agentic tasks with an action-mapping to high-level motion primitives (pages 2‚Äì3). It further studies transfer via targeted SFT and introduces an auto-think RL strategy that suppresses unnecessary deliberation to jointly improve perception and reasoning (abstract).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning to Switch: Meta-Controllers for Auto-Think in Spatial MLLMs: Develop adaptive policies that dynamically decide when to reason versus act intuitively, optimizing performance across L1‚ÄìL4.<br>‚Ä¢ From Primitives to Policies: End-to-End Spatial Agents on SpatialTree L4: Learn motion primitives and closed-loop policies jointly from demonstrations and feedback, extending the L4 action mapping to continuous control and real-time interaction.<br>‚Ä¢ Cross-Level Curriculum for Spatial Intelligence: Mitigating L1 Interference while Maximizing High-Level Synergy: Design curriculum and sampling strategies that reduce negative transfer within L1 and amplify beneficial cross-level transfer discovered by SFT.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">MemEvolve: Meta-Evolution of Agent Memory Systems</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.18746" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.18746" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to 17.06%; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Static, hand-engineered memory architectures are fixed and cannot adapt to diverse task regimes, leading to suboptimal or brittle performance across domains (pp. 1‚Äì3; Fig. 2).<br>‚Ä¢ Prior self-improving agents evolve experience only (store trajectories, tips, tools) but not the memory architecture itself, so gains are limited and often inconsistent across benchmarks (pp. 3‚Äì4, 10‚Äì11; Table 3).<br>‚Ä¢ The memory design space is vast and heterogeneous (encode/store/retrieve/manage), lacking a unified, modular substrate for fair comparison and controllable optimization (pp. 4‚Äì5; Table 1).<br>‚Ä¢ Practitioners lack a principled, resource-aware way to balance performance with token cost and latency when designing memory; current systems rarely consider Pareto trade-offs (pp. 7‚Äì8).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>MemEvolve performs dual (bilevel) evolution: an inner loop accumulates experience under a fixed candidate memory, while an outer loop meta-evolves the memory architecture itself over a unified four-module interface (Encode, Store, Retrieve, Manage) using a diagnose-and-design operator with Pareto selection on performance, cost, and delay (Fig. 3). EvolveLab provides the standardized codebase and search space by re-implementing 12 representative memories under this modular interface.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Continual Meta-Evolution under Non‚ÄëStationary Task Streams: Extend MemEvolve to online, lifelong settings with regret-aware selection and rapid adaptation to distribution shifts.<br>‚Ä¢ Memory Architecture Distillation Across LLMs and Frameworks: Learn transferable memory "programs" that can be distilled and retargeted across backbones and agent scaffolds with minimal re-evolution.<br>‚Ä¢ Safety‚Äë and Privacy‚ÄëAware Meta‚ÄëEvolving Memories: Incorporate compliance, leakage prevention, and selective forgetting constraints into the outer-loop design to guarantee safe, auditable memory evolution.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Step-DeepResearch Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.20491" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.20491" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Search ‚â† research: existing systems and benchmarks optimize multi-hop retrieval not end-to-end research (intent decomposition, planning, synthesis), causing fragmented reasoning and hallucinations.<br>‚Ä¢ Evaluation misalignment: multi-hop QA becomes the optimization target, while real user needs are open-ended; especially limited realistic benchmarks in Chinese for practical deep research.<br>‚Ä¢ Orchestration-heavy pipelines: many solutions hardcode complex multi-agent/workflows, increasing system complexity and cost, yet still lack robust depth in specialized scenarios.<br>‚Ä¢ Missing atomic capabilities: prior end-to-end works focus on search efficacy but under-train planning, cross-source verification, reflection, and structured reporting.<br>‚Ä¢ Data scarcity and structure: lack of high-value, logically dense training data and strategies to teach long-horizon decision-making; weak supervision signals for open-ended report quality.<br>‚Ä¢ Cost-effectiveness gap: strong performance often requires large proprietary models; industry needs expert-level research at much lower deployment and inference cost.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Decompose deep research into atomic capabilities (planning, deep information seeking, reflection/verification, reporting) and synthesize targeted data, then train a 32B model via a progressive pipeline‚Äîagentic mid-training (32K‚Üí128K), supervised fine-tuning, and rubric-driven RL with a checklist-style judge‚Äîto internalize expert-like, ReAct-based single-agent behavior with reference-preserving tools.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Consensus-Driven Multi-Agent Deep Research with Specialized Roles: Introduce planner/retriever/verifier/writer roles and agreement mechanisms to reduce hallucinations and improve evidence coherence.<br>‚Ä¢ Learning to Research in Dynamic, Partially Observable Web Environments: Train agents for continuous exploration, error recovery, and tool use under API drift and changing pages using environment-level RL.<br>‚Ä¢ Reward Modeling for Verifiable, Citation-Linked Research Reports: Develop richer rubric and preference models that optimize factual consistency, traceable citations, and structural clarity with process-based supervision.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">Reinforcement Learning for Self-Improving Agent with Skill Library</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.17102" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.17102" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLM agents struggle to continually self-improve and adapt when deployed in new environments; RL-trained agents often overfit to specific training scenarios and fail to leverage ongoing experience.<br>‚Ä¢ Existing skill-library approaches are predominantly prompt-based, leading to inconsistent skill definitions/usage and quality that is capped by the base model‚Äôs instruction-following ability.<br>‚Ä¢ Prior frameworks define reusable skills only after completing tasks, inflating context length in long-horizon settings and creating mismatches between execution and post-hoc skill induction.<br>‚Ä¢ Standard RL optimizes per-task outcome rewards and offers no credit assignment for generating/using reusable skills across related tasks, limiting transfer and efficiency gains.<br>‚Ä¢ Efficient agents should compress repeated action sequences into executable skills to reduce steps/tokens; however, current methods lack mechanisms to systematically learn, validate, and reuse such skills during training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SAGE (Skill Augmented GRPO for self-Evolution) extends GRPO with Sequential Rollout over chains of similar tasks and a Skill-integrated Reward that explicitly credits both task success and the generation/usage of executable skills. A unified skill-library agent represents actions as functions (define‚Äìcall‚Äìupdate‚Äìsave), is initialized via SFT on expert trajectories, and is then trained end-to-end with SAGE.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Beyond AppWorld: Generalizing SAGE to Diverse Tool-Use Benchmarks: Evaluate and adapt SAGE across web, desktop, and embodied tool-using environments to test robustness and domain transfer.<br>‚Ä¢ Skill-Aware Retrieval: Learning Retrievers for Function-Level Reuse: Train code- and API-semantic retrievers that match tasks to reusable skills, outperforming n-gram/embedding baselines in accuracy and efficiency.<br>‚Ä¢ Long-Horizon SAGE: Stabilizing Credit Assignment over Extended Task Chains: Reduce gradient variance and reward imbalance for chains >2 tasks via variance reduction, hierarchical credit assignment, and curriculum scheduling.<br>‚Ä¢ Online SAGE: Safe Continual Self-Improvement in the Wild: Enable deployment-time learning with guardrails (verification, rollback, sandboxing) to safely add/update skills during real usage.<br>‚Ä¢ Automatic Skill Discovery, Abstraction, and Compression: Discover higher-level macros, deduplicate and compress overlapping skills, and learn when to inline vs. call skills to optimize latency/cost.<br>‚Ä¢ Verifiable RL for Skill Libraries: Integrating RLVR with Skill-Integrated Rewards: Combine verifiable checks for tool execution with SAGE to tighten reward signals and reduce spurious skill induction.<br>‚Ä¢ Cross-Agent Skill Sharing and Transfer: Federated or multi-agent sharing of vetted skills across users/tasks, with provenance tracking and trust metrics for safe reuse.<br>‚Ä¢ Joint Planning‚ÄìProgramming Training for Skillful Agents: Co-train planning, code synthesis, and skill selection modules end-to-end to improve consistency between high-level plans and executable skills.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SAM Audio: Segment Anything in Audio</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.18099" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.18099" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>General audio source separation is a key capability for multimodal AI systems that can perceive and reason about sound. Despite substantial progress in recent years, existing separation models are either domain-specific, designed for fixed categories such as speech or music, or limited in controllability, supporting only a single prompting modality such as text. In this work, we present SAM Audio, a foundation model for general audio separation that unifies text, visual, and temporal span prompting within a single framework. Built on a diffusion transformer architecture, SAM Audio is trained with flow matching on large-scale audio data spanning speech, music, and general sounds, and can flexibly separate target sources described by language, visual masks, or temporal spans. The model achieves state-of-the-art performance across a diverse suite of benchmarks, including general sound, speech, music, and musical instrument separation in both in-the-wild and professionally produced audios, substantially outperforming prior general-purpose and specialized systems. Furthermore, we introduce a new real-world separation benchmark with human-labeled multimodal prompts and a reference-free evaluation model that correlates strongly with human judgment.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing source separation models are domain-specific with fixed stem ontologies (e.g., speech/music), lacking a general, open-domain solution that users can control flexibly.<br>‚Ä¢ Current methods usually support only a single prompting modality (typically text), which cannot disambiguate instances when multiple similar sources co-occur or when fine-grained distinctions are needed.<br>‚Ä¢ Text-prompted systems underperform in specialized domains (e.g., instrument demixing, multi-speaker speech) compared to domain-specific models like Demucs.<br>‚Ä¢ Visual-prompted separation is underexplored; real videos mix on-/off-screen sounds and have ambiguous audio‚Äìvision correspondences, and existing evaluations are small or synthetic.<br>‚Ä¢ Lack of unified, realistic benchmarks covering speech/music/general SFX with multimodal prompts; common SDR-like metrics correlate poorly with human perception and require clean references.<br>‚Ä¢ Synthetic mixing often yields unrealistic training data; real stems are scarce, slowing progress and limiting generalization.<br>‚Ä¢ Long-form separation is challenging: naive chunking causes boundary artifacts and temporal incoherence.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SAM Audio is a generative separation foundation model that jointly generates target and residual stems in a DAC-VAE latent space using a Diffusion Transformer trained with flow matching, conditioned on multimodal prompts (text via T5 cross-attention, visual masks via PE/SAM2, and frame-synchronous temporal spans). Training adds an AED-based representation alignment loss; inference can auto-predict spans (PEA-Frame), uses multi-diffusion for long audios, and re-ranks candidates with a learned reference-free judge.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning Stronger Audio‚ÄìVisual Grounding for Instance-Level Separation: Improve visual prompting with better AV alignment for on-/off-screen sources, mask quality, and ambiguity resolution in real videos.<br>‚Ä¢ Real-Time SAM Audio: Causal, Low-Latency Multimodal Separation: Distill or redesign the flow-matching DiT for streaming/interactive use with few ODE steps and limited compute.<br>‚Ä¢ Confidence-Aware Self-Training at Scale for Open-Domain Separation: Enhance pseudo-labeling with uncertainty estimation and curriculum to expand realistic training triplets without manual stems.<br>‚Ä¢ Span-Conditioned Interactive Audio Editing and Refinement: Iteratively refine separations with user-provided spans/clicks and on-the-fly re-prompting for precise control.<br>‚Ä¢ Spatial and Multi-Microphone SAM Audio for 3D Scene Separation: Incorporate spatial cues (e.g., multichannel/binaural/array data) to separate by direction and room acoustics.<br>‚Ä¢ A Unified Reference-Free Perceptual Judge for Separation and Editing: Extend SAM Audio Judge to multi-language speech, music subgenres, and editing tasks; calibrate across domains and difficulty.<br>‚Ä¢ Cross-Domain Continual Pretraining for Universal Separation: Continually adapt the foundation model to new domains (bioacoustics, podcasts, Foley) while preserving previous skills.<br>‚Ä¢ Joint Separation, Diarization, and Transcription via Multimodal Prompts: Combine target separation with speaker/entity tracking and ASR for conversational AV content.<br>‚Ä¢ Unifying Generation and Separation with Concept-Conditioned Diffusion: Leverage shared latent spaces to both synthesize and isolate sources with consistent prompt semantics.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">INTELLECT-3: Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.16144" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.16144" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Open-source RL post-training gap: Existing open frameworks are monolithic and hard to extend, lacking a modular, reproducible, end-to-end stack for agentic RL with tools and long-context reasoning.<br>‚Ä¢ Inefficient rollout generation at scale: Synchronous/on-policy RL stalls inference and underutilizes GPUs for long sequences, creating bottlenecks for reasoning models with high variance in rollout lengths.<br>‚Ä¢ Trainer‚Äìinference mismatch and instability: Off-policy drift and distribution shift between training and inference can crash long runs; popular algorithms (e.g., GSPO) show instability in highly async regimes.<br>‚Ä¢ Long-context training limits: FSDP memory bounds make 64k+ contexts difficult; CP can degrade accuracy and reduce data parallelism; activation memory becomes dominant.<br>‚Ä¢ Multi-node inference scaling bottlenecks: vLLM‚Äôs standard multi-node data parallel strategy plateaus with more nodes, limiting rollout throughput.<br>‚Ä¢ Secure high-throughput code execution: Naive Kubernetes exec paths (API/etcd-bound) add seconds of latency, making large-scale coding RL with thousands of concurrent sandboxes impractical.<br>‚Ä¢ Efficient MoE post-training: Delivering high MFU without expert-parallel overhead at long sequence lengths while retaining HF/vLLM compatibility.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>They introduce prime-rl, an open, asynchronous RL framework with disaggregated FSDP training and vLLM inference, continuous batching, and in-flight weight updates, tightly integrated with verifiers environments and Prime Sandboxes for secure, high-throughput code execution. Using masked token-level importance sampling with double-sided masking (ICEPOp-style), Distributed Muon for FSDP, activation offloading for 72k contexts, and a multi-client orchestrator for linear inference scaling, they train INTELLECT-3 (106B MoE, 12B active) to state-of-the-art results for its size on 512 H200 GPUs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Scaling Agentic RL with Diverse Tool-Using Environments: Continue long-horizon RL across Deep Research and SWE settings to harvest further gains beyond current non-plateaued curves.<br>‚Ä¢ Environments Hub 2.0: A Standardized, Multimodal, High-Quality Suite for Agentic RL: Expand community-contributed, versioned environments to cover more tools, modalities, and real-world workflows with verifiable rewards.<br>‚Ä¢ Learning to Manage Context in Long-Horizon Agents: RL for Self-Cutting, Branching, and External Memory: Let models optimize their own context budgets to combat long-context ‚Äúrot‚Äù and improve end-to-end task completion.<br>‚Ä¢ Stable Async Off-Policy RL for LLMs: Theory and Methods for In-Flight Updates and IS Masking: Formalize stability conditions, improve masking/clipping schemes, and reduce trainer‚Äìinference mismatch at scale.<br>‚Ä¢ Beyond 100K Tokens: Memory/Compute Co-Design for Long-Context RL with FSDP, CP, and Activation Offloading: Systematically study accuracy‚Äìthroughput trade-offs to safely scale context length without MFU collapse.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">FaithLens: Detecting and Explaining Faithfulness Hallucination</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.20182" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.20182" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLMs frequently produce faithfulness hallucinations (claims unsupported or contradicted by the given context) in RAG, summarization, and dialogue, undermining reliability in real-world use.<br>‚Ä¢ LLM-as-judge approaches (prompting advanced models like GPT-4-class) are accurate but too costly and inefficient for deployment.<br>‚Ä¢ Existing lightweight detectors are largely black-box binary classifiers without explanations, limiting trust, debuggability, and user adoption.<br>‚Ä¢ Generalization across tasks is inconsistent; detectors trained for one domain (e.g., summarization) often fail on others (e.g., RAG, multi-hop reasoning) due to task-specific hallucination patterns.<br>‚Ä¢ High-quality labeled data are scarce and expensive; prior synthetic pipelines lack rigorous quality control (label correctness, explanation usefulness, diversity), leading to overfitting on easy cases and weak explanations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>FaithLens trains an 8B detector that jointly predicts faithfulness and generates human-readable explanations via a two-stage pipeline: (1) cold-start SFT on LRM-synthesized data filtered for label correctness, explanation quality (perplexity gain), and diversity (clustering + probe influence); (2) rule-based RL (GRPO) with composite rewards for prediction correctness, explanation quality (enabling a novice model to answer correctly), and format compliance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ FaithLens-MM: Multimodal Faithfulness Detection with Cited Evidence ‚Äî Extend FaithLens to image/audio/table+text grounding with modality-aware explanation formats and evaluation.<br>‚Ä¢ From Binary to Fine-Grained: Taxonomy-Aware Faithfulness Detection with Interpretable Rationales ‚Äî Predict detailed error types (unsupported, contradiction, scope shift, temporal, entity swap) and highlight evidence spans that justify each label.<br>‚Ä¢ Distilling Explanatory Detectors for Real-Time Guardrails ‚Äî Compress the RL-tuned, explanation-capable model into a low-latency student with selective/triggered explanation generation for production RAG and summarization systems.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Scaling Laws for Code: Every Programming Language Matters</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.13472" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.13472" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing scaling laws treat code as language-agnostic, but different programming languages (PLs) scale differently, causing inaccurate performance prediction and suboptimal compute/data allocation (see Figure 2 for per-PL heterogeneity).<br>‚Ä¢ Real-world codebases are multilingual, yet prior work largely ignores cross-lingual dynamics; we lack understanding of synergy/interference between languages and how to mix them optimally (Table 1 quantifies these effects).<br>‚Ä¢ Training top-tier code LLMs is prohibitively expensive, preventing systematic ablations on language composition and pre-training strategies; practitioners lack principled guidance for token allocation under fixed budgets.<br>‚Ä¢ There is no established data organization strategy to teach cross-lingual alignment efficiently (random shuffling vs. parallel pairing), nor scaling laws that predict supervised and zero-shot translation quality.<br>‚Ä¢ Compute-optimal training for multilingual code requires a proportion-aware scaling law that incorporates language-specific parameters and cross-lingual transfer, which existing methods do not provide.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Run >1000 controlled pre-training experiments across seven PLs (0.2B‚Äì14B parameters; up to 1T tokens) to fit language-specific Chinchilla-style scaling laws, measure bilingual synergy via a mixing matrix, and compare data organization strategies (random shuffling vs. document-level parallel pairing). Using these measurements, propose a proportion-dependent multilingual scaling law that includes per-language exponents/irreducible loss and cross-lingual transfer coefficients (œÑ_ij) to compute optimal token allocations, validated on MultiPL-E and a 42-direction translation set.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Universal Multilingual Code Scaling Laws Beyond Seven Languages: Extend language-specific and proportion-dependent scaling to 30+ PLs, including low-resource and domain-specific languages (e.g., SQL, CUDA, DSLs), and recalibrate synergy matrices across diverse corpora.<br>‚Ä¢ Extreme-Scale Validation of Proportion-Dependent Training: Test the proposed multilingual scaling law at 100B+ parameters and multi-trillion tokens to verify exponents, optimal N:D ratios, and allocation policies under compute-optimal constraints.<br>‚Ä¢ Adaptive Curriculum for Multilingual Code Pre-training via Online Synergy Estimation: Develop training-time estimators of cross-lingual transfer (œÑ_ij) and dynamically reallocate sampling probabilities to maximize marginal utility, comparing against static token allocations.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.17648" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.17648" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Streaming Speech-to-Text Translation (StreamST) requires producing translations concurrently with incoming speech, imposing strict latency constraints and demanding models that balance partial-information decision-making with high translation quality. Research efforts on the topic have so far relied on the SimulEval repository, which is no longer maintained and does not support systems that revise their outputs. In addition, it has been designed for simulating the processing of short segments, rather than long-form audio streams, and it does not provide an easy method to showcase systems in a demo. As a solution, we introduce simulstream, the first open-source framework dedicated to unified evaluation and demonstration of StreamST systems. Designed for long-form speech processing, it supports not only incremental decoding approaches, but also re-translation methods, enabling for their comparison within the same framework both in terms of quality and latency. In addition, it also offers an interactive web interface to demo any system built within the tool.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ The community lacks a maintained, unified framework to evaluate and demo streaming speech-to-text translation (StreamST) systems; SimulEval is archived, supports only incremental decoding (no token deletions), targets short segments, and offers no demo interface.<br>‚Ä¢ It is hard to fairly compare re-translation and incremental paradigms under the same setup, especially on long-form audio and with both quality and latency (including computationally aware) metrics, as well as stability (flickering) tracking.<br>‚Ä¢ Real-world use requires long-form streaming, accurate tracking of emitted and deleted tokens, and easy demoing; current tools either evaluate without demos (SLTev) or demo without accessible evaluation frameworks (Lecture Translator).<br>‚Ä¢ Progress in StreamST hinges on balancing latency and quality under partial information and enabling reproducible benchmarking across models and policies.<br>‚Ä¢ Researchers need an easy path to port legacy SimulEval agents and to extend metrics without re-running experiments, which existing tools do not provide.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Simulstream is an open-source, paradigm-agnostic toolkit built around a WebSocket server and pluggable streaming processors (sliding window, StreamAtt, VAD wrapper, SimulEval agent proxy) that operate on long-form audio, emit incremental outputs, and log token emissions/deletions. It integrates unified evaluation (BLEU, COMET, StreamLAAL and computationally aware StreamLAAL, NE, RTF), automatic re-segmentation, and an interactive web demo for real-time visualization and system comparison.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Unified Benchmarking of Re-translation and Incremental StreamST at Scale: Extend simulstream to more datasets/languages with human evaluations and additional metrics capturing stability and user experience.<br>‚Ä¢ Flicker-Aware Decoding Policies for Streaming Speech Translation: Learn decoding/controllers that minimize normalized erasure while preserving quality and latency, benchmarked via simulstream.<br>‚Ä¢ Computationally-Aware Training and Scheduling for Real-Time StreamST: Develop models and runtime schedulers optimized for computationally-aware latency and RTF constraints within the simulstream evaluation loop.<br>‚Ä¢ Long-Form Memory for Streaming ST via Adaptive Audio-Text History Selection: Improve StreamAtt-style memory management with learned/adaptive mechanisms robust to topic shifts and hours-long streams.<br>‚Ä¢ Demo-Driven, User-Centric Evaluation of StreamST Systems: Integrate A/B testing and perceptual metrics (perceived latency, trust, comprehension) into the simulstream web interface to guide model and policy design.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">Active Intelligence in Video Avatars via Closed-loop World Modeling</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.20615" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.20615" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing video avatars are passive (speech/pose-driven) and cannot autonomously pursue long-horizon, goal-oriented tasks, limiting applications like autonomous livestream hosting and product demos.<br>‚Ä¢ Maintaining state under partial observability is hard: the agent only sees its own generated clips, and stochastic I2V models produce variable outcomes that can corrupt internal belief without verification.<br>‚Ä¢ Open-domain, semantic action spaces make execution fragile: high-level captions are underspecified and model-specific, causing misalignment and inconsistent generations without hierarchical grounding.<br>‚Ä¢ Prior agent frameworks optimize per-clip aesthetics/refinement or assume near-deterministic transitions, lacking closed-loop reflection, belief tracking, and robust multi-step planning in generative environments.<br>‚Ä¢ Current evaluations overlook goal-directed success; there is no benchmark to measure autonomous, multi-step task completion with meaningful object interactions in stochastic generative worlds.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes ORCA, a training-free, Internal World Model‚Äìinspired agent that formulates avatar control as a POMDP and runs a closed-loop OTAR (Observe‚ÄìThink‚ÄìAct‚ÄìReflect) cycle with continuous belief updating and outcome verification. A hierarchical dual-system design lets System 2 plan subgoals and predict next states while System 1 grounds them into detailed, model-specific action captions for precise I2V execution, evaluated on the new L-IVA benchmark for long-horizon interactive tasks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ 3D Belief World Models for Generative Avatars: Build explicit monocular 3D scene memory and object permanence modules to resolve depth/occlusion ambiguity and improve state estimation from generated clips.<br>‚Ä¢ Reflexive Fine-Tuning of I2V with OTAR Feedback: Use accept/reject and correction signals from the Reflect stage to fine-tune diffusion-based I2V models for stronger instruction following and reduced hallucination.<br>‚Ä¢ Reinforcement-Learned Action Grounding for Open-Domain Control: Replace prompt-engineered System 1 with an RL/DPO-trained caption compiler optimized for action fidelity and task success in stochastic generative environments.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.20352" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.20352" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa (Œ∫) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability (Œ∫= 0.907, cosine=95.3%), followed by GPT-4o (Œ∫= 0.853, cosine=92.6%) and Claude (Œ∫= 0.842, cosine=92.1%). All three models achieve a high agreement (Œ∫> 0.80), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Traditional qualitative analysis requires multiple human coders, is time‚Äëintensive and costly, and often yields only moderate inter‚Äërater agreement (Œ∫‚âà0.40‚Äì0.60).<br>‚Ä¢ Existing LLM approaches emphasize topic modeling/key‚Äëphrases rather than full Braun & Clarke‚Äìstyle thematic analysis (iterative, contextual, affective).<br>‚Ä¢ Current LLM workflows lack systematic, reproducible validation; single‚Äërun outputs provide no reliability indicators and ignore semantic equivalence beyond exact matches.<br>‚Ä¢ High variability across models and cultural interpretation challenges undermine trust without cross‚Äërun/model reliability checks.<br>‚Ä¢ Researchers need transparent, configurable, structure‚Äëagnostic, and cost‚Äëeffective methods with explicit reliability metrics for AI‚Äëassisted qualitative research.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A multi-run ensemble framework performs 1‚Äì6 seeded analyses per LLM at a set temperature, then quantifies reliability using pairwise Cohen‚Äôs Kappa on theme presence/absence and cosine similarity of theme descriptions, and derives consensus themes via semantic clustering (similarity >0.70; retained if ‚â•50% of runs). It supports custom prompts with variable substitution, robust structure-agnostic JSON parsing, and client-side embedding computation for privacy and reproducibility.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Thematic Saturation for Dynamic Ensemble Sizing: Learn stopping rules that detect saturation and automatically determine the minimal number of runs needed instead of a fixed N.<br>‚Ä¢ Cross-Architecture Ensemble Thematic Analysis: Fuse outputs from multiple LLM families (e.g., Gemini, GPT-4o, Claude) to prioritize model-invariant themes and downweight model-specific artifacts.<br>‚Ä¢ Human‚ÄìAI Agreement Benchmarks Across Domains and Languages: Establish Œ∫ and semantic similarity benchmarks between AI consensus themes and multi-coder human analyses over diverse datasets and non-English contexts.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.20092" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.20092" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Long, noisy multi-session dialogues hinder agents from locating temporally pertinent evidence, causing event misordering and cross-session conflation‚Äîundermining factual consistency in long-term assistants.<br>‚Ä¢ General long-context LLMs treat dialogue history as flat text, fail to resolve/ground ambiguous temporal expressions (e.g., ‚Äúlast week‚Äù, ‚Äúthe week before that‚Äù), and degrade sharply with longer contexts (lost-in-the-middle).<br>‚Ä¢ Time-aware pipelines (e.g., timeline summaries) depend on accurate timestamps and explicit expressions; they accumulate errors and are brittle to implicit/ambiguous references.<br>‚Ä¢ Prior RL approaches rely on structured metadata or answer-only supervision, yielding sparse signals and weak retrieval policies for unstructured multi-session dialogues.<br>‚Ä¢ There is a need for a scalable, robust memory selection method that remains accurate under extensive, noisy histories (up to 128k tokens) with minimal latency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Memory-T1 uses a coarse-to-fine retrieval pipeline: an LLM predicts the query‚Äôs time scope to hard-filter sessions, BM25 ranks relevance to form a high-recall candidate pool, and an RL policy (GRPO) jointly selects evidence and answers via a structured output. A dense multi-level reward‚Äîanswer accuracy, evidence grounding, and temporal consistency (session-level proximity and utterance-level fidelity)‚Äîtrains a temporally aware selection policy robust to ambiguity and long contexts.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Self-Supervised Temporal Rewards for Memory Agents: Learn temporal consistency and evidence-grounding signals from unlabeled dialogue logs via pseudo-timestamp inference and cycle-consistency, removing dependence on manual time/event annotations.<br>‚Ä¢ Multimodal Memory-T1: Temporal Reasoning over Long Vision‚ÄìLanguage Dialogues: Extend Memory-T1 to align dialogue with images/videos and capture times, enabling temporally grounded retrieval and reasoning in VLM agents on long multimodal histories.<br>‚Ä¢ Neuro-Symbolic Calendar Agents with RL-Guided Memory Selection: Integrate symbolic event graphs, calendar/time-zone APIs, and recurrence modeling with RL-based memory selection to resolve complex relative times and recurring events at scale.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.19526" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.19526" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Lack of quantitative evaluation of physical reasoning in VLMs: existing benchmarks are mainly qualitative VQA or multiple-choice, which cannot assess numerical accuracy or closeness of predictions.<br>‚Ä¢ Multiple-choice insensitivity to numeric error: answers like 3.1 m vs 31 m are scored equally wrong, masking meaningful differences in quantitative reasoning.<br>‚Ä¢ Real-world need for metric kinematics from video: embodied AI, AR/VR, and autonomous driving require size, velocity, and acceleration in physical units without full calibration.<br>‚Ä¢ Missing standardized tasks, data, and scoring: no unified protocol to test 2D/3D motion with static/dynamic priors and evaluate with robust numeric metrics.<br>‚Ä¢ Unclear input faithfulness: current VLMs may rely on memorized world knowledge rather than using the provided video and priors; this must be diagnosed.<br>‚Ä¢ Limited understanding of prompt/scene effects: the field lacks controlled analyses of background complexity, counterfactual priors, and reasoning prompts on quantitative performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>QuantiPhy defines a kinematic inference benchmark where a VLM is given a video and a single physical prior (size, velocity, or acceleration) and must estimate other kinematic properties in world units by resolving a pixel-to-world scale. It provides 3.3K video‚Äìtext instances spanning 2D/3D and static/dynamic settings from simulation, lab, and web data, a standardized prompt, and a Mean Relative Accuracy metric, plus diagnostic protocols (prior-only, counterfactual scaling, chain-of-thought) to test input faithfulness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Toward Input-Faithful Kinematic Reasoners: Physics-Guided Training and Tool Use for VLMs: Integrate differentiable pixel-measurement tools (tracking/optical flow) and physics-aware losses to force models to use visual evidence and priors.<br>‚Ä¢ Beyond Translation: Benchmarking Rotational Dynamics and Deformable Objects in Multimodal Physical Reasoning: Extend QuantiPhy to rotations, soft bodies, and contact-rich interactions for broader physical coverage.<br>‚Ä¢ Calibration-Free 3D Kinematics Under Moving Cameras: Learning Robust Metric Reasoning from Monocular Video: Generalize tasks to dynamic viewpoints by jointly inferring camera motion and scale for world-unit predictions.<br>‚Ä¢ Programs-of-Thought for Numbers: Modular Pipelines that Combine Vision Tools with Symbolic Kinematics: Compose step-wise programs that estimate pixels‚Üíscale‚Üítarget with verification, improving over free-form CoT.<br>‚Ä¢ Physics-Consistency for Video Generation: Training Generative Models with Quantitative Kinematic Constraints: Use QuantiPhy-style numeric checks as training or RL signals to enforce physically consistent video synthesis.<br>‚Ä¢ Data Engines for Physics-Rich Pretraining: Automatic Video Mining, Annotation, and Curriculum for Quantitative Reasoning: Build scalable pipelines that curate and annotate kinematics at scale to pretrain VLMs on numeric physics tasks.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Toxicity Ahead: Forecasting Conversational Derailment on GitHub</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.15031" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.15031" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Toxic interactions in Open Source Software (OSS) communities reduce contributor engagement and threaten project sustainability. Preventing such toxicity before it emerges requires a clear understanding of how harmful conversations unfold. However, most proactive moderation strategies are manual, requiring significant time and effort from community maintainers. To support more scalable approaches, we curate a dataset of 159 derailed toxic threads and 207 non-toxic threads from GitHub discussions. Our analysis reveals that toxicity can be forecast by tension triggers, sentiment shifts, and specific conversational patterns. We present a novel Large Language Model (LLM)-based framework for predicting conversational derailment on GitHub using a two-step prompting pipeline. First, we generate Summaries of Conversation Dynamics (SCDs) via Least-to-Most (LtM) prompting; then we use these summaries to estimate the likelihood of derailment. Evaluated on Qwen and Llama models, our LtM strategy achieves F1-scores of 0.901 and 0.852, respectively, at a decision threshold of 0.3, outperforming established NLP baselines on conversation derailment. External validation on a dataset of 308 GitHub issue threads (65 toxic, 243 non-toxic) yields an F1-score up to 0.797. Our findings demonstrate the effectiveness of structured LLM prompting for early detection of conversational derailment in OSS, enabling proactive and explainable moderation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing toxicity detection in OSS is largely post‚Äëhoc, catching harmful language only after it appears and missing chances to prevent harm.<br>‚Ä¢ Manual proactive moderation is impractical at scale across issues/PRs/discussions; maintainers need automated, early-warning support.<br>‚Ä¢ GitHub toxicity is often subtle (e.g., entitlement, impatience, miscommunication) and SE jargon confuses generic toxicity models.<br>‚Ä¢ Limited understanding and datasets of GitHub-specific derailment signals (timing, linguistic markers, tonal shifts, triggers) hamper forecasting.<br>‚Ä¢ Prior derailment predictors are tuned to platforms like Reddit/Wikipedia and lack interpretability and domain adaptation for technical threads.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A two-step LLM pipeline: (1) generate GitHub-specific Summaries of Conversation Dynamics via Least-to-Most prompting that capture intentions, conversation strategies, sentiment/tonal shifts, and tension triggers while excluding technical details; (2) apply a simple prediction prompt to the SCD to output a derailment probability (0‚Äì1) before any toxic comment appears.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ SCD-Tuned LLMs for GitHub Moderation: Instruction-tune models on human-written SCDs to reduce errors in sarcasm, subtle tone, and trigger detection.<br>‚Ä¢ Streaming Derailment Forecasting for Real-Time Moderation: Build incremental SCDs that update per new comment for low-latency, cost-efficient alerts.<br>‚Ä¢ Cross-Platform, Multilingual Derailment Benchmarks in Software Communities: Create standardized datasets (GitHub/GitLab/JIRA) with aligned labels for derailment points, tones, and outcomes.<br>‚Ä¢ Fusing Behavioral and Network Signals with SCDs: Integrate participant history, reply graphs, and temporal rhythms to improve robustness and generalization.<br>‚Ä¢ Field Trial of Thresholded Early-Warning Bots on GitHub: Deploy intervention policies (auto-reminders vs. mod alerts) and measure impact on civility, false alarms, and maintainer workload.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Learning to Refocus with Video Diffusion Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.19823" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.19823" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing and unlocking a range of downstream applications. We release a large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at www.learn2refocus.github.io</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Autofocus often fails in dynamic/complex scenes, and users want realistic post-capture focus control from a single photo.<br>‚Ä¢ Existing solutions commonly require specialized hardware (light-field cameras), multiple captures, depth maps, or dual-pixel data‚Äîimpractical for everyday photography.<br>‚Ä¢ Prior single-image refocusing (e.g., RefocusGAN) relies on synthetic training data and multi-stage/iterative stacks, hurting realism and robustness.<br>‚Ä¢ Many restoration methods target all-in-focus outputs, not true optical defocus/bokeh, leading to less natural results.<br>‚Ä¢ A lack of large, real-world focal stack datasets limits training and fair evaluation under realistic conditions.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Cast single-image refocusing as focal stack (video) generation and fine-tune a latent video diffusion model (Stable Video Diffusion) to synthesize an entire focal stack in one pass from a single defocused image. Introduce position-dependent classifier-free guidance that injects the input frame‚Äôs latent only at its focal index to resolve positional ambiguity and produce perceptually realistic, temporally coherent stacks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Pixel-Accurate Refocusing with Pixel-Space Diffusion: Replace/augment the latent VAE with pixel-space or hybrid latent‚Äìpixel diffusion to recover fine textures and identity at small focus shifts.<br>‚Ä¢ Aperture-Conditioned Refocusing: Joint Control of Focus and DoF: Add camera/aperture/focal-length conditioning and train on large-aperture data to handle extreme bokeh and enable controllable DoF edits.<br>‚Ä¢ Refocus-Consistent Video Diffusion: Temporally Coherent Refocusing for Videos: Extend the method with temporal constraints to refocus entire videos without flicker or temporal artifacts.<br>‚Ä¢ Cross-Device Generalization via Multi-Sensor Supervision: Fuse data from DSLRs, smartphones, and dual-pixel/light-field sensors to learn device-agnostic priors and robust defocus behavior.<br>‚Ä¢ RefocusBench: A Large-Scale, Real-World Focal Stack Benchmark and Metrics: Curate a diverse, multi-camera dataset and propose perceptual/defocus-aware metrics beyond PSNR for fair evaluation.<br>‚Ä¢ Learning Physically Grounded Defocus Priors: Integrate thin-lens/PSF models into diffusion training to improve realism and consistency under varying optics and scene depth.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 6</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2025-12-24 23:06:48 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 6;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>