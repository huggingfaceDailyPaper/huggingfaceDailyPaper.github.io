<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Papers Analysis - January 01, 2026</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: "Times New Roman", Times, serif;
            line-height: 1.7;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
            padding-top: 80px;
            font-size: 16px;
            font-weight: 400;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            font-weight: 600;
            letter-spacing: -0.02em;
        }
        
        .header .date {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 0;
        }
        
        .paper {
            border-bottom: 1px solid #eee;
            padding: 40px;
            transition: background-color 0.3s ease;
        }
        
        .paper:last-child {
            border-bottom: none;
        }
        
        .paper:hover {
            background-color: #f8f9fa;
        }
        
        .paper-title {
            font-size: 1.9em;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 18px;
            line-height: 1.4;
            letter-spacing: -0.01em;
        }
        
        .paper-links {
            margin-bottom: 25px;
        }
        
        .paper-links a {
            display: inline-block;
            padding: 8px 16px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }
        
        .paper-links a:hover {
            background-color: #2980b9;
        }
        
        .paper-links a.pdf {
            background-color: #e74c3c;
        }
        
        .paper-links a.pdf:hover {
            background-color: #c0392b;
        }
        
        .abstract {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .abstract h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .analysis-section {
            margin-bottom: 25px;
        }
        
        .analysis-section h3 {
            color: #1a202c;
            margin-bottom: 18px;
            font-size: 1.35em;
            font-weight: 600;
            display: flex;
            align-items: center;
            letter-spacing: -0.01em;
        }
        
        .analysis-section h3 .emoji {
            margin-right: 10px;
            font-size: 1.2em;
        }
        
        .motivation {
            border-left: 4px solid #f39c12;
            background-color: #fdf6e3;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .method {
            border-left: 4px solid #27ae60;
            background-color: #f0fff4;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .results {
            border-left: 4px solid #8e44ad;
            background-color: #f8f4ff;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .idea {
            border-left: 4px solid #e67e22;
            background-color: #fef9e7;
            padding: 24px;
            border-radius: 8px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        .motivation p, .method p, .idea p {
            margin: 0;
            font-weight: 450;
            color: #374151;
        }
        
        .no-analysis {
            color: #7f8c8d;
            font-style: italic;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 6px;
            text-align: center;
        }
        
        .pagination {
            text-align: center;
            padding: 20px;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        .pagination button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .pagination button:hover {
            background-color: #2980b9;
        }
        
        .pagination button:disabled {
            background-color: #bdc3c7;
            cursor: not-allowed;
        }
        
        .pagination button.active {
            background-color: #e74c3c;
        }
        
        .page-btn {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 8px 12px;
            margin: 0 2px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s ease;
        }
        
        .page-btn:hover {
            background-color: #2980b9;
        }
        
        .page-btn.active {
            background-color: #e74c3c;
        }
        
        .page {
            display: none;
        }
        
        .page.active {
            display: block;
        }
        
        .page-info {
            color: #7f8c8d;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .footer {
            text-align: center;
            padding: 30px;
            color: #7f8c8d;
            background-color: #f8f9fa;
            border-top: 1px solid #eee;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .paper {
                padding: 20px;
            }
            
            .paper-title {
                font-size: 1.4em;
            }
        }
    </style>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
        <div class="container-lg">
            <a class="navbar-brand"><strong>Heng Zhou</strong></a>
            <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <i class="fas fa-map"></i> Menu
            </button>

            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    
                    <li class="nav-item active">
                        <a class="nav-link" href="https://henggg.cn/">Home</a>
                    </li>
                    
                    <li class="nav-item ">
                        <a class="nav-link" href="https://henggg.cn/publications">Publications</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    <div class="container">
        <div class="header">
            <h1>Daily Papers Analysis</h1>
            <div class="date">January 01, 2026</div>
        </div>
        
        <div class="content">
            <div class="page active" id="page-1">

    <div class="paper">
        <h2 class="paper-title">mHC: Manifold-Constrained Hyper-Connections</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24880" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24880" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Hyper-Connections (HC) compromise the identity mapping property of residual connections, causing severe training instability.<br>‚Ä¢ The loss of identity mapping in HC restricts scalability when training large models.<br>‚Ä¢ Diversified connectivity and widened residual streams in HC introduce notable memory access overhead, reducing efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Manifold-Constrained Hyper-Connections (mHC) project HC residual connection matrices onto a specific constrained manifold to restore the identity mapping property, coupled with infrastructure optimizations to reduce memory access overhead and improve efficiency.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Manifold Selection for Hyper-Connections: Learn or select task-specific manifolds to balance stability and expressivity across architectures and datasets.<br>‚Ä¢ Theoretical Foundations of Manifold-Constrained Residual Topologies: Analyze stability, conditioning, and expressivity guarantees when residual spaces are constrained to particular manifolds.<br>‚Ä¢ Hardware-Aware mHC: Co-design and optimize mHC implementations to minimize memory access overhead and maximize throughput on modern accelerators.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24618" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24618" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled "Commonsense-STEM-Agent" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Large LLMs are expensive to train and deploy; there is a need for compact models that retain strong reasoning and agentic performance for latency- and resource-constrained settings.<br>‚Ä¢ Existing small-model approaches (distillation/instruction tuning/architectural simplifications) align outputs but do not cultivate native planning/reasoning, leading to weak robustness and generalization on agentic tasks.<br>‚Ä¢ There is no principled pre-training route to agentic capability in lightweight models, compounded by long-context demands and noisy/redundant CoT data and data-quality/contamination risks.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Youtu-LLM is a 1.96B dense MLA model pre-trained from scratch with a 128k context and a STEM-oriented tokenizer under a staged Commonsense‚ÜíSTEM‚ÜíAgent curriculum plus scalable agentic mid-training. It uses ~11T high-quality general tokens and 200B structured Agentic-CoT/math/code/deep-research/tool-use trajectories filtered by a domain/quality scoring pipeline to induce native planning, reflection, and tool-use without distillation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Scaling Laws for Agentic Capability in Sub-2B LLMs: Quantify how agentic performance scales with data composition, trajectory volume, context length, and architecture in lightweight models.<br>‚Ä¢ Executable Agentic-CoT: From Structured Reasoning to Verified Tool Use: Augment Agentic-CoT with programmatic tool execution and result verification to improve reliability and reduce hallucinations.<br>‚Ä¢ Continual Agentic Pre-Training for On-Device Lightweight Agents: Develop online/continual agentic pre-training with streaming trajectories, addressing stability, safety, and catastrophic forgetting for edge deployment.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24873" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24873" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLMs struggle with agentic crafting‚Äîplanning, acting, observing, and iterating over long horizons‚Äîbecause most systems are optimized for one-shot responses to simple tasks.<br>‚Ä¢ The open-source ecosystem lacks an end-to-end, production-ready agentic pipeline (from data and training to deployment), slowing practical development and adoption of agents.<br>‚Ä¢ Token-level policy optimization is unstable for long interactions; robust credit assignment across extended trajectories is needed to improve reliability and learning efficiency.<br>‚Ä¢ There is a shortage of high-quality, verified agentic datasets and composition protocols that ensure safety, security, and validity.<br>‚Ä¢ Existing benchmarks lack scale, domain coverage, and contamination control, hindering rigorous evaluation and comparison of agentic models.<br>‚Ä¢ Context engineering for tool- and environment-mediated workflows is ad hoc and inefficient, limiting reproducibility and performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces the open Agentic Learning Ecosystem (ALE), composed of ROCK (environment orchestration for trajectory generation), ROLL (post-training weight optimization), and iFlow CLI (a configurable agent framework for efficient context engineering), alongside curated, verified data protocols and an end-to-end training pipeline. It proposes the IPA policy optimization algorithm that assigns credit over semantic interaction chunks rather than tokens, trains the open-source ROME agent on over one million trajectories, and releases Terminal Bench Pro for rigorous evaluation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Hierarchical IPA: Multi-Scale Credit Assignment for Long-Horizon Agentic Learning: Extend IPA with hierarchical/adaptive chunking and uncertainty-aware credit to improve stability and sample efficiency across diverse tasks.<br>‚Ä¢ Safe and Secure Agentic Data Curation at Scale: Automated Protocols, Audits, and Contamination Control: Build formal pipelines for trajectory synthesis with automated safety/security verification, provenance tracking, and robust contamination detection.<br>‚Ä¢ Adaptive Context Engineering with iFlow: Learning to Optimize Tooling, Memory, and Prompts On-the-Fly: Develop learning methods that dynamically configure tools, prompt layouts, retrieval, and memory policies during interaction to boost reliability and performance.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-2">

    <div class="paper">
        <h2 class="paper-title">GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.25073" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.25073" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a 25times speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Sparse-view 3D reconstruction yields holes, ghosting, and broken geometry due to insufficient coverage of unobserved regions.<br>‚Ä¢ Diffusion-based novel view generation introduces geometric and photometric inconsistencies across views, which worsen with more generated viewpoints and overlap.<br>‚Ä¢ Novel view pipelines require complex trajectory planning and are computationally expensive, slowing reconstruction.<br>‚Ä¢ Prior/regularization-based methods struggle to extend content beyond the periphery of known views, limiting completeness.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>GaMO performs zero-shot, geometry-aware multi-view diffusion outpainting that expands the FOV of existing views, using coarse 3DGS priors (opacity mask and coarse render), multi-view conditioning (Pl√ºcker ray embeddings, warped CCM/RGB), and mask latent blending with iterative mask scheduling and noise resampling; the outpainted wide-FOV views then refine 3DGS for consistent, high-fidelity reconstruction.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Joint GaMO-3DGS Optimization: End-to-End Training for Consistent Reconstruction: Co-optimize diffusion outpainting and 3D Gaussian Splatting with differentiable rendering to further reduce artifacts and improve convergence.<br>‚Ä¢ Temporal GaMO: Geometry-Aware Multi-View Outpainting for Dynamic Scenes: Extend GaMO with video diffusion priors and temporal consistency constraints to handle moving objects and camera motion.<br>‚Ä¢ Uncertainty-Guided GaMO: Confidence-Weighted Mask Scheduling and Conditioning: Integrate uncertainty estimation from coarse geometry and diffusion predictions to adapt mask blending and noise resampling for robust outpainting under extreme sparsity.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23380" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23380" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Log anomaly detection is crucial for preserving the security of operating systems. Depending on the source of log data collection, various information is recorded in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, multimodal methods fail to handle the interactions between these modalities. Applying multimodal sentiment analysis to log anomaly detection, we propose CoLog, a framework that collaboratively encodes logs utilizing various modalities. CoLog utilizes collaborative transformers and multi-head impressed attention to learn interactions among several modalities, ensuring comprehensive anomaly detection. To handle the heterogeneity caused by these interactions, CoLog incorporates a modality adaptation layer, which adapts the representations from different log modalities. This methodology enables CoLog to learn nuanced patterns and dependencies within the data, enhancing its anomaly detection capabilities. Extensive experiments demonstrate CoLog's superiority over existing state-of-the-art methods. Furthermore, in detecting both point and collective anomalies, CoLog achieves a mean precision of 99.63%, a mean recall of 99.59%, and a mean F1 score of 99.61% across seven benchmark datasets for log-based anomaly detection. The comprehensive detection capabilities of CoLog make it highly suitable for cybersecurity, system monitoring, and operational efficiency. CoLog represents a significant advancement in log anomaly detection, providing a sophisticated and effective solution to point and collective anomaly detection through a unified framework and a solution to the complex challenges automatic log data analysis poses. We also provide the implementation of CoLog at https://github.com/NasirzadehMoh/CoLog.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Unimodal log anomaly detectors ignore complementary modalities (sequence vs. semantic), losing cross-modal context and missing complex anomalies.<br>‚Ä¢ Existing multimodal fusion (early/intermediate/late or separate models) leads to high dimensionality, noise sensitivity, feature incompatibility, network complexity, redundancy, and limited cross-modal interaction.<br>‚Ä¢ No unified framework to jointly detect both point and collective anomalies in logs; prior works typically target only one type or rely on fixed-length windows/scoring without learning modality relationships.<br>‚Ä¢ Heterogeneity across log sources and evolving logging statements demand adaptive representations and balanced modality contributions.<br>‚Ä¢ Large-scale, dynamic logs render manual/rule-based methods impractical; class imbalance and real-time constraints require robust, end-to-end supervised models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>CoLog formulates log anomaly detection as multimodal sentiment analysis and uses a collaborative transformer with multi-head impressed attention and a modality adaptation layer to jointly encode sequence and semantic modalities, fuse them in a latent space, and adaptively balance their contributions. This end-to-end design enables unified detection of both point and collective anomalies with state-of-the-art performance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Unsupervised CoLog: Collaborative Transformers for Label-Free Multimodal Log Anomaly Detection: Extend CoLog with self-supervised pretraining and anomaly scoring to remove reliance on labeled data.<br>‚Ä¢ Federated CoLog: Privacy-Preserving Multimodal Log Anomaly Detection across Distributed Systems: Adapt CoLog to federated settings with heterogeneous clients and secure aggregation.<br>‚Ä¢ Explainable CoLog: Impressed Attention for Transparent Root-Cause Analysis in System Logs: Leverage attention maps and cross-modal attributions to provide human-interpretable diagnoses.<br>‚Ä¢ Domain-Adaptive CoLog: Continual Learning under Evolving Log Formats and Concepts: Introduce adapters and meta-learning for robust performance under log drift and concept shifts.<br>‚Ä¢ Real-Time CoLog: Streaming Collaborative Transformers for High-Throughput Log Monitoring: Optimize latency and memory via lightweight adapters and streaming attention for production use.<br>‚Ä¢ LLM-Augmented CoLog: Retrieval and Prompting to Enhance Cross-Domain Generalization: Integrate retrieval-augmented generation and prompts to improve zero-/few-shot transfer across systems.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Scaling Open-Ended Reasoning to Predict the Future</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.25070" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.25070" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Open-ended world-event forecasting needs large, diverse, resolved questions, but prediction-market data is scarce, mostly binary, and distributionally skewed.<br>‚Ä¢ Online retrieval via web search causes outcome leakage in backtests due to dynamic documents and unreliable date cutoffs.<br>‚Ä¢ Outcome-based RL on binary questions yields noisy rewards (high 50% baseline), reinforcing incorrect reasoning and hurting exploration.<br>‚Ä¢ Iterative training is bottlenecked by waiting for events to resolve; scalable, automated question synthesis without future leakage is lacking.<br>‚Ä¢ Existing evaluations emphasize structured (binary/multiple-choice) formats, missing ‚Äúunknown unknowns‚Äù where foresight requires open-ended answers and calibrated probabilities.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Automatically synthesize ~50k open-ended forecasting questions (OpenForesight) from static monthly news snapshots, retrieve relevant context from an offline corpus constrained to pre-resolution windows, and train Qwen3 thinking models with GRPO using a composite reward (accuracy + adapted Brier score) and model-based semantic answer matching to improve accuracy, calibration, and consistency while preventing future-information leakage.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Safe Real-Time Forecasting with Leakage-Resistant Retrieval: Develop protocols and audits to integrate live information (static caching, temporal filters, leakage detectors) without compromising backtesting integrity.<br>‚Ä¢ Adaptive Multi-Objective RL for Calibrated Open-Ended Forecasting: Learn dynamic reward weighting and exploration policies that balance accuracy, calibration, and coverage across question difficulty.<br>‚Ä¢ Beyond News: Multi-Modal Question Synthesis for Open-Ended Event Forecasting: Extend automated question generation to scientific literature, corporate filings, social media, and multimedia to broaden event coverage and outcome diversity.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-3">

    <div class="paper">
        <h2 class="paper-title">PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24551" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24551" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or learn implicit physical reasoning. The scarcity of training data with rich physics interactions and phenomena is also a problem. In this paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that leverages a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training dataset, PhyVidGen-135K. Then we formulate a principled Physics-aware Groupwise Direct Preference Optimization, PhyGDPO, framework that builds upon the groupwise Plackett-Luce probabilistic model to capture holistic preferences beyond pairwise comparisons. In PhyGDPO, we design a Physics-Guided Rewarding (PGR) scheme that embeds VLM-based physics rewards to steer optimization toward physical consistency. We also propose a LoRA-Switch Reference (LoRA-SR) scheme that eliminates memory-heavy reference duplication for efficient training. Experiments show that our method significantly outperforms state-of-the-art open-source methods on PhyGenBench and VideoPhy2. Please check our project page at https://caiyuanhao1998.github.io/project/PhyGDPO for more video results. Our code, models, and data will be released at https://github.com/caiyuanhao1998/Open-PhyGDPO</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Current text-to-video models produce visually appealing but physically inconsistent videos, lacking robust implicit physics reasoning.<br>‚Ä¢ Graphics-based approaches depend on simplified simulators and hand-specified parameters, failing to generalize to complex real-world scenes.<br>‚Ä¢ LLM-based prompt extension outsources reasoning, suffers from weak/erroneous physics guidance, and is limited by models‚Äô prompt-following capability.<br>‚Ä¢ Scarcity of physics-rich datasets and absence of negative/contrastive signals impede learning of physical plausibility.<br>‚Ä¢ Vanilla DPO relies on pairwise Bradley‚ÄìTerry comparisons, missing holistic preferences (e.g., motion smoothness, causal plausibility), and duplicates a heavy reference model; GRPO is slow and unstable, limiting practical training.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper introduces PhyGDPO, a physics-aware groupwise DPO that uses a Plackett‚ÄìLuce model with physics-guided rewarding and a LoRA-Switch Reference to align T2V models toward physical consistency. It is complemented by PhyAugPipe, which constructs the 135K-sample PhyVidGen dataset via VLM chain-of-thought filtering, action clustering, and difficulty-weighted sampling using a physics-aware VLM.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Curriculum PhyGDPO: Adaptive Difficulty Scheduling for Physics-Consistent Text-to-Video: Develop dynamic curricula that adjust sampling weights, group sizes, and reward strength based on online physics error and category-wise performance.<br>‚Ä¢ Differentiable Physics Constraints in Groupwise Preference Optimization: Integrate lightweight, differentiable physics validators and temporal consistency losses into the GDPO objective to provide stronger, model-aware rewards without relying solely on external VLMs.<br>‚Ä¢ Human-in-the-Loop Physics Preference Benchmarks for T2V: Build standardized benchmarks and collect human preference annotations on physical plausibility to train hybrid human+VLM preference models and reduce bias in physics reward signals.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">GR-Dexter Technical Report</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24210" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24210" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Vision-language-action (VLA) models have enabled language-conditioned, long-horizon robot manipulation, but most existing systems are limited to grippers. Scaling VLA policies to bimanual robots with high degree-of-freedom (DoF) dexterous hands remains challenging due to the expanded action space, frequent hand-object occlusions, and the cost of collecting real-robot data. We present GR-Dexter, a holistic hardware-model-data framework for VLA-based generalist manipulation on a bimanual dexterous-hand robot. Our approach combines the design of a compact 21-DoF robotic hand, an intuitive bimanual teleoperation system for real-robot data collection, and a training recipe that leverages teleoperated robot trajectories together with large-scale vision-language and carefully curated cross-embodiment datasets. Across real-world evaluations spanning long-horizon everyday manipulation and generalizable pick-and-place, GR-Dexter achieves strong in-domain performance and improved robustness to unseen objects and unseen instructions. We hope GR-Dexter serves as a practical step toward generalist dexterous-hand robotic manipulation.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing VLA policies largely target gripper-based robots; dexterous, bimanual manipulation with high-DoF hands remains underexplored.<br>‚Ä¢ High-DoF hands expand the action space and cause frequent hand-object occlusions, making perception and policy learning significantly harder.<br>‚Ä¢ Collecting diverse, high-quality real-robot trajectories for dexterous bimanual manipulation is costly and teleoperation is challenging.<br>‚Ä¢ Generalist policies must execute long-horizon tasks and robustly generalize to unseen objects and abstract language instructions, which current methods struggle to achieve.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>GR-Dexter introduces a holistic hardware‚Äìmodel‚Äìdata pipeline: a compact 21-DoF ByteDexter V2 hand and a Meta Quest + Manus glove teleoperation interface for collecting bimanual dexterous trajectories, paired with a VLA policy built on a pre-trained VLM and co-trained across robot, vision-language, cross-embodiment, and human datasets to control a 56-DoF system.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Tactile-Enhanced VLA Policies for Bimanual Dexterous Manipulation: Integrate fingertip tactile sensing into the VLA model to improve contact reasoning, occlusion robustness, and grasp stability.<br>‚Ä¢ Cross-Embodiment Alignment via Shared Hand Kinematics for Generalist Manipulation: Learn a unified latent action space aligning human hand motion datasets with robotic dexterous hands to scale training beyond teleoperation.<br>‚Ä¢ Hierarchical Language-Conditioned Planning for Long-Horizon Dexterous Tasks: Combine high-level language planners with low-level dexterous controllers to improve multi-step task success and generalization to unseen instructions.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23343" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23343" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, we systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along a progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with a focus on multimodal memory systems and skill acquisition.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Interdisciplinary gap: existing works are siloed or biologically shallow, hindering deep integration of cognitive neuroscience memory mechanisms with LLM-driven agents.<br>‚Ä¢ LLM statelessness: models cannot maintain cross-session continuity or accumulate experience; parametric memory is static, prone to hallucinations and temporal lag, and expensive to update.<br>‚Ä¢ Context window limits: capacity‚Äìcost tradeoffs and positional bias make working memory unsuitable for long-term, stable knowledge.<br>‚Ä¢ Need for agent-centric memory: agents require persistent, structured, and dynamic memory to enable identity persistence, experiential accumulation, and long-horizon planning.<br>‚Ä¢ Lack of unified taxonomy and lifecycle: missing systematic classification (nature/scope), storage formats/locations, and end-to-end management (encode, update, retrieve, utilize).<br>‚Ä¢ Insufficient evaluation and security: fragmented benchmarks and overlooked memory security (attacks like extraction/backdoors/noise; defenses across retrieval and lifecycle).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>A unified survey synthesizing cognitive neuroscience and LLM-agent literature, proposing a dual-dimension taxonomy (nature and scope), analyzing storage locations and formats, detailing full lifecycle management (extraction, updating, retrieval, utilization), reviewing semantic and episodic benchmarks, and systematizing memory security from attack and defense perspectives, with future directions on multimodal memory and skill acquisition.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Towards Multimodal Agent Memory Systems: Cross-Modal Encoding, Integration, and Retrieval: Design unified representations and scheduling that seamlessly handle text, image, audio, and video memory.<br>‚Ä¢ Composable Skill Memories for Heterogeneous Agents: Learning, Sharing, and Transfer: Build reusable procedural skill libraries enabling memory sharing and transfer across diverse agent architectures and domains.<br>‚Ä¢ Adaptive Memory Scheduling via Reinforcement Learning for Long-Horizon Agents: Learn policies to retain, overwrite, and retrieve memory under resource constraints and dynamic environments.<br>‚Ä¢ Secure Memory for Autonomous Agents: Attack Taxonomies and End-to-End Defense Pipelines: Formalize threats (extraction, backdoors, noise/conflicts) and develop purification, real-time blocking, and lifecycle protection.<br>‚Ä¢ Benchmarking Agent Memory: A Unified Semantic‚ÄìEpisodic Evaluation Suite: Create standardized tasks that jointly measure internal state maintenance, higher-order cognition, and domain-specific episodic performance.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-4">

    <div class="paper">
        <h2 class="paper-title">Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23988" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23988" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we propose an unsupervised framework (namely, RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discovering reasoning vectors, which we define as directions in the activation space that encode distinct reasoning behaviors. By segmenting chain-of-thought traces into sentence-level 'steps' and training sparse auto-encoders (SAEs) on step-level activations, we uncover disentangled features corresponding to interpretable behaviors such as reflection and backtracking. Visualization and clustering analyses show that these behaviors occupy separable regions in the decoder column space. Moreover, targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors, altering inference trajectories without retraining. Beyond behavior-specific disentanglement, SAEs capture structural properties such as response length, revealing clusters of long versus short reasoning traces. More interestingly, SAEs enable the discovery of novel behaviors beyond human supervision. We demonstrate the ability to control response confidence by identifying confidence-related vectors in the SAE decoder space. These findings underscore the potential of unsupervised latent discovery for both interpreting and controllably steering reasoning in LLMs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ LLM reasoning mechanisms are opaque; we lack tools to identify and interpret fine-grained behaviors within chain-of-thought.<br>‚Ä¢ Existing analyses and activation steering rely on human-defined, token-level labels (e.g., reflection/backtracking), which are narrow, hard to annotate at scale, and miss overlapping/fluid behaviors.<br>‚Ä¢ DiffMean and contrastive approaches require supervised oppositional concepts, making them ill-suited for complex, non-binary reasoning dynamics.<br>‚Ä¢ Prior SAE use focused on capability transfer rather than uncovering the internal geometry of reasoning, limiting mechanistic insight and controllable intervention.<br>‚Ä¢ There is a need for disentangled, interpretable latent features that enable causal control of reasoning (e.g., reflection, confidence) without retraining.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>RISE segments chain-of-thought into sentence-level steps and trains a sparse autoencoder on delimiter-token residual-stream activations; decoder columns emerge as disentangled reasoning vectors that can be visualized, clustered, and injected during inference to amplify or suppress specific behaviors (e.g., reflection, backtracking, confidence) without retraining.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Layer-Wise Cartography of Reasoning Vectors: Systematically characterize where behaviors emerge and peak separability occurs across transformer layers, and measure causal efficacy of interventions by layer.<br>‚Ä¢ Training-Time Steering with SAE-Discovered Behaviors: Integrate reasoning vectors into reinforcement learning or supervised fine-tuning to regularize verbosity, enhance verification, and optimize accuracy-cost tradeoffs.<br>‚Ä¢ Cross-Model Alignment of Reasoning Geometry: Develop unsupervised alignment methods to transfer and generalize reasoning vectors across architectures and scales, evaluating robustness and portability.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.25075" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.25075" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Unified space‚Äìtime control in generative video is missing: current models can re-pose cameras but cannot retime motion (e.g., slow/reverse/bullet time) while maintaining coherence.<br>‚Ä¢ Existing approaches (geometry-based 4D, warp-and-inpaint, multi-view diffusion) either produce sparse frames, rely on heavy reconstruction, or break under large viewpoint/time changes.<br>‚Ä¢ Lack of training data: no paired videos of the same dynamic scene with continuous temporal variations; datasets often entangle camera and time and assume identical first frames.<br>‚Ä¢ Standard temporal position embeddings (e.g., RoPE) interfere with camera signals, causing space‚Äìtime entanglement and limiting precise control.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>SpaceTimePilot is a video diffusion model that disentangles space and time by jointly conditioning on source/target camera poses and a learnable animation-time embedding (sinusoidal + 1D conv) while training with temporal-warping augmentation and the synthetic Cam√óTime dataset that covers the full camera‚Äìtime grid. A source-aware camera-conditioning mechanism enables altering the camera from the first frame and supports coherent, longer sequences via autoregressive generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ SpaceTimePilot-Real: A Large-Scale Real-World Space‚ÄìTime Grid Dataset for Dynamic Scenes: Collect and align multi-camera, multi-time captures to provide full space‚Äìtime coverage without synthetic rendering.<br>‚Ä¢ Online Space‚ÄìTime Pilot: Real-Time Interactive Camera and Temporal Control for Video Diffusion: Optimize inference and conditioning for low-latency user-driven navigation across space and time.<br>‚Ä¢ Geometry-Aware SpaceTimePilot: Hybrid Diffusion‚ÄìScene Representations for Extreme Novel Views: Integrate lightweight 3D priors (depth/point clouds/Gaussians) to improve robustness under large viewpoint changes.<br>‚Ä¢ Object-Centric Space‚ÄìTime Editing: Disentangled Temporal Control of Multiple Actors and Parts: Extend time embeddings to per-object and per-region control for selective motion retiming and bullet-time.<br>‚Ä¢ Long-Range Space‚ÄìTime Generation: Scaling Disentangled Control to Minutes-Long Videos: Develop memory and continuity mechanisms to maintain identity and dynamics over extended durations.<br>‚Ä¢ Semantically-Guided Space‚ÄìTime Control: Aligning Motion Retiming with Text and Audio Cues: Condition animation-time on language/audio to produce semantically meaningful temporal edits (beats, emphasis, narrative timing).</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Pretraining Frame Preservation in Autoregressive Video Memory Compression</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.23851" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.23851" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>We present PFP, a neural network structure to compress long videos into short contexts, with an explicit pretraining objective to preserve the high-frequency details of single frames at arbitrary temporal positions. The baseline model can compress a 20-second video into a context at about 5k length, where random frames can be retrieved with perceptually preserved appearances. Such pretrained models can be directly fine-tuned as memory encoders for autoregressive video models, enabling long history memory with low context cost and relatively low fidelity loss. We evaluate the framework with ablative settings and discuss the trade-offs of possible neural architecture designs.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Autoregressive video models face a fundamental trade-off between context length and context quality; sliding windows drop long-range history and hurt narrative coherence.<br>‚Ä¢ Existing compression (highly compressed VAEs, token merging, large patch sizes) reduces context length but degrades high-frequency image details crucial for identity, objects, and scene fidelity.<br>‚Ä¢ Sparse/linear attention lowers compute but still processes long sequences and does not directly preserve per-frame detail; linear layers remain costly for training/inference.<br>‚Ä¢ Lack of an explicit, measurable objective for memory quality makes it hard to optimize the balance between compression rate and perceptual fidelity.<br>‚Ä¢ Training and inference with long histories are computationally prohibitive; practical AR video generation needs compact memory (e.g., ~5k tokens for 20s) that preserves arbitrary frames with high fidelity on commodity GPUs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Pretrain a lightweight memory encoder to compress long video history into a short context by maximizing reconstruction of randomly indexed frames via a diffusion-based retrieval objective that preserves high-frequency details. The encoder reuses the DiT patchifier/first projection, adds residual features at inner channels with 3D conv + attention under spatiotemporal downsampling, and is then finetuned as the history memory for AR video diffusion to provide >20s effective context at ~5k tokens.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive-Rate Memory Compression for Autoregressive Video Generation: Learn per-shot/scene-adaptive spatial/temporal compression rates to optimize fidelity under a fixed context budget.<br>‚Ä¢ Hybrid Retrieval-and-Compression Memory for Long-Form Video: Combine semantic retrieval of salient segments with compact continuous memory to balance relevance and detail preservation.<br>‚Ä¢ Cross-Attention Bridges for Token-Efficient Memory Conditioning in DiTs: Design lightweight cross-attention adapters that maximize consistency gains per token from compressed memory.<br>‚Ä¢ Drift-Resistant AR Video with Consistency Regularization over Compressed Histories: Add training-time rollout simulation and temporal consistency losses to mitigate error accumulation.<br>‚Ä¢ Multimodal Memory Compression with Audio and Text for Narrative Coherence: Jointly compress video, audio, and storyboard cues to improve long-range plot and lip-sync consistency.<br>‚Ä¢ Hierarchical Spatiotemporal Memory Encoders for Multi-Shot Storytelling: Build multi-scale memory (shot-, scene-, global-level) to better handle transitions and long-range dependencies.<br>‚Ä¢ Benchmarking the Length‚ÄìQuality Pareto Frontier in Video Memory Compression: Create standardized datasets and metrics to map context length vs perceptual/frame reconstruction trade-offs.<br>‚Ä¢ Online Streaming Memory Updates for Real-Time AR Video Generation: Enable on-the-fly incremental compression and concatenation without recomputation for live generation.<br>‚Ä¢ Instruction-Tuned Memory Encoders for Controllable Recall in Video Generation: Allow users to prioritize entities (faces, text, objects) in memory to steer what details are preserved.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-5">

    <div class="paper">
        <h2 class="paper-title">Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22564" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22564" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Respiratory sound classification is hindered by the limited size, high noise levels, and severe class imbalance of benchmark datasets like ICBHI 2017. While Transformer-based models offer powerful feature extraction capabilities, they are prone to overfitting and often converge to sharp minima in the loss landscape when trained on such constrained medical data. To address this, we introduce a framework that enhances the Audio Spectrogram Transformer (AST) using Sharpness-Aware Minimization (SAM). Instead of merely minimizing the training loss, our approach optimizes the geometry of the loss surface, guiding the model toward flatter minima that generalize better to unseen patients. We also implement a weighted sampling strategy to handle class imbalance effectively. Our method achieves a state-of-the-art score of 68.10% on the ICBHI 2017 dataset, outperforming existing CNN and hybrid baselines. More importantly, it reaches a sensitivity of 68.31%, a crucial improvement for reliable clinical screening. Further analysis using t-SNE and attention maps confirms that the model learns robust, discriminative features rather than memorizing background noise.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Small, noisy, and imbalanced respiratory sound datasets (e.g., ICBHI2017) cause overfitting and poor generalization, especially for Transformers.<br>‚Ä¢ Transformers trained on constrained medical data converge to sharp minima, making models brittle to patient/device variations and background noise.<br>‚Ä¢ Severe class imbalance (Normal vs. Adventitious) reduces sensitivity, risking false negatives in clinical screening.<br>‚Ä¢ Zero-padding introduces artificial silence that dilutes diagnostic cues; preprocessing can inadvertently remove subtle pathologies.<br>‚Ä¢ CNNs capture local features but miss global temporal context; RNN hybrids are computationally heavy and unstable on small datasets.<br>‚Ä¢ Prior methods emphasize denoising or complex architectures, often improving specificity at the expense of sensitivity critical for screening.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Integrate an Audio Spectrogram Transformer (pre-trained on AudioSet) with geometry-aware Sharpness-Aware Minimization and weighted sampling, plus signal-preserving cyclic padding to avoid silence artifacts, thereby steering training toward flat minima and boosting sensitivity on imbalanced, noisy data.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Flatness-Constrained Training for Biomedical Audio: Combine SAM with specificity-aware hybrid losses (e.g., focal/Tversky) to reduce false positives while preserving high sensitivity.<br>‚Ä¢ Self-Supervised Pretraining for Respiratory Sound Transformers at Scale: Use masked audio modeling and contrastive SSL on large unlabeled auscultation corpora to surpass AudioSet transfer and enhance robustness.<br>‚Ä¢ Domain-Robust Lung Sound Classification via Geometry-Aware Adaptation: Integrate SAM with domain adaptation (e.g., adversarial or adapter-based) to handle device, environment, and patient variability without sacrificing sensitivity.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24885" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24885" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Strategic dialogue requires agents to execute distinct dialogue acts, for which belief estimation is essential. While prior work often estimates beliefs accurately, it lacks a principled mechanism to use those beliefs during generation. We bridge this gap by first formalizing two core acts Adversarial and Alignment, and by operationalizing them via probabilistic constraints on what an agent may generate. We instantiate this idea in BEDA, a framework that consists of the world set, the belief estimator for belief estimation, and the conditional generator that selects acts and realizes utterances consistent with the inferred beliefs. Across three settings, Conditional Keeper Burglar (CKBG, adversarial), Mutual Friends (MF, cooperative), and CaSiNo (negotiation), BEDA consistently outperforms strong baselines: on CKBG it improves success rate by at least 5.0 points across backbones and by 20.6 points with GPT-4.1-nano; on Mutual Friends it achieves an average improvement of 9.3 points; and on CaSiNo it achieves the optimal deal relative to all baselines. These results indicate that casting belief estimation as constraints provides a simple, general mechanism for reliable strategic dialogue.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Existing dialogue systems can estimate interlocutor beliefs but lack a principled mechanism to use those beliefs during generation, leading to unstructured, indiscriminate disclosure of information.<br>‚Ä¢ Strategic dialogue requires distinct acts (e.g., adversarial vs. alignment) tied to beliefs/common knowledge; without formalization, models fail to decide what to reveal or conceal to achieve goals.<br>‚Ä¢ Prior methods emphasize belief estimation accuracy but underdevelop how to operationalize beliefs as constraints on generation, yielding suboptimal outcomes in adversarial/cooperative tasks.<br>‚Ä¢ There is a need for a simple, general, and robust framework that bridges belief estimation and utterance generation across diverse settings (debate, cooperation, negotiation).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>BEDA formalizes Adversarial and Alignment dialogue acts as probabilistic constraints over events in a structured world set, using a belief estimator to model P_A(E) and P_A(K_B E) and a conditional generator (LLM) that selects events satisfying the chosen act and generates utterances accordingly. Event selection follows a maximum-entropy principle over feasible events, while a BERT-based discriminator estimates beliefs and an LLM realizes the utterance.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Learning World Sets and Events from Raw Dialogue: Automatically induce and update structured event spaces from open-domain conversations to replace the predefined world set.<br>‚Ä¢ End-to-End Joint Training of Belief-Constrained Generators: Train belief estimators and generators jointly with differentiable probabilistic constraints to improve calibration and strategic consistency.<br>‚Ä¢ Recursive Theory-of-Mind for Multi-Party Strategic Dialogue: Extend BEDA to higher-order and multi-agent belief modeling, enabling richer act taxonomies and coordination in complex negotiations.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24385" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24385" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Foundation models remain largely single-modal; integrating heterogeneous onboard sensors (camera, LiDAR, radar, event) into a unified spatial understanding is still challenging.<br>‚Ä¢ Supervised pipelines depend on costly manual annotations, creating scalability and generalization bottlenecks; self-supervised multi-modal pre-training is needed.<br>‚Ä¢ Cross-modal alignment is hard due to spatio-temporal calibration, modality-specific noise/sparsity, and the semantic‚Äìgeometric gap between images and 3D geometry.<br>‚Ä¢ Platform-specific constraints and dataset diversity (vehicles, UAVs, USVs, rail, legged robots) hinder transferability and robust open-world perception.<br>‚Ä¢ Lack of a unified framework linking single-modality, cross-modal, and unified pre-training to downstream tasks (3D detection, occupancy, planning) limits progress.<br>‚Ä¢ Computational efficiency and model scalability for real-time, edge deployment remain critical bottlenecks.<br>‚Ä¢ Bridging passive perception to active planning requires integrating textual supervision and occupancy/world-model representations for open-world reasoning.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>The paper proposes a unified taxonomy and framework for multi-modal sensor pre-training, categorizing single-modality, cross-modal (camera-/LiDAR-centric), and unified approaches, and mapping them to platform-specific datasets and applications. It synthesizes empirical evidence, identifies bottlenecks, and outlines a roadmap that integrates textual supervision and occupancy representations toward general-purpose multi-modal foundation models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Physically Consistent Generative World Simulators for Autonomous Systems: Enforce differentiable physics and geometric constraints in world-model pre-training to produce action-conditioned, physically plausible 4D occupancy dynamics.<br>‚Ä¢ Real-Time VLA: Uncertainty-Aware Vision‚ÄìLanguage‚ÄìAction Models for Edge Autonomy: Design lightweight, interpretable VLA architectures with efficient tokenization and uncertainty quantification for millisecond-level decision-making.<br>‚Ä¢ Semantic Lifting in 4D Gaussian Worlds: Unifying Geometry and Semantics Over Time: Imbue continuous 3D Gaussian representations with dense, temporally consistent semantics to bridge visual rendering quality and actionable scene understanding.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-6">

    <div class="paper">
        <h2 class="paper-title">Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24297" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24297" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Complex reasoning problems often involve implicit spatial, geometric, and structural relationships that are not explicitly encoded in text. While recent reasoning models have achieved strong performance across many domains, purely text-based reasoning struggles to represent global structural constraints in complex settings. In this paper, we introduce FIGR, which integrates active visual thinking into multi-turn reasoning via end-to-end reinforcement learning. FIGR externalizes intermediate structural hypotheses by constructing visual representations during problem solving. By adaptively regulating when and how visual reasoning should be invoked, FIGR enables more stable and coherent reasoning over global structural properties that are difficult to capture from text alone. Experiments on challenging mathematical reasoning benchmarks demonstrate that FIGR outperforms strong text-only chain-of-thought baselines. In particular, FIGR improves the base model by 13.12% on AIME 2025 and 11.00% on BeyondAIME, highlighting the effectiveness of figure-guided multimodal reasoning in enhancing the stability and reliability of complex reasoning.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Complex reasoning tasks (e.g., geometry) require maintaining global spatial and structural constraints that text-only chain-of-thought struggles to represent, leading to algebraic and logical errors.<br>‚Ä¢ Multimodal models that generate images within the reasoning process often produce noisy or imprecise visuals, which cannot reliably encode fine-grained geometric constraints and propagate inaccuracies into symbolic computation.<br>‚Ä¢ Tool-augmented LVLMs are limited to predefined operations on existing images (e.g., zoom/crop) and cannot construct task-specific diagrams from scratch, restricting expressive flexibility for structural reasoning.<br>‚Ä¢ There is a need for a controllable, precise, and interpretable mechanism to actively construct visual representations within the reasoning loop, and a training paradigm that adaptively decides when visual reasoning should be invoked without supervised cold-start.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>FIGR interleaves textual reasoning with executable code that constructs precise figures, using GRPO-based reinforcement learning and an adaptive reward to regulate when visual reasoning is invoked. Rendered diagrams provide dynamic, interpretable feedback that enforces geometric consistency and improves solution accuracy.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Hierarchical Figure-Guided Policies for Complex Reasoning: Introduce multi-level controllers that plan when and what to draw, combining global layout planning with local geometric refinements.<br>‚Ä¢ Differentiable Rendering in the Reasoning Loop: Integrate differentiable renderers so visual construction can be optimized end-to-end with gradient signals from visual consistency and answer accuracy.<br>‚Ä¢ Cross-Domain FIGR for Scientific Reasoning: Extend figure-steered reasoning to physics, chemistry, and systems diagrams, evaluating gains on kinematics, reaction networks, and software architecture tasks.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Guiding a Diffusion Transformer with the Internal Dynamics of Itself</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24176" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24176" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>The diffusion model presents a powerful ability to capture the entire (conditional) data distribution. However, due to the lack of sufficient training and data to learn to cover low-probability areas, the model will be penalized for failing to generate high-quality images corresponding to these areas. To achieve better generation quality, guidance strategies such as classifier free guidance (CFG) can guide the samples to the high-probability areas during the sampling stage. However, the standard CFG often leads to over-simplified or distorted samples. On the other hand, the alternative line of guiding diffusion model with its bad version is limited by carefully designed degradation strategies, extra training and additional sampling steps. In this paper, we proposed a simple yet effective strategy Internal Guidance (IG), which introduces an auxiliary supervision on the intermediate layer during training process and extrapolates the intermediate and deep layer's outputs to obtain generative results during sampling process. This simple strategy yields significant improvements in both training efficiency and generation quality on various baselines. On ImageNet 256x256, SiT-XL/2+IG achieves FID=5.31 and FID=1.75 at 80 and 800 epochs. More impressively, LightningDiT-XL/1+IG achieves FID=1.34 which achieves a large margin between all of these methods. Combined with CFG, LightningDiT-XL/1+IG achieves the current state-of-the-art FID of 1.19.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Diffusion models must cover the full (conditional) data distribution but lack sufficient data/training to model low-probability regions, degrading image quality for rare conditions.<br>‚Ä¢ Classifier-free guidance (CFG) improves alignment but can overshoot with high coefficients, leading to over-simplified or distorted samples.<br>‚Ä¢ Guiding with a degraded or ‚Äúbad‚Äù version of the model improves quality/diversity but typically requires carefully designed degradations, extra training, and/or additional sampling steps, limiting scalability.<br>‚Ä¢ There is a need for a simple, compute-efficient guidance mechanism tailored to Diffusion Transformers that boosts quality without extra sampling cost and scales to large models.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Internal Guidance (IG) adds an auxiliary supervision at an intermediate Transformer layer so it can output a weaker denoising prediction; at sampling, the model extrapolates between the intermediate and deeper-layer outputs to guide the trajectory toward high-probability regions, achieving Autoguidance-like gains without extra sampling steps and remaining compatible with CFG and guidance-interval scheduling.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Adaptive Internal Guidance: Layer Selection and Strength Scheduling for Diffusion Transformers: Learn to choose guidance layers and dynamically schedule extrapolation strength per timestep/prompt to optimize the quality‚Äìdiversity trade-off.<br>‚Ä¢ Internal Guidance for Video and Multimodal Diffusion Transformers: Extend IG to text-to-video and other modalities, studying temporal consistency, cross-modal alignment, and efficiency at scale.<br>‚Ä¢ Understanding Internal Dynamics for Guidance in Diffusion Transformers: A Theoretical and Empirical Study: Analyze why intermediate-to-final extrapolation improves sampling, linking representational geometry and score estimates to stability and sample quality.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Factorized Learning for Temporally Grounded Video-Language Models</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.24097" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.24097" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D^2VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a "grounding then answering with evidence referencing" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Video LLMs struggle with accurate temporal event-level grounding, which undermines both localization tasks and the correctness of textual answers.<br>‚Ä¢ Existing methods interleave time tokens with text tokens, coupling grounding and answering objectives and yielding sub-optimal training.<br>‚Ä¢ Special grounding tokens largely encode timestamps but fail to capture event-level visual semantics needed as context for subsequent text generation.<br>‚Ä¢ Preference optimization in multimodal/video LLMs seldom models temporal grounding explicitly, and suitable datasets for factorized preference learning are lacking.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>D2VLM decouples generation into a pure temporal grounding stage followed by interleaved text‚Äìevidence answering, using evidence tokens that capture event-level visual semantics via similarity aggregation and enforcing cross-stage consistency. Factorized Preference Optimization integrates explicit grounding probabilities (derived from evidence‚Äìframe similarities) with textual token probabilities, trained on a synthetic dataset built by event-level, factorized perturbations.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Hierarchically Grounded Video-Language Models: Joint Temporal‚ÄìSpatial Evidence Tokens and Factorized Preferences: Extend evidence tokens to spatial grounding and learn joint preferences for when and where events occur.<br>‚Ä¢ Long-Video Reasoning with Memory-Augmented Evidence Referencing: Scale D2VLM to hours-long videos using memory modules and multi-instance evidence chains while maintaining cross-segment consistency.<br>‚Ä¢ Human-Curated Preference Benchmarks for Temporal Grounding in Video LLMs: Develop standardized datasets with human-annotated preferences and explicit event-level grounding to validate and improve FPO beyond synthetic perturbations.</p>
            </div>
        </div>    </div></div>
<div class="page" id="page-7">

    <div class="paper">
        <h2 class="paper-title">JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22905" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22905" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Unified modeling of synchronized sounding videos (joint audio‚Äìvideo) is underexplored; precise temporal alignment for both comprehension and generation is missing.<br>‚Ä¢ Existing MLLMs treat audio and video as independent inputs and rely on naive concatenation/interleaving, failing to capture fine-grained spatio-temporal interactions required for synchronization.<br>‚Ä¢ Pipeline-based multimodal generation with separate decoders causes error propagation and desynchronized outputs; end-to-end joint AV generation is lacking.<br>‚Ä¢ Current systems have limited instruction-following and interleaved text‚Äìaudio‚Äìvideo reasoning/generation in multi-turn dialogues.<br>‚Ä¢ There is a scarcity of high-quality, synchrony-aware instruction datasets to train JA V-capable LLMs.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>JavisGPT is an encoder‚ÄìLLM‚Äìdecoder MLLM that introduces SyncFusion to fuse audio and video via cross-attention for fine-grained spatio-temporal synchrony, and synchrony-aware learnable queries that align LLM hidden states to semantic and spatio-temporal prior conditions of a frozen JAV-DiT generator via alignment losses. A three-stage pipeline (multimodal pretraining, AV fine-tuning, and multimodal instruction-tuning) with the 200K JavisInst-Omni dataset enables unified comprehension and synchronized generation.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ SyncFusion++: Streaming-aligned Audio‚ÄìVideo Fusion for Low-latency Multimodal LLMs: Develop adaptive, streaming cross-modal fusion to handle variable frame rates and online inputs while preserving synchrony.<br>‚Ä¢ RLHF for Sounding Video Synchrony: Human-in-the-loop Optimization of Multimodal LLM Generators: Optimize perceived audio‚Äìvideo alignment, realism, and instruction adherence using human preference feedback.<br>‚Ä¢ Self-supervised JA V Pretraining at Scale: Leveraging Unlabeled Sounding Videos for Unified AV LLMs: Use contrastive and cycle-consistency objectives to align audio and visual events without labels, improving robustness and reducing supervision.</p>
            </div>
        </div>    </div>
    <div class="paper">
        <h2 class="paper-title">Valori: A Deterministic Memory Substrate for AI Systems</h2>
        
        <div class="paper-links">
            <a href="https://huggingface.co/papers/2512.22280" target="_blank">Open in Hugging Face</a>
            <a href="https://arxiv.org/pdf/2512.22280" target="_blank" class="pdf">Open PDF</a>
        </div>
        
        <div class="abstract">
            <h3>Abstract</h3>
            <p>Modern AI systems rely on vector embeddings stored and searched using floating-point arithmetic. While effective for approximate similarity search, this design introduces fundamental non-determinism: identical models, inputs, and code can produce different memory states and retrieval results across hardware architectures (e.g., x86 vs. ARM). This prevents replayability and safe deployment, leading to silent data divergence that prevents post-hoc verification and compromises audit trails in regulated sectors. We present Valori, a deterministic AI memory substrate that replaces floating-point memory operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. Valori guarantees bit-identical memory states, snapshots, and search results across platforms. We demonstrate that non-determinism arises before indexing or retrieval and show how Valori enforces determinism at the memory boundary. Our results suggest that deterministic memory is a necessary primitive for trustworthy AI systems. The reference implementation is open-source and available at https://github.com/varshith-Git/Valori-Kernel (archived at https://zenodo.org/records/18022660).</p>
        </div>
    
        <div class="analysis-section">
            <h3><span class="emoji">üéØ</span>Research Motivation</h3>
            <div class="motivation">
                <p>‚Ä¢ Floating-point non-determinism across architectures (FMA choices, reduction order, SIMD differences) yields bit-level divergence in embeddings and distance computations, breaking replayability and auditability.<br>‚Ä¢ Existing vector databases and ANN indices prioritize speed with f32, leading to hardware-dependent index topology and retrieval orderings that cannot be reproduced across environments.<br>‚Ä¢ Safety-critical and regulated applications require bit-identical memory states and verifiable audit trails, which current floating-point-based AI memory systems cannot guarantee.</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üîß</span>Research Method</h3>
            <div class="method">
                <p>Valori enforces determinism by converting all external floating-point inputs to fixed-point (Q16.16) at the kernel boundary and executing all indexing and distance operations inside a pure no-std Rust state-machine, using integer arithmetic and deterministic ordering. It adapts HNSW to remove stochasticity (fixed insertion order, data-dependent decisions), supports snapshot/restore for bit-identical cross-platform state, and treats precision as a configurable integer contract (e.g., Q16.16, Q32.32).</p>
            </div>
        </div>
        <div class="analysis-section">
            <h3><span class="emoji">üí°</span>Research Ideas</h3>
            <div class="idea">
                <p>‚Ä¢ Deterministic Neural Inference: Integer-Only Transformers for Replayable Embeddings ‚Äî Develop quantized, fixed-point inference pipelines that produce bit-identical embeddings aligned with Valori‚Äôs precision contracts.<br>‚Ä¢ Verified Deterministic ANN Graphs: Formal Proofs and Error Bounds for HNSW Under Fixed-Point Arithmetic ‚Äî Provide machine-checked proofs of determinism and derive tight bounds on retrieval fidelity versus precision.<br>‚Ä¢ SIMD-Accelerated Fixed-Point Memory Kernels: Portable Integer Pipelines on x86, ARM, RISC-V, and WASM ‚Äî Design and evaluate vectorized integer kernels to match float32 throughput while preserving determinism and auditability.</p>
            </div>
        </div>    </div></div>

        </div>
        
        <div class="pagination">
            <div class="page-info">
                <span id="current-page-info">Page 1 of 7</span>
            </div>
            <button id="prev-btn" onclick="changePage(-1)" disabled>‚Üê ‰∏ä‰∏ÄÈ°µ</button>
            <span id="page-numbers"><button class="page-btn active" onclick="goToPage(1)">1</button><button class="page-btn" onclick="goToPage(2)">2</button><button class="page-btn" onclick="goToPage(3)">3</button><button class="page-btn" onclick="goToPage(4)">4</button><button class="page-btn" onclick="goToPage(5)">5</button><button class="page-btn" onclick="goToPage(6)">6</button><button class="page-btn" onclick="goToPage(7)">7</button></span>
            <button id="next-btn" onclick="changePage(1)">‰∏ã‰∏ÄÈ°µ ‚Üí</button>
        </div>
        
        <div class="footer">
            <p>Generated on 2026-01-01 23:22:55 | Powered by GPT-5 Analysis</p>
        </div>
    </div>

    <script>
        let currentPage = 1;
        const totalPages = 7;
        
        function showPage(pageNum) {
            // Hide all pages
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            
            // Show target page
            const targetPage = document.getElementById(`page-${pageNum}`);
            if (targetPage) {
                targetPage.classList.add('active');
            }
            
            // Update page info
            document.getElementById('current-page-info').textContent = `Page ${pageNum} of ${totalPages}`;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = pageNum === 1;
            document.getElementById('next-btn').disabled = pageNum === totalPages;
            
            // Update page number buttons
            const pageButtons = document.querySelectorAll('.page-btn');
            pageButtons.forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === pageNum) {
                    btn.classList.add('active');
                }
            });
            
            currentPage = pageNum;
        }
        
        function changePage(direction) {
            const newPage = currentPage + direction;
            if (newPage >= 1 && newPage <= totalPages) {
                showPage(newPage);
            }
        }
        
        function goToPage(pageNum) {
            showPage(pageNum);
        }
        
        // Initialize first page
        document.addEventListener('DOMContentLoaded', function() {
            showPage(1);
        });
    </script>
    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>